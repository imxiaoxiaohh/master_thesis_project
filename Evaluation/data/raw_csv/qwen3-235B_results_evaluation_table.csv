paper_id,equation_id,context,ground_truth_eq,ground_truth_description,generated_equation,generated_description
2024.acl-short.12,1,"## 3 Preliminaries

Traditional works rely on text frequencies to define whether the instance is long-tail or not; thus, low-frequency texts tend to be classified into long-tail classes. For LLMs, computing text frequencies of previously unknown user queries is by no means an easy task. As in (Aimar et al., 2023; Zhong et al., 2021; Xu et al., 2021), _Expected Calibration Error_ (ECE) provides a new perspective to measure ""long-tailness"". ECE measures how well a model's estimated probabilities match true (observed) probabilities (Guo et al., 2017). In the calculation of ECE, the confidence of each instance is allocated to a specific interval and obtained by the model predicted probability. The accuracy is determined by the comparison of the predicted label and the ground truth. The absolute margin between confidence and accuracy of each instance represents the calibration degree. The expected calibration degree of the whole dataset indicates the reliance of the model. Formally, ECE can be formulated as:",ECE=\sum_{i=1}^{B}\frac{n_{b_{i}}}{N}|acc(b_{i})-conf(b_{i})|,"where \(i\) denotes \(i\)-th bin, \(N\) is the total instance count of the dataset, \(acc(b_{i})\) and \(conf(b_{i})\) represent the accuracy and confidence of the bin \(b_{i}\), and \(n_{b_{i}}\) is the instance number of the bin \(b_{i}\)","\begin{equation}
\text{ECE} = \sum_{k=1}^{K} \frac{|B_k|}{N} \left| \text{acc}(B_k) - \text{conf}(B_k) \right|
\end{equation}","ECE is calculated by dividing instances into K bins based on confidence intervals, where each bin B_k contains instances with similar confidence levels, and computes the weighted average of the absolute difference between accuracy and confidence within each bin."
2024.acl-short.12,2,"\). \(B\) is the count of bins in the interval of \([0,1]\). In our work, we extend ECE for NLP, particularly for the LLM text generation scenario.

## 4 Methodology

### Metric-based Long-tailness Detection

As long-tail knowledge is crucial for RAG, we propose the GECE metric to detect the instance ""long-tailness"". Here, we transform the traditional ECE formula with METEOR (Banerjee and Lavie, 2005) and average prediction probability:

* Accuracy in ECE is to measure the agreement between prediction and ground truth. Inthe generative scenario, we utilize METEOR Banerjee and Lavie (2005) to measure coherence and relevance between predicted candidates and ground truth.
* Confidence in ECE is the predicted probability produced by the model itself. Similarly, we employ the average token probability output by LLMs.

Moreover, to enhance our metric with long-tail detection abilities, we further integrate the following two factors, which assist us to further separate common and long-tail instances apart:

* Average word frequency, as word frequency is a basic indication of long-tail texts.
* Dot product between the mean gradient of the total dataset and the gradient of a specific instance is leveraged to evaluate the discrepancy Chen et al. (2022). This is because the gradient of a long-tail instance has a large disparity with the mean gradient of the total dataset, and vice versa.

From the above analysis, we construct GECE as:","GECE=\frac{|M(pred,ref)-\frac{1}{n}\sum_{i=1}^{n}p(t_{i})|}{\alpha\cdot[E( \bigtriangledown_{ins})\cdot\bigtriangledown_{ins}]}","where \(pred\) and \(ref\) represent the generated text and the referenced ground truth, respectively","GECE = \sum_{i=1}^{B} \frac{n_{b_i}}{N} \left| \text{METEOR}(y_{\text{pred}}, y_{\text{true}}) - \text{AvgProb}(y_{\text{pred}}) + \lambda_1 \cdot \text{AvgFreq}(x) + \lambda_2 \cdot \nabla(x) \cdot \nabla_{\text{mean}} \right|","where $ \text{METEOR}(y_{\text{pred}}, y_{\text{true}}) $ measures coherence and relevance between predicted and ground truth texts, $ \text{AvgProb}(y_{\text{pred}}) $ is the average token probability of the prediction, $ \text{AvgFreq}(x) $ is the average word frequency in input $ x $, $ \nabla(x) \cdot \nabla_{\text{mean}} $ evaluates the gradient discrepancy, and $ \lambda_1, \lambda_2 $ are weighting coefficients."
2024.acl-short.14,1,"### Forward Sentence-Level Generation

In this part, we decompose the generation of the text to describe the full KG into a sequential decoding problem: the model sequentially generate a semantically complete sentence with the sentence-specific subset of KG triples. Then the model generates the full text of KG based on the triples and the sentence-level generation. The generation process can be formulated as follows:","P(\mathbf{cot},T|\mathbf{KG}) || =P((s_{1},t_{1}),\cdots,(s_{n},t_{n}),T|\mathbf{KG}) || =\prod_{i=1}^{n}P((s_{i},t_{i})|(s_{1},t_{1}),\cdots,(s_{i-1},t_ {i-1}),\mathbf{KG})\cdot || P(T|(s_{1},t_{1}),\cdots,(s_{n},t_{n}),\mathbf{KG}) || =\prod_{i=1}^{n}P(t_{i}|(s_{1},t_{1}),\cdots,(s_{i-1},t_{i-1}), \mathbf{KG})\cdot || \prod_{i=1}^{n}P(s_{i})|t_{i},(s_{1},t_{1}),\cdots,(s_{i-1},t_{i- 1}),\mathbf{KG})\cdot || P(T|(s_{1},t_{1}),\cdots,(s_{n},t_{n}),\mathbf{KG})",,"P(Y|X) = \prod_{t=1}^{T} P(y_t | X, y_{<t})","Equation 1 defines the probability of generating the full text $ Y $ given the input KG $ X $, decomposed as a product of conditional probabilities for each sentence $ y_t $ given $ X $ and the previously generated sentences $ y_{<t} $."
2024.acl-short.14,2,"es.

In the example in Figure 2, There are two semantically complete sentences in the target text, i.e. \(n\) = 2. The first sentence \(s_{1}\) is ""Aarhus University in Denmark is affiliated with the European University Association, which has its HQ in Brussels"", which describes the ""Aarhus University"" and its affiliation. The triplet subset corresponding to this sentence \(t_{1}\) is ""<Aarhus University, country, Denmark>; <Aarhus University, affiliation, European University Association>; <European University Association, headquarters, Brussels>"". The second sentence \(s_{2}\) describes the entity ""Denmark"".

The cross-entropy loss is utilized to optimize the model:","L_{seq}=-\mathbf{log}P((s_{1},t_{1}),\cdots,(s_{n},t_{n}),T| \mathbf{KG}) || =-\sum_{i=1}^{n}\mathbf{log}P(t_{i}|(s_{1},t_{1}),\cdots,(s_{i-1},t_{i-1}), || \mathbf{KG})-\sum_{i=1}^{n}\mathbf{log}P(s_{i})|t_{i},(s_{1},t_{1 }),\cdots,(s_{i-1}, || t_{i-1}),\mathbf{KG})-\sum_{i=1}^{n}\mathbf{log}P(T|(s_{1},t_{1 }),\cdots,(s_{n}, || t_{n}),\mathbf{KG})",,"\mathcal{L} = -\sum_{i=1}^{n} \log P(t_i | \mathbf{KG}, \text{prefix}_i) - \sum_{i=1}^{n} \log P(s_i | t_i, \mathbf{KG}, \text{prefix}_i)","The loss function combines the negative log-likelihoods of predicting each sentence-specific triplet subset and the corresponding sentence, conditioned on the knowledge graph and previously generated context."
2024.acl-short.14,3,"### Backward Relation Extraction

To help the model capture the correct relational information between the head and tail entities, we de

Figure 2: Pipeline of our approach BDMG. It includes forward sequential sentence-level generation and backward relation extraction.

sign the backward relation extraction task. Specifically, we randomly sample a number of triples from the KG and prompt the model to infer the relation between its head and tail entities based on the text generation of the KG. Such as the triple ""European University Association, headquarters, Brussels"", we prompt the model as ""what is the relation between European University Association and Brussels based on the text \(\cdots\)"", and the target answer is ""The headquarters of European University Association are in Brussels"". The objective function is as follows:","L_{re} =-\textbf{log}P(r|h,t,T) || =-\textbf{log}\prod_{i=1}^{m}P(r_{i}|r_{<i},h,t,T)",,"L_{b}=-\sum_{(h,r,t)\in \mathbf{KG}}\mathbf{log}P(r|h,t,T)","The backward relation extraction loss function, which encourages the model to correctly infer relations between head and tail entities given the generated text."
2024.acl-short.14,4,"th.

### Training and Inference

Our model is jointly optimized by the sequential sentence-level generation loss and the backward RE loss:",L=\alpha_{1}L_{seq}+\alpha_{2}L_{re},where \(\alpha_{1}\) and \(\alpha_{2}\) are parameters to tune,L_{total} = L_{seq} + \lambda L_{re},Total loss combining sequential generation and relation extraction objectives
2024.acl-short.15,1,"### Task Description

As previously discussed in SS1, SLU in dialog systems contains two subtasks: intent detection and slot filling. Since the two subtasks are highly correlated (Goo et al., 2018), it is common to adopt a joint SLU model that can capture shared knowledge. Formally, given an input sentence \(\mathbf{x}\) in a target language, zero-shot cross-lingual SLU means the joint model is trained in a source language dataset, _e.g._, English, and directly applied to the target language datasets, _e.g._, Chinese:","(\mathbf{o}^{I},\mathbf{o}^{S})=f(\mathbf{x}),",where \(f(\cdot)\) is the joint model; \(\mathbf{o}^{I}\) and \(\mathbf{o}^{S}\) denotes an intent label and a slot sequence,"\begin{equation}
\mathbf{y} = f_{\theta}(\mathbf{x})
\end{equation}","Equation 1 defines the joint SLU model $f_{\theta}$ that maps an input sentence $\mathbf{x}$ to the output $\mathbf{y}$, representing intent and slot labels, in zero-shot cross-lingual settings."
2024.acl-short.15,2,"ce. Note that multiple target languages are considered, while only English serves as the source language in our setting.

### Representation-Level Alignment

In existing zero-shot cross-lingual SLU studies, a bunch of works (Liang et al., 2022; Qin et al., 2022) have employed contrastive learning to explicitly align code-switched sentences with original sentences. However, this token-to-token hard alignment disrupts the inherent structure of languages (Zhu et al., 2023). Therefore, we introduce optimal transport (OT) (Peyre et al., 2019) to facilitate soft alignment at the representation level, which aims to find a mapping that transitions probability from one distribution to another with a minimized cost. The OT problem considers two point sets \(\mathbf{A}=\{\alpha_{i}\}_{i=1}^{n}\) and \(\mathbf{B}=\{\beta_{i}\}_{i=1}^{m}\), and a transport cost matrix \(\mathbf{C}\) with components \(\mathbf{C}_{[i,j]}=c(\alpha_{i},\beta_{j})\) specifying the cost of aligning a pair of points. The goal of OT is to compute a mapping or an alignment matrix \(\mathbf{Q}\) that pushes the probability mass of \(\mathbf{A}\) toward that of \(\mathbf{B}\), while minimizing the sum of costs weighted by the alignments: \(\mathcal{L}_{\mathbf{C}}=\sum_{[i,j]}\mathbf{C}_{[i,j]}\mathbf{Q}_{[i,j]}\), where the alignment matrix \(\mathbf{Q}\) can be determined using certain OT solution algorithm (_e.g._, relaxed OT (Kusner et al., 2015), Sinkhorn-Knopp (Sinkhorn and Knopp, 1967) and IPOT (Xie et al., 2020)).

In this work, we denote the original and corresponding code-switched sentence as \(\mathbf{x}=\{w_{1},w_{2},\ldots,w_{L}\}\) and \(\mathbf{x}^{\prime}=\{w_{1},w^{\prime}_{2},\ldots,w_{L}\}\), where \(w^{\prime}_{i}\) means the replaced source language token by target languages. For a sample \(\mathbf{x}\) and its code-switched sentence \(\mathbf{x}^{\prime}\), the multilingual pre-trained language model (mPLM) will produce two different representations \(\mathbf{h},\mathbf{h}^{\prime}\) (prepended [CLS]

Figure 1: Overview of our proposed RePe.

and appended [SEP]). Then, we treat \(\mathbf{h}\) and \(\mathbf{h}^{\prime}\) as two point sets and assume each token is uniformly distributed. The cost matrix \(\mathbf{C}\) is obtained by computing the cosine distance between contextualized representations in \(\mathbf{h}\) and \(\mathbf{h}^{\prime}\). As for the solutions, we use IPOT in this work to obtain the alignment matrix \(\mathbf{Q}\), which improves the training speed without degrading the performance as shown in SS4.1. The final alignment matrix \(\mathbf{\hat{Q}}\) is computed by:","\hat{\mathbf{Q}}_{[i,j]}=\mathrm{norm}(\mathbf{Q}_{[i,j]}),","where \(\mathrm{norm}(\cdot)\) denotes row normalization, which constrains the values to lie between 0 and 1","\mathbf{\hat{Q}} = \mathop{\text{argmin}}\limits_{\mathbf{Q} \in \Pi(\mathbf{h}, \mathbf{h}^{\prime})} \sum_{i,j} \mathbf{C}_{[i,j]} \mathbf{Q}_{[i,j]},","where $\Pi(\mathbf{h}, \mathbf{h}^{\prime})$ denotes the set of all possible transport plans between the representations $\mathbf{h}$ and $\mathbf{h}^{\prime}$, and $\mathbf{\hat{Q}}$ is the optimal alignment matrix obtained via IPOT."
2024.acl-short.15,3,"1. The value \(\mathbf{\hat{Q}}_{[i,j]}=1\) indicates the extent of alignment between \(\mathbf{h}_{i}\) and \(\mathbf{h}^{\prime}_{j}\). In this manner, the resulting alignment matrix is used as weak supervision to encourage soft alignment between original and code-switched sentences. The training loss for representation-level alignment is defined as:","\mathcal{L}_{\text{RLA}}=-\sum_{[i,j]}\mathbf{\hat{Q}}_{[i,j]}\log(\sigma(1- \mathbf{C}_{[i,j]})),","where \(\sigma\) denotes the sigmoid function, and \(1-\mathbf{C}_{[i,j]}\) denotes the cosine similarity between \(\mathbf{h}_{i}\) and \(\mathbf{h}^{\prime}_{j}\)","\mathcal{L}_{\text{align}} = -\sum_{i,j} \hat{\mathbf{Q}}_{[i,j]} \log \hat{\mathbf{Q}}_{[i,j]}.",the training loss for representation-level alignment using the alignment matrix $\mathbf{\hat{Q}}$ as weak supervision
2024.acl-short.15,4,"\).

### Prediction-Level Alignment

For intent detection task, we then feed the whole sentence representations of \(\mathbf{h}_{\text{CLS}}\) and \(\mathbf{h}^{\prime}_{\text{CLS}}\) into a classification layer (decoder\({}^{I}\)):","\mathbf{P}^{I}_{\mathbf{x}} =\mathrm{softmax}(\mathbf{W}^{I}\mathbf{h}_{\text{CLS}}+\mathbf{ b}^{I}), || \mathbf{P}^{I}_{\mathbf{x}^{\prime}} =\mathrm{softmax}(\mathbf{W}^{I}\mathbf{h}^{\prime}_{\text{CLS}}+ \mathbf{b}^{I}),",,"\mathbf{p}^{I} = \text{softmax}(\text{decoder}^{I}(\mathbf{h}_{\text{CLS}})),",where $\mathbf{p}^{I}$ denotes the predicted intent probability distribution for the original sentence
2024.acl-short.15,5,"rs.

For slot filling task, we similarly feed each hidden state \(\mathbf{h}_{[1:-1]}\) and \(\mathbf{h}^{\prime}_{[1:-1]}\) into a classification layer (decoder\({}^{S}\)):","\mathbf{P}^{S}_{\mathbf{x}} =\mathrm{softmax}(\mathbf{W}^{S}\mathbf{h}_{[1:-1]}+\mathbf{b}^{ S}), || \mathbf{P}^{S}_{\mathbf{x}^{\prime}} =\mathrm{softmax}(\mathbf{W}^{S}\mathbf{h}^{\prime}_{[1:-1]}+ \mathbf{b}^{S}).",,"\mathbf{P}^{S}_{\mathbf{x}} =\mathrm{softmax}(\mathbf{W}^{S}\mathbf{h}_{[1:-1]}+\mathbf{b}^{S}),",where $\mathbf{P}^{S}_{\mathbf{x}}$ denotes the predicted slot sequence probabilities for the original sentence $\mathbf{x}$
2024.acl-short.15,6,"The learning objective is to train the classifier to match predicted labels of the original sentence with the ground truth, thus the intent detection loss \(\mathcal{L}_{I}\) and slot filling loss \(\mathcal{L}_{S}\) are defined as:","\mathcal{L}_{I} =\mathrm{CE}(\mathbf{P}^{I}_{\mathbf{x}},\mathbf{P}^{I}), || \mathcal{L}_{S} =\frac{1}{L}\sum_{i=1}^{L}\mathrm{CE}(\mathbf{P}^{S}_{[\mathbf{x},i]},\mathbf{P}^{S}_{i}),",,"\mathcal{L}_{I} = -\log \mathbf{P}^{I}_{\mathbf{x}}(y^{I}),",where $y^{I}$ is the ground truth intent label for the original sentence $\mathbf{x}$
2024.acl-short.15,7,"en.

On the other hand, we hope the output produced by the decoder\({}^{I}\) and decoder\({}^{S}\) are language-invariant. Toward this goal, we leverage multi-view learning (Li et al., 2018) to exploit prediction-level alignment from multiple views, which usually contain complementary insights.

Concretely, we consider two distinct views: the probability distribution of original and code-switched sentences. Then, we strive to establish a consensus between these two views, ensuring that the predicted distributions across both two views for each subtask should be as closely aligned as possible:","\mathcal{L}_{\text{PLA}}=\underbrace{\mathrm{KL}(\mathbf{P}^{I}_{\mathbf{x}^{ \prime}}||\mathbf{P}^{I}_{\mathbf{x}})}_{\text{Intent PLA}}+\underbrace{ \mathrm{KL}(\mathbf{P}^{S}_{\mathbf{x}^{\prime}}||\mathbf{P}^{S}_{\mathbf{x}}) }_{\text{Slot PLA}},","where \(\mathrm{KL}(\cdot)\) denotes Kullback-Leibler divergence (Kullback and Leibler, 1951) to measure the difference between two distributions","\mathcal{L}_{\text{PLA}} =\mathrm{KL}(\mathbf{P}^{I}_{\mathbf{x}}\parallel \mathbf{P}^{I}_{\mathbf{x}^{\prime}})+\frac{1}{L}\sum_{i=1}^{L}\mathrm{KL}(\mathbf{P}^{S}_{[\mathbf{x},i]}\parallel \mathbf{P}^{S}_{[\mathbf{x}^{\prime},i]}),",where $\mathrm{KL}(\cdot \parallel \cdot)$ denotes the Kullback-Leibler divergence measuring the difference between two probability distributions
2024.acl-short.15,8,"ns.

### Self-distillation

To enhance the stability of alignment at both the representation and prediction levels, we introduce a self-distillation (SD) layer to improve the model's robustness. Self-distillation minimizes KL divergence between the current prediction and the previous one (Yun et al., 2020). Specifically, we denote \(\mathbf{P}^{t}_{\mathbf{x}}\) as the probability distribution of the input \(\mathbf{x}\) predicted by the model at the \(t\)-th epoch, respectively. The whole SD loss \(\mathcal{L}_{\text{SD}}\) is combined with its intent- and slot-specific losses expressed as:","\mathcal{L}_{\text{SD}}=\underbrace{\mathrm{KL}(\mathbf{P}^{[I,t-1]}_{\text{ kT}}||\mathbf{P}^{[I,t]}_{\text{latent SD}})}_{\text{Intent SD}}+\underbrace{\frac{1}{L}\sum_{i=1}^{L}\mathrm{KL}( \mathbf{P}^{[S,t-1]}_{[\mathbf{x},i]}||\mathbf{P}^{[S,t]}_{[\mathbf{x},i]}) }_{\text{Slot SD}},","where \(\mathbf{P}^{[I,t]}_{\mathbf{x}}\) denotes the probability distribution of intent, \(\mathbf{P}^{[S,t]}_{[\mathbf{x},i]}\) of slot at \(i\)-th token","\mathcal{L}_{\text{SD}}=\underbrace{\mathrm{KL}(\mathbf{P}^{t}_{\mathbf{x}}|| \mathbf{P}^{t-1}_{\mathbf{x}})}_{\text{Intent SD}}+\underbrace{\mathrm{KL}( \mathbf{P}^{t}_{\mathbf{x}^{\prime}}||\mathbf{P}^{t-1}_{\mathbf{x}^{\prime}} )}_{\text{Slot SD}},","where $\mathrm{KL}(\cdot)$ denotes Kullback-Leibler divergence, and the loss encourages consistency between current and previous predictions for both intent and slot tasks"
2024.acl-short.15,9,"en. Note that \(\mathbf{P}^{[I,0]}_{\mathbf{x}}\) denotes the one-hot vector of the intent label and \(\mathbf{P}^{[S,0]}_{[\mathbf{x},i]}\) denotes the one-hot vector of the slot label.

Finally, we train the proposed RePe with a combination of the proposed objectives jointly:",\mathcal{L}=\mathcal{L}_{I}+\mathcal{L}_{S}+\mathcal{L}_{\text{RLA}}+\mathcal{L} _{\text{PLA}}+\mathcal{L}_{\text{SD}}.,,"\mathcal{L}_{\text{total}} = \mathcal{L}_{I} + \mathcal{L}_{S} + \lambda_{1} \mathcal{L}_{\text{RLA}} + \lambda_{2} \mathcal{L}_{\text{PLA}} + \lambda_{3} \mathcal{L}_{\text{SD}},","where $\lambda_{1}$, $\lambda_{2}$, and $\lambda_{3}$ are hyperparameters that balance the contributions of the representation-level alignment, prediction-level alignment, and self-distillation losses, respectively."
2024.acl-short.16,1,"## 4 AFLoRA: Methodology

**Module Structure.** Inspired by the framework proposed by Kopiczko et al. (2024), we design the LoRA module to encompass four components, namely, the down-projection linear layer (\(lora_{A}\)), the up-projection linear layer (\(lora_{B}\)), and two feature transform vectors (\(s_{d}\), and \(s_{b}\)) placed before and after \(lora_{B}\). However, unlike (Kopiczko et al., 2024), **we keep both the projection matrices (\(lora_{A}\) and \(lora_{B}\)) and vectors trainable at the beginning and keep the rank very low**. The module processes a given input \(X\) through these components to produce an output \(Y\). The complete operation for a layer \(l\) can be described as follows:",Y=W_{0}^{l}X+\Lambda_{b}^{l}B^{l}\Lambda_{d}^{l}A^{l}X,,"\begin{equation}
Y = \sigma\left(X \cdot W + s_{d} \odot \left(lora_{B} \cdot \left(lora_{A} \cdot X\right)\right) + s_{b}\right)
\end{equation}","The output $Y$ is computed by applying a non-linear activation $\sigma$ to the sum of the base transformation $X \cdot W$, the scaled low-rank adaptation, and the bias vector $s_{b}$."
2024.acl-short.16,2,"Here, \(A^{l}\) and \(B^{l}\) are the trainable LoRA tensors of \(lora_{A}^{l}\) and \(lora_{B}^{l}\), respectively. \(\Lambda_{d}\) and \(\Lambda_{b}\) are the vectors of \(s_{d}\), and \(s_{b}\), respectively. \(W_{0}^{l}\) represents the frozen pre-trained weights. We use Kaiming Uniform initialization for \(A^{l}\) and \(B^{l}\), and follow (Kopiczko et al., 2024) to initialize the vectors.

Figure 2: Performance of ELoRA with two different ranks of the frozen projection matrices.

**Adaptive Freezing.** In pruning literature (Han et al., 2015; Molchanov et al., 2019; Zhang et al., 2022; Yin et al., 2024; Kundu et al., 2021, 2022), sensitivity is gauged to reflect weight variability, necessitating consideration of both the weights' magnitudes and their gradients. Small weight values suggest minimal impact, while minor gradient values indicate stability. Taking inspiration from this idea, here we introduce the concept of a ""freezing score"". However, unlike pruning where both magnitude and gradient play a critical role in identifying insignificant weight, we leverage only gradient as a proxy to compute the freezing score. This is because, we assume large magnitude weights with negligible change has the same priority to be frozen as that for small magnitude weights. This score quantifies the degree to which weights vary throughout the training process. Consequently, when the expected changes to the weights become negligible, we may consider them to be frozen, thereby saving computational resources and energy. The following equation describes the freezing score evaluation steps for a low-rank tensor \(A^{l}\).","I_{A^{l}}=|\nabla\mathcal{L}(\mathbf{\theta})|,\overline{T}^{(t)}_{A^{l }}=\beta_{1}\overline{T}^{(t-1)}_{A^{l}}+(1-\beta_{1})I^{(t)}_{A^{l}} || U^{(t)}_{A^{l}}=|I^{(t)}_{A^{l}}-\overline{I}^{(t)}_{A^{l}}|, \overline{U}^{(t)}_{A^{l}}=\beta_{2}\overline{U}^{(t-1)}_{A^{l}}+(1-\beta_{2} )U^{(t)}_{A^{l}} || s^{(t)}_{A^{l}}=mean(\overline{T}^{(t)}_{A^{l}}\circ\overline{U}^{(t)}_{A^{l}})",,\mathcal{F}_{A^{l}} = \frac{1}{T} \sum_{t=1}^{T} \left\| \nabla A^{l}_{t} \right\|_{2}^{2},The freezing score $\mathcal{F}_{A^{l}}$ quantifies the average squared L2 norm of gradients for the LoRA tensor $A^{l}$ over $T$ training steps.
2024.acl-short.16,3,"Here, for each projection tensor at iteration \(t\), we compute a smoothed gradient (\(\overline{T}^{(t)}_{A^{l}}\)) and uncertainly tensor (\(\overline{U}^{(t)}_{A^{l}}\)), as shown in Eq. 2 and 3, respectively. We then evaluate the freezing score \(s^{(t)}_{A^{l}}\), as the mean of the tensor generated via Hadamard product (\(\circ\)) between \(\overline{T}^{(t)}_{A^{l}}\) and \(\overline{U}^{(t)}_{A^{l}}\).

To apply thresholding on the LoRA freezing scores, we use the cubic schedule as (Zhang et al., 2022). In specific, we keep the projection matrices trainable for the initial \(t_{i}\) training steps, and then progressively freeze them by calculating the freezing fraction \(r(t)\) as shown in Eq. 5. Finally, all the projection matrices freeze beyond \(T-t_{f}\) steps. Note, at step \(t\), for a computed freezing fraction \(k\), we freeze the lowest \(k\%\) projection matrices.",r(t)=\{\begin{array}{ll}0&0\leq t<t_{i}\\ 1-(1-\frac{t-t_{i}}{T-t_{i}-t_{f}})^{3}&t_{i}\leq t<T-t_{f}\\ 1&\text{otherwise}\end{array}.,"where \(t\) refers to current #step, \(T\) is the total number of fine-tuning steps","r(t)=\begin{cases} 
0 & \text{if } t < t_i \\
1 - \left(\frac{T - t}{T - t_i}\right)^3 & \text{if } t_i \leq t < T - t_f \\
1 & \text{if } t \geq T - t_f 
\end{cases}","Freezing fraction $r(t)$ calculated using a cubic schedule, determining the proportion of projection matrices to be frozen at training step $t$."
2024.acl-short.20,1,"## 3 DynaSemble

Our goal is to dynamically ensemble \(k\) KGC models \(\mathtt{M}_{\mathtt{i}}\), which may be textual or structure-based, to maximize performance. Each model \(\mathtt{M}_{\mathtt{i}}\) assigns a score \(\mathtt{M}_{\mathtt{i}}(\mathbf{h},\mathbf{r},\mathbf{t})\) to all candidate tails \(\mathbf{t}\in\mathcal{E}\) for query \(\mathbf{q}=(\mathbf{h},\mathbf{r},?)\). These models are trained independently and their parameters are frozen before ensembling. We formulate the ensemble \(\mathtt{E}\) as:","\mathtt{E}(\mathbf{h},\mathbf{r},\mathbf{t})=\sum_{i=1}^{k}\mathbf{w}_{ \mathtt{i}}(\mathtt{q})\mathtt{M}_{\mathtt{i}}(\mathtt{h},\mathbf{r},\mathbf{t})","where \(\mathtt{E}(\mathtt{h},\mathbf{r},\mathbf{t})\) is the ensemble score for \(\mathbf{t}\) given query \(\mathtt{q}=(\mathbf{h},\mathbf{r},?)\)","\begin{equation}
\mathtt{E}(\mathbf{h}, \mathbf{r}, \mathbf{t}) = \sum_{i=1}^{k} \alpha_i \cdot \mathtt{M}_i(\mathbf{h}, \mathbf{r}, \mathbf{t})
\end{equation}","The ensemble E combines the scores of k KGC models using learnable weights α_i, where each model's contribution is weighted by its relevance to the query."
2024.acl-short.20,2,"\). We first normalize these scores as described below.

**Normalization:** To bring the distribution of scores assigned by each model \(\mathtt{M}_{\mathtt{i}}\) over all \(\mathbf{t}\in\mathcal{E}\) in the same range for each query, we max-min normalize the scores obtained from all models \(\mathtt{M}_{\mathtt{i}}\) separately:","\mathtt{M}_{\mathtt{i}}(\mathtt{h},\mathbf{r},\mathbf{t})\gets\mathtt{M}_ {\mathtt{i}}(\mathtt{h},\mathbf{r},\mathbf{t})-\min_{\mathbf{t}^{\prime}\in \mathcal{E}}\mathtt{M}_{\mathtt{i}}(\mathtt{h},\mathbf{r},\mathbf{t}^{\prime}) || \mathtt{M}_{\mathtt{i}}(\mathtt{h},\mathbf{r},\mathbf{t})arrow\frac{ \mathtt{M}_{\mathtt{i}}(\mathtt{h},\mathbf{r},\mathbf{t})}{\max_{t^{\prime}\in \mathcal{E}}\mathtt{M}_{\mathtt{i}}(\mathtt{h},\mathbf{r},\mathbf{t}^{\prime})}",,"\mathbf{s}_{\mathtt{i}}(\mathtt{q},\mathbf{t}) = \frac{\mathtt{M}_{\mathtt{i}}(\mathtt{h},\mathbf{r},\mathbf{t}) - \min_{\mathbf{t}'\in\mathcal{E}}\mathtt{M}_{\mathtt{i}}(\mathtt{h},\mathbf{r},\mathbf{t}')}{\max_{\mathbf{t}'\in\mathcal{E}}\mathtt{M}_{\mathtt{i}}(\mathtt{h},\mathbf{r},\mathbf{t}') - \min_{\mathbf{t}'\in\mathcal{E}}\mathtt{M}_{\mathtt{i}}(\mathtt{h},\mathbf{r},\mathbf{t}')}.","where $\mathbf{s}_{\mathtt{i}}(\mathtt{q},\mathbf{t})$ is the normalized score for model $\mathtt{M}_{\mathtt{i}}$ and candidate tail $\mathbf{t}$ given query $\mathtt{q}=(\mathbf{h},\mathbf{r},?)$"
2024.acl-short.20,3,"The scores obtained after normalization lie in the range [0,1] for all models. We next describe the simple model used to learn the query-dependent ensemble weights \(\mathtt{w}_{\mathtt{i}}\).

**Model:** We extract the following features from the score distribution of each model \(\mathtt{M}_{\mathtt{i}}\):","\mathtt{f}(\mathtt{M}_{\mathtt{i}},\mathtt{q})=\underset{\mathbf{t}^{\prime} \in\mathcal{E}}{\mathrm{mean}}(\mathtt{M}_{\mathtt{i}}(\mathtt{h},\mathbf{r}, \mathbf{t}^{\prime}))||\underset{\mathbf{t}^{\prime}\in\mathcal{E}}{\mathrm{ var}}(\mathtt{M}_{\mathtt{i}}(\mathtt{h},\mathbf{r},\mathbf{t}^{\prime}))",,"\mathbf{f}_{\mathtt{i}}(\mathtt{q}) = \left\langle \mu_{\mathtt{i}}(\mathtt{q}), \sigma_{\mathtt{i}}(\mathtt{q}), \text{top}_k\text{score}_{\mathtt{i}}(\mathtt{q}) \right\rangle","where $\mathbf{f}_{\mathtt{i}}(\mathtt{q})$ represents the feature vector for model $\mathtt{M}_{\mathtt{i}}$ and query $\mathtt{q}$, composed of the mean $\mu_{\mathtt{i}}(\mathtt{q})$, standard deviation $\sigma_{\mathtt{i}}(\mathtt{q})$, and top-k scores of the normalized scores for $\mathtt{M}_{\mathtt{i}}$ over all $\mathbf{t} \in \mathcal{E}$"
2024.acl-short.20,4,"In the above equations, \(\mathrm{mean}()\) and \(\mathrm{var}()\) are the standard mean and variance functions respectively,whose outputs are concatenated to obtain the feature. This choice is driven by the insight that the variance and mean of the distribution of scores computed by any model over \(\mathcal{E}\) is correlated to the model confidence. A more detailed discussion, along with an exploration of other possible feature sets can be found in Appendix C.

Next, we concatenate these features for all \(\mathtt{M_{1}}\) to obtain a final feature vector that is passed to an independent 2-layer MLP (\(\mathtt{MLP_{1}}\)) for each model \(\mathtt{M_{4}}\) to learn query-dependent \(\mathtt{w_{1}}\):","\mathtt{w_{1}}(\mathtt{q})=\mathtt{MLP_{1}}(\mathtt{f}(\mathtt{M_{1}}, \mathtt{q})||\mathtt{f}(\mathtt{M_{2}},\mathtt{q})||...||\mathtt{f}(\mathtt{M_ {k}},\mathtt{q}))",,"\mathbf{w}_{\mathtt{i}}(\mathtt{q})=\mathtt{MLP}_{\mathtt{i}}\left( \bigoplus_{j=1}^{k}\mathtt{f}(\mathtt{M}_{\mathtt{j}},\mathtt{q})\right)","where $\mathbf{w}_{\mathtt{i}}(\mathtt{q})$ is the query-dependent weight for model $\mathtt{M}_{\mathtt{i}}$, obtained using a 2-layer MLP that takes as input the concatenated features from all models $\mathtt{M}_{\mathtt{j}}$"
2024.acl-short.20,5,"Intuitively, this concatenation informs each MLP about the relative confidence of all models regarding their predictions, enhancing the ensemble weight computation for corresponding models. Note that our approach is agnostic to models \(\mathtt{M_{4}}\).

Our experiments in this paper mostly involve only one textual model. Therefore, we learn the ensemble weights for the other models with respect to this textual model, which is assigned a fixed weight of 1. This decreases the parameter count while still being as expressive as learning distinct ensemble weights for all models. The method for learning these other weights is unchanged.

**Loss Function:** We train DynaSemble on the validation set (traditionally used to tune ensemble weights) using margin loss between the score of the gold entity and a set of negative samples. The train set is not used since all models are likely to give high-confidence predictions on its triples (Appendix D). If the gold entity is \(\mathtt{t^{*}}\) and the set of negative samples is \(\mathtt{N}\), the loss function \(\mathcal{L}\) for query \(\mathtt{q}=(\mathtt{h},\mathtt{r},?)\) is:","\mathcal{L}=\sum_{\mathtt{t}\in\mathtt{N}}\max(\mathtt{E}(\mathtt{h}, \mathtt{r},\mathtt{t})-\mathtt{E}(\mathtt{h},\mathtt{r},\mathtt{t^{*}})+ \mathtt{m},\mathtt{0})",where \(\mathtt{m}\) is the margin hyperparameter,"\mathcal{L}(\mathtt{q}) = \sum_{\mathtt{t}^{-} \in \mathtt{N}} \max\left(0, \gamma - \mathtt{E}(\mathtt{h}, \mathtt{r}, \mathtt{t}^{*}) + \mathtt{E}(\mathtt{h}, \mathtt{r}, \mathtt{t}^{-})\right)","where $\gamma$ is the margin hyperparameter and $\mathtt{E}(\mathtt{h}, \mathtt{r}, \mathtt{t})$ is the ensemble score for entity $\mathtt{t}$ given query $(\mathtt{h}, \mathtt{r}, ?)$"
2024.acl-short.25,1,"### Differentiable Neuron Mask Learning

Since the training loss is not differentiable for binary masks, we resort to a practical method to learn subnetworks Louizos et al. (2017), which employs a smoothing approximation of the discrete Bernoulli distribution Maddison et al. (2016). Following Zheng et al. (2022), we assume mask \(m_{i}\) corresponding to each neuron to be an independent random variable that follows a hard concrete distribution HardConcrete(log \(\alpha_{i}\), \(\beta_{i}\)) with temperature \(\beta_{i}\) and location \(\alpha_{i}\)Louizos et al. (2017):","s_{i}=\sigma(\frac{1}{\beta_{i}}(\text{log}\frac{\mu_{i}}{1- \mu_{i}}+\textit{log}\alpha_{i})), || m_{i}=\text{min}(1,\text{max}(0,s_{i}(\zeta-\gamma)+\gamma)),",,"\begin{equation}
    m_{i} \sim \text{HardConcrete}(\log \alpha_{i}, \beta_{i})
\end{equation}",Each neuron mask $m_{i}$ is modeled as an independent random variable following a hard concrete distribution parameterized by $\log \alpha_{i}$ and temperature $\beta_{i}$.
2024.acl-short.25,2,"on. \(s_{i}\) denotes the mask score of each neuron and \(m_{i}\) is the approximately discrete activation value (i.e., almost 0 or 1) of \(s_{i}\). \(\gamma\) and \(\zeta\) are constants, and \(\mu_{i}\) is the random sample drawn from uniform distribution \(\mathcal{U}\)(0, 1). In this work, we also treat \(\beta_{i}\) as a constant, thus only \(\alpha\) is the set of differentiable parameters for \(m\). During the inference stage, the mask \(m_{i}\) can be calculated through a hard concrete gate:","\text{min}(1,\text{max}(0,\sigma(\text{log}\alpha_{i})(\zeta-\gamma)+\gamma)).",,"m_{i} = \text{min}(1, \text{max}(0, \sigma(\log \alpha_{i}) (\zeta - \gamma) + \gamma))","The mask $m_{i}$ is computed using a hard concrete gate during inference, where $\sigma$ is the sigmoid function, $\alpha_{i}$ is the learnable parameter, and $\gamma$, $\zeta$ are constants controlling the gate's behavior."
2024.acl-short.25,3,"```
0: mask parameters \(\alpha\), pre-trained language model \(f(\theta)\) with frozen parameter \(\theta\), training corpus \(X\), hyper-parameters \(\beta\), \(\gamma\), \(\zeta\), \(\eta\), learning rate lr.
1: Initialize \(s\leftarrow\sigma(\frac{1}{\beta}(\text{log}\frac{\mu}{1-\mu}+\textit{log} \alpha))\), where \(\mu\sim\mathcal{U}(0,1)\)
2: Initialize \(m\leftarrow\text{min}(1,\text{max}(0,s(\zeta-\gamma)+\gamma))\)
3: Initialize \(f(\theta)\gets f(m\odot\theta)\)
4:for epoch in num_epochs do
5:for\(x\) in \(X\)do
6: Generate \(f(m\odot\theta)\) with step1-3
7:if optimizer_idx == 0 then
8:\(\mathcal{L}=\mathcal{L}_{m}(f(m\odot\theta),x)\) + \(\eta\)\(R(m)\)
9:else
10:\(\mathcal{L}=\mathcal{L}_{adv}(f(m\odot\theta),x)\) + \(\eta\)\(R(m)\)
11:endif
12:\(\alpha=\alpha-\text{lr}\cdot\nabla_{\alpha}(\mathcal{L})\)
13:endfor
14:endfor
15:\(m\leftarrow\text{min}(1,\text{max}(0,\sigma(\text{log}\alpha)(\zeta-\gamma)+ \gamma))\)
16:return\(m\)
```

**Algorithm 1** Neuron Localization Algorithm.

### Adversarial Privacy Neuron Localization

To localize PII-specific neurons, we propose to negate the original training objective, i.e., maximizing the negative log-likelihood of the PII token sequences. Specifically, given a sequence of tokens \(x\) = [\(x_{1}\),..., \(x_{T}\)] from the training corpus and PII tokens [\(x_{p},...,x_{p+I}\)], our training objective is:","\mathcal{L}_{m}(f(m\odot\theta),x)=\sum_{i=1}^{I}\text{log}(P(x_{p+i}|x_{<p+i})).",,"\mathcal{L}_{m}(f(m\odot\theta),x) = -\sum_{t=p}^{p+I} \log P_{f}(x_{t}|x_{<t};m\odot\theta),",This equation defines the training objective for localizing privacy-related neurons by maximizing the negative log-likelihood of PII token sequences.
2024.acl-short.25,4,"On the other hand, to preserve the original language modeling ability of \(f(m\odot\theta)\), we propose to perform further training on the corpus, utilizing the pre-training loss as the adversarial loss:","\mathcal{L}_{adv}(f(m\odot\theta),x)=-\sum_{t=1}^{T}\text{log}(P(x_{t}|x_{<t})).",,"\mathcal{L}_{adv}(f(m\odot\theta),x)=\sum_{t=1}^{T}\text{log}(P(x_{t}|x_{<t})).","This equation defines the adversarial loss as the standard language modeling objective, which maximizes the log-likelihood of the entire token sequence to preserve the model's language generation capability."
2024.acl-short.25,5,"Figure 1: An illustration of our neuron localization method.

Finally, to minimize the number of localized neurons, we penalize the number of localized neurons by minimizing the \(L_{0}\) complexity of mask scores which are zero:",R(m)=-\frac{1}{|m|}\sum_{i=1}^{|m|}\sigma(\text{log}\alpha_{i}-\beta_{i}\text{log }\frac{-\gamma}{\zeta}).,,R(m) = \|m\|_0.,The regularization term $R(m)$ minimizes the $L_0$ norm of the mask $m$ to reduce the number of selected neurons.
2024.acl-short.3,1,"### TSED on Programming Languages

Applying the TSED evaluation method, initially designed for SQL analysis, we have undergone modifications to extend its applicability to various programming languages. The fundamental TSED approach, illustrated in Figure 1, encompasses AST parsing, AST Editing Distance Calculation, and normalization, closely resembling the methodology outlined in the original paper. However, we have made modifications to both the AST parsing and normalization.

**Code Parsing:** Parsing in the domain of programming languages involves parsing raw codetext into its associated AST. This parsing underscores the complexity of interpreting various programming constructs and converting them into a structured grammar tree representation.

We use tree-sitter2 as our AST parser which is based on GLR(generalized left-to-right rightmost), a powerful parsing algorithm commonly found in the literature (Latif et al., 2023; Tomita, 1991; Clem and Thomson, 2021).

Footnote 2: [https://tree-sitter.github.io/tree-sitter/](https://tree-sitter.github.io/tree-sitter/)

**Tree Distance Computation:** For calculating tree edit distance as \(\Delta\), we utilize the same function as outlined in the TSED paper, which is APTED(All Path Tree Edit Distance) algorithm (Pawlik and Augsten, 2015, 2016). Considering \(G_{1}\) as predicted code's AST and \(G_{2}\) as AST from ground-truth:","\Delta(G_{1},G_{2})=\min_{ops}\sum_{i=1}^{n}w(op_{i})",,"\Delta = \text{APTED}(G_{1}, G_{2})",The tree edit distance $\Delta$ between predicted AST $G_{1}$ and ground-truth AST $G_{2}$ is computed using the APTED algorithm.
2024.acl-short.3,2,"Here, \(ops\) is a sequence of edit operations transforming \(G_{1}\) into \(G_{2}\), with \(w(op_{i})\) as the cost for the \(i^{th}\) operation.

**Normalization:** Normalization of tree edit distances accounts for the complexity of the code by considering the maximum number of nodes between two trees, and we add a ramp function to avoid some extreme situations:","TSED=\max\{1-\frac{\delta}{MaxNodes(G_{1},G_{2})},0\}",,"\text{Normalized\_TSED}(G_{1}, G_{2}) = \frac{\Delta(G_{1}, G_{2})}{\max(|G_{1}|, |G_{2}|) + \mathbb{I}[\min(|G_{1}|, |G_{2}|) < \tau]}",Normalized tree edit distance with a ramp function to handle extreme cases based on a threshold $\tau$.
2024.acl-short.39,1,"## 2 Methodology

DataWe use the WikiBio dataset Lebret et al. (2016) for training, consisting of 728,321 biographies from real English Wikipedia pages where the infobox and first paragraph from the articles are provided. On average, each infobox contains 12.5 personal attributes. We explicitly add the gender label (_male_, _female_ or _non-binary/identifiable_), inferring from the pronouns in the paragraph Dereraaga et al. (2019), to the infobox. We remove the biographies where the nationality is not available.

To mitigate the cross-contamination of training and evaluation sets Roberts et al. (2020); Li and Flanigan (2023), we use the Synthbio dataset Yuan et al. (2021) for evaluation, which is a synthetic dataset consisting of structured attributes--which we refer as _true attributes_--describing fictional individuals. It consists of 2,237 infoboxes and each infobox has on average 19 personal attributes and multiple fictional biographies. The comparison of the Wikibio and Synthbio datasets is shown in Table 1.

Personal Attributes of InterestWe study the impact of two common personal attributes:2 (1) _Gender_. Following the gender attributes in the Synthbio dataset, we consider _male_, _female_, and _non-binary_, and, (2) _Region_. Inspired by Min et al. (2023), we manually map the 40 nationalities to 6 regions based on Wikipedia continent categories:3_North America_ (NA), _Europe_ (EU), _Middle East_ (ME), _Asia-Pacific_ (AP), _South/Latin America_ (SA), and _Africa_ (AF).4

Footnote 2: Attribute distributions are shown in Appendix A

Semantic Matching and SentimentWe study the generated biographies from two dimensions: (1) _Semantic Matching_. We use Data-QuestEvalRebuffel et al. (2021), a reference-free semantic evaluator curated for data-to-text evaluation developed in a QA format. Specifically, this metric adopted T5 Kale and Rastogi (2020) for QG/QA models on both data and text. It measures the answer correctness given the text and generates questions from data, and vice versa. and, (2) _Sentiment_. Since recent sentiment evaluators are deployed for social media text Hutto and Gilbert (2014); Camachocollados et al. (2022) which is not suitable for our task, we use a lexical-based method, obtaining the sentiment score by retrieving SentiWords Gatti et al. (2015), a dictionary associating positive or negative scores with approximately 155,000 words. We calculate the sentiment score of the biography by averaging the associated sentiment scores for each word.

In line with the study of _sentiment_, we additionally experiment with the _regard_ evaluation Sheng et al. (2019), a metric measuring if the regard towards a particular identity/demographic group is positive or negative. We observe similar patterns to that of _sentiment_ (Appendix F).

\begin{table}
\begin{tabular}{l r r} \hline \hline  & Wikibio & Synthbio \\ \hline Number of Infoboxs & 105,469 & 2,237 \\ Number of Biographies & 105,469 & 4,270 \\ Avg. \#attributes/Infobox & 12.1 & 19.0 \\ Avg. \#sentences/Biography & 4.3 & 7.0 \\ Avg. \#words/Biography & 101.7 & 110.3 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Statistics of the Wikibio and Synthbio datasets. For the Wikibio dataset, we consider the training partition and filter out the infoboxs that do not have name and nationality attributes.

Biopraphy GenerationOur biography data-to-text task can be formulated as:","Bio(m,co(m))=f_{gen}(m,co(m)),",where biography is generated by the model \(f_{gen}\) given the personal attribute of interest (\(m\)) and the co-occurring attributes (\(co(m)\)),"\begin{equation}
    \mathcal{B} = G(\mathcal{A}, \theta)
\end{equation}","where $ \mathcal{B} $ denotes the generated biography, $ \mathcal{A} $ represents the structured personal attributes from the infobox, and $ G(\cdot; \theta) $ is the generation model parameterized by $ \theta $."
2024.acl-short.39,2,")). We use FlannT5-base Chung et al. (2022), an instruction fine-tuned model, to generate biographies. Following Yuan et al. (2021), we construct the infobox as the data-to-text format described in Kale and Rastogi (2020)5 and finetune Flan-T5-base on WikiBio for 10,000 steps on one P100 GPU, with a batch size of 8, to instruct the model to generate biography based on given attributes. To generate biographies on the Synthbio, we use a beam search of 5.

Footnote 5: The detailed construction is provided in Appendix C.

## 3 True Attributed Biography Generation

First, we validate that the fine-tuned Flan-T5 model generates biographies based on the given personal attributes. To explore the effect of personal attributes, we compare the semantic matching and sentiment on the generated biographies with true attributes (Equation (1)) against those without given the particular attribute (Masked), i.e.,","Bio(\phi,co(m))=f_{gen}(\phi,co(m)).",,"\Delta_{sem} = \text{Data-QuestEval}(Bio(m,co(m))) - \text{Data-QuestEval}(Bio(\text{Masked},co(m)))","Equation 2 defines the semantic matching difference ($\Delta_{sem}$) between biographies generated with a specific attribute $m$ and those generated when the attribute is masked, using the Data-QuestEval metric."
2024.acl-short.39,3,"Model ValidationOur fine-tuned Flan-T5 model outperforms the T5 model Raffel et al. (2020) reported in the Synthbio dataset Yuan et al. (2021), with a RougeL score of 26.4 (vs., 22.6) and a PARENT-F score Dhingra et al. (2019) of 0.114 (vs., 0.049).

We first validate whether the personal attribute of interest can be inferred from the biographies. Specifically, for gender, we use the pronouns as the proxy of gender De-Arteaga et al. (2019) and compare it against the given gender attribute. For the region, since there is no direct method to predict the nationality from the biography, we consider whether the nationality or related country name is mentioned in the biography as the proxy of the nationality encoded in the biography. We do not train a classifier for nationality as the biography contains rich personal information--the classifier may remember the training instances instead of the nationality signals. We then group the results for nationality based on the region.

As shown in Table 2 (Column: True), for gender, we achieve higher than 0.8 accuracy across gender groups, confirming that the given gender is encoded in generated biographies. However, the results in region groups vary. To ensure the generation quality for our analysis and obtain a sufficient amount of data for the analysis, we consider regions with scores higher than 0.75 based on our empirical experience where similar patterns are observed with different thresholds among different region groups: EU, AF, AP, and NA.

True Attributed BiographyDo LLMs generate different biographies for different gender and nationality groups? Figure 2 shows that generated biographies are significantly different among different gender groups (purple bars, gender) in semantic

\begin{table}
\begin{tabular}{l c c c} \hline \hline
**Attributes** & **True** & **Masked** & 
\begin{tabular}{c} **Counterfactual** \\ Raw/Selected \\ \end{tabular} \\ \hline \multicolumn{4}{c}{Gender} \\ \hline Male & 0.999 & 0.963 & 0.991 \\ Female & 0.972 & 0.514 & 0.978 \\ Non-Binary & 0.837 & 0.057 & 0.824 \\ Overall & 0.936 & 0.509 & 0.931 \\ \hline \multicolumn{4}{c}{Region} \\ \hline Europe & 0.837 & 0.732 & 0.488/0.770 \\ South/L. America & 0.674 & 0.618 & 0.234 / - \\ Africa & 0.805 & 0.573 & 0.432/0.856 \\ Middle East & 0.527 & 0.420 & 0.090 / - \\ Asia–Pacific & 0.854 & 0.742 & 0.586/0.819 \\ North America & 0.939 & 0.833 & 0.740 / - \\ Overall & 0.804 & 0.684 & 0.459/0.809 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Results of inferring personal attribute of interest from generated biographies.

Figure 2: Semantic matching and sentiment for different attribute groups. Gender: (M=Male, F=Female, NB=Non-Binary); For true attributed biography (purple bars), pairwise significant differences are reported according to Welch’s t-test at p<0.1 (*) and p<0.05 (**).

matching and sentiment.6 For region, we observe significant differences in some region groups, e.g., AF vs., AP in both measurements, indicating the potential bias among region groups. However, we do not observe constant significant differences for any particular region.

Footnote 6: We conducted a preliminary qualitative analysis on the correlation between the length of generated biographies and evaluation scores in Appendix G and we do not find a strong correlation among them.

True vs., Masked Attributed BiographyTo study the effect of individual personal attributes, we evaluate the semantic matching and sentiment of the generated biographies where given identical attributes but without attributes of interest (Figure 2, green bars). Compared to truly attributed biographies (Figure 2, purple bars), we do not observe significant differences in gender and region. Given that the model mostly cannot infer the masked attributes from the generated text (Table 2, Column: Masked), this indicates that co-occurring attributes also have a strong influence on the biography generation. Masking the personal attributes alone is not effective in understanding the influence of individual personal attributes.

## 4 Counterfactual Attributed Generation

We apply our counterfactual methodology based on our fine-tuned Flan-T5 model. We manipulate only the personal attributes of interest and keep the co-occurring attribute unchanged to study the effect of individual attributes. Specifically, we change the personal attribute (Figure 1, male, top-left) to a different attribute (Figure 1, female, bottom-left) and compare the true (Equation (1)) and counterfactual attributed biographies (Figure 1, top-right vs., bottom-right, respectively), formulating as:","Bio(f,co(m))=f_{gen}(f,co(m)),do(m\to f),","where \(do(m\to f)\) denotes the do operator (Pearl, 2009), e","Bio(m',co(m))=f_{gen}(m',co(m)).",where counterfactual biography is generated by replacing the personal attribute $m$ with $m'$ while keeping the co-occurring attributes $co(m)$ unchanged
2024.acl-short.40,1,"## 3 SEM-based Architectures

In our work we build two systems that revolve around textual sentence embeddings, SEM, as depicted in Figure 1. The figure presents two independent modules sign2sem and sem2text (left plots) that we later combine in sign2sem2text and sign2(sem+text) in an end2end setting (right plot).

Figure 1: sign2sem and sem2text independent modules for the SLT task (left plot). End2end architectures: pipeline system sign2sem2text and multitask system sign2(sem+text) (right plot).

\(\bullet\)**sign2sem Module** This module predicts an intermediate SEM vector. Given a set of frames (video) features, sign2sem produces a vector representing the sentence signed in the video using a transformer encoder.

**Pretraining** the visual feature sentence embedding model on text. We follow Reimers and Gurevych (2019) and train a Siamese network with twin subnetworks 1 and 2. We compute the loss as the minimum squared error (MSE):","\mathcal{L}_{e}=\frac{1}{N}\sum_{i=1}^{N}(\cos(S_{1,i},S_{2,i})-\cos(E_{1,i},E_{2,i}))^{2}","where \(N\) is the batch size, and \(S\) and \(E\) contain the target text SEM vectors and the predicted output SEM vectors respectively","tags, and the description in <description> tags. Also, check if the summation is correctly applied over the vector elements. Alternatively, if the vectors are of dimension d, maybe the summation is over each element. But the context says ""minimum squared error (MSE)"", which is a scalar loss. So yes, the equation as written makes sense.
</think>","tags. Also, check if the summation is correctly applied over the vector elements. Alternatively, if the vectors are of dimension d, maybe the summation is over each element. But the context says ""minimum squared error (MSE)"", which is a scalar loss. So yes, the equation as written makes sense.
</think>

<latex>
\begin{equation}
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} \left\| \mathbf{y}_1^{(i)} - \mathbf{y}_2^{(i)} \right\|^2
\end{equation}
</latex>"
2024.acl-short.40,2,"ly. In our experiments, the target SEM vector is given by sBERT (Reimers and Gurevych, 2019) here and in our models below.

\(\bullet\)**sem2text Module** This module is responsible for the text reconstruction from sentence embeddings SEM. It produces the text translation of the video features encoded in a given SEM vector. The core sem2text model is a transformer model; we compare encoder-decoder and only decoder systems for the task:

**- Encoder-decoder (SLTr)**: this version uses the sign language transformer (SLTr) architecture as in Camgoz et al. (2020). We use a transformer base with a linear projection from the SEM vector input instead of the usual word embedding layer.

**- Decoder only with pretrained mBART**: this version uses a pre-trained mBART-25 decoder and a linear layer to project the SEM vectors into the mBART model dimensions.

**Pretraining** We train both transformers (SLTr from scratch and the already pretrained mBART-25) with Wikipedia data and then finetune them on the SL datasets. We compute the translation output loss as the cross-entropy:","\mathcal{L}_{o}=\mathrm{CE}(T,O)=-\frac{1}{N}\sum_{i=1}^{N}\sum_{j=1}^{M} (\mathrm{T}_{ij}\cdot\log(\mathrm{O}_{ij}))","where \(N\) is the batch size, \(M\) the vocabulary size, \(T\) is the target text and \(O\) is the output text","\mathcal{L}_{t} = -\frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \log P(Y_{i,t} | SEM_i)","where $N$ is the batch size, $T$ is the sequence length, $Y$ contains target text tokens, and $P(Y_{i,t} | SEM_i)$ represents the predicted probability of token $t$ given SEM vector $SEM_i$"
2024.acl-short.41,1,"## 3 Downstream Tasks

While the visual analysis of topics is often very helpful in analyzing a large corpus, the contents of documents often also have effects on other variables. Roberts et al. (2018) e.g. introduced a model that captures the effects of additional tabular variables on topics. STREAM offers the possibility to analyze the effects of topics and additional tabular variables on any given target variable, via implementing a downstream NAM1. The general form of a NAM can be written as:

Footnote 1: see an example in the appendix","\mathbb{E}(y)=h(\beta+\sum_{j=1}^{J}f_{j}(x_{j})),","where \(h(\cdot)\) is the activation function used in the output layer, e","\begin{equation}
f(x) = \sum_{i=1}^{n} f_i(x_i)
\end{equation}","The equation represents the general form of a Neural Additive Model (NAM), where the function $ f(x) $ is expressed as the sum of individual functions $ f_i $ applied to each input feature $ x_i $. This formulation allows for modeling the additive effects of individual features on the target variable."
2024.acl-short.41,2,"e.g. linear activation for a simple regression task or softmax activation for a classification task. \(x\in\mathbb{R}^{j}\) are the input features, \(\beta\) describes the intercept. The shape-functions are expressed as \(f_{j}:\mathbb{R}\rightarrow\mathbb{R}\) and represent the Multi-Layer Perceptron (MLP) corresponding to the \(j\)-th feature. The model structure of a simple NAM is given in Figure 3.

Further, let \(\mathbf{x}\equiv(\mathbf{x}_{tab},\mathbf{x}_{doc})\) denote the categorical and numerical (continuous) structural features \(\mathbf{x}_{tab}\) and \(\mathbf{x}_{doc}\) denote the documents. After fitting a topic model (see section 2), STREAM extracts the documents topical prevalences and thus ""creates"" \(\mathbf{z}\equiv(\mathbf{x}_{tab},\mathbf{x}_{top})\), a probability vector over the documents and topics. Note, that \(x_{j(tab)}^{(i)}\) denotes the \(j\)-th tabular feature of the \(i\)-th observation and \(x_{k(top)}^{(i)}\) denotes document \(i\)-th topical prevalence for topic \(k\). In order to preserve interpretability the available downstream model is given by:","h(\mathbb{E}[y])=\beta+\sum_{j=1}^{J}f_{j}(x_{j(tab)})+\sum_{k=1}^{ K}f_{k}(x_{k(top)}),",,\mathbb{E}(y)=h\left(\beta+\sum_{j=1}^{J_{tab}}f_{j}(x_{j(tab)})+\sum_{k=1}^{K}f_{J_{tab}+k}(x_{k(top)})\right),"This equation extends the NAM framework by incorporating both tabular features and topic prevalences into the downstream model, maintaining interpretability through additive shape functions."
2024.acl-short.43,1,"### FastText-based LIDs

In this paper, we explore the use of MaskLID for LIDs based on the FastText Bojanowski et al. (2017) architecture. However, it is also possible to apply MaskLID to other LIDs, as long as they enable to determine how much each feature (e.g., word) contributes to each supported language. FastText is one of the most popular LID architectures due to its open-source nature, high performance, ease of use, and efficiency. FastText classifier is a multinomial logistic classifier that represents the input sentence as a set of feature embeddings, making it easy to assess each feature's contribution to the final prediction.

Given a sentence \(s\), let \(f_{1},f_{2},\ldots,f_{T}\) represent the features extracted from \(s\). Note that these features are linearly ordered, i.e., \(f_{i}\) precedes \(f_{i+1}\) in \(s\). FastText maps these features onto vectors in \(\mathbb{R}^{d}\) via feature embeddings \(\mathbf{x}_{1},\mathbf{x}_{2},\ldots,\mathbf{x}_{T}\). The dimensionality of these embeddings, denoted \(d\), is a hyperparameter. A base LID using FastText architecture computes the posterior probability for a language \(c\in[1:N]\) by applying the softmax function over logits as:",P(c|s)=\frac{\exp(\mathbf{b}_{c}\cdot\frac{1}{T}\sum_{t=1}^{T}\mathbf{x}_{t})}{ \sum_{c^{\prime}=1}^{N}\exp(\mathbf{b}_{c^{\prime}}\cdot\frac{1}{T}\sum_{t=1}^ {T}\mathbf{x}_{t})}.,,"\begin{equation}
p(c \mid s) = \frac{\exp\left(\frac{1}{T} \sum_{i=1}^{T} \mathbf{w}_{c}^{\top} \mathbf{x}_{i} + b_{c}\right)}{\sum_{c'=1}^{N} \exp\left(\frac{1}{T} \sum_{i=1}^{T} \mathbf{w}_{c'}^{\top} \mathbf{x}_{i} + b_{c'}\right)},
\end{equation}","Equation 1 defines the posterior probability of a language $c$ given a sentence $s$ using the FastText architecture, where $\mathbf{w}_{c}$ and $b_{c}$ are the weight vector and bias term for class $c$, respectively."
2024.acl-short.43,2,"\(P(c|s)\) is the base LID probability of the input text \(s\) belonging to language \(c\), \(\mathbf{b}_{c}\) is the weight vector for language \(c\), and \(N\) is the total number of classes supported by the base LID.

To evaluate how much each feature contributes to each supported language, we need to compute logits separately for each feature. For simplicity and alignment with the FastText tokenizer (which considers white-spaces as token boundaries), we set the level of granularity of features to be the word level. The word-level feature embedding is obtained as the summation of all feature embeddings that build each word. Noting \(W\) the number of words in a sentence \(s\), we define the \(N\times W\) matrix \(\mathbf{V}(s)\), where each element \(\mathbf{V}_{c,t}(s)\) represents the logits for language \(c\) and word-level feature \(\mathbf{x}_{t}\):","\mathbf{V}_{c,t}(s)=\mathbf{b}_{c}\cdot\mathbf{x}_{t}.",,"\mathbf{V}_{c,t}(s) = \mathbf{b}_{c} \cdot \mathbf{x}_{t}.","Each element $\mathbf{V}_{c,t}(s)$ represents the contribution (logits) of word-level feature $\mathbf{x}_{t}$ to language $c$ in sentence $s$."
2024.acl-short.49,1,"## 3 Methods

We identify two significant drawbacks with the CT:

1. It does not test whether impactful features are _more likely_ to be mentioned than less impactful ones. There is a trivial strategy that leads to 0% unfaithfulness as measured by the CT: repeat all input t ext verbatim as the explanation, which means explanations will never fail to mention the IA. This demonstrates an important property of useful explanations: they are useful only if they both mention impactful features and _leave out_ non-impactful features.
2. It measures impactfulness as binary, i.e. whether the intervention results in a change in the model's top predicted label. But this ignores changes in the model's predicted class likelihoods: it would label an intervention that changes the predicted probability of a class from \(49\%\) to \(51\%\) as relevant, while an intervention that changes the probability from \(1\%\) to \(49\%\) would be labelled as irrelevant, even though the latter caused a larger shift.

To address these drawbacks, we propose the metric **Correlational Explanatory Faithfulness (CEF)**, which can be applied to any tests with three given properties:

1. An _intervention_: a function mapping an input example to its modified version.

2. A _prediction impact measure_: a function mapping an input example, intervention, and model to a scalar representing how impactful the intervention was on the model's prediction. We call the output of this function the _prediction impact_ or \(\mathcal{I}\).
3. An _explanation mention measure_: a function mapping an input example, intervention, and explanation to a scalar representing the extent to which the explanation attributes importance to the intervened factors. We call the output of this function the _mention importance_ or \(\mathcal{M}\).

If an intervention has higher prediction impact, a faithful explanation should assign it higher mention importance. We quantify this relationship by measuring the Pearson correlation coefficient between prediction impact and mention importance:",\text{CEF}=\frac{\sum_{i=0}^{n}(\mathcal{I}_{i}-\overline{\mathcal{I}} )(\mathcal{M}_{i}-\overline{\mathcal{M}})}{\sqrt{\sum_{i=1}^ {n}(\mathcal{I}_{i}-\overline{\mathcal{I}})^{2}}\sqrt{\sum_{i=1}^ {n}(\mathcal{M}_{i}-\overline{\mathcal{M}})^{2}}},where \(\overline{x}=\frac{1}{n}\sum_{i=1}^{n}x_{i}\) (the sample mean),\text{CEF} = \frac{\sum_{i=1}^{N} (\mathcal{I}_i - \bar{\mathcal{I}})(\mathcal{M}_i - \bar{\mathcal{M}})}{\sqrt{\sum_{i=1}^{N} (\mathcal{I}_i - \bar{\mathcal{I}})^2 \sum_{i=1}^{N} (\mathcal{M}_i - \bar{\mathcal{M}})^2}},"Equation 1 defines the Correlational Explanatory Faithfulness (CEF) metric as the Pearson correlation coefficient between prediction impact ($\mathcal{I}$) and mention importance ($\mathcal{M}$) across $N$ interventions, addressing the limitations of the CT by capturing the continuous relationship between impact and explanation."
2024.acl-short.49,2,"n). Being a correlation, it lies in the interval \([-1,1]\), with 0 indicating no relationship and positive values indicating higher mention importance for more impactful interventions.

We can then apply this metric to the CT, which gives us the **Correlational Counterfactual Test (CCT)**. In our work, the intervention inserts an IA. To quantify the degree of prediction impact in a continuous manner, we measure the total shift in the model's predictions due to the IA. There are a number of ways to measure shifts in probability distributions over discrete classes; we use the _total variation distance_ (TVD), i.e:","\text{TVD}(P,Q)=\frac{1}{2}\sum_{x}|P(x)-Q(x)|",where P and Q are probability distributions over discrete classes,\mathcal{I}_{\text{TVD}} = \frac{1}{2} \sum_{c \in \mathcal{C}} \left| p(c \mid \mathbf{x}) - p(c \mid \mathbf{x}') \right|,"where $\mathcal{C}$ is the set of all classes, $\mathbf{x}$ is the original input, and $\mathbf{x}'$ is the modified input with the IA inserted."
2024.acl-short.49,3,"es. We take P and Q to be the model's predicted distributions before and after the intervention, so that TVD measures the absolute change in probabilities assigned to each class. Compared to other common statistical distances such as the relative entropy (KL divergence), TVD gives less weight to shifts between very small probabilities (which are unlikely to impact classification) and has the advantage of symmetry.

To measure mention importance, we use the original CT's binary metric: does the explanation mention the word? Note that in this case our metric represents the _point-biserial correlation_, a special case of the Pearson correlation coefficient where one variable is continuous and the other is dichotomous. We can then write CCT as:","\text{CCT}=\frac{\mathbb{E}_{M}(\text{TVD})-\mathbb{E}_{-M}(\text{TVD})}{ \text{STD}(\text{TVD})}\sqrt{\frac{|M||\neg M|}{|M\cup\neg M|^{2}}},","where \(M\) indicates that the explanation mentions the IA, and \(|M|\) indicates the number of examples with explanation mentions",\text{CCT}=\frac{\sum_{i=1}^{n}(\text{TVD}_i-\overline{\text{TVD}})(M_i-\overline{M})}{\sqrt{\sum_{i=1}^{n}(\text{TVD}_i-\overline{\text{TVD}})^2}\sqrt{\sum_{i=1}^{n}(M_i-\overline{M})^2}},the point-biserial correlation between prediction impact (measured via TVD) and mention importance (binary indicator of whether the explanation mentions the intervened word)
2024.acl-short.5,1,"### Contrastive Decoding

The intrinsic rationale of contrastive decoding (CD) is that amateur LMs have stronger systematic undesirable tendencies to produce undesirable patterns (e.g., hallucination) than expert LMs. By contrasting the token distributions between expert and amateur LMs, such tendencies can be alleviated. There have been successively proposed two versions of contrastive decoding by Li et al. (2023) and O'Brien and Lewis (2023), which we term as _Original_ contrastive decoding and _Improved_ contrastive decoding. The final contrastive logit scores for the original contrastive decoding \(s_{\text{ori}}(x_{i}|x_{<i})\) and the improved contrastive decoding \(s_{\text{imp}}(x_{i}|x_{<i})\) are respectively:","s_{\text{ori}}(x_{i}|x_{<i})= || \{\begin{array}{ll}\log P_{\mathcal{M}_{e}}(x_{i}|x_{<i}) -\log P_{\mathcal{M}_{a}}(x_{i}|x_{<i}),&x_{i}\in\mathcal{V}_{\text{ori},i}^{ \alpha}\\ -\infty,&x_{i}\notin\mathcal{V}_{\text{ori},i}^{\alpha}\\ s_{\text{imp}}(x_{i}|x_{<i})=\\ \{\begin{array}{ll}(1+\beta)Y_{\mathcal{M}_{a}}(x_{i}|x_{<i})-\beta Y_{ \mathcal{M}_{a}}(x_{i}|x_{<i}),&x_{i}\in\mathcal{V}_{\text{imp},i}^{\alpha}\\ -\infty,&x_{i}\notin\mathcal{V}_{\text{imp},i}\end{array}.\end{array}. || s_{\text{int}}(x_{i}|x_{<i})= || \{\begin{array}{ll}(1+\beta)Y_{\mathcal{M}_{e}}(x_{i}|x_{< i})-\beta Y_{\mathcal{M}_{a}}(x_{i}|x_{<i}),&x_{i}\in\mathcal{V}_{\text{imp},i}^{ \alpha}\\ -\infty,&x_{i}\notin\mathcal{V}_{\text{imp},i}\end{array}.",,s_{\text{ori}}(x_{i}|x_{<i}) = \log p_{\text{expert}}(x_{i}|x_{<i}) - \log p_{\text{amateur}}(x_{i}|x_{<i}),Original contrastive decoding logit score as the difference between expert and amateur LM log probabilities.
2024.acl-short.5,2,"\). and \(Y\). are respectively the token probability and logit generated from LMs. \(\mathcal{V}_{\cdot,i}^{\alpha}\) denotes the adaptive plausibility constraint that dynamically restricts the logits from producing the erroneous modes. The adaptive plausibility constraints are calculated as","\mathcal{V}_{\text{ori},i}^{\alpha}=\{w|P_{\mathcal{M}_{e}}(w|x_{<i})> \alpha\max_{w\in\mathcal{V}}P_{\mathcal{M}_{e}}(w|x_{<i})\}, || \mathcal{V}_{\text{imp},i}^{\alpha}=\{w|Y_{\mathcal{M}_{e}}(w| x_{<i})>\log\alpha+\max_{w\in\mathcal{V}}Y_{\mathcal{M}_{e}}(w|x_{<i})\}.",,"\mathcal{V}_{\cdot,i}^{\alpha}=\left\{x_{i}\in\mathcal{V}\mid P_{\mathcal{M}_{e}}(x_{i}|x_{<i})\geq\alpha\cdot\max_{x'\in\mathcal{V}}P_{\mathcal{M}_{e}}(x'|x_{<i})\right\},","The adaptive plausibility constraint $\mathcal{V}_{\cdot,i}^{\alpha}$ restricts the token generation to those whose expert LM probability is at least a fraction $\alpha$ of the maximum probability among all tokens at position $i$."
2024.acl-short.53,1,"## 3 Method

We mainly fine-tune the base LLMs including LLaMA-7B, Falcon-7B, and Mistral-7B with the PEFT approach including LoRA and P-tuning and minimize the following loss function:","\mathcal{L}_{FT}=\frac{1}{|\mathcal{D}_{M}|}\sum_{d\in\mathcal{D}_{M}}L(d;\Phi _{0},\Delta\Phi)","where \(\mathcal{D}_{M}\) is the KE dataset and \(d\) is a fact edit example, \(L\) is the cross entropy loss function applied to autoregressive models, \(\Phi_{0}\) denotes the set of original weights of the language model that are kept frozen, and \(\Delta\Phi\) denotes the additional parameters used by the PEFT adapters",\mathcal{L} = \sum_{i=1}^{N} \left( y_i - \hat{y}_i \right)^2 + \lambda \Omega(\theta),Loss function combining prediction error and regularization term for fine-tuning base LLMs with PEFT approaches.
2024.acl-short.53,2,"rs.

**LoRA.** LoRA uses low-rank matrices \(B\in\mathbb{R}^{d\times r}\) and \(A\in\mathbb{R}^{r\times k}\) and \(r\ll\min(d,k)\). The low rank matrices \(A\) and \(B\) are trainable parameters:",h=W_{0}x+BAx=(W_{0}+BA)x.,,\Delta\Phi_{\text{LoRA}} = B \cdot A,"Equation defining the PEFT adapter parameters via low-rank matrix decomposition, where $B\in\mathbb{R}^{d\times r}$ and $A\in\mathbb{R}^{r\times k}$ are trainable low-rank matrices with $r\ll\min(d,k)$"
2024.acl-short.56,1,"### Conditional Variational IME

Following the previous work of CVM, CV-IME primarily consists of four components: a encoder-decoder model, a prior network \(p_{\theta}(z|c)\), a recognition network \(q_{\phi}(z|r,c)\) and a discrete latent variable matrix \(M\). \(c\), \(r\) and \(z\) represent the user input (i.e., context and pinyin sequence), the character result and the continues latent variable.

Hybrid Latent Variable.Previous researches indicate that continuous latent variables can enhance diversity but may reduce relevance, whereas discrete latent variables strengthen relevance but lack diversity (Gao et al., 2019; Bao et al., 2020; Sun et al., 2021, 2023). Therefore, a promising direction is to hybrid the continuous and discrete latent variables, leveraging their respective strengths to complement and offset their weaknesses. To build the hybrid latent variables \(H\), we follow Sun et al. (2023), adding sentence-level continuous latent variable \(z^{\prime}_{s}\) to the discrete latent variables \(M\): \(H=(z^{\prime}_{s}+M[1],\cdots,z^{\prime}_{s}+M[k])\), where \(K\) represents the number of discrete latent variables.

Continuous Latent Variables.We initially employ the model encoder to transform \(c\) and \(c+r\) into prior memory \(\mathbf{h}\) and posterior memory \(\mathbf{h}^{\prime}\). Given that there is a degree of alignment between the pinyin and character sequences in the task of pinyin-to-character conversion, relying solely on the encoder's self-attention mechanism for interaction may not yield effective information. Therefore, we have introduced an interaction between the prior memory and the posterior memory:",\mathbf{h}^{\prime}=\mathrm{SoftMax}(\mathbf{h}\cdot\mathbf{h}^{\prime T}) \cdot\mathbf{h}^{\prime},,"H = \left( z^{\prime}_{s} + M[1], \cdots, z^{\prime}_{s} + M[k] \right)",Hybrid latent variable $H$ combines continuous variable $z^{\prime}_{s}$ with discrete latent variables $M$ to balance diversity and relevance.
2024.acl-short.56,2,"To enhance the recognition process, we use \(\mathbf{h}\) and \(\mathbf{h}^{\prime}\) together to estimate the isotropic Gaussian distribution \(q_{\phi}(z|c,r)\sim\mathcal{N}(\mu^{\prime},\sigma^{\prime 2}\mathbf{I})\) :","\begin{pmatrix}\mu^{\prime}_{1},...,\mu^{\prime}_{n}\\ \log(\sigma^{\prime 2}_{1}),...,\log(\sigma^{\prime 2}_{n})\end{pmatrix}= \begin{pmatrix}[h_{1};h^{\prime}_{1}]\\ \cdots\\ [h_{n};h^{\prime}_{n}]\end{pmatrix}W^{\prime}_{u}\,,","where \(W^{\prime}_{u}\) is trainable parameters of \(q_{\phi}(z|r,c)\)","q_{\phi}(z|c,r) \sim \mathcal{N}(\mu^{\prime}, \sigma^{\prime 2} \mathbf{I})","Distribution of the continuous latent variable $z$ conditioned on context $c$ and character result $r$, modeled as an isotropic Gaussian with mean $\mu^{\prime}$ and variance $\sigma^{\prime 2}$."
2024.acl-short.57,1,"## 3 Problem Definition

To model a CQA setting, at dialog turn \(k\), a model receives a question (\(q_{k}\)), a document containing the answer (\(D\)), and the history of the question (\(H_{k}\)), which is represented as a set of tuples, such as \(H_{k}=\{(q_{0},a_{0}^{pred}),\cdots,(q_{k-1},a_{k-1}^{pred})\}\), where \(a_{j}^{pred}\) is the model's prediction for \(q_{j}\). It's important to note that the model may utilize only some of this information. For instance, we only employ history questions while excluding history answers. The objective is to predict the answer \(a_{k}^{pred}\) for \(q_{k}\).","a_{k}^{pred}=\operatorname*{arg\,max}_{a_{k}}\!P(a_{k}|q_{k},H_{k},D)",,"a_k^{\text{pred}} = f(q_k, D, H_k)","The predicted answer $a_k^{\text{pred}}$ is a function of the current question $q_k$, document $D$, and history $H_k$, where $H_k$ contains prior question-answer pairs but only historical questions are utilized in this model."
2024.acl-short.57,2,"## 4 Methodology

We seek to make the reasoning robust to irrelevant history implicitly by augmenting the dataset. To this end, for question \(q_{k}\), we augment its history by injecting some synthetic questions. Let \(H_{k}^{\star}\) be the augmented history. The intuition is that irrespective of whether the reasoning is performed with \(H_{k}\) or \(H_{k}^{\star}\), the result should be the same. In other words:","P(a_{k}|q_{k},H_{k},D)=P(a_{k}|q_{k},H_{k}^{\star},D)",,"a_{k}^{pred}=\operatorname*{arg\,max}_{a_{k}}\!P(a_{k}|q_{k},H_{k},D) = \operatorname*{arg\,max}_{a_{k}}\!P(a_{k}|q_{k},H_{k}^{\star},D)",Equation 2 enforces robustness by ensuring that the predicted answer remains consistent regardless of whether the original or augmented history is used.
2024.acl-short.57,3,"To achieve this goal, we establish a two-stage pipeline. Our pipeline consists of a history augmentation module, whose goal is to augment the history and a question-answering module, whose objective is to consistently train a QA network so that the reasoning is consistent. The overall architecture of our model is depicted in Figure 1.

### History Augmentation Module

This module includes a conversational question generator, denoted as \(CQG_{\theta}\), where \(\theta\) represents the parameter set of the generator, and a question selector, denoted as \(QS\), which is responsible for choosing a set of \(S\) synthetic questions generated to augment the history.

TrainingThe first step involves training \(CQG_{\theta}\). While there has been research aimed at generating conversational questions Gu et al. (2021); Pan et al. (2019), for the sake of simplifying the implementation, we employ a straightforward generative transformer for this task. To train this network, we input \(D\), \(H_{k}\), and \(a_{k}\) into the network, intending to generate \(q_{k}\). We train this network using cross-entropy loss in an auto-regressive manner.

Question GenerationAfter training \(CQG_{\theta}\), we aim to generate synthetic conversational questions for the training set. Suppose that we want to generate synthetic conversational questions for \(q_{k}\). We iteratively generate synthetic questions between \(q_{j}\) and \(q_{j+1}\) for \(1\leq j\leq k-1\). Suppose that \(a_{j}\) is located in the \(i\)-th sentence of the document. We extract noun phrases from sentences \(i-1\), \(i\), and \(i+1\) as potential answers. We make this choice because we want these answers to be similar to the flow of conversation, and if these answers are extracted from local regions, the likelihood increases. Let one of these answers be called \(a^{syn}\). We feed \(D\), \(H_{j+1}\) (all the questions and answers before \(a^{syn}\)), and \(a^{syn}\) to \(CQG_{\theta}\) to obtain the synthetic question of \(q^{syn}\). We refer to all generated synthetic questions and real questions of history as the pool of questions (\(P_{k}\)) for \(q_{k}\).

Question Filtering & InjectionWe could set \(P_{k}\) as \(H_{k}^{\star}\); however, \(P_{k}\) contains a multitude of synthetic questions which induces too much noise. Additionally, in the consistency training setting, the noise (perturbation) should be small. Thus, we only select \(S\) of synthetic questions from \(P_{k}\), where \(S\) is a hyperparameter. Not all synthetic questions are helpful, necessitating the need to filter out degenerate ones. We want our selected synthetic questions to be similar and relevant to the trend of the conversation. To this end, we compute a score for each synthetic question and only keep the top \(M\) synthetic questions with the highest score. To compute the score, each question (real or synthetic) is encoded with LaBSE (Feng et al., 2022). For each synthetic question \(q^{syn}\) which is located between history turns \(q_{j}\) and \(q_{j+1}\), the score is computed as \(Sim(h(q_{j}),h(q^{syn}))+Sim(h(q_{j+1}),h(q^{syn}))\), where \(Sim\) is the cosine similarity function and \(h(\mathbf{x})\) is the LaBSE's encoding of the sentence \(\mathbf{x}\). Additionally, sometimes, we generate questions that are too similar to previous or future questions, which are invaluable. Thus, we compare the similarity of the generated question \(q^{syn}\) with questions in \(\{q_{k}\}\bigcup H_{k}\) and if the similarity is above \(\gamma\), \(q^{syn}\) is discarded. This situation is depicted in Figure 1, where \(P_{k}\) contains real history questions, depicted in blue, and synthetic questions, depicted in red and green. Those synthetic questions that have high similarity with \(\{q_{k}\}\bigcup H_{k}\) are depicted in red. As it can be seen, the two questions ""Did she have any children"" and ""How many children did they have"" have high similarity with the question ""Did they have children"", and thus, they're discarded. In addition, we need to set a distribution to guide the selection of \(S\) number of generated questions. We conduct experiments using two distributions: uniform and linear. In the uniform setting, the generated questions are selected with the same probability. For the linear, if \(q^{syn}\) is located between \(q_{j}\) and \(q_{j+1}\), its probability of being selected (\(P(q^{syn})\)) is \(P(q^{syn})\propto j\). We opt for the linear distribution, as we believe that closer synthetic questions to the original question might contribute to greater robustness, as questions that are further away are likely less relevant.

### Question Answering Module

For each question \(q_{k}\), as illustrated in Figure 1, we feed \(q_{k}\), \(H_{k}\), and \(D\) to the QA network (\(QA_{\theta^{\prime}}\)) to compute the answer distribution. In parallel, we feed \(q_{k}\), \(H_{k}^{\star}\), and \(D\) to the QA network to compute another answer distribution. As mentioned in Section 4, we need to impose the condition outlined in Equation (2). To achieve this, we employ KL-Divergence between the answer distributions.

Figure 1: **Architecture of the Model**: For a given question \(q_{k}\), the conversational question generator \(CQG_{\theta}\) constructs a pool of questions denoted as \(P_{k}\). Questions in \(H_{k}\) are shown in blue. The synthetic questions are depicted in red and green: those similar to \(H_{k}\) questions are in red, and the dissimilar ones are in green. The question selector \(QS\) selects \(M\) questions with the highest scores, discards red questions, and chooses \(S=3\) synthetic questions from the green questions according to uniform distribution, along with \(H_{k}\) questions, to create \(H_{k}^{\star}\). The QA network \(QA_{\theta^{\prime}}\) computes its output using both \(H_{k}\) and \(H_{k}^{\star}\) as input. The QA network is trained by minimizing the cross-entropy loss (\(L_{CE}\)) and consistency loss (\(L_{Cons}\)). \(q_{k}\) and \(H_{k}\) are from the QuAC dataset.

Additionally, we use cross-entropy loss to train the QA network for answer prediction. The losses are calculated as per Equation (3), where \(L_{CE}\), \(L_{Cons}\), and \(L_{T}\) represent the cross-entropy loss, consistency loss, and total loss. \(\lambda\) is a hyperparameter used to determine the ratio of the two losses.","L_{CE}=CE(QA_{\theta^{\prime}}(q_{k},H_{k},D),a_{k}^{gold}) || L_{Cons}=D_{KL}(QA_{\theta^{\prime}}(q_{k},H_{k},D), || QA_{\theta^{\prime}}(q_{k},H_{k}^{\star},D)) || L_{T}=L_{CE}+\lambda L_{Cons}",,"L_{CE} = -\log P(a_{k}|q_{k},H_{k},D), \quad L_{Cons} = D_{KL}(P(a_{k}|q_{k},H_{k},D) \parallel P(a_{k}|q_{k},H_{k}^{\star},D)), \quad L_{T} = L_{CE} + \lambda L_{Cons}","Equation 3 defines the loss functions used to train the QA network, including cross-entropy loss (L_CE), consistency loss (L_Cons) based on KL-divergence, and the total loss (L_T) as a weighted sum of the two, where λ is a hyperparameter balancing the contributions of the losses."
2024.acl-short.62,1,"### Token-level reward modeling

In this section, we will first introduce the RL environment and then define our token-level reward modeling scheme.

Language generation can be defined as a Markov Decision Process (MDP) \(\langle\mathcal{S},\mathcal{A},\mathcal{R},\mathcal{P},\gamma\rangle\). \(\mathcal{S}\) refers to the state space and we define the start state \(s_{1}\) as the input prompts \(\{x\}\). An action at t-step \(a_{t}\) is a generated token. The transition function of the environment is denoted as \(\mathcal{P}:\mathcal{S}\times\mathcal{A}\rightarrow\mathcal{S}\), \(s_{t}=\{x,a_{1},...,a_{t-1}\}\). A response \(y\) of length \(T\) is then \(y=\{a_{1},..,a_{T}\}\). In our token-level reward scheme, a reward is assigned to each generated token \(a_{t}\) by \(\mathcal{R}\colon\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}\), where at each time step \(t\) there is a learned reward function \(r_{t}=r_{\phi}(s_{t},a_{t})\). Therefore, for each response, we have a trajectory \(\tau=\{s_{1},a_{1},r_{1},...s_{t},a_{t},r_{t},...s_{T},a_{T},r_{T}\}\).

We define the reward of the whole trajectory as the average of rewards assigned to each token:",R(\tau)=\frac{1}{T}\sum_{t=1}^{T}r_{t}.,,"\begin{equation}
R(\tau) = \frac{1}{T} \sum_{t=1}^{T} r_{\phi}(s_t, a_t)
\end{equation}",This equation defines the total reward of a trajectory as the average of the token-level rewards obtained at each step in the sequence.
2024.acl-short.62,2,"Following the Bradley-Terry (BT) model (Bradley and Terry, 1952) for preference modeling, we formulate the distribution of human preference for responses as below:","\begin{split} p(\tau^{i}\succ\tau^{j})&=\frac{ \exp(R(\tau^{i}))}{\exp(R(\tau^{i}))+\exp(R(\tau^{j}))}\\ &=\sigma(R(\tau^{i})-R(\tau^{j})),\end{split}",where \(\tau^{i}\) and \(\tau^{j}\) represent two different responses generated from the same prompt,"p_{\text{BT}}(y_{i}\succ y_{j})=\frac{e^{\mu_{i}}}{e^{\mu_{i}}+e^{\mu_{j}}}},","where $y_i$ and $y_j$ are two responses to the same prompt, and $\mu_i$, $\mu_j$ are their respective reward scores."
2024.acl-short.62,3,"pt. Under the setting of our fine-grained supervision dataset, we assume \(\tau^{i}\) only makes edits on \(\tau^{j}\) while maintaining most parts unchanged. We define \(U_{0}=\{t|a_{t}^{i}=a_{t}^{j}\}\) and \(U_{1}=\{t|a_{t}^{i}\neq a_{t}^{j}\}\) to represent the unchanged and changed parts.

Regarding the reward model as a binary classifier, we use negative log-likelihood as the loss function. By plugging in Equation 1, we have:","\begin{split}&\mathcal{L}=-\mathbb{E}_{(\tau^{i},\tau^{j}) \sim\mathcal{D}}[\log\sigma(R(\tau^{i})-R(\tau^{j}))]\\ &=-\mathbb{E}_{(\tau^{i},\tau^{j})\sim\mathcal{D}}[\log\sigma(( \frac{1}{T^{i}}-\frac{1}{T^{j}})\sum_{t\in U_{0}}r_{t}\\ &+\frac{1}{T^{i}}\sum_{t\in U_{1}}r_{t}^{i}-\frac{1}{T^{j}}\sum_ {t\in U_{1}}r_{t}^{j})],\end{split}",,"\begin{split} \mathcal{L}_{\text{RM}} & = -\log\, p(\tau^{i}\succ\tau^{j}) \\ & = -\log\, \sigma\left(\frac{1}{T}\sum_{t=1}^{T}(r_{t}^{i} - r_{t}^{j})\right) \\ & = -\log\, \sigma\left(\frac{1}{T}\sum_{t\in U_{1}}(r_{t}^{i} - r_{t}^{j})\right), \end{split}","Loss function for the reward model under the assumption that only parts of the response are changed between $\tau^{i}$ and $\tau^{j}$, using the sigmoid function and focusing only on the changed token positions $U_1$."
2024.acl-short.62,4,"Ideally, we aim for the unchanged part to maintain a consistent reward. Under this assumption, and if the two responses are of equal length, the first term of the loss function can be removed:","\begin{split}\mathcal{L}\approx-\mathbb{E}_{(\tau^{i},\tau^{j} )\sim\mathcal{D}}[\log\sigma(\frac{1}{T^{i}}\sum_{t\in U_{1}}r_{t}^{i}-\frac{ 1}{T^{j}}\sum_{t\in U_{1}}r_{t}^{j})]\end{split}",,"\begin{split}&\mathcal{L}_{\text{equal}}=-\mathbb{E}_{(\tau^{i},\tau^{j}) \sim\mathcal{D}}[\log\sigma(\frac{1}{T}\sum_{t\in U_{1}}r_{t}^{i}-\frac{1}{T}\sum_{t\in U_{1}}r_{t}^{j})].\end{split}",This equation simplifies the loss function under the assumption of equal response length and consistent reward for unchanged parts.
2024.acl-short.66,1,"## 2 Experimental Settings

Our analysis is based on the most widely used SiMT models and datasets. This section introduces these models and datasets as follows.

SiMT Models and Datasets.SiMT models translate by reading partial source sentences. Ma et al. (2019) proposed widely used Wait-\(k\) models for SiMT. It involves reading k words initially and then iteratively generating each word until the end of the sentence. We conducted experiments on it. We use two standard benchmarks from IWSLT14 De\(\leftrightarrow\)En Cettolo et al. (2013) and MuST-C Release V2.0 Zh\(\rightarrow\)En Cettoni et al. (2021) to conduct experiments. Appendix A provides detailed settings. Due to space limitation, we only present the experimental results for the De\(\rightarrow\)En benchmark. The results for Zh\(\rightarrow\)En and En\(\rightarrow\)De are similar, as shown in Appendix D and C.

Hallucination Metric.In SiMT, Chen et al. (2021) pioneers the definition of Hallucination Metrics based on word alignment \(a\). A target word \(\hat{y}_{t}\), is a hallucination if there is no alignment to any source word \(x_{j}\). This is formally represented as:","H(t,a)=\mathds{1}[\{(i,t)\in a\}=\varnothing].",,"\begin{equation}
    \text{Hallucination}(\hat{y}_t) = 
    \begin{cases}
        1 & \text{if } \nexists j \text{ such that } a_t = j, \\
        0 & \text{otherwise}.
    \end{cases}
\end{equation}","This equation defines the hallucination metric for a target word in SiMT based on word alignment, where a target word is considered a hallucination if it has no alignment to any source word."
2024.acl-short.66,2,"Conversely, a target word \(\hat{y}_{t}\), is not a hallucination if there is alignment to any source word \(x_{j}\).

The Hallucination Rate (HR) is defined as following:","\text{HR}(x,\hat{y},a)=\frac{1}{|\hat{y}|}\sum_{t=1}^{|\hat{y}|}H(t,a).",,"HR = \frac{1}{T} \sum_{t=1}^{T} H(t, a)","The Hallucination Rate (HR) is calculated as the average of the hallucination metric $H(t, a)$ over all target words $t$ in the translation, where $T$ is the total number of target words."
2024.acl-short.66,3,"Deng et al. (2022) propose GHall to measure hallucination in Wait-\(k\). Formally, a word is a hallucination if it does not align with the current source:","H_{wait-k}(t,a)=\mathds{1}[\{(s,t)\in a\mid s\geq t+k\}=\varnothing].",,"H_{\text{GHall}}(t, a, s) = \mathds{1}[\{(i, t) \in a \mid x_i \in s\} = \varnothing].","This equation defines the GHall hallucination metric, where a target word is considered hallucinated if it does not align with any source word in the current source segment $s$."
2024.acl-short.66,4,"The definition of HR remains consistent with Chen et al. (2021). We utilize GHall metrics to conduct experiments. We use Awesome-align Dou and Neubig (2021) as the word aligner \(a\).

## 3 Understanding Hallucination Words from Distribution

Hallucination is severe in SiMT.We measure HR of Wait-\(k\) models, illustrated in Table 11. We obtain that Wait-\(k\) models suffer more from hallucinations than Full-sentence MT. Furthermore, with \(k\) decreasing, hallucinations increase clearly. This shows that hallucination is an important issue and it is worth the in-depth study.

### Understanding Hallucination from Frequency Distribution

Hallucination words are with high distribution entropy.To investigate hallucination words in Wait-\(k\), we compare frequency distributions of hallucination and overall words. Figure 1 and Table 2 illustrate that their distributions are remarkably similar and both exhibit high entropy. It suggests that understanding hallucination from high distribution entropy is challenging.

\begin{table}
\begin{tabular}{c|c c c c c c} \hline \hline \(k\) & \(1\) & \(3\) & \(5\) & \(7\) & \(9\) & \(\infty\) \\ \hline HR \(\%\) & 31.28 & 22.57 & 18.58 & 16.41 & 15.21 & 11.50 \\ \hline \hline \end{tabular}
\end{table}
Table 1: HR on valid set of wait-\(k\), where \(k=\infty\) means Full-sentence MT.

Figure 1: Word frequency of Hallucination and Overall on valid hypotheses set of wait-\(1\) (x-axis is ordered randomly, with additional \(k\) results in Appendix B.1).

### Understanding Hallucination from Predictive Distribution

We investigate **Confidence** and **Uncertainty** of the predictive distribution. We define the Confidence of a word as its probability and the Uncertainty of a word as the entropy of its predictive distribution.

**Hallucination words are difficult to translate.** To explore the difficulty of translating hallucination and non-hallucination words, we calculate the average confidence and uncertainty on the valid set. The results in the left of Table 3 reveal that during decoding hallucination words, the models exhibit higher uncertainty. Additionally, the confidence is lower. It suggests that models encounter challenges in accurately translating hallucination words.

**Hallucination words are difficult to memorize.** To investigate the reasons behind the difficulty in translating hallucination words, we measure confidence and uncertainty for hallucination and non-hallucination words on the training data. We sample examples from the training data as a training subset with the same size as the valid set. The results in the right of Table 3 illustrate that even in previously encountered contexts, models remain uncertain when dealing with hallucination words. These findings suggest that models do not fit well with hallucination words during training, leading to a limited ability to generalize to similar contexts on the valid set. Consequently, the difficulty in translating hallucination words can be attributed to challenges in memorization during the training. Additionally, we observe that as \(k\) increases, the uncertainty decreases significantly. It can be attributed to the model encountering source-side context more, enabling a improved memorization.

## 4 Analysis of Target Context Usage for Hallucination Words

To verify the hypothesis that using more on target-side context leads to the emergence of hallucination, we propose to analyze the usage of target-side context.

**Measure on Target-side Context Usage.** To explicitly measure Target Context Usage, we adapt an interpretive approach that evaluates the relevance of both target and source words. It involves deactivating connections between the corresponding words and the network. We compute the relevance between the words in the source or target and the next word to be generated and determine the maximum absolute relevance as source or target relevance. It allows us to calculate the **T**arget-Side Relevance to **S**ource-Side Relevance **'s R**atio (TSSR).

To begin with, we assess the relevance of target-side words and source-side words to the next word to be generated. This evaluation is conducted by selectively deactivating the connection between \(\text{x}_{j}\) or \(\text{y}_{j}\) and the encoder or decoder network in a deterministic manner, following the approach described in Li et al. (2019). More formally, the relevance \(R(\text{y}_{i},\text{x}_{j})\) or \(R(\text{y}_{i},\text{y}_{j})\) in Wait-\(k\) is directly determined through the dropout effect on \(\text{x}_{j}\) or \(\text{y}_{j}\), as outlined below:","R(\text{y}_{i},\text{x}_{j}) =P(\text{y}_{i}\mid\text{y}_{<i},\text{x}_{\leq i+k-1}) || \quad-P(\text{y}_{i}\mid\text{y}_{<i},\text{x}_{\leq i+k-1, (j,\textbf{0})}). || R(\text{y}_{i},\text{y}_{j}) =P(\text{y}_{i}\mid\text{y}_{<i},\text{x}_{\leq i+k-1}) || \quad-P(\text{y}_{i}\mid\text{y}_{<i,(j,\textbf{0} )},\text{x}_{\leq i+k-1}).",,"R(y_i, x_j) = \left| \frac{\partial \log p(y_i \mid x_{1:n}, y_{1:i-1})}{\partial \text{dropout}(x_j)} \right|",Relevance of source word $x_j$ to target word $y_i$ measured via dropout sensitivity in Wait-$k$ models.
2024.acl-short.66,5,"The relevance of the source-side and target-side is determined by selecting the maximum absolute value of the word's relevance on the current source-side and the current target-side. Formally, this can be expressed as:","R(\text{y}_{i})_{source-side}=\max\{|R(\text{y}_{i},\text{x}_ {j})|\}. || R(\text{y}_{i})_{target-side}=\max\{|R(\text{y}_{i},\text{y} _{j})|\}.",,"\text{SourceRelevance}(i) = \max_{j} |R(\text{y}_{i}, \text{x}_{j})|, \quad \text{TargetRelevance}(i) = \max_{j < i} |R(\text{y}_{i}, \text{y}_{j})|.",SourceRelevance and TargetRelevance capture the maximum absolute relevance of source and target words to the current target word $\text{y}_{i}$.
2024.acl-short.66,6,"\begin{table}
\begin{tabular}{c|c c c c c} \hline \hline \(k\) & \(1\) & \(3\) & \(5\) & \(7\) & \(9\) \\ \hline Hallucination & 7.82 & 8.22 & 8.19 & 8.10 & 8.07 \\ Overall & 8.70 & 8.97 & 9.00 & 9.01 & 9.02 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Word frequency distribution entropy of Hallucination and Overall on the valid set of wait-\(k\).

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \hline  & \multicolumn{4}{c}{**Valid set**} & \multicolumn{4}{c}{**Training subset**} \\ \cline{2-9}
**Wait-\(k\)** & **Uncertainty** & **Confidence** & **Uncertainty** & **Confidence** \\ \cline{2-9}  & **H** & **NH** & **H** & **NH** & **H** & **NH** & **H** & **NH** \\ \hline \(k\)=1 & 3.53 & 2.35 & 0.40 & 0.61 & 3.47 & 2.13 & 0.41 & 0.65 \\ \(k\)=3 & 3.00 & 2.04 & 0.48 & 0.66 & 2.98 & 1.90 & 0.49 & 0.69 \\ \(k\)=5 & 2.81 & 1.97 & 0.52 & 0.67 & 2.76 & 1.90 & 0.52 & 0.69 \\ \(k\)=7 & 2.55 & 1.89 & 0.55 & 0.69 & 2.48 & 1.81 & 0.57 & 0.70 \\ \(k\)=9 & 2.48 & 1.92 & 0.57 & 0.68 & 2.42 & 1.96 & 0.58 & 0.69 \\ \hline \hline \end{tabular}
\end{table}
Table 3: The Uncertainty and Confidence of Hallucinationation (**H**) and Non-Hallucination (**NH**) on the valid set and training subset of wait-\(k\) models.

Finally, the ratio of target-side relevance to source-side relevance(TSSR) is calculated. A larger TSSR indicates a higher usage of target-side context in generating the next word \(y_{i}\).",TSSR(y_{i})=\frac{R(y_{i})_{target-side}}{R(y_{i} )_{source-side}}.,,\text{TSSR}(\text{y}_{i}) = \frac{R(\text{y}_{i})_{target-side}}{R(\text{y}_{i})_{source-side}}.,"The TSSR measures the ratio of target-side relevance to source-side relevance for a generated word $y_i$, indicating the extent to which target-side context is used during generation."
2024.acl-short.68,1,"### Retrieval Augmentation w/ Medical Codes

Existing approaches often treat each visit as context-free vectors, which fail to capture the concrete semantics of medical codes. Being aware of this, we aim to create the summarized knowledge for each medical code \(c_{i}\) using its surface name \(s_{i}\) via retrieval augmentation with additional contexts. **Multi-source Corpus Creation**. Retrieval augmentation requires additional corpora as external knowledge. To ensure the coverage of clinical knowledge, we collect a diverse external resources \(\mathcal{M}=\{d_{1},d_{2},\ldots,d_{|\mathcal{M}|}\}\). We represent each knowledge unit as a raw text to facilitate retrieval. The detailed information of \(\mathcal{M}\) is in Appendix C.

**Passage Retrieval**. Given a collection of \(|\mathcal{M}|\) passages, the objective of the retriever is to transform passages in a _dense_ vector, so that it can efficiently retrieve the most relevant information to the input query. In our work, we adopt Dragon [15], a dual-encoder model with strong performance across domains as the retriever. Specifically, we first use the passage encoder \(R_{D}(\cdot)\) to build an index for corpus \(\mathcal{M}\) to support retrieval. Then, at runtime, we use the query encoder \(R_{Q}(\cdot)\) to map the input to an embedding (same dimension as the passage embedding) and calculate the similarity as \(f(q,d)=R_{Q}(q)^{\top}R_{D}(d)\). For the medical code \(c_{i}\) with the surface name \(s_{i}\), we retrieve top-\(k\) (\(k=5\) in this work) passages \(\mathcal{T}_{i}\) from the corpus \(\mathcal{M}\) as","\mathcal{T}_{i}=\operatorname*{Top\_}{d\in\mathcal{M}}k\ f(s_{i},d).",,"\mathcal{T}_{i} = \text{Top-}k_{d \in \mathcal{M}} \left( \text{sim}\left( R_{Q}(s_{i}), R_{D}(d) \right) \right)",Retrieval of top-k passages from the corpus $\mathcal{M}$ for a medical code $c_i$ based on the surface name $s_i$ using dense vector similarity.
2024.acl-short.68,2,"The top retrieved passages are considered as the external knowledge for the medical code \(c_{i}\).

**Summarized Knowledge Generation**. Although \(\mathcal{T}_{i}\) contains the most relevant information for \(c_{i}\) from \(\mathcal{M}\), directly using them to assist predictions can be suboptimal, as simply concatenating these passages often leads to long contexts, and some of the retrieved passages can also be irrelevant [26]. Motivated by the fact that LLMs have strong capabilities in text summarization [15], we propose to use the off-the-shelf LLM (gpt-3.5-turbo-0613) to generate the summarized knowledge \(e_{i}\) for medical code \(c_{i}\) as","e_{i}=\operatorname{LLM}([\text{Prompt},t_{i,1},\cdots,t_{i,k}]),",where \(t_{i}\in\mathcal{T}_{i}\) stands for the retrieved passages in Eq,"e_{i} = \text{LLM}(\mathcal{T}_{i}),",where $e_{i}$ denotes the summarized knowledge generated by the LLM from the top retrieved passages $\mathcal{T}_{i}$ for medical code $c_{i}$.
2024.acl-short.68,3,"Eq.(1). We incorporate information related to the downstream task within our prompt to ensure the generated summaries are task-specific. Detailed prompt designs can be found in Appendix F.

**Remark**. The retrieval step is efficient as the corpus indexing only needs to be done _once_ before applying to prediction tasks. It only needs one extra ANN retrieval operation per query, which is efficiently supported by FAISS [11]. Besides, we cache the summarized knowledge for each medical code to avoid redundant operations.

### Augmenting Patient Visits with

**Summarized Knowledge via Co-training**

Recall that patient visits and summarized knowledge encode complementary information for clinical prediction tasks -- visits capture _cooccurence relationships_, while summarized knowledge encodes _semantic information_. To effectively aggregate these two types of information, we design a co-training approach, detailed as follows.

**Augmented Model \(g_{\phi}\) with Summarized Knowledge**. For patient \(p_{i}\) having the hospital visit \(v_{i}\) with involved medical codes \(\mathcal{C}_{i}\), we decompose \(\mathcal{C}_{i}\) into three subsets: \(\mathcal{C}_{i}^{\text{d}}\) for diseases, \(\mathcal{C}_{i}^{\text{m}}\) for medications, and \(\mathcal{C}_{i}^{\text{p}}\) for procedures. For each type of medical code, we flatten the visit into a document by concatenating all the codes and their summarized knowledge in a reversed sequential order. For example, for disease code \(\mathcal{C}_{i}^{\text{d}}\), the flattened document can be \(X_{i}^{\text{d}}=\{[\text{CLS}],D_{t},D_{t-1},\ldots,D_{1}\}\), where \(D_{i}=||_{c\in D_{i}}(c,e)\) is the concatenation of disease code and its summarized knowledge (Eq. 2) within the \(i\)-th visit. We then use a pre-trained language model (PLM) with a multi-layer perceptron (MLP) classification head as \(g_{\phi}\) for prediction with flattened documents as inputs:","\mathbf{h}_{i}^{k}=\mathrm{PLM}(X_{i}^{k}),\;\;\widehat{y}_{i,1}=\mathrm{MLP} (||_{k\in\mathcal{S}}\mathbf{h}_{i}^{k}).",,"X_{i}^{d} = \{[\text{CLS}], D_{t}, D_{t-1}, \ldots, D_{1}\}, \text{ where } D_{i} = \big\Vert_{c \in D_{i}} (c, e)","Flattened document representation for disease codes, incorporating summarized knowledge via concatenation within the i-th visit."
2024.acl-short.68,4,"Here \(\mathcal{S}=\{\mathrm{p},\mathrm{m},\mathrm{d}\}\), \(\mathbf{h}_{i}\) is the representation of [CLS] token of \(X_{i}\), \(\widehat{y}_{i,1}\) is the prediction for the target task. We share PLM weights for three types of medical codes to improve efficiency.

**Local Model \(f_{\theta}\) with Visit Information**. To harness the visit-level information, various deep learning architectures have been proposed. In principle, \(g_{\phi}\) can be combined with any \(f_{\theta}\) to improve performance. In main experiments, we use a hypergraph transformer (HyGT, Cai et al. (2022); Xu et al. (2023a)) due to its strong ability to capture high-order relationships between visits and medical codes. It first builds hypergraphs \(\mathcal{G}=(\mathcal{V},\mathcal{E})\) by treating medical codes as nodes and patients as hyperedges, then leverages self-attention for aggregating neighborhood information. The details for HyGT are in Appendix E. We obtain the prediction \(\widehat{y}_{i,2}\) with \(f_{\theta}\) as","\mathbf{e}_{i}=\mathrm{HyGT}(\mathcal{G},V_{i}),\widehat{y}_{i,2}=\mathrm{MLP}( \mathbf{e}_{i}),",where \(\mathbf{e}_{i}\) is the representation of patient \(i\) after hypergraph transformer,"\widehat{y}_{i,2}=f_{\theta}(v_{i}),","where $v_{i}$ represents the visit information for patient $p_{i}$, and $f_{\theta}$ is the local model used to generate predictions based on visit-level data."
2024.acl-short.68,5,"er.

**Co-training**. We integrate the two predictors into a co-training framework, with the learning objective:","\mathcal{L}_{\text{aug}}= \mathbb{E}_{(V_{i},y_{i})\sim\mathcal{P}}\;\ell(\widehat{y}_{i,1},y_{i})+\lambda\mathcal{D}_{\mathrm{KL}}(\widehat{y}_{i,1},\widetilde{y}), || \mathcal{L}_{\text{loc}}= \mathbb{E}_{(V_{i},y_{i})\sim\mathcal{P}}\;\ell(\widehat{y}_{i,2},y_{i})+\lambda\mathcal{D}_{\mathrm{KL}}(\widehat{y}_{i,2},\widetilde{y}),",,"\mathcal{L}=\mathcal{L}_{\text{task}}(\widehat{y}_{i,1},y_{i})+\mathcal{L}_{\text{task}}(\widehat{y}_{i,2},y_{i})+\lambda\mathcal{L}_{\text{consistency}}(\widehat{y}_{i,1},\widehat{y}_{i,2}),","where the total loss combines task-specific losses from both models and a consistency loss between their predictions, weighted by $\lambda$."
2024.acl-short.71,1,"## 2 Background and Related Work

Sampling-based MBR Eikema and Aziz (2020) selects a translation \(\mathit{hyp}^{*}\) out of a set of translation hypotheses \(\mathit{hyp}_{1},\ldots,\mathit{hyp}_{n}\in\mathit{hyp}\) by maximizing (expected) utility:","\mathit{hyp}^{*}=\operatorname*{arg\,max}_{\mathit{hyp}\in\mathit{hyp}s} \operatorname{utility}(\mathit{hyp}).",,"\mathit{hyp}^{*} = \argmax_{\mathit{hyp} \in \mathit{hyp}} \mathbb{E}_{R|S}[U(\mathit{hyp}, R)]",Selects the optimal translation hypothesis by maximizing expected utility under the reference distribution given the source.
2024.acl-short.71,2,"The set of hypotheses is sampled from the model distribution \(p(\mathit{hyp}|\mathit{src})\). Eikema and Aziz (2020) propose to approximate the utility using MC sampling: sample a set of pseudo-references \(\mathit{refs}=\{\mathit{ref}_{1},\ldots,\mathit{ref}_{m}\}\sim p(\mathit{refs} \mathit{src})\) from the model andcalculate a metric against each sampled reference:","\text{utility}(hyp)\approx\frac{1}{m}\sum_{\textit{ref}\in\textit{refs}}\text{ metric}(hyp,\textit{ref}).",,"\hat{U}(\mathit{hyp}) = \frac{1}{m} \sum_{i=1}^{m} \operatorname{metric}(\mathit{hyp}, \mathit{ref}_i)",Monte Carlo approximation of expected utility using $m$ pseudo-references sampled from the model distribution
2024.acl-short.71,3,"For machine translation, typical such metrics are ChrF (Popovic, 2015) and BLEU (Papineni et al., 2002), which are based on n-gram statistics, or neural metrics such as COMET (Rei et al., 2020) and BLEURT (Sellam et al., 2020).

A line of research has focused on improving the efficiency of sampling-based MBR. Eikema and Aziz (2022) propose _coarse-to-fine MBR_, which prunes the hypotheses based on a cheaper metric, and _N-by-S MBR_, which uses fewer references than hypotheses. Cheng and Vlachos (2023) propose _confidence-based pruning_, where the number of hypotheses is iteratively reduced based on an increasing number of references. Jinnai and Ariu (2024) interpret sampling-based MBR as an instance of _medoid identification_ and apply an established approximation algorithm to this problem. A line of work uses MBR outputs as a training reward, avoiding the inefficiency of MBR during deployment (Finkelstein et al., 2023; Yang et al., 2023). Finally, alternative reranking approaches that do not require pairwise comparisons have been proposed (Fernandes et al., 2022).

Several other works investigate the aggregation of reference representations to develop a faster variant of MBR decoding. DeNero et al. (2009) perform reference aggregation in the context of statistical machine translation (SMT). Since SMT does not afford random sampling of pseudo-references, they aggregate references from translation forests or \(k\)-best lists. Our study shows the effectiveness of reference aggregation from sampled pseudo-references, and for neural metrics such as COMET. Furthermore, concurrent to our work, Deguchi et al. (2024) propose to aggregate the sentence embeddings of COMET, and use \(k\)-means to group the references into multiple clusters.

## 3 Reference Aggregation

Our approach is based on the observation that most metrics that are commonly used for MBR make use of feature representations that can be aggregated. For example, the n-gram statistics used by ChrF can be aggregated by averaging the counts of the n-grams across all references; and the sentence embeddings used by COMET can be aggregated by calculating an average sentence embedding.

For simplicity, we re-use the above notation, where _hyp_ is a hypothesis and _ref_ is a reference, but we now assume that they are represented in an averageable form. We then combine the set of references _refs_ into an aggregate representation \(\overline{\textit{ref}}\):",\overline{\textit{ref}}=\frac{1}{m}\sum_{\textit{ref}\in\textit{refs}}\textit{ ref}.,,\overline{\textit{ref}} = \frac{1}{m} \sum_{\textit{ref} \in \textit{refs}} \textit{ref}.,The aggregate reference representation $\overline{\textit{ref}}$ is computed by averaging the individual reference representations across all $m$ sampled references.
2024.acl-short.71,4,We approximate the expected the utility of a sampled hypothesis by calculating a single metric score against this aggregate representation:,"\text{utility}(hyp)\approx\text{metric}(hyp,\overline{\textit{ref}}).",,"\text{utility}(hyp)\approx\text{metric}(hyp,\overline{\textit{ref}}).",This equation approximates the utility of a hypothesis by computing a single metric score against the aggregated reference representation $\overline{\textit{ref}}$.
2024.acl-short.71,5,"Like with standard sampling-based MBR, it is possible to interpret this approximation as MC sampling: By averaging over representations of sampled references, we estimate a representation of the ""true"" reference, which we then use for approximating the expected utility of each sampled hypothesis. Importantly, the computational complexity of our approach is in \(O(|\textit{hyps}|+|\textit{refs}|)\) rather than \(O(|\textit{hyps}|\cdot|\textit{refs}|)\); see Appendix D for a discussion.

### Application to chrF Metric

ChrF (Popovic, 2015) is defined as an F-score over character n-grams:","\text{ChrF}_{\beta}=\frac{(1+\beta^{2})\cdot\text{ChrP}\cdot\text{ChrR}}{ \beta^{2}\cdot\text{ChrP}+\text{ChrR}}, || \text{ChrP}=\frac{|hyp\cap\textit{ref}|}{|hyp|}\text{ and }\text{ChrR}=\frac{|hyp \cap\textit{ref}|}{|\textit{ref}|},","where

\[\text{ChrP}=\frac{|hyp\cap\textit{ref}|}{|hyp|}\text{ and }\text{ChrR}=\frac{|hyp \cap\textit{ref}|}{|\textit{ref}|},\]

and the parameter \(\beta\) controls the relative importance of precision and recall","\text{ChrF}(hyp, ref) = \frac{(1 + \beta^2) \cdot \text{matching\_ngrams}(hyp, ref)}{\beta^2 \cdot \text{hyp\_ngrams}(hyp) + \text{ref\_ngrams}(ref)}.","ChrF metric computes an F-score based on character n-grams between a hypothesis and a reference, with $\beta$ balancing precision and recall."
2024.acl-short.71,6,"and the parameter \(\beta\) controls the relative importance of precision and recall. The representations _hyp_ and _ref_ are bags of n-grams, i.e., objects that map each n-gram to its count in the string.

We apply reference aggregation to ChrF by averaging the counts of n-grams across all references:","\overline{\textit{ref}}=\frac{1}{m}\underbrace{\biguplus}_{\textit{ref}\in \textit{refs}}\textit{ref},",where \(\biguplus\) is an operation that sums up the counts of each n-gram,\overline{\textit{ref}}_{\text{n-gram}}=\frac{1}{m}\sum_{\textit{ref}\in\textit{refs}}\textit{ref}_{\text{n-gram}}.,The aggregate reference representation for ChrF is obtained by averaging the n-gram counts across all reference samples.
2024.acl-short.71,7,"am. We then approximate the expected utility of a hypothesis by calculating ChrF\({}_{\beta}(hyp,\overline{\textit{ref}})\). Appendix A provides a more formal definition of reference aggregation for ChrF.

### Application to COMET Metric

COMET (Rei et al., 2020) is a pre-trained Transformer model (Vaswani et al., 2017) that has been fine-tuned to predict human judgments of translation quality. In this paper, we focus on the Estimator model architecture, which directly estimates a quality score given a hypothesis, a reference and the source sequence. COMET separately encodes these three inputs into fixed-size embeddings:","\textbf{{hyp}},\ \textbf{{ref}},\ \textbf{{src}}=\text{emb}(\textbf{{hyp}}),\ \text{emb}(\textbf{{ref}}),\ \text{emb}(\textbf{{src}}).",,"\text{utility}(hyp)\approx\text{ChrF}_{\beta}(hyp,\overline{\textit{ref}}).",This equation approximates the expected utility of a hypothesis by calculating the ChrF score between the hypothesis and the aggregated reference representation.
2024.acl-short.71,8,"The three embeddings are then fed into a feed-forward module, which outputs a scalar score:","\text{comet}(\textbf{{hyp}})=\text{score}(\textbf{{hyp}},\ \textbf{{ref}},\ \textbf{{src}}).",,\text{score} = \text{FFN}([\textbf{hyp}; \textbf{ref}; \textbf{src}]).,"The scalar score is computed by a feed-forward network (FFN) that takes the concatenated embeddings of the hypothesis, reference, and source as input."
2024.acl-short.71,9,We apply reference aggregation to COMET by averaging the reference embeddings:,"\overline{\textbf{{ref}}}=\frac{1}{m}\sum_{\textbf{{ref}}\in\textbf{{ref}}} \text{emb}(\textbf{{ref}}),",,\overline{\textbf{{ref}}}=\frac{1}{m}\sum_{\textit{ref}\in\textit{refs}}\textbf{{ref}}.,Average reference embeddings across all references to form a single aggregate reference representation for COMET metric.
2024.acl-short.71,10,calculating a single score per hypothesis:,"\text{comet}(\textbf{{hyp}})\approx\text{score}(\textbf{{hyp}},\ \overline{\textbf{{ref}}},\ \textbf{{src}}).",,"\text{comet}(\textbf{{hyp}})=\text{score}(\textbf{{hyp}},\ \overline{\textbf{{ref}}},\ \textbf{{src}}).",Reference aggregation is applied to COMET by using the average reference embedding to compute a single score per hypothesis.
2024.acl-short.72,1,"### Web Scraping with the Neural Method

In this subsection, we introduce our neural modeling method to build the web scraper. To process the textual sequences \(X=\{x_{1},x_{2},...,x_{n}\}\), we build a hierarchical architecture for node-level prediction.

Specifically, to guarantee the efficiency of NeuScraper, we use the first layer of the XLM-Roberta (Conneau et al., 2020) model to encode the text representation \(x_{i}\) of the \(i\)-th DOM node as the 768-dimensional node representation \(h_{i}\):","h_{i}=\text{XLMRoberta-Layer}^{1}(x_{i}),","where \(h_{i}\) is the representation of the ""[CLS]"" token","\begin{equation}
h_{i} = \text{XLM-Roberta}(x_{i})
\end{equation}",Equation 1 defines the encoding of the $i$-th DOM node's text representation $x_i$ into a 768-dimensional node representation $h_i$ using the first layer of the XLM-Roberta model.
2024.acl-short.72,2,"en. Then we feed these node representations \(H=\{h_{1},h_{2},...,h_{n}\}\) into a 3-layer transformer model (Vaswani et al., 2017) with 8 attention heads to get the encoded node representations:","\hat{h}_{i}=\text{Transformer}(\text{Linear}(h_{i})),",where the linear layer projects \(h_{i}\) to 256-dimensional embeddings for efficient modeling,"H'=\text{Transformer}(H),",where $H'$ denotes the encoded node representations obtained through a 3-layer transformer model with 8 attention heads.
2024.acl-short.72,3,"ng. Following previous work (Overwijk et al., 2022), the DOM nodes can be categorized into six kinds of labels \(y^{k}\), including primary content, heading, title, paragraph, table, and list. Then we calculate the label prediction probability \(P(y^{k}_{i}=1|x_{i})\) of the \(k\)-th category label \(y^{k}_{i}\) of the \(i\)-th node:",P(y^{k}_{i}=1|x_{i})=\text{Sigmoid}(\text{MLP}(\hat{h}_{i})),,"P(y^{k}_{i}=1|x_{i}) = \text{Softmax}(\hat{h}_{i} W_{k} + b_{k}),",where the probability is obtained through a softmax function over the transformed node representations
2024.acl-short.72,4,"Finally, NeuScraper is trained using the loss \(L\):","L=\sum_{k=1}^{6}\sum_{i=1}^{n}\text{CrossEntropy}(P(y^{k}_{i}|x_{i}),\mathcal{ Y}^{k}_{i}),",where \(\mathcal{Y}^{k}_{i}\) is the ground truth label,L = -\frac{1}{N} \sum_{i=1}^{N} \sum_{k=1}^{K} y_{i}^{k} \log(P(y_{i}^{k}=1|x_{i})) + (1 - y_{i}^{k}) \log(1 - P(y_{i}^{k}=1|x_{i})),where the loss function is a binary cross-entropy over all nodes and label categories
ICLR_2024_oral_1,1,"## 3 Preliminaries

We formulate our decision-making setting as Markov Decision Processes (MDPs). An MDP is defined as a tuple \(M^{\star}=(\mathcal{S},\mathcal{A},\{P_{h}^{\star}\}_{h=1}^{H},P_{0},\{r_{h} \}_{h=1}^{H})\), where \(\mathcal{S}\) is the set of states, \(\mathcal{A}\) the set of actions, \(P_{h}:\mathcal{S}\times\mathcal{A}\rightarrow\triangle_{\mathcal{S}}\) the transition kernel, \(P_{0}\in\triangle_{\mathcal{S}}\) the initial state distribution, and \(r_{h}:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}\) the reward function. We consider the episodic setting. At the beginning of an episode, the environment samples a state \(s_{1}\sim P_{0}\). The agent observes this state, plays some action \(a_{1}\in\mathcal{A}\), and transitions to state \(s_{2}\sim P_{1}(\cdot\mid s_{1},a_{1})\), receiving reward \(r_{1}(s_{1},a_{1})\). After \(H\) steps, the environment resets and the process repeats. Our primary goal is to learn a _policy_\(\pi\)--a mapping from states to actions--that maximizes reward in the true environment. We denote the value of a policy by \(V_{0}^{\pi}:=\mathbb{E}_{M^{\star},\pi}[\sum_{h=1}^{H}r_{h}(s_{h},a_{h})]\), where the expectation is over trajectories induced playing policy \(\pi\) on MDP \(M^{\star}\). We think of the reward \(r\) as encoding our _downstream task_, and our end goal is to find a policy that solves our task, maximizing \(V_{0}^{\pi}\). We denote such policies as \(\pi_{\text{task}}\).

In the sim2real setting considered in this work, we assume that the reward is known, but that the dynamics of the real environment, \(P^{\star}=\{P_{h}^{\star}\}_{h=1}^{H}\), are initially unknown. However, we assume that they belong to some known parametric family \(\mathcal{P}:=\{P_{\mathbf{\theta}}~{}:~{}\mathbf{\theta}\in\Theta\}\), so that there exists some \(\mathbf{\theta}^{\star}\in\Theta\) such that \(P^{\star}=P_{\mathbf{\theta}}\). Here we take \(\mathbf{\theta}\) to be some unknown parameter (for example, mass, friction, etc.), and \(P_{\mathbf{\theta}}\) the dynamics under parameter \(\theta\) (which we might know from physics, first principles, etc.). For any \(\mathbf{\theta}\) and policy \(\pi\), the dynamics \(P_{\mathbf{\theta}}\) induce a distribution over state-action trajectories, \(\mathbf{\tau}=(s_{1},a_{1},s_{2},\ldots,s_{H},a_{H})\), which we denote by \(p_{\mathbf{\theta}}(\cdot\mid\pi)\). We can think of our simulator as instantiating \(p_{\mathbf{\theta}}(\cdot\mid\pi)\)--we assume our simulator is able to accurately mimic the dynamics of an MDP with parameter \(\mathbf{\theta}\) under policy \(\pi\), generating samples \(\mathbf{\Upsilon}\sim p_{\mathbf{\theta}}(\cdot\mid\pi)\).

In addition, we also assume that samples from our simulator are effectively ""free""--for any \(\mathbf{\theta}\) and policy \(\pi\), we can generate as many trajectories \(\mathbf{\Upsilon}\sim p_{\mathbf{\theta}}(\cdot\mid\pi)\) as we wish. Given this, it is possible to find the optimal policy under \(\mathbf{\theta}\) by simply running any standard RL algorithm in simulation. With knowledge of the true parameters \(\mathbf{\theta}^{\star}\), we can then easily find the optimal policy in real by sampling trajectories from the simulated environment with parameter \(\mathbf{\theta}^{\star}\). It follows that, if we can identify the true parameter \(\mathbf{\theta}^{\star}\) in real, we can solve the goal task.

We consider the following learning protocol:

1. Learner chooses exploration policy \(\pi_{\mathrm{exp}}\) and plays it in real for a _single_ episode, generating trajectory \(\mathbf{\Upsilon}_{\mathrm{real}}\sim p_{\mathbf{\theta}^{\star}}(\cdot\mid\pi_{ \mathrm{exp}})\).
2. Using \(\mathbf{\Upsilon}_{\mathrm{real}}\) and the simulator in any way they wish, the learner obtains some policy \(\pi_{\mathrm{task}}\).
3. Learner deploys \(\pi_{\mathrm{task}}\) in real and suffers loss \(\max_{\pi}V_{0}^{\pi}-V_{0}^{\pi_{\mathrm{task}}}\).

The goal of the learner is then to learn as much useful information as possible about the real environment from a single episode of interaction and use this information to obtain a policy that can solve the task in real as effectively as possible.

Parameter Estimation and Fisher Information:The _Fisher information matrix_ plays a key role in the choice of our exploration policy, \(\pi_{\mathrm{exp}}\). Recall that, for a distribution \(p_{\mathbf{\theta}}\), satisfying certain regularity conditions, the Fisher information matrix is defined as:",\mathcal{I}(\mathbf{\theta}):=\mathbb{E}_{\mathbf{\Upsilon}\sim p_{\mathbf{\theta}}} [\nabla_{\mathbf{\theta}}\log p_{\mathbf{\theta}}(\mathbf{\Upsilon})\cdot\nabla_{\bm {\theta}}\log p_{\mathbf{\theta}}(\mathbf{\Upsilon})^{\top}].,,"\begin{equation}
\mathcal{I}_{\mathbf{\theta}} := \mathbb{E}_{p_{\mathbf{\theta}}}\left[\nabla_{\mathbf{\theta}}\log p_{\mathbf{\theta}}(\mathbf{\Upsilon}) \cdot \nabla_{\mathbf{\theta}}\log p_{\mathbf{\theta}}(\mathbf{\Upsilon})^{\top}\right].
\end{equation}",The Fisher information matrix quantifies the amount of information that an observed trajectory carries about the unknown parameter $\mathbf{\theta}$.
ICLR_2024_oral_1,2,"Assume that we have access to data \(\mathfrak{D}=(\mathbf{\Upsilon}_{t})_{t=1}^{T}\), where \(\mathbf{\Upsilon}_{t}\sim p_{\mathbf{\theta}^{\star}}\) for \(t=1,\dots,T\), and let \(\widehat{\mathbf{\theta}}(\mathfrak{D})\) denote some unbiased estimator of \(\mathbf{\theta}^{\star}\). Then the Cramer-Rao lower bound (see e.g. Pronzato and Pazman (2013)) states that, under certain regularity conditions, the covariance of \(\widehat{\mathbf{\theta}}(\mathfrak{D})\) satisfies:",\mathbb{E}_{\mathfrak{D}\sim p_{\mathbf{\theta}^{\star}}}([\widehat{\mathbf{\theta}} (\mathfrak{D})-\mathbf{\theta}^{\star})(\widehat{\mathbf{\theta}}(\mathfrak{D})-\mathbf{ \theta}^{\star})^{\top}]\succeq T^{-1}\cdot\mathcal{I}(\mathbf{\theta}^{\star})^ {-1}.,,"\mathrm{Cov}(\widehat{\mathbf{\theta}}(\mathfrak{D})) \succeq \mathcal{I}(\mathbf{\theta}^{\star})^{-1},",The covariance of any unbiased estimator of the true parameter θ⋆ is lower bounded by the inverse of the Fisher information matrix.
ICLR_2024_oral_1,3,From this it follows that the Fisher information serves as a lower bound on the mean-squared error:,\mathbb{E}_{\mathfrak{D}\sim p_{\mathbf{\theta}^{\star}}}[\|\widehat{\mathbf{\theta}} (\mathfrak{D})-\mathbf{\theta}^{\star}\|_{2}^{2}]=\mathrm{tr}(\mathbb{E}_{\mathfrak{ D}\sim p_{\mathbf{\theta}^{\star}}}[(\widehat{\mathbf{\theta}}(\mathfrak{D})-\mathbf{ \theta}^{\star})(\widehat{\mathbf{\theta}}(\mathfrak{D})-\mathbf{\theta}^{\star})^{ \top}])\geq T^{-1}\cdot\mathrm{tr}(\mathcal{I}(\mathbf{\theta}^{\star})^{-1}).,,"\mathbb{E}_{\mathfrak{D}\sim p_{\mathbf{\theta}^{\star}},\widehat{\mathbf{\theta}}}\left[\left\|\widehat{\mathbf{\theta}}(\mathfrak{D}) - \mathbf{\theta}^{\star}\right\|^{2}\right] \geq T^{-1} \cdot \operatorname{tr}\left(\mathcal{I}(\mathbf{\theta}^{\star})^{-1}\right).",This equation expresses the lower bound on the expected squared error of an unbiased estimator in terms of the trace of the inverse Fisher information matrix scaled by the number of samples.
ICLR_2024_oral_1,4,"This is in general tight--for example, the maximum likelihood estimator satisfies (1) with equality as \(T\to\infty\)(Van der Vaart, 2000). The Fisher information thus serves as a fundamental lower bound on parameter estimation error, a key motivation for our exploration procedure.

Asid: Targeted Exploration for Test-Time Simulation Construction, Identification, and Policy Optimization

In this section, we present our proposed approach, Asid, a three-stage pipeline illustrated in Figure 2. We describe each component of Asid in the following.

Figure 2: **Overview of Asid: (1) Train an exploration policy \(\pi_{\mathrm{exp}}\) that maximizes the Fisher information, leveraging the vast amount of cheap simulation data. (2) Roll out \(\pi_{\mathrm{exp}}\) in real to collect informative data that can be used to (3) run system identification to identify physics parameters and reconstruct, _e.g._, geometric, collision, and kinematic properties. (4) Train a task-specific policy \(\pi_{\mathrm{task}}\) in the updated simulator and (5) zero-shot transfer \(\pi_{\mathrm{task}}\) to the real world.**

### Exploration via Fisher Information Maximization

As motivated in Section 3, to learn a policy effectively accomplishing our task, it suffices to accurately identify \(\mathbf{\theta}^{\star}\). In the exploration phase, step 1 in our learning protocol, our goal is to then play an exploration policy \(\pi_{\mathrm{exp}}\) which generates a trajectory on the real environment that provides as much information on \(\mathbf{\theta}^{\star}\) as possible. Following Section 3, the Fisher information gives a quantification of the usefulness of the data collected, which motivates our approach.

In our setting, the distribution over trajectories generated during exploration in real, \(\mathbf{\tau}_{\mathrm{real}}\sim p_{\mathbf{\theta}^{\star}}(\cdot\mid\pi_{\mathrm{ exp}})\), depends on the exploration policy, \(\pi_{\mathrm{exp}}\), being played. As the Fisher information depends on the data distribution, it too scales with the choice of exploration policy:","\mathcal{I}(\mathbf{\theta}^{\star},\pi_{\mathrm{exp}}):=\mathbb{E}_{\mathbf{\tau} \sim p_{\mathbf{\theta}^{\star}}(\cdot\mid\pi_{\mathrm{exp}})}[\nabla_{\mathbf{ \theta}}\log p_{\mathbf{\theta}^{\star}}(\mathbf{\tau}\mid\pi_{\mathrm{exp}})\cdot \nabla_{\mathbf{\theta}}\log p_{\mathbf{\theta}^{\star}}(\mathbf{\tau}\mid\pi_{\mathrm{ exp}})^{\top}].",,\mathcal{I}(\mathbf{\theta}^{\star};\pi_{\mathrm{exp}}):=\mathbb{E}_{\mathbf{ \Upsilon}\sim p_{\mathbf{\theta}^{\star}}(\cdot\mid\pi_{\mathrm{exp}})}[ \nabla_{\mathbf{\theta}}\log p_{\mathbf{\theta}}(\mathbf{\Upsilon})\cdot\nabla_{ \mathbf{\theta}}\log p_{\mathbf{\theta}}(\mathbf{\Upsilon})^{\top}]\Big|_{ \mathbf{\theta}=\mathbf{\theta}^{\star}}.,"The Fisher information matrix conditioned on the exploration policy $\pi_{\mathrm{exp}}$, quantifying how much information a trajectory from this policy provides about the true parameter $\mathbf{\theta}^{\star}$."
ICLR_2024_oral_1,5,"Following (1), if we collect trajectories by playing \(\pi_{\mathrm{exp}}\) and set \(\mathbf{\widehat{\theta}}\) to any unbiased estimator of \(\mathbf{\theta}^{\star}\) on these trajectories, the mean-squared error of \(\mathbf{\widehat{\theta}}\) will be lower bounded by \(\mathrm{tr}(\mathcal{I}(\mathbf{\theta}^{\star},\pi_{\mathrm{exp}})^{-1})\). The optimal exploration policy--the exploration policy which allows for the smallest estimation error--is, therefore, the policy which solves1

Footnote 1: In the experiment design literature, this is known as an _A-optimal experiment design_(Pukelsheim, 2006).","\arg\min_{\pi}\mathrm{tr}(\mathcal{I}(\mathbf{\theta}^{\star},\pi)^{-1}).",,"\pi_{\mathrm{exp}}^{\star}=\arg\min_{\pi_{\mathrm{exp}}}\mathrm{tr}(\mathcal{I}(\mathbf{\theta}^{\star},\pi_{\mathrm{exp}})^{-1}).","This equation defines the optimal exploration policy as the one that minimizes the trace of the inverse of the Fisher information matrix, corresponding to the A-optimal experiment design criterion."
ICLR_2024_oral_1,6,"As an intuitive justification for this choice of exploration policy, note that the Fisher information is defined in terms of the gradient of the log-likelihood with respect to the unknown parameter. Thus, if playing some \(\pi_{\mathrm{exp}}\) makes the Fisher information ""large"", making \(\mathrm{tr}(\mathcal{I}(\mathbf{\theta}^{\star},\pi_{\mathrm{exp}})^{-1})\) small, this suggests \(\pi_{\mathrm{exp}}\) induces trajectories that are very sensitive to the unknown parameters, _i.e._, trajectory that are significantly more likely under one set of parameters than another. By exploring to maximize the Fisher information, we, therefore, will collect trajectories that are maximally informative about the unknown parameters, since we will observe trajectories much more likely under one set of parameters than another. Motivated by this, we therefore seek to play a policy during exploration that solves (2).

Implementing Fisher Information Maximization:In practice, several issues arise in solving (2), which we address here. First, the form of \(\mathcal{I}(\mathbf{\theta},\pi)\) can be quite complicated, depending on the structure of \(p_{\mathbf{\theta}}(\cdot\mid\pi)\), and it may not be possible to efficiently obtain a solution to (2). To address this, we make a simplifying assumption on the dynamics, that our next state, \(s_{h+1}\), evolves as:","s_{h+1}=f_{\mathbf{\theta}}(s_{h},a_{h})+w_{h},","where \(s_{h}\) and \(a_{h}\) are the current state and action, \(w_{h}\sim\mathcal{N}(0,\sigma_{w}^{2}\cdot I)\) is Gaussian process noise, and \(f_{\mathbf{\theta}}\) are the nominal dynamics","\nabla_{\mathbf{\theta}}\log p_{\mathbf{\theta}}(\mathbf{\tau}\mid\pi)=\sum_{h=1}^{H}\nabla_{\mathbf{\theta}}\log p_{\mathbf{\theta}}(s_{h+1}\mid s_{h},a_{h}),","This equation decomposes the gradient of the log-likelihood of the trajectory with respect to the parameters into a sum of gradients over transitions, under the assumption of Markovian dynamics."
ICLR_2024_oral_1,7,"cs. Under these dynamics, the Fisher information matrix reduces to","\mathcal{I}(\mathbf{\theta},\pi)=\sigma_{w}^{-2}\cdot\mathbb{E}_{p_{\mathbf{\theta}}( \cdot\mid\pi)}[\sum_{h=1}^{H}\nabla_{\mathbf{\theta}}f_{\mathbf{\theta}}(s_{h},a_ {h})\cdot\nabla_{\mathbf{\theta}}f_{\mathbf{\theta}}(s_{h},a_{h})^{\top}].",,"\mathcal{I}(\mathbf{\theta},\pi)=\frac{1}{\sigma_{w}^{2}}\mathbb{E}_{\mathbf{ \tau}\sim p_{\mathbf{\theta}}(\cdot\mid\pi)}\left[\sum_{h=1}^{H}\nabla_{\mathbf{ \theta}}f_{\mathbf{\theta}}(s_{h},a_{h})^{\top}\nabla_{\mathbf{\theta}}f_{\mathbf{ \theta}}(s_{h},a_{h})\right].","The Fisher information matrix under Gaussian process noise dynamics, expressed in terms of the expectation of the sum of outer products of the gradients of the nominal dynamics with respect to the parameters."
ICLR_2024_oral_1,8,"We argue that solving (2) with this form of \(\mathcal{I}(\mathbf{\theta},\pi)\) is a very intuitive objective, even in cases when the dynamics may not follow (3) exactly. Indeed, this suggests that during exploration, we should aim to reach states for which the dynamics \(f_{\mathbf{\theta}}\) have a large gradient with respect to \(\mathbf{\theta}\)--states for which the next state predicted by the dynamics is very sensitive to \(\mathbf{\theta}\). In such states, observing the next state gives us a significant amount of information on \(\mathbf{\theta}\), allowing us to accurately identify \(\mathbf{\theta}^{\star}\).

A second challenge in solving (2) is that we do not know the true parameter \(\mathbf{\theta}^{\star}\), which the optimization (2) depends on. To circumvent this, we rely on domain randomization in choosing our exploration policy, solving instead:","\pi_{\mathrm{exp}}=\arg\min_{\pi}\mathbb{E}_{\mathbf{\theta}\sim q_{0}}[\mathrm{tr} (\mathcal{I}(\mathbf{\theta},\pi)^{-1})]",,"\arg\min_{\pi}\mathbb{E}_{\mathbf{\theta}\sim\mathcal{U}(\Theta)}\left[\mathrm{tr}(\mathcal{I}(\mathbf{\theta},\pi)^{-1}\right)].","This equation represents the domain randomization approach to exploration policy optimization, where the expectation over $\mathbf{\theta}$ is taken with respect to a uniform distribution over the parameter space $\Theta$."
ICLR_2024_oral_1,9,"for some distribution over parameters \(q_{0}\). While this is only an approximation of (2), in practice we find that this approximation yields effective exploration policies since, as described in Section 1, in many cases exploration policies require only a coarse model of the dynamics, and can therefore often be learned without precise knowledge of the unknown parameters.

A final challenge is that, in general, we may not have access to a differentiable simulator, and our dynamics themselves may not be differentiable. In such cases, \(\nabla_{\mathbf{\theta}}f_{\mathbf{\theta}}(s_{h},a_{h})\) is unknown orundefined, and the above approach cannot be applied. As a simple solution to this, we rely on a finite-differences approximation to the gradient, which still provides an effective measure of how sensitive the next state is to the unknown parameter. In practice, to solve (4) and obtain an exploration policy, we rely on standard policy optimization algorithms, such as PPO (Schulman et al., 2017).

### System Identification

Asid runs the exploration policy \(\pi_{\mathrm{exp}}\) (Section 4.1) in the real environment to generate a single trajectory \(\mathbf{\uptau}_{\mathrm{real}}\sim p_{\mathbf{\uptheta}}\left(\cdot\mid\pi_{\mathrm{ exp}}\right)\). In the system identification phase, Asid then updates the simulator parameters using the collected trajectory. The goal is to find a distribution over simulator parameters that yield trajectories that match \(\mathbf{\uptau}_{\mathrm{real}}\) as closely as possible. In particular, we wish to find some distribution over simulation parameters, \(q_{\mathbf{\phi}}\), which minimizes:",\mathbb{E}_{\mathbf{\theta}\sim q_{\mathbf{\theta}}}[\mathbb{E}_{\mathbf{\tau}_{\mathrm{ sim}}\sim p_{\mathbf{\theta}}(\cdot\mid\mathcal{A}(\mathbf{\uptau}_{\mathrm{real}}) )}[\|\mathbf{\uptau}_{\mathrm{real}}-\mathbf{\uptau}_{\mathrm{sim}}\|_{2}^{2}]],"where \(p_{\mathbf{\theta}}(\cdot\mid\mathcal{A}(\mathbf{\uptau}_{\mathrm{real}}))\) denotes the distribution over trajectories generated by the simulator with parameter \(\mathbf{\theta}\), and playing the same sequence of actions as were played in \(\mathbf{\uptau}_{\mathrm{real}}\)","\min_{q_{\mathbf{\phi}}}\mathrm{KL}\left(q_{\mathbf{\phi}}(\mathbf{\theta})\|p(\mathbf{\theta})\right)-\mathbb{E}_{\mathbf{\theta}\sim q_{\mathbf{\phi}}}\left[\log p_{\mathbf{\theta}}(\mathbf{\uptau}_{\mathrm{real}}\mid\pi_{\mathrm{exp}})\right],",
ICLR_2024_oral_10,1,"### Representing Cameras with Rays

**Distributed Ray Representation.** Typically, a camera is parameterized by its extrinsics (rotation \(\mathbf{R}\in\mathrm{SO}(3)\), translation \(\mathbf{t}\in\mathbb{R}^{3}\)) and intrinsics matrix \(\mathbf{K}\in\mathbb{R}^{3\times 3}\). Although this parameterization compactly relates the relationship of world coordinates to pixel coordinates using camera projection (\(\mathbf{u}=\mathbf{K}[\mathbf{R}\mid\mathbf{T}]\mathbf{x}\)), we hypothesize that it may be difficult for a neural network to directly regress this low-dimensional representation. Instead, inspired by generalized camera models (Grossberg and Nayar, 2001; Schops et al., 2020) used for calibration, we propose to _over-parameterize_ a camera as a collection of rays:","\mathcal{R}=\{\mathbf{r}_{1},\dots,\mathbf{r}_{m}\},",where each ray \(\mathbf{r}_{i}\in\mathbb{R}^{6}\) is associated with a known pixel coordinate \(\mathbf{u}_{i}\),"\begin{equation}
    \mathcal{C} = \left\{ \mathbf{r}_i = \left( \mathbf{o}_i, \mathbf{d}_i \right) \mid i = 1, \dots, N \right\}
\end{equation}","A camera is represented as a collection of rays, where each ray $\mathbf{r}_i$ is defined by its origin $\mathbf{o}_i$ and direction $\mathbf{d}_i$, forming an over-parameterized representation $\mathcal{C}$ consisting of $N$ rays."
ICLR_2024_oral_10,2,"\). We parameterize each ray \(\mathbf{r}\) traveling in direction \(\mathbf{d}\in\mathbb{R}^{3}\) through any point \(\mathbf{p}\in\mathbb{R}^{3}\) using Plucker coordinates (Plucker, 1828):","\mathbf{r}=\langle\mathbf{d},\mathbf{m}\rangle\in\mathbb{R}^{6},","where \(\mathbf{m}=\mathbf{p}\times\mathbf{d}\in\mathbb{R}^{3}\) is the moment vector, and importantly, is agnostic to the specific point on the ray used to compute it","\mathbf{r} = \begin{bmatrix} \mathbf{d} \\ \mathbf{p} \times \mathbf{d} \end{bmatrix},",where each ray $\mathbf{r}\in\mathbb{R}^{6}$ is represented by its direction $\mathbf{d}\in\mathbb{R}^{3}$ and the cross product $\mathbf{p} \times \mathbf{d}$ of a point $\mathbf{p}\in\mathbb{R
ICLR_2024_oral_10,3,"it. When \(\mathbf{d}\) is of unit length, the norm of the moment \(\mathbf{m}\) represents the distance from the ray to the origin.

**Converting from Camera to Ray Bundle.** Given a known camera and a set of 2D pixel coordinates \(\{\mathbf{u}_{i}\}_{m}\), the directions \(\mathbf{d}\) can be computed by unprojecting rays from the pixel coordinates, and the moments \(\mathbf{m}\) can be computed by treating the camera center as the point \(\mathbf{p}\) since all rays intersect at the camera center:","\mathbf{d}=\mathbf{R}^{\top}\mathbf{K}^{-1}\mathbf{u},\qquad\mathbf{m}=(-\mathbf{R}^{\top}\mathbf{t}) \times\mathbf{d}.",,"\mathbf{d}_{i} = \frac{\mathbf{K}^{-1}\mathbf{u}_{i}}{\|\mathbf{K}^{-1}\mathbf{u}_{i}\|}, \quad \mathbf{m}_{i} = \mathbf{p} \times \mathbf{d}_{i}.","Equation 3 defines how to compute the direction and moment vectors of each ray in the bundle given a known camera and a set of 2D pixel coordinates, using unprojection and the camera center as the reference point."
ICLR_2024_oral_10,4,"In practice, we select the points \(\{\mathbf{u}_{i}\}_{m}\) by uniformly sampling points on a grid across the image or image crop, as shown in Fig. 2. This allows us to associate each patch in the image with a ray passing through the center of the patch, which we will use later to design a patch- and ray-conditioned architecture.

**Converting from Ray Bundle to Camera.** Given a collection of rays \(\mathcal{R}=\{\mathbf{r}_{i}\}_{m}\) associated with 2D pixels \(\{\mathbf{u}_{i}\}_{m}\), we show that one can recover the camera extrinsics and intrinsics. We start by solving for the camera center \(\mathbf{c}\) by finding the 3D world coordinate closest to the intersection of all rays in \(\mathcal{R}\):","\mathbf{c}=\operatorname*{arg\,min}_{\mathbf{p}\in\mathbb{R}^{3}}\ \sum_{\langle\mathbf{d},\mathbf{m} \rangle\in\mathcal{R}}\lVert\mathbf{p}\times\mathbf{d}-\mathbf{m}\rVert^{2}.",,"\mathbf{c}=\arg\min_{\mathbf{x}\in\mathbb{R}^{3}}\sum_{i=1}^{m}\left\|\left(\mathbf{I}-\frac{\mathbf{d}_{i}\mathbf{d}_{i}^{\top}}{\|\mathbf{d}_{i}\|^{2}}\right)(\mathbf{x}-\mathbf{p}_{i})\right\|^{2},",where the camera center $\mathbf{c}$ is estimated by minimizing the geometric distance to all rays in the bundle $\mathcal{R}$
ICLR_2024_oral_10,5,"Figure 3: **Denoising Ray Diffuser Network. Given a noisy ray corresponding to an image patch, our denoising ray diffusion model predicts the denoised ray. We concatenate spatial image features (Oquab et al., 2023) with noisy rays, represented with 6-dimensional Plücker coordinates (Plücker, 1828) that are visualized as 3-channel direction maps and 3-channel moment maps. We use a transformer to jointly process all image patches and associated noisy rays to predict the original denoised rays.**

To solve for the rotation \(\mathbf{R}\) (and intrinsics \(\mathbf{K}\)) for each camera, we can solve for the optimal homography matrix \(\mathbf{P}\) that transforms per-pixel ray directions from the predicted ones to those of an 'identity' camera (\(\mathbf{K}=\mathbf{I}\) and \(\mathbf{R}=\mathbf{I}\)):","\mathbf{P}=\operatorname*{arg\,min}_{\|\mathbf{H}\|=1}\sum_{i=1}^{m}\|\mathbf{H}\mathbf{ \mathrm{d}}_{i}\times\mathbf{\mathrm{u}}_{i}\|.",,"\mathbf{P} = \operatorname*{arg\,min}_{\mathbf{P}}\ \sum_{i=1}^{m} \lVert \mathbf{d}_i^{\text{pred}} - \mathbf{P} \mathbf{u}_i \rVert^2,",where $\mathbf{P} \in \mathbb{R}^{3 \times 3}$ is the homography matrix that aligns predicted ray directions $\mathbf{d}_i^{\text{pred}}$ with pixel coordinates $\mathbf{u}_i$ under an identity camera model.
ICLR_2024_oral_10,6,"The matrix \(\mathbf{P}\) can be computed via DLT (Abdel-Aziz et al., 2015) and can allow recovering \(\mathbf{R}\) using RQ-decomposition as \(\mathbf{K}\) is an upper-triangular matrix and \(\mathbf{R}\) is orthonormal. Once the camera rotation \(\mathbf{R}\) and camera center \(\mathbf{c}\) are recovered, the translation \(\mathbf{t}\) can be computed as \(\mathbf{t}=-\mathbf{R}\mathbf{c}\).

### Pose Estimation via Ray Regression

We now describe an approach for predicting the ray representation outlined in Sec. 3.1 for camera pose estimation given \(N\) images \(\{I_{1},\dots,I_{N}\}\). Given ground truth camera parameters, we can compute the ground truth ray bundles \(\{\mathcal{R}_{1},\dots,\mathcal{R}_{N}\}\). As shown in Fig. 2, we compute the rays over a uniform \(p\times p\) grid over the image such that each ray bundle consists of \(m=p^{2}\) rays (eq. (1)).

To ensure a correspondence between rays and image patches, we use a spatial image feature extractor and treat each patch feature as a token:",f_{\text{feat}}(I)=\mathbf{f}\in\mathbb{R}^{p\times p\times d}.,,"\mathbf{F}_{i}=\Phi(I_{i})\in\mathbb{R}^{m\times d},","where $\mathbf{F}_{i}$ represents the spatial image features extracted from image $I_{i}$, with $m$ patches and feature dimension $d$"
ICLR_2024_oral_10,7,"To make use of the crop parameters, we also concatenate the pixel coordinate \(\mathbf{u}\) (in normalized device coordinates with respect to the uncropped image) to each spatial feature. We use a transformer-based architecture (Dosovitskiy et al. (2021); Peebles and Xie (2023)) that jointly processes each of the \(p^{2}\) tokens from \(N\) images, and predicts the ray corresponding to each patch:","\{\hat{\mathcal{R}}\}_{i=1}^{N}=f_{\text{Regress}}(\{\mathbf{f}_{i}, \mathbf{u}_{i}\}_{i=1}^{N\cdot p^{2}}).",,"\hat{\mathcal{R}}_{1},\dots,\hat{\mathcal{R}}_{N}=f_{\text{ray}}(\{\mathbf{f}_{1},\dots,\mathbf{f}_{N}\}),",where $f_{\text{ray}}$ is a transformer-based model that predicts ray bundles $\hat{\mathcal{R}}_i$ for each input image $I_i$ by jointly processing spatial features and pixel coordinates.
ICLR_2024_oral_10,8,We train the network by computing a reconstruction loss on the predicted camera rays:,\mathcal{L}_{\text{recon}}=\sum_{i=1}^{N}\|\hat{\mathcal{R}}_{i}-\mathcal{ R}_{i}\|_{2}^{2}.,,"\mathcal{L}_{\text{ray}} = \sum_{i=1}^{N} \sum_{j=1}^{m} \left\| \hat{\mathbf{r}}_{i,j} - \mathbf{r}_{i,j} \right\|^{2},","where $\hat{\mathbf{r}}_{i,j}$ is the predicted ray and $\mathbf{r}_{i,j}$ is the ground truth ray for the $j$-th patch in the $i$-th image."
ICLR_2024_oral_10,9,"### Pose Estimation via Denoising Ray Diffusion

While the patchwise regression-based architecture described in Sec. 3.2 can effectively predict our distributed ray-based parametrization, the task of predicting poses (in the form of rays) may still be ambiguous given sparse views. To handle inherent uncertainty in the predictions (due to symmetries and partial observations), we extend the previously described regression approach to instead learn a diffusion-based probabilistic model over our distributed ray representation.

Figure 4: **Visualizing the Denoising Process Using Our Ray Diffuser. Given the 2 images of the suitcase (_Bottom Right_), we visualize the denoising process starting from randomly initialized camera rays. We visualize the noisy rays using the Plucker representation (ray directions and moments) in the bottom row and their corresponding 3D positions in the top row. In the rightmost column, we recover the predicted cameras (green) and compare them to the ground truth cameras (black).**

Denoising diffusion models (Ho et al., 2020) approximate a data likelihood function by inverting a noising process that adds time-dependent Gaussian noise to the original sample \(x_{0}\):","x_{t}=\sqrt{\alpha_{t}}x_{0}+\sqrt{1-\bar{\alpha}_{t}}\epsilon,","where \(\epsilon\sim\mathcal{N}(0,\mathbf{I})\) and \(\alpha_{t}\) is a hyper-parameter schedule of noise weights such that \(x_{T}\) can be approximated as a standard Gaussian distribution","\mathbf{x}_{t} = \sqrt{\alpha_{t}}\mathbf{x}_{0} + \sqrt{1 - \alpha_{t}}\mathbf{\epsilon}_{t},\quad \mathbf{\epsilon}_{t} \sim \mathcal{N}(0, \mathbf{I}),","where $\mathbf{x}_{t}$ is the noisy sample at time $t$, $\alpha_{t}$ is the time-dependent noise schedule, and $\mathbf{\epsilon}_{t}$ is standard Gaussian noise."
ICLR_2024_oral_10,10,"on. To learn the reverse process, one can train a denoising network \(f_{\theta}\) to predict the denoised sample \(\mathbf{x}_{0}\) conditioned on \(\mathbf{x}_{t}\):","\mathcal{L}(\theta)=\mathbb{E}_{t,\mathbf{x}_{0},\epsilon}\|x_{0}-f_{\theta }(x_{t},t)\|^{2}.",,"\hat{x}_{0}=f_{\theta}(\mathbf{x}_{t},t).",The denoising network predicts the original noise-free sample $x_0$ from a noisy input $x_t$ and the noise level $t$.
ICLR_2024_oral_10,11,"We instantiate this denoising diffusion framework to model the distributions over patchwise rays conditioned on the input images. We do this by simply modifying our ray regression network from Sec. 3.2 to be additionally conditioned on noisy rays (concatenated with patchwise features and pixel coordinates) and a positionally encoded (Vaswani et al., 2017) time embedding \(t\):","\{\hat{\mathcal{R}}\}_{i=1}^{N}=f_{\text{Diffusion}}(\{\mathbf{f}_{i}, \mathbf{u}_{i},\mathbf{r}_{i,t}\}_{i=1}^{N\cdot p^{2}},t), || \mathbf{r}_{i,t}=\sqrt{\bar{\alpha}_{t}}\mathbf{r}_{i}+\sqrt{1-\bar{\alpha}_{t}}\epsilon.","where the noisy rays \(\mathbf{r}_{i,t}\) can be computed as:

\[\mathbf{r}_{i,t}=\sqrt{\bar{\alpha}_{t}}\mathbf{r}_{i}+\sqrt{1-\bar{\alpha}_{t}}\epsilon","\{\hat{\mathcal{R}}_{t}\}_{i=1}^{N}=f_{\theta}(\{\mathbf{f}_{i}, \mathbf{u}_{i}, \mathbf{r}_{i}^{(t)}, t\}_{i=1}^{N\cdot p^{2}}).","We modify the ray regression network to condition on noisy rays and a time embedding, enabling it to learn the denoising process for distributed ray representations."
ICLR_2024_oral_11,1,"### The Proposed Pruning Method

Here, we introduce the proposed CLSP method that aims to eliminate potential false candidate labels in the candidate label set of each training PLL instance. Differently from conventional learning-centric adopt PLL methods that focus on training a desired deep neural network, we pursue a data-centric solution that delves into the PLL data itself. Motivated by the clusterability setting in the conventional supervised learning (Zhu et al., 2022), we further focus on a label distinguishability setting on the PLL dataset, i.e., nearby PLL instances are likely to have the same true label (vs. false candidate label) inside their candidate label sets with a high probability (vs. a limited probability), which could be formally defined below.

**Definition 2** ((\(k,\delta_{k},\rho_{k}\))-label distinguishability).: _A PLL dataset \(\mathcal{D}=\{(\mathbf{x}_{i},Y_{i})\}_{i=1}^{n}\) satisfies (\(k,\delta_{k},\rho_{k}\)) label distinguishability if: \(\forall(\mathbf{x}_{i},Y_{i})\in\mathcal{D}\), the true label \(y_{i}\in Y_{i}\) is inside the candidate label set \(Y^{(j)}_{i}\) of its each \(k\)-NN example \((\mathbf{x}^{(j)}_{i},Y^{(j)}_{i})\), with probability at least \(1-\delta_{k}\), and each false candidate label \(y^{\prime}_{i}\in Y_{i}\backslash\{y_{i}\}\) is inside the candidate label set \(Y^{(j)}_{i}\) of its each \(k\)-NN example \((\mathbf{x}^{(j)}_{i},Y^{(j)}_{i})\) with probability no more than \(\rho_{k}\)._

Definition 2 characterizes the candidate label distribution in the local representation space, which has two-fold implications. First, the true label of instances in the local representation space appears in their candidate label sets with a high probability. Second, each false candidate label of instances in the local representation space appears in their candidate label sets with a limited probability.

Intuitively, the candidate label of an instance that appears most frequently in its \(k\)-NN instances' candidate label sets is more likely to be the true label, and the candidate label that rarely appears in its \(k\)-NN instances' candidate label sets has a high probability of being a false label. Motivated by this intuition, we introduce a per-instance label-wise metric \(O_{ij}\) towards the \(i\)-th instance (\(\mathbf{x}_{i},Y_{i}\)) associated with the \(j\)-th candidate label, to measure the possibility of the \(j\)-th candidate label is _not_ the true label of the instance \(\mathbf{x}_{i}\), which implies that we should prune the \(j\)-th label from the candidate label set if \(O_{ij}\) is large. Concretely, \(O_{ij}\) is calculated by counting the times of its \(j\)-th candidate label not appearing in its \(k\)-NN instances' candidate label set \(Y_{i}^{(k)}\):","O_{ij}=\sum_{v=1}^{k}\mathbb{I}[y_{ij}\neq y_{ij}^{(v)}],\forall j\in Y_{i},",where \(\mathbb{I}[\cdot]\) is the indicator function and \(y_{ij}\) is the \(j\)-th candidate label of \(\mathbf{x}_{i}\),"\begin{equation}
O_{ij} = \sum_{\substack{(\mathbf{x}_i^{(j)}, Y_i^{(j)}) \in \mathcal{N}_k(\mathbf{x}_i)}} \mathbb{I}(y_j \notin Y_i^{(j)}),
\end{equation}","where $O_{ij}$ quantifies the number of times the $j$-th candidate label is absent in the $k$-NN instances of the $i$-th example, indicating a higher likelihood of being a false label."
ICLR_2024_oral_11,2,"\). In this way, the value of \(O_{ij}\) denotes the number of the \(k\)-NN instances of \(\mathbf{x}_{i}\) whose candidate label set does not include the \(j\)-th candidate label of \(\mathbf{x}_{i}\). Particularly, the calculating process in Eq. (1) is like a ""down-voting"" procedure where each nearest neighbor instance of the concerned instance serves a voter to down-vote the candidate labels that are not inside its candidate label set \(Y_{i}^{(k)}\). Furthermore, we define a specified parameter \(\tau\) to control the pruning extent. Specifically, the number of eliminated candidate labels of \(\mathbf{x}_{i}\) is","\gamma_{i}=\lceil\tau(|Y_{i}|-1)\rceil,",where \(\lceil\cdot\rceil\) is the ceiling function that returns the least integer greater than or equal to the given number,"E_{i} = \sum_{j \in Y_{i}} \mathbb{I}[O_{ij} \geq \tau],","where $E_i$ represents the count of candidate labels in $Y_i$ whose corresponding $O_{ij}$ score meets or exceeds the pruning threshold $\tau$, indicating they are likely false labels and should be removed."
ICLR_2024_oral_11,3,"er. Then, for each instance \((\mathbf{x}_{i},Y_{i})\), we can eliminate a subset of candidate labels that possess a high down-voting value:","\widetilde{Y}_{i}=\mathrm{Top}\text{-}\gamma_{i}\text{-}\mathrm{argmax}_{j \in Y_{i}}(O_{ij}),",where \(\mathrm{Top}\text{-}\gamma_{i}\text{-}\mathrm{argmax}_{i}\) returns a subset of indices (i,"\mathcal{P}_{i} = \text{top}_{\gamma_{i}}\{O_{ij} \mid j \in Y_{i}\},",which represents the set of candidate labels to be pruned based on the top-$\gamma_i$ highest down-voted labels.
ICLR_2024_oral_11,4,"(i.e., candidate labels) that have the highest \(\gamma_{i}\) down-voting values of \(O_{ij}\) for \(j\in Y_{i}\). After eliminating the selected subset of candidate labels \(\widetilde{Y}_{i}\) from \(Y_{i}\) for each instance \(\mathbf{x}_{i}\), we can obtain the pruned PLL dataset \(\widetilde{\mathcal{D}}=\{(\mathbf{x}_{i},\widetilde{Y}_{i})\}_{i=1}^{n}\) where \(\widetilde{Y}_{i}=Y_{i}\backslash\widetilde{Y}_{i}\). The pseudo-code of the proposed algorithm for CLSP is shown in Appendix A. Notably, an incorrectly pruned PLL instance \((\mathbf{x}_{i},Y_{i})\) whose true label \(y_{i}\) is inside the eliminated candidate label set \(\widetilde{Y}_{i}\) would become a noisy PLL instance (Yan and Guo, 2023; Xu et al., 2023; Lv et al., 2023; Wang et al., 2024), which is more challenging for conventional PLL methods. To alleviate this issue, we would like to analyze the upper bound of the pruning error in the proposed method.

### Theoretical Analysis of Pruning Error

Given an example \((\mathbf{x}_{i},Y_{i})\) and its \(k\)-NN examples \(\{(\mathbf{x}_{i}^{(j)},Y_{i}^{(j)})\}_{j=1}^{k}\) in a PLL dataset \(\mathcal{D}\) that satisfies the (\(k,\delta_{k},\rho_{k}\)) label distinguishability, the probability of the true label \(y_{i}\in Y_{i}\) appearing in each \(k\)-NN instance's candidate label set \(Y_{i}^{(j)}\) is denoted by \(t\geq 1-\delta_{k}\), and correspondingly the probability of each false candidate label \(y_{i}^{\prime}\in\hat{Y}_{i}^{\prime}=Y_{i}\backslash\{y_{i}\}\) appearing in each \(k\)-NN instance's candidate label set \(Y_{i}^{(j)}\) is denoted by \(q\leq\rho_{k}\). We assume that the true label and false candidate labels of each PLL example appear in its \(k\)-NN examples' candidate label sets independently. Then, the down-voting statistic of the true label \(O_{iy}\) follows a binomial distribution \(B(k,1-t)\), and the down-voting statistic of each false candidate label \(O_{iy^{\prime}}\) follows a binomial distribution \(B(k,1-q)\). In this case, there are one random variable \(O_{iy}\sim B(k,1-t)\) and \(|Y_{i}^{\prime}|\)_i.i.d._ random variables \(O_{iy^{\prime}}\sim B(k,1-q)\). Given the number of eliminated candidate labels \(\gamma_{i}\in[1,|Y_{i}^{\prime}|]\), an incorrect pruning event for a PLL example \((\mathbf{x}_{i},Y_{i})\) occurs in the proposed algorithm when \(O_{iy}\) is \(\mathrm{Top}\text{-}\gamma_{i}\text{-}\operatorname{arg\,max}_{j}(O_{ij})\). In other words, the incorrect pruning implies that the down-voting statistic of the true label \(O_{iy}\) is larger than the \(\gamma_{i}\)-th highest down-voting statistic of the false candidate label. Formally, based on the definition of the \(k\)-th _order statistic_\(O_{iy^{\prime}}^{(k)}\) which is the \(k\)-th item in the ascending order statistics \([O_{iy^{\prime}}^{(1)},O_{iy}^{(k)},\cdots,O_{iy^{\prime}}^{(|Y_{i}^{\prime}|)}]\), the incorrect pruning event can be defined as \([O_{iy^{\prime}}^{(|Y_{i}^{\prime}|-\gamma_{i}+1)}<O_{iy}]\). An upper bound for the probability of getting such an event is shown below.

**Theorem 1**.: _Assume that the \((k,\delta_{k},\rho_{k})\)-label distinguishability is satisfied. For each PLL example (\(\mathbf{x}_{i},Y_{i}\)), let us denote that the \(y\)-th label in the candidate label set \(Y_{i}\) is the true label, and the \(y^{\prime}\)-th label in the false candidate label set \(Y_{i}^{\prime}=Y_{i}\backslash\{y\}\) is an arbitrary false candidate label, i.e., \(y^{\prime}\neq y\). Given the number of eliminated candidate labels \(\gamma_{i}\), then the probability of getting an incorrect pruning can be upper bounded by_","\mathbb{P}(O_{iy^{\prime}}^{(\xi_{i})}<O_{iy})\leq\sum_{j=1}^{k}\sum_{m=\xi_{i}} ^{|Y_{i}^{\prime}|}\binom{|Y_{i}^{\prime}|}{m}\eta^{m}(1-\eta)^{(|Y_{i}^{\prime }|-m)}b_{\delta_{k}}(k,j),",,,
ICLR_2024_oral_11,5,"_where \(\xi_{i}=(|Y^{\prime}_{i}|-\gamma_{i}+1)\), \(\binom{n}{r}=\frac{n!}{r!(n-r)!}\) is the combination formula, \(b_{\delta_{k}}(k,j)=\binom{k}{j}\delta_{k}^{j}(1-\delta_{k})^{k-j}\) denotes the probability mass function of a binomial distribution \(B(k,\delta_{k})\), and \(\eta=I_{\rho_{k}}(k-j+1,j)\) where \(I_{\rho_{k}}(k,j)=\int_{0}^{\rho_{k}}t^{k-1}(1-t)^{j-1}dt\) is the regularized incomplete beta function._

The proof is provided in Appendix B. Note that the above upper bound is too complicated to perform the mathematical quantitative analysis for four key factors \(k\), \(\gamma_{i}\), \(\delta_{k}\), and \(\rho_{k}\). Hence, we aim to conduct an empirical analysis of the four factors to derive technical insights. Specifically, in the practical scenario of utilizing the proposed algorithm, given a PLL dataset (including the employed feature extractor), \(\delta_{k}\), and \(\rho_{k}\) are fixed (or can be estimated on the validation set). We need to choose the appreciate values of \(k\) and \(\gamma_{i}\) (\(\tau\)). Before formally introducing the empirical experiment of evaluating the effects of varying \(k\) and \(\gamma_{i}\), let us additionally come up with another conclusion shown below.

**Theorem 2**.: _Given the same assumption of the (\(k,\delta_{k},\rho_{k}\))-label distinguishability and notations in Theorem 1, when increasing the number of eliminated candidate labels (i.e., \(\gamma_{i}^{2}>\gamma_{i}^{1}\)), the extra pruning error can be bounded by_","\mathbb{P}(O_{iy}^{(\xi_{i}^{2})}<O_{iy})-\mathbb{P}(O_{iy}^{(\xi_{i}^{1})}<O _{iy})\leqslant\sum_{j=1}^{k}\sum_{m=\xi_{i}^{2}}^{\xi_{i}^{1}-1}\binom{|Y^{ \prime}_{i}|}{m}\eta^{m}(1-\eta)^{|Y^{\prime}_{i}|-m}b_{\delta_{k}}(k,j),",,"\mathbb{P}(O_{iy^{\prime}}^{(\xi_{i}^{2})}<O_{iy})-\mathbb{P}(O_{iy^{\prime}}^{(\xi_{i}^{1})}<O_{iy})\leq\sum_{j=1}^{k}\sum_{m=\xi_{i}^{2}}^{\xi_{i}^{1}-1}\binom{|Y_{i}^{\prime}|}{m}\eta^{m}(1-\eta)^{(|Y_{i}^{\prime }|-m)}b_{\delta_{k}}(k,j),","where $\xi_{i}^{1}=(|Y^{\prime}_{i}|-\gamma_{i}^{1}+1)$ and $\xi_{i}^{2}=(|Y^{\prime}_{i}|-\gamma_{i}^{2}+1)$ denote the order statistics corresponding to two different pruning extents $\gamma_{i}^{1}$ and $\gamma_{i}^{2}$, and the bound quantifies the additional pruning error introduced by increasing the number of eliminated candidate labels."
ICLR_2024_oral_13,1,"### Preliminary

We begin by defining the essential notation associated with diffusion models (Ho et al., 2020; Song and Ermon, 2020; Dhariwal and Nichol, 2021). For a data point \(x_{0}\) drawn from the real data distribution \(q(x_{0})\), a forward diffusion process comprises a fixed Markov chain spanning \(T\) steps, where each step introduces a predetermined amount of Gaussian noise. Specifically:","q(x_{t}|x_{t-1})=\mathcal{N}(x_{t};\sqrt{1-\beta_{t}}x_{t},\beta_{t}\mathbf{I} ),\quad\text{for }t\in\{1,...,T\},","where \(\beta_{t}\in(0,1)\) is the scheduled variance at step \(t\)","x_t = \sqrt{1 - \beta_t} x_{t-1} + \sqrt{\beta_t} \epsilon \quad \text{for } t=1,\dots,T","Forward diffusion process defining $x_t$ as a Markov chain with Gaussian noise scaled by $\beta_t$, where $\epsilon \sim \mathcal{N}(0, I)$."
ICLR_2024_oral_13,2,\). The closed-form for this sampling is,"x_{t}=\sqrt{\bar{\alpha}_{t}}x_{0}+\sqrt{1-\bar{\alpha}_{t}}\epsilon,","where, \(\bar{\alpha}_{t}=\prod_{i=1}^{t}(1-\beta_{t})\)","q(x_{t}|x_{0})=\mathcal{N}(x_{t};\sqrt{\bar{\alpha}_{t}}x_{0},(1-\bar{\alpha}_{t})\mathbf{I}),",where $\bar{\alpha}_{t} = \prod_{s=1}^{t} (1 - \beta_{s})$ represents the cumulative product of the noise schedule up to step $t$.
ICLR_2024_oral_13,3,"\).

In the reverse diffusion process, a Gaussian vector \(x_{T}\sim\mathcal{N}(0,1)\) is denoised to map to an image \(x_{0}\in q(x)\). At each denoising step, a trained noise-predictor \(\epsilon_{\theta}\) anticipates the noise \(\epsilon_{\theta}(x_{t})\) that was added to \(x_{0}\). Based on Eq. (2), the estimation of \(x_{0}\) can be formulated as:",\hat{x}_{0}^{t}=\frac{x_{t}-\sqrt{1-\bar{\alpha}_{t}}\epsilon_{\theta}(x_{t}) }{\sqrt{\bar{\alpha}_{t}}}.,,\hat{x}_{0}=\frac{1}{\sqrt{\bar{\alpha}_{t}}}x_{t}-\sqrt{1-\bar{\alpha}_{t}}\epsilon_{\theta}(x_{t}),where $\hat{x}_{0}$ is the estimated original data point from the noisy input $x_{t}$ using the predicted noise $\epsilon_{\theta}(x_{t})$
ICLR_2024_oral_13,4,"Then, we can predict \(x_{t-1}\) as:","x_{t-1}=\sqrt{\bar{\alpha}_{t-1}}\hat{x}_{0}^{t}+\sqrt{1-\Text-conditional diffusion models, such as Stable Diffusion (Rombach et al., 2022), employ classifier-free diffusion guidance (Rombach et al., 2022) to steer the sampling process. Given a text prompt \(p\), its embedding \(e_{p}=f(p)\) is computed using a pre-trained CLIP text encoder \(f(\cdot)\)(Radford et al., 2021; Cherti et al., 2023). In the reverse process, the conditional sampling adheres to Eq. (3) and Eq. (4), but the predicted noise \(\epsilon_{\theta}(x_{t})\) is changed to:

\[\epsilon_{\theta}(x_{t},e_{\theta})+s(\underbrace{\epsilon_{\theta}(x_{t},e_ {p})-\epsilon_{\theta}(x_{t},e_{\theta})}_{\text{text-conditional noise prediction}}),","where, \(e_{\theta}\) represents the prompt embedding of an empty string, and \(s\) determines the guidance strength, controlling the alignment of the generation to the prompt",\hat{x}_{t-1} = \frac{x_{t} - \sqrt{1 - \bar{\alpha}_{t}} \epsilon_{\theta}(x_{t})}{\sqrt{\bar{\alpha}_{t}}} \cdot \sqrt{\bar{\alpha}_{t-1}} + \sqrt{1 - \bar{\alpha}_{t-1}} \epsilon_{\theta}(x_{t}).,Equation predicts the previous step $x_{t-1}$ using the estimated noise $\epsilon_{\theta}(x_{t})$ and the known variance schedule $\bar{\alpha}_{t}$.
ICLR_2024_oral_13,5,"pt. We refer to the term \(\epsilon_{\theta}(x_{t},e_{p})-\epsilon_{\theta}(x_{t},e_{\theta})\) as the text-conditional noise prediction for future reference.

### Motivation

When provided with the same text prompt but different initializations, diffusion models can generate a diverse set of images. Conversely, when given different text prompts but the same initialization, the resulting images often display semantic similarities. These similarities include analogous layouts and color themes, as demonstrated in Appendix Fig. 6. Such a phenomenon might arise when the final generation remains closely tied to its initialization, and the textual guidance is not particularly dominant. This observation is consistent with findings from Wen et al. (2023b), suggesting that one can trace the origin to the initial seed even without knowing the text condition.

Interestingly, when it comes to memorized prompts, the initialization appears to be irrelevant. The generated images consistently converge to a specific memorized visualization. This behavior implies that the model might be overfitting to both the prompt and a certain denoising trajectory, which leads to the memorized image. Consequently, the final image deviates substantially from its initial state.

These insights provide a foundation for a straightforward detection strategy: scrutinizing the magnitude of text-conditional noise predictions. A smaller magnitude signals that the final image is closely aligned with its initialization, hinting that it is likely not a memorized image. On the other hand, a larger magnitude could indicate potential memorization. The correlation between magnitude and memorization is depicted in Fig. 2(a).

### An Effective Detection Method

Following the intuition above, we introduce a straightforward yet effective detection method centered on the magnitude of text-conditional noise predictions. For a prompt embedding \(p\) and a sampling step of \(T\), the detection metric is defined as","d=\frac{1}{T}\sum_{t=1}^{T}\|\epsilon_{\theta}(x_{t},e_{p})-\epsilon_{\theta} (x_{t},e_{\theta})\|_{2}.",,"\mathcal{M}(p) = \frac{1}{T} \sum_{t=1}^{T} \left\| \epsilon_{\theta}(x_{t}, e_{p}) - \epsilon_{\theta}(x_{t}, e_{\theta}) \right\|_{2},",where $\mathcal{M}(p)$ quantifies the average Euclidean norm of the text-conditional noise prediction across all sampling steps for a given prompt $p$.
ICLR_2024_oral_13,6,"Figure 2: Statistics of the magnitude of text-conditional noise predictions.

Memorization is then identified if the detection metric falls beneath a tunable threshold \(\gamma\).

In practice, we also find that even the detection metric of a single generation can provide a strong signal of memorization. Consequently, our method remains effective and reliable with the number of generations restricted to \(1\). In contrast, earlier studies, such as those examining generation density over a large number of generations (Carlini et al., 2023), require the simultaneous generation of multiple images, with some cases necessitating over a hundred generations. This might impose an extra computational burden on the service provider by generating more images than the user requested. Moreover, another method presented in (Sompalli et al., 2023) identifies memorized prompts by directly comparing the generated images with the original training data. Unlike this method, our approach allows a third party to use the detection method without needing access to the large training dataset, thereby protecting training data privacy.

Another distinct advantage of our approach is its adaptability in calculating the detection metric. Strong detection does not mandate collecting the metric from all sampling steps. Based on our empirical findings, even when the metric is collated solely from the first step, reliable detection remains attainable. This efficiency enables model owners to identify memorized prompts promptly. By stopping generation early, they can then opt for post-processing, like declining the generated output or reinitializing the generation with corrective strategies in place.

### Experiments

Experimental Setup.To evaluate our detection method, we use 500 memorized prompts identified in Webster (2023) for Stable Diffusion v1 (Rombach et al., 2022), where the SSCD similarity score (Pizzi et al., 2022) between the memorized and the generated images exceeds \(0.7\). The memorized prompts gathered in Webster (2023) include three types of memorization: 1) matching verbatim: where the images generated from the memorized prompt are an exact pixel-by-pixel match with the original paired training image; 2) retrieval verbatim: the generated images perfectly align with some training images, albeit paired with different prompts; 3) template verbatim: generated images bear a partial resemblance to the training image, though variations in colors or styles might be observed.

Additionally, we use another \(2,000\) prompts, evenly distributed from sources LAION (Schuhmann et al., 2022), COCO (Lin et al., 2014), Lexica.art (Santana, 2022), and randomly generated strings. For this set of prompts, we assume they are not memorized by the model. All generations employ DDIM (Song et al., 2020) with 50 inference steps.

In our comparison, we use the detection method from Carlini et al. (2023) as a baseline. This method determines memorization by analyzing generation density, computed using the pairwise \(\ell_{2}\) distance between non-overlapping tiles. While Carlini et al. (2023) utilizes the \(\ell_{2}\) distance in pixel space, we introduce an additional baseline that calculates the distance in the SSCD feature space (Pizzi et al., 2022). This adjustment is inspired by Sompalli et al. (2022), who underscore the effectiveness of

\begin{table}
\begin{tabular}{c c c c} \hline \hline Method & 1st Step & First 10 Steps & Last Step \\  & \multicolumn{2}{c}{AUC\(\uparrow\) / TPR\(@1\%\)FPR\(\uparrow\) / Time in Seconds\(\downarrow\)} \\ \hline \hline Density\({}_{\ell_{2}}\), n=4 & \(0.520\) / \(0.012\) / \(0.810\) & \(0.652\) / \(0.225\) / \(5.314\) & \(0.659\) / \(0.288\) / \(9.904\) \\ Density\({}_{\ell_{2}}\), n=16 & \(0.506\) / \(0.000\) / \(3.570\) & \(0.656\) / \(0.175\) / \(24.78\) & \(0.676\) / \(0.271\) / \(40.66\) \\ Density\({}_{\ell_{2}}\), n=32 & \(0.510\) / \(0.000\) / \(8.092\) & \(0.664\) / \(0.175\) / \(59.43\) & \(0.681\) / \(0.266\) / \(81.44\) \\ Density\({}_{\text{SSCD}}\), n=4 & \(0.537\) / \(0.019\) / \(0.809\) & \(0.405\) / \(0.005\) / \(5.421\) & \(0.878\) / \(0.525\) / \(9.892\) \\ Density\({}_{\text{SSCD}}\), n=16 & \(0.515\) / \(0.000\) / \(3.186\) & \(0.375\) / \(0.000\) / \(21.02\) & \(0.934\) / \(0.523\) / \(39.55\) \\ Density\({}_{\text{SSCD}}\), n=32 & \(0.506\) / \(0.000\) / \(6.341\) & \(0.370\) / \(0.000\) / \(42.12\) & \(0.940\) / \(0.530\) / \(79.47\) \\ \hline
**Ours**, n=1 & \(0.960\) / \(0.760\) / \(\mathbf{0.199}\) & \(0.989\) / \(0.944\) / \(\mathbf{1.866}\) & \(0.989\) / \(0.934\) / \(\mathbf{9.584}\) \\
**Ours**, n=4 & \(0.990\) / \(0.912\) / \(0.794\) & \(0.998\) / \(0.982\) / \(7.471\) & \(0.996\) / \(0.978\) / \(37.27\) \\
**Ours**, n=32 & \(\mathbf{0.996}\) / \(\mathbf{0.954}\) / \(1.606\) & \(\mathbf{0.999}\) / \(\mathbf{0.988}\) / \(14.96\) & \(\mathbf{0.998}\) / \(\mathbf{0.986}\) / \(74.75\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Memorization detection results with AUC, TPR\(@1\%\)FPR, and the running time of the method in seconds. In this table, “n” represents the number of generations per prompt.

SSCD. As a deep learning-informed distance metric, SSCD offers enhanced resilience to particular augmentations, like color shift -- a critical advantage when the training image is only partially memorized.

We use the area under the curve (AUC) of the receiver operating characteristic (ROC) curve and the True Positive Rate at the False Positive Rate of \(1\%\) (TPR\(@1\%\)FPR) as metrics. Meanwhile, we report the running time in seconds with a batch size of \(4\) on a single NVIDIA RTX A6000.

Results.In Fig. 2(b), we display a density plot comparing the detection metrics for the memorized prompts against the non-memorized ones, calculated over \(50\) steps with \(4\) generations per prompt. The distribution of memorized prompts is bimodal. This dichotomy stems from the fact that the template verbatim scenario often exhibits a slightly smaller metric than the matching verbatim scenario, given that the memorization occurs only partially.

In Table 1, we highlight the balance between the precision and efficiency of our proposed method. Our method is able to achieve very strong detection performance. When generating \(32\) images and using the metrics from the first \(10\) steps, our method is able to achieve an AUC of \(0.999\) and TPR\(@1\%\)FPR of \(0.988\). Remarkably, even when operating with a single generation, our method can achieve TPR\(@1\%\)FPR of \(0.760\) from the very first step within merely \(0.2\) seconds. This feature provides a significant advantage in terms of time and computational resource savings, allowing model operators the flexibility to terminate generation early if necessary. In contrast, the baseline methods show noticeably reduced detection capability. In particular, the baseline methods can only achieve relatively high detection accuracy when generating more than \(16\) images and relying on the image generation from the final step, where it requires at least \(40\) seconds. Yet, in real-world applications, service providers like _Midjourney_ or _DALL-E_ 2(Ramesh et al., 2022) typically generate a mere \(4\) images concurrently for each prompt.

Interestingly, our method surpasses the baselines in speed even when using the metric with equivalent generations and steps. This superiority emerges since our method doesn't need to decode latent noise into image space and perform subsequent calculations.

## 4 Mitigate Memorization

### A Straightforward Method to Detect Trigger Tokens

As observed by Sompalli et al. (2022), certain words or tokens in memorized prompts play a significant influence on the generation process. Even when only these specific ""trigger tokens"" are present in the prompt, the memorization effect remains evident. One potential approach to identify these trigger tokens involves probing with various n-gram combinations to discern which combinations induce memorization. However, this heuristic becomes notably inefficient, particularly when the prompt contains a vast number of tokens. Our earlier observations offer a more streamlined method for discerning the significance of each token in relation to memorization: by checking the magnitude of the change applied to each token while minimizing the magnitude of text-conditional noise prediction. A token undergoing substantial change suggests its crucial role in steering the prediction; conversely, a token with minimal change is less important.

Given a prompt embedding \(e\) of prompt \(p\) with \(N\) tokens, we form the objective of the minimization problem as:","\mathcal{L}(x_{t},e)=\|\epsilon_{\theta}(x_{t},e)-\epsilon_{\theta}(x_{t},e_{ \emptyset})\|_{2}.",,"\min_{\delta_{1},...,\delta_{N}}\sum_{t=1}^{T}\|\epsilon_{\theta}(x_{t},e+\delta)-\epsilon_{\theta}(x_{t},e_{\theta})\|_{2}\quad\text{subject to}\quad\|\delta_{i}\|_{2}\leq\lambda,\forall i,","where $\delta_{i}$ denotes the perturbation applied to the $i$-th token embedding, and $\lambda$ controls the maximum allowed perturbation magnitude for each token."
ICLR_2024_oral_13,7,"We then determine the significance score for each token at position \(i\in[0,N-1]\) as:","\text{SS}_{e^{i}}=\frac{1}{T}\sum_{t=1}^{T}\|\nabla_{e^{i}}\mathcal{L}(x_{t}, e)\|_{2}.",,"s_{i}=\left\|\frac{\partial\mathcal{L}(x_{t},e)}{\partial e_{i}}\right\|_{2},",where the significance score $s_{i}$ quantifies the sensitivity of the text-conditional noise prediction with respect to changes in the embedding of the token at position $i$.
ICLR_2024_oral_14,1,"## 3 Preliminaries

**Probabilistic interpretation of dense correspondence.** Let us denote a pair of images, i.e., source and target, as \(I_{\text{src}}\) and \(I_{\text{tgt}}\) that represent visually or semantically similar images, and feature descriptors extracted from \(I_{\text{src}}\) and \(I_{\text{tgt}}\) as \(D_{\text{src}}\) and \(D_{\text{tgt}}\), respectively. The objective of dense correspondence is to find a correspondence field \(F\) that is defined at each pixel \(i\), which warps \(I_{\text{src}}\) towards \(I_{\text{tgt}}\) such that \(I_{\text{tgt}}(i)\sim I_{\text{src}}(i+F(i))\) or \(D_{\text{tgt}}(i)\sim D_{\text{src}}(i+F(i))\).

This objective can be formulated within probabilistic interpretation (Simoncelli et al., 1991; Sun et al., 2008; Ham et al., 2016; Kim et al., 2017), where we seek to find \(F^{*}\) that maximizes the posterior probability of the correspondence field given a pair of feature descriptors \(D_{\text{src}}\) and \(D_{\text{tgt}}\), i.e., \(p(F|D_{\text{src}},D_{\text{tgt}})\). According to Bayes' theorem (Joyce, 2003), the posterior can be decomposed such that \(p(F|D_{\text{src}},D_{\text{tgt}})\propto p(D_{\text{src}},D_{\text{tgt}}|F )\cdot p(F)\). To find the matching field \(F^{*}\) that maximizes the posterior, we can use the maximum a posteriori (MAP) approach (Greig et al., 1989):","\begin{split} F^{*}&=\operatorname*{argmax}_{F}p(F|D _{\text{src}},D_{\text{tgt}})=\operatorname*{argmax}_{F}p(D_{\text{src}},D_{ \text{tgt}}|F)\cdot p(F)\\ &=\operatorname*{argmax}_{F}\{\underbrace{\log p(D_{\text{src}},D _{\text{tgt}}|F)}_{\text{data term}}+\underbrace{\log p(F)}_{\text{prior term}}\}.\end{split}",,"F^{*} = \arg\max_{F} \left[ p(D_{\text{src}}, D_{\text{tgt}} | F) \cdot p(F) \right]","The optimal correspondence field $F^{*}$ is obtained by maximizing the product of the likelihood $p(D_{\text{src}}, D_{\text{tgt}} | F)$ and the prior $p(F)$ over all possible correspondence fields $F$."
ICLR_2024_oral_14,2,"In this probabilistic interpretation, the first term, referred to as _data_ term, represents the matching evidence between feature descriptors \(D_{\text{src}}\) and \(D_{\text{tgt}}\), and the second term, referred to as _prior_ term, encodes prior knowledge of the matching field \(F\).

**Conditional diffusion models.** The diffusion model is a type of generative model, and can be divided into two categories: unconditional models (Sohl-Dickstein et al., 2015; Ho et al., 2020) and conditional models (Batzolis et al., 2021; Dhariwal and Nichol, 2021). Specifically, unconditional diffusion models learn an explicit approximation of the data distribution, denoted as \(p(X)\). On the other hand, conditional diffusion models estimate the data distribution given a certain condition \(K\), e.g., text prompt (Dhariwal and Nichol, 2021), denoted as \(p(X|K)\).

In the conditional diffusion model, the data distribution is approximated by recovering a data sample from the Gaussian noise through an iterative denoising process. Given a sample \(X_{0}\), it is transformed to \(X_{t}\) through the forward diffusion process at a time step \(t\in\{T,T-1,\ldots,1\}\), which consists of Gaussian transition at each time step \(q(X_{t}|X_{t-1}):=\mathcal{N}(\sqrt{1-\beta_{t}}X_{t-1},\beta_{t}I)\). The forward diffusion process follows the pre-defined variance schedule \(\beta_{t}\) such that","X_{t}=\sqrt{\alpha_{t}}X_{0}+\sqrt{1-\alpha_{t}}Z,\quad Z\sim\mathcal{N}(0,I),",where \(\alpha_{t}=\prod_{i=1}^{t}(1-\beta_{i})\),"\begin{equation} q(X_{t}|X_{0})=\mathcal{N}(X_{t};\sqrt{\bar{\alpha}_{t}}X_{0},(1-\bar{\alpha}_{t})I), \end{equation}","The forward diffusion process at time step $t$ given the initial sample $X_0$, characterized by a Gaussian distribution with mean $\sqrt{\bar{\alpha}_t}X_0$ and variance $(1-\bar{\alpha}_t)I$."
ICLR_2024_oral_14,3,"\). After training, we can sample data from the learned distribution through iterative denoising with the pre-defined range of time steps, called the reverse diffusion process, following the non-Markovian process of DDIM (Song et al., 2020), which is parametrized as another Gaussian transition \(p_{\theta}(X_{t-1}\mid X_{t}):=\mathcal{N}(X_{t-1};\mu_{\theta}(X_{t},t),\sigma _{\theta}(X_{t},t)I)\). To this end, the diffusion network \(\mathcal{F}_{\theta}(X_{t},t;K)\) predicts the denoised sample \(X_{t}\), given \(X_{t}\), \(t\) and \(K\). One step in the reverse diffusion process can be formulated such that","X_{t-1}=\sqrt{\alpha_{t-1}}\mathcal{F}_{\theta}(X_{t},t;K)+\frac{\sqrt{1- \alpha_{t-1}-\sigma_{t}^{2}}}{\sqrt{1-\alpha_{t}}}\Big{(}X_{t}-\sqrt{\alpha_{ t}}\mathcal{F}_{\theta}(X_{t},t;K)\Big{)}+\sigma_{t}Z",where \(\sigma_{t}\) is the covariance value of Gaussian distribution at time step \(t\),"X_{t-1}=\sqrt{\alpha_{t-1}}\left(\frac{X_{t}-\sqrt{1-\alpha_{t}}Z_{t}}{\sqrt{\alpha_{t}}}\right)+\sqrt{1-\alpha_{t-1}}\mathcal{F}_{\theta}(X_{t},t;K),","where $Z_{t} \sim \mathcal{N}(0, I)$ and $\mathcal{F}_{\theta}(X_{t},t;K)$ is the denoising network conditioned on $K$."
ICLR_2024_oral_14,4,"\).

This iterative denoising process can be viewed as finding \(X^{*}=\operatorname*{argmax}_{X}\log p(X|K)\) through the relationship between the conditional sampling process of DDIM (Song et al., 2020) and conditional score-based generative models (Batzolis et al., 2021).

## 4 Methodology

### Motivation

Recent learning-based methods (Kim et al., 2017; Sun et al., 2018; Rocco et al., 2017, 2020; Truong et al., 2020; Min and Cho, 2021; Kim et al., 2018; Jiang et al., 2021; Cho et al., 2021, 2022) have employed deep neural networks \(\mathcal{F}(\cdot)\) to directly approximate the _data_ term, i.e., \(\operatorname*{argmax}_{F}\log p(D_{\text{src}},D_{\text{tgt}}|F)\), without explicitly considering the _prior_ term. For instance, GLU-Net (Truong et al., 2020) and GOCor (Truong et al., 2020) construct a cost volume along candidates \(F\) between source and target features \(D_{\text{src}}\) and \(D_{\text{tgt}}\), and regresses the matching fields \(F^{*}\) within deep neural networks, which might be analogy to \(\operatorname*{argmax}_{F}\log p(D_{\text{src}},D_{\text{tgt}}|F)\). In this setting,

Figure 2: **Overall network architecture of DiffMatch. Given source and target images, our conditional diffusion-based network estimates the dense correspondence between the two images. We leverage two conditions: the initial correspondence \(F_{\text{init}}\) and the local matching cost \(C^{l}\), which finds long-range matching and embeds local pixel-wise interactions, respectively.**dense correspondence \(F^{*}\) is estimated as follows:","F^{*}=\mathcal{F}_{\theta}(D_{\mathrm{src}},D_{\mathrm{tgt}})\approx\underset{F }{\mathrm{argmax}}\ \underbrace{\log p(D_{\mathrm{src}},D_{\mathrm{tgt}}|F)}_{\text{data term}},","where \(\mathcal{F}_{\theta}(\cdot)\) and \(\theta\) represent a feed-forward network and its parameters, respectively","F^{*}=\operatorname*{argmax}_{F}\log p(D_{\text{src}},D_{\text{tgt}}|F)","Estimation of the dense correspondence field by maximizing the data term, which represents the matching evidence between source and target feature descriptors."
ICLR_2024_oral_14,5,"ly.

These approaches assume that the matching prior can be learned within the model architecture by leveraging the high capacity of deep networks (Truong et al., 2020; Jiang et al., 2021; Cho et al., 2021, 2022; Min & Cho, 2021) and the availability of large-scale datasets. While there exists obvious performance improvement, they typically focus on the data term without explicitly considering the matching prior. This can restrict ability of the model to learn the manifold of matching field and result in poor generalization.

### Formulation

To address these limitations, for the first time, we explore a conditional generative model for dense correspondence to explicitly learn both the _data_ and _prior_ terms. Unlike previous discriminative learning-based approaches (Perez et al., 2013; Drulea & Nedevschi, 2011; Werlberger et al., 2010; Kim et al., 2017; Sun et al., 2018; Rocco et al., 2017), we achieve this by leveraging a conditional generative model that jointly learns the data and prior through optimization of the following objective that _explicitly_ learn \(\underset{F}{\mathrm{argmax}}_{F}p(F|D_{\mathrm{src}},D_{\mathrm{tgt}})\):","F^{*} =\mathcal{F}_{\theta}(D_{\mathrm{src}},D_{\mathrm{tgt}})\approx \underset{F}{\mathrm{argmax}}\ p(F|D_{\mathrm{src}},D_{\mathrm{tgt}}) || =\underset{F}{\mathrm{argmax}}\{\underbrace{\log p(D_{\mathrm{ src}},D_{\mathrm{tgt}}|F)}_{\text{data term}}+\underbrace{\log p(F)}_{\text{prior term}}\}.",,"\begin{split} F^{*} = \operatorname*{argmax}_{F} \left[ \log p(D_{\mathrm{src}}, D_{\mathrm{tgt}} | F) + \log p(F) \right], \end{split}",where the objective explicitly maximizes the posterior probability of the correspondence field by jointly considering both the data term and the prior term.
ICLR_2024_oral_14,6,"We leverage the capacity of a conditional diffusion model, which generates high-fidelity and diverse samples aligned with the given conditions, to search for accurate matching within the learned correspondence manifold.

Specifically, we define the forward diffusion process for dense correspondence as the Gaussian transition such that \(q(F_{t}|F_{t-1}):=\mathcal{N}(\sqrt{1-\beta_{t}}F_{t-1},\beta_{t}I)\), where \(\beta_{t}\) is a predefined variance schedule. The resulting latent variable \(F_{t}\) can be formulated as Eq. 2:","F_{t}=\sqrt{\alpha_{t}}F_{0}+\sqrt{1-\alpha_{t}}Z,\quad Z\sim\mathcal{N}(0,I),",where \(F_{0}\) is the ground-truth correspondence,"F_{t}=\sqrt{\alpha_{t}}F_{0}+\sqrt{1-\alpha_{t}}Z,\quad Z\sim\mathcal{N}(0,I),",where $\alpha_{t}=\prod_{i=1}^{t}(1-\beta_{i})$
ICLR_2024_oral_14,7,"ce. In addition, the neural network \(\mathcal{F}_{\theta}(\cdot)\) is subsequently trained to reverse the forward diffusion process. During the reverse diffusion phase, the initial latent variable \(F_{T}\) is iteratively denoised following the sequence \(F_{T-1},F_{T-2},\ldots,F_{0}\), using Eq. 3:","F_{t-1}=\sqrt{\alpha_{t-1}}\mathcal{F}_{\theta}(X_{t},t;D_{\mathrm{src}},D_{ \mathrm{tgt}})+\frac{\sqrt{1-\alpha_{t-1}-\sigma_{t}^{2}}}{\sqrt{1-\alpha_{t }}}\Big{(}X_{t}-\sqrt{\alpha_{t}}\mathcal{F}_{\theta}(F_{t},t;D_{\mathrm{src} },D_{\mathrm{tgt}})\Big{)}+\sigma_{t}Z,","where \(\mathcal{F}_{\theta}(F_{t},t;D_{\mathrm{src}},D_{\mathrm{tgt}})\) directly predicts the denoised correspondence \(\hat{F}_{0,t}\) with source and target features, \(D_{\mathrm{src}}\) and \(D_{\mathrm{tgt}}\), as conditions","F_{t-1}=\sqrt{\alpha_{t-1}}\mathcal{F}_{\theta}(F_{t},t;C^{l},F_{\text{init}})+\frac{\sqrt{1-\alpha_{t-1}-\sigma_{t}^{2}}}{\sqrt{1-\alpha_{t}}}\Big{(}F_{t}-\sqrt{\alpha_{t}}\mathcal{F}_{\theta}(F_{t},t;C^{l},F_{\text{init}})\Big{)}+\sigma_{t}Z,","where $C^{l}$ and $F_{\text{init}}$ are the local matching cost and the initial correspondence used as conditions, respectively."
ICLR_2024_oral_14,8,"ns.

The objective of this denoising process is to find the optimal correspondence field \(F^{*}\) that satisfies \(\underset{F}{\mathrm{argmax}}_{F}\log p(F|D_{\mathrm{src}},D_{\mathrm{tgt}})\). The detailed explanation of the objective function of the denoising process will be explained in Section 4.5.

Figure 3: **Visualization of the reverse diffusion process in DiffMatch:** (from left to right) source and target images, and warped source images by estimated correspondences as evolving time steps. The source image is progressively warped into the target image through an iterative denoising process.

### Network architecture

In this section, we discuss how to design the network architecture \(\mathcal{F}_{\theta}(\cdot)\). Our goal is to find accurate matching fields given feature descriptors \(D_{\text{src}}\) and \(D_{\text{tgt}}\) from \(I_{\text{src}}\) and \(I_{\text{tgt}}\), respectively, as conditions. An overview of our proposed architecture is provided in Figure 2.

**Cost computation.** Following conventional methods (Rocco et al., 2020; Truong et al., 2020), we first compute the matching cost by calculating the pairwise cosine similarity between localized deep features from the source and target images. Given image features \(D_{\text{src}}\) and \(D_{\text{tgt}}\), the matching cost is constructed by taking scalar products between all locations in the feature descriptors, formulated as:","C(i,j)=\frac{D_{\text{src}}(i)\cdot D_{\text{tgt}}(j)}{\|D_{\text{src}}(i) \|\|D_{\text{tgt}}(j)\|},","where \(i\in[0,h_{\text{src}})\times[0,w_{\text{src}})\), \(j\in[0,h_{\text{tgt}})\times[0,w_{\text{tgt}})\), and \(\|\cdot\|\) denotes \(l\)-2 normalization","C(i,j)=\frac{D_{\text{src}}(i)^{\top}D_{\text{tgt}}(j)}{\|D_{\text{src}}(i)\|\cdot\|D_{\text{tgt}}(j)\|},","where $C(i,j)$ represents the cosine similarity between feature descriptors at locations $i$ and $j$"
ICLR_2024_oral_14,9,"on.

Forming the global matching cost by computing all pairwise feature dot products is robust to long-range matching. However, it is computationally unfeasible due to its high dimensionality such that \(C\in\mathbb{R}^{h_{\text{src}}\times w_{\text{src}}\times h_{\text{tgt}} \times w_{\text{tgt}}}\). To alleviate this, we can build the local matching cost by narrowing down the target search region \(j\) within a neighborhood of the source location \(i\), constrained by a search radius \(R\). Compared to the global matching cost, the local matching cost \(C^{l}\in\mathbb{R}^{h_{\text{src}}\times w_{\text{src}}\times R\times R}\) is suitable for small displacements and, thanks to its constrained search range of \(R\), is more feasible for large spatial sizes and can be directly used as a condition for the diffusion model. Importantly, the computational overhead remains minimal, with the only significant increase being \(R\times R\) in the channel dimension.

**Conditional denoising diffusion model.** As illustrated in Figure 2, we introduce a modified U-Net architecture based on (Nichol and Dhariwal, 2021). Our aim is to generate an accurate matching field that aligns with the given conditions. A direct method to condition the model is simply concatenating \(D_{\text{src}}\) and \(D_{\text{tgt}}\) with the noisy flow input \(F_{\text{t}}\). However, this led to suboptimal performance in our tests. Instead, we present two distinct conditions for our network: the initial correspondence and the local matching cost.

First, our model is designed to learn the residual of the initially estimated correspondence, which leads to improved initialization and enhanced stability. Specifically, we calculate the initial correspondence \(F_{\text{init}}\) using the soft-argmax operation (Cho et al., 2021) based on the global matching cost \(C\) between \(D_{\text{src}}\) and \(D_{\text{tgt}}\). This assists the model to find long-range matches. Secondly, we integrate pixel-wise interactions between paired images. For this, each pixel \(i\) in the source image is mapped to \(i^{\prime}\) in the target image through the estimated initial correspondence \(F_{\text{init}}\). We then compute the local matching cost \(C^{l}\) as an additional condition with \(F_{\text{init}}\). This local cost guides the model to focus on the neighborhood of the initial estimation, helping to find a more refined matching field. With these combined, our conditioning strategies enable the model to precisely navigate the matching field manifold while preserving its generative capability. Finally, under the conditions \(F_{\text{init}}\) and \(C^{l}\), the noised matching field \(F_{t}\) at time step \(t\) passes through the modified U-net (Nichol and Dhariwal, 2021), which comprises convolution and attention, and generates the denoised matching field \(\hat{F}_{t,0}\) aligned with the given conditions.

### Flow upsampling

The inherent input resolution limitations of the diffusion model is a major hindrance. Inspired by recent super-resolution diffusion models (Ho et al., 2022; Ryu and Ye, 2022), we propose a cascaded pipeline tailored for flow upsampling. Our approach begins with a low-resolution denoising diffusion model, followed by a super-resolution model, successively upsampling and adding fine-grained details to the matching field. To achieve this, we simply finetune the pre-trained conditional denoising diffusion model, which was trained at a coarse resolution. Specifically, instead of using \(F_{\text{init}}\) from the global matching cost, we opt for a downsampled ground-truth flow field as \(F_{\text{init}}\). This simple modification effectively harnesses the power of the pretrained diffusion model for flow upsampling. The efficacy of our flow upsampling model is demonstrated in Table 4.

### Training

In training phase, the denoising diffusion model, as illustrated in Section 4.3, learns the prior knowledge of the matching field with the initial correspondence \(F_{\mathrm{init}}\) to give a matching hint and the local matching cost \(C^{l}\) to provide additional pixel-wise interactions. In other words, we redefine the network \(\mathcal{F}_{\theta}(F_{t},t;D_{\mathrm{src}},D_{\mathrm{tgt}})\) as \(\mathcal{F}_{\theta}(F_{t},t;F_{\mathrm{init}},C^{l})\), given that \(F_{\mathrm{init}}\) and \(C^{l}\) are derived from \(D_{\mathrm{src}}\) and \(D_{\mathrm{tgt}}\) as described in Section 4.3. The loss function for training diffusion model is defined as follows:","\mathcal{L}=\mathbb{E}_{F_{0},t,Z\sim\mathcal{N}(0,I),D_{\mathrm{src}},D_{ \mathrm{tgt}}}[\|F_{0}-\mathcal{F}_{\theta}(F_{t},t;F_{\mathrm{init} },C^{l})\|^{2}].",,"\mathcal{L}_{\text{diffusion}} = \mathbb{E}_{F_{0},Z,t}\left[\left\| \epsilon_{\theta}(F_{t},t;F_{\text{init}},C^{l}) - Z \right\|^{2}\right],","where $ \epsilon_{\theta}(\cdot) $ is the neural network that predicts the noise $ Z $ added to the ground-truth correspondence field $ F_{0} $ at time step $ t $, given the initial correspondence $ F_{\text{init}} $ and the local matching cost $ C^{l} $ as conditions."
ICLR_2024_oral_15,1,"### Generative Gaussian Splatting

Gaussian splatting (Kerbl et al., 2023) represents 3D information with a set of 3D Gaussians. It has been proven effective in reconstruction settings (Kerbl et al., 2023; Laiten et al., 2023) with high inference speed and reconstruction quality under similar modeling time with NeRF. However, its usage in a generative manner has not been explored. We identify that the 3D Gaussians can be efficient for 3D generation tasks too.

Specifically, the location of each Gaussian can be described with a center \(\mathbf{x}\in\mathbb{R}^{3}\), a scaling factor \(\mathbf{s}\in\mathbb{R}^{3}\), and a rotation quaternion \(\mathbf{q}\in\mathbb{R}^{4}\). We also store an opacity value \(\alpha\in\mathbb{R}\) and a color feature \(\mathbf{c}\in\mathbb{R}^{3}\) for volumetric rendering. Spherical harmonics are disabled since we only want to model simple diffuse color. All the above optimizable parameters is presented by \(\Theta\), where \(\Theta_{i}=\{\mathbf{x}_{i},\mathbf{s}_{i},\mathbf{q}_{i},\alpha_{i},\mathbf{ c}_{i}\}\) is the parameter for the \(i\)-th Gaussian. To render a set of 3D Gaussians, we need to project them onto the image plane as 2D Gaussians. Volumetric rendering is then performed for each pixel in front-to-back depth order to evaluate the final color and alpha. In this work, we use the highly optimized renderer implementation from Kerbl et al. (2023) to optimize \(\Theta\).

We initialize the 3D Gaussians with random positions sampled inside a sphere, with unit scaling and no rotation. These 3D Gaussians are periodically densified during optimization. Different from the reconstruction pipeline, we start from fewer Gaussians but density it more frequently to align with the generation progress. We follow the recommended practices from previous works (Poole et al., 2022; Huang et al., 2023; Lin et al., 2023) and use SDS to optimize the 3D Gaussians (Please refer to Section A.1 for more details on SDS loss). At each step, we sample a random camera pose \(p\) orbiting the object center, and render the RGB image \(I_{\text{RGB}}^{p}\) and transparency \(I_{\text{A}}^{p}\) of the current view. Similar to Dreamtime (Huang et al., 2023), we decrease the timestep \(t\) linearly during training, which is used to weight the random noise \(\epsilon\) added to the rendered RGB image. Then, different 2D diffusion priors \(\phi\) can be used to optimize the underlying 3D Gaussians through SDS.

**Image-to-3D.** For the image-to-3D task, an image \(\tilde{I}_{\text{RGB}}^{r}\) and a foreground mask \(\tilde{I}_{\text{A}}^{r}\) are given as input. Zero-1-to-3 XL (Liu et al., 2023; Deitke et al., 2023) is adopted as the 2D diffusion prior. The

Figure 2: **DreamGaussian Framework. 3D Gaussians are used for efficient initialization of geometry and appearance using single-step SDS loss. We then extract a textured mesh and refine the texture image with a multi-step MSE loss.**

SDS loss can be formulated as:","\nabla_{\Theta}\mathcal{L}_{\text{SDS}}=\mathbb{E}_{t,p,\epsilon}[w(t)( \epsilon_{\phi}(I_{\text{RGB}}^{p};t,\tilde{I}_{\text{RGB}}^{r},\Delta p)- \epsilon)\frac{\partial I_{\text{RGB}}^{p}}{\partial\Theta}]","where \(w(t)\) is a weighting function, \(\epsilon_{\phi}(\cdot)\) is the predicted noise by the 2D diffusion prior \(\phi\), and \(\Delta p\) is the relative camera pose change from the reference camera \(r\)","\mathcal{L}_{\text{SDS}} = \mathbb{E}_{t,\epsilon,p}\left[\left\|\nabla_{\mathbf{x}}\log p_{\phi}(I_{\text{RGB}}^{p} + \sqrt{t}\epsilon \mid t) - \nabla_{\mathbf{x}}\log p_{\theta}(I_{\text{RGB}}^{p} + \sqrt{t}\epsilon \mid t)\right\|^{2}\right],",The SDS loss minimizes the difference between score gradients of the diffusion prior and the generated image with respect to injected noise.
ICLR_2024_oral_15,2,"\). Additionally, we optimize the reference view image \(I_{\text{RGB}}^{r}\) and transparency \(I_{\text{A}}^{r}\) to align with the input:",\mathcal{L}_{\text{Ref}}=\lambda_{\text{RGB}}||I_{\text{RGB}}^{r}-\tilde{I}_ {\text{RGB}}^{r}||_{2}^{2}+\lambda_{\text{A}}||I_{\text{A}}^{r}-\tilde{I}_{ \text{A}}^{r}||_{2}^{2},where \(\lambda_{\text{RGB}}\) and \(\lambda_{\text{A}}\) are the weights which are linearly increased during training,\mathcal{L}_{\text{img}}=\left\|I_{\text{RGB}}^{r}-\tilde{I}_{\text{RGB}}^{r}\right\|_{2}+\lambda_{\text{A}}\left\|I_{\text{A}}^{r}-\tilde{I}_{\text{A}}^{r}\right\|_{1},"Additionally, we optimize the reference view image and transparency to align with the input by minimizing the L2 loss between rendered and input images, and the L1 loss between rendered and input transparency."
ICLR_2024_oral_15,3,"ng. The final loss is the weighted sum of the above three losses.

**Text-to-3D.** The input for text-to-3D is a single text prompt. Following previous works, Stable-diffusion (Rombach et al., 2022) is used for the text-to-3D task. The SDS loss can be formulated as:","\nabla_{\Theta}\mathcal{L}_{\text{SDS}}=\mathbb{E}_{t,p,\epsilon}[w(t)( \epsilon_{\phi}(I_{\text{RGB}}^{p};t,e)-\epsilon)\frac{\partial I_{\text{RGB }}^{p}}{\partial\Theta}]",where \(e\) is the CLIP embeddings of the input text description,"\nabla_{\Theta}\mathcal{L}_{\text{SDS}}=\mathbb{E}_{t,p,\epsilon}[w(t)( \epsilon_{\phi}(I_{\text{RGB}}^{p};t,\tilde{I}_{\text{RGB}}^{r},\Delta p)- \epsilon)\frac{\partial I_{\text{RGB}}^{p}}{\partial\Theta}]","where $ \epsilon_{\phi}(\cdot) $ is the predicted noise by the 2D diffusion prior $ \phi $, $ w(t) $ is a weighting function, and $ \Delta p $ is the relative camera pose change from the reference camera $ r $"
ICLR_2024_oral_15,4,"on.

**Discussion.** We observe that _the generated Gaussians often look blurry and lack details even with longer SDS training iterations_. This could be explained by the ambiguity of SDS loss. Since each optimization step may provide inconsistent 3D guidance, it's hard for the algorithm to correctly densify the under-reconstruction regions or prune over-reconstruction regions as in reconstruction. This observation leads us to the following mesh extraction and texture refinement designs.

### Efficient Mesh Extraction

Polygonal mesh is a widely used 3D representation, particularly in industrial applications. Many previous works (Poole et al., 2022; Lin et al., 2023; Tsalicoglou et al., 2023; Tang et al., 2023) export the NeRF representation into a mesh-based representation for high-resolution fine-tuning. We also seek to convert the generated 3D Gaussians into meshes and further refine the texture.

To the best of our knowledge, the polygonal mesh extraction from 3D Gaussians is still an unexplored problem. _Since the spatial density is described by a large number of 3D Gaussians, brute-force querying of a dense 3D density grid can be slow and inefficient._ It's also unclear how to extract the appearance in 3D, as the color blending is only defined with projected 2D Gaussians (Kerbl et al., 2023). Here, we propose an efficient algorithm to extract a textured mesh based on block-wise local density query and back-projected color.

**Local Density Query.** To extract the mesh geometry, a dense density grid is needed to apply the Marching Cubes (Lorensen and Cline, 1998) algorithm. An important feature of the Gaussian splatting algorithm is that over-sized Gaussians will be split or pruned during optimization. This is the foundation of the tile-based culling technique for efficient rasterization (Kerbl et al., 2023). We also leverage this feature to perform block-wise density queries.

We first divide the 3D space of \((-1,1)^{3}\) into \(16^{3}\) overlapping blocks, then cull the Gaussians whose centers are located outside each local block. This effectively reduces the total number of Gaussians to query in each block. We then query a \(8^{3}\) dense grid inside each block, which leads to a final \(128^{3}\) dense grid. For each query at grid position \(\mathbf{x}\), we sum up the weighted opacity of each remained 3D Gaussian:",d(\mathbf{x})=\sum_{i}\alpha_{i}\exp(-\frac{1}{2}(\mathbf{x}-\mathbf{x}_{i})^ {T}\Sigma_{i}^{-1}(\mathbf{x}-\mathbf{x}_{i})),where \(\Sigma_{i}\) is the covariance matrix built from scaling \(\mathbf{s}_{i}\) and rotation \(\mathbf{q}_{i}\),\sigma(\mathbf{x}) = \sum_{i} \alpha_i \exp\left(-\frac{1}{2} (\mathbf{x} - \mathbf{x}_i)^T \mathbf{Q}_i \text{diag}(\mathbf{s}_i)^{-2} \mathbf{Q}_i^T (\mathbf{x} - \mathbf{x}_i)\right),"where $\alpha_i$ is the opacity and $\mathbf{x}_i$, $\mathbf{s}_i$, $\mathbf{q}_i$ are the position, scaling, and rotation of the $i$-th Gaussian"
ICLR_2024_oral_15,5,"\). An empirical threshold is then used to extract the mesh surface through Marching Cubes. Decimation and remeshing (Cignoni et al., 2008) are applied to post-process the extracted mesh to make it smoother and more compact.

**Color Back-projection.** Since we have acquired the mesh geometry, we can back-project the rendered RGB image to the mesh surface and bake it as the texture. We first unwrap the mesh's UV coordinates (Young, 2021) (detailed in Section A.1) and initialize an empty texture image. Then, we uniformly choose 8 azimuths and 3 elevations, plus the top and bottom views to render the corresponding RGB image. Each pixel from these RGB images can be back-projected to the texture image based on its UV coordinate. Following Richardson et al. (2023), we exclude the pixels with a small camera space z-direction normal to avoid unstable projection at mesh boundaries. This back-projected texture image serves as an initialization for the next texture fine-tuning stage.

### UV-space Texture Refinement

We further use a second stage to refine the extracted coarse texture. Different from texture generation (Richardson et al., 2023; Chen et al., 2023; Cao et al., 2023), we hope to enhance the details given a coarse texture. However, fine-tuning the UV-space directly with SDS loss leads to artifacts as shown in Figure 3, which is also observed in previous works (Liao et al., 2023). This is due to the impmap texture sampling technique used in differentiable rasterization (Laine et al., 2020). With ambiguous guidance like SDS, the gradient propagated to each impmap level results in over-saturated color blocks. Therefore, we seek more definite guidance to fine-tune a blurry texture.

We draw inspiration from the image-to-image synthesis of SDEdit (Meng et al., 2021) and the reconstruction settings. Since we already have an initialization texture, we can render a blurry image \(I^{p}_{\text{coarse}}\) from an arbitrary camera view \(p\). Then, we perturb the image with random noise and apply a multi-step denoising process \(f_{\phi}(\cdot)\) using the 2D diffusion prior to obtaining a refined image:","I^{p}_{\text{fine}}=f_{\phi}(I^{p}_{\text{coarse}}+\epsilon(t_{\text{start}}) ;t_{\text{start}},c)","where \(\epsilon(t_{\text{start}})\) is a random noise at timestep \(t_{\text{start}}\), \(c\) is \(\Delta p\) for image-to-3D and \(e\) for text-to-3D respectively",I^{p}_{\text{refined}} = f_{\phi}(I^{p}_{\text{coarse}} + \text{noise}),
ICLR_2024_oral_15,6,"ly. The starting timestep \(t_{\text{start}}\) is carefully chosen to limit the noise strength, so the refined image can enhance details without breaking the original content. This refined image is then used to optimize the texture through a pixel-wise MSE loss:",\mathcal{L}_{\text{MSE}}=||I^{p}_{\text{fine}}-I^{p}_{\text{coarse}}||^{2}_{2},,\mathcal{L}_{\text{MSE}}=\lambda_{\text{MSE}}||I^{p}_{\text{fine}}-I^{p}_{ \text{coarse}}||_{2}^{2},where $\lambda_{\text{MSE}}$ is the weight for the pixel-wise MSE loss used to refine the texture image
ICLR_2024_oral_16,1,"### Episodic control in MARL

Episodic control was introduced from the analogy of a brain's hippocampus for memory utilization (Lengyel and Dayan, 2007). After the introduction of deep Q-network, Blundell et al. (2016) adopted this idea of episodic control to the model-free setting by storing the highest return of a given state, to efficiently estimate the Q-values of the state. This recalling of the high-reward experiences helps to increase sample efficiency and thus expedites the overall learning process (Blundell et al., 2016; Pritzel et al., 2017; Lin et al., 2018). Please see Appendix A for related works and further discussions.

At timestep \(t\), let us define a global state as \(s_{t}\). When utilizing episodic control, instead of directly using \(s_{t}\), researchers adopt a state embedding function \(f_{\phi}(s):S\rightarrow\mathbb{R}^{k}\) to project states toward a \(k\)-dimensional vector space. With this projection, a representation of global state \(s_{t}\) becomes \(x_{t}=f_{\phi}(s_{t})\). The episodic control memorizes \(H(f_{\phi}(s_{t}))\), i.e., the highest return of a given global state \(s_{t}\), in episodic buffer \(\mathcal{D}_{E}\)(Pritzel et al., 2017; Lin et al., 2018; Zheng et al., 2021). Here, \(x_{t}\) is used as a key to the highest return, \(H(x_{t})\); as a key-value pair in \(\mathcal{D}_{E}\). The episodic control in Lin et al. (2018) updates \(H(x_{t})\) with the following rules.","H(x_{t})=\{\begin{array}{cc}\max\{H(\hat{x}_{t}),R_{t}(s_{t},\mathbf{a_{t} })\},&\text{if}\;||\hat{x}_{t}-x_{t}||_{2}<\delta\\ R_{t}(s_{t},\mathbf{a_{t}}),&\text{otherwise}\;,\end{array}.","where \(R_{t}(s_{t},\mathbf{a_{t}})\) is the return of a given \((s_{t},\mathbf{a_{t}})\); \(\delta\) is a threshold value of state-embedding difference; and \(\hat{x}_{t}=f_{\phi}(\hat{s}_{t})\) is \(x_{t}=f_{\phi}(s_{t})\)'s nearest neighbor in \(\mathcal{D}_{E}\)","H(x_t) \leftarrow \max\left(H(x_t), G_t\right)","Updates the highest return $H(x_t)$ in the episodic buffer $\mathcal{D}_{E}$ to the maximum of its current value and the observed return $G_t$, following Lin et al. (2018)."
ICLR_2024_oral_16,2,"\). If there is no similar projected state \(\hat{x}_{t}\) such that \(||\hat{x}_{t}-x_{t}||_{2}<\delta\) in the memory, then \(H(x_{t})\) keeps the current \(R_{t}(s_{t},\mathbf{a_{t}})\). Leveraging the episodic memory, EMC (Zheng et al., 2021) presents the one-step TD memory target \(Q_{EC}(f_{\phi}(s_{t}),\mathbf{a_{t}})\) as","Q_{EC}(f_{\phi}(s_{t}),\mathbf{a_{t}})=r_{t}(s_{t},\mathbf{a_{t}})+\gamma H(f_{\phi}(s_ {t+1})).",,"Q_{EC}(f_{\phi}(s_{t}),\mathbf{a_{t}}) = R_{t}(s_{t},\mathbf{a_{t}}) + \gamma \cdot \max_{\mathbf{a}} H(f_{\phi}(s_{t+1})),",where $\gamma$ is the discount factor and $H(f_{\phi}(s_{t+1}))$ represents the highest return of the next state's embedding.
ICLR_2024_oral_16,3,"Then, the loss function \(L_{\theta}^{EC}\) for training can be expressed as the weighted sum of one-step TD error and one-step TD memory error, i.e., Monte Carlo (MC) inference error, based on \(Q_{EC}(f_{\phi}(s_{t}),\mathbf{a_{t}})\).","L_{\theta}^{EC}=(y(s,\mathbf{a})-Q_{tot}(s,\mathbf{a};\theta))^{2}+\lambda(Q_{EC}(f_{ \phi}(s),\mathbf{a})-Q_{tot}(s,\mathbf{a};\theta))^{2},","where \(y(s,\mathbf{a})\) is one-step TD target; \(Q_{tot}\) is the joint Q-value function parameterized by \(\theta\); and \(\lambda\) is a scale factor","L_{\theta}^{EC} = \mathbb{E}_{t}\left[\left(Q_{EC}(f_{\phi}(s_{t}),\mathbf{a_{t}}) - Q_{\theta}(s_{t},\mathbf{a_{t}})\right)^2 + \lambda\left(R_{t}(s_{t},\mathbf{a_{t}}) - Q_{\theta}(s_{t},\mathbf{a_{t}})\right)^2\right].","Here, $\lambda$ is a hyperparameter that balances the contribution of the one-step TD error and the Monte Carlo inference error in the loss function."
ICLR_2024_oral_16,4,"or.

Problem of the conventional episodic control with random projectionRandom projection is useful for dimensionality reduction as it preserves distance relationships, as demonstrated by the Johnson-Lindenstrauss lemma (Dasgupta and Gupta, 2003). However, a random projection adopted for \(f_{\phi}(s)\) hardly has a semantic meaning in its embedding \(x_{t}\), as it puts random weights on the state features without considering the patterns of determining the state returns. Additionally, when recalling the memory from \(\mathcal{D}_{E}\), the projected state \(x_{t}\) can abruptly change even with a small change of \(s_{t}\) because the embedding is not being regulated by the return. This results in a sparse selection of semantically similar memories, i.e. similar states with similar or better rewards. As a result, conventional episodic control using random projection only recalls identical states and relies on its own Monte-Carlo (MC) return to regulate the one-step TD target inference, limiting exploration of nearby states on the embedding space.

The problem intensifies when the high-return states in the early training phase are indeed local optima. In such cases, the naive utilization of episodic control is prone to converge on local minima. As a result, for the super hard tasks of SMAC, EMC (Zheng et al., 2021) had to decrease the magnitude of this regularization to almost zero, i.e., not considering episodic memories anymore.

## 3 Methodology

This section introduces **E**fficient episodic **M**emory **U**tilization (EMU) (Figure 1). We begin by explaining how to construct **(1) semantic memory embeddings** to better utilize the episodic memory, which enables memory recall of similar, more promising states. To further improve memory utilization, as an alternative to the conventional episodic control, we propose **(2) episodic incentive** that selectively encourages desirable transitions while preventing local convergence towards undesirable trajectories.

### Semantic Memory Embedding

**Episodic Memory Construction** To address the problems of a random projection adopted in episodic control, we propose a trainable embedding function \(f_{\phi}(s)\) to learn the state embedding patterns affected by the highest return. The problem of a learnable embedding network \(f_{\phi}\) is that the match between \(H(f_{\phi}(s_{t}))\) and \(s_{t}\) breaks whenever \(f_{\phi}\) is updated. Hence, we save the global state \(s_{t}\) as well as a pair of \(H_{t}\) and \(x_{t}\) in \(\mathcal{D}_{E}\), so that we can update \(x=f_{\phi}(s)\) whenever \(f_{\phi}\) is updated. In addition, we store the desirability \(\xi\) of \(s_{t}\) according to Definition 1. Appendix E.1 illustrates the details of memory construction proposed by this paper.

**Learning framework for State Embedding** When training \(f_{\phi}(s_{t})\), it is critical to extract important features of a global state that affect its value, i.e., the highest return. Thus, we additionally adopt a decoder structure \(\bar{H}_{t}=f_{\psi}(x_{t})\) to predict the highest return \(H_{t}\) of \(s_{t}\). We call this embedding function as **EmbNet**, and its learning objective of \(f_{\phi}\) and \(f_{\psi}\) can be written as","\mathcal{L}(\mathbf{\phi},\mathbf{\psi})=(H_{t}-f_{\psi}(f_{\phi}(s_{t})))^ {2}.",,"\min_{\phi,\psi}\mathbb{E}_{s_{t}\sim\mathcal{D}_{E}}\left[\left(H(f_{\phi}(s_{t})) - f_{\psi}(f_{\phi}(s_{t}))\right)^{2}\right].","where $H(f_{\phi}(s_{t}))$ is the highest return of the embedded state, and $f_{\psi}(f_{\phi}(s_{t}))$ is the predicted return via the decoder structure."
ICLR_2024_oral_16,5,"When constructing the embedding space, we found that an additional consideration of reconstruction of state \(s\) conditioned on timestep \(t\) improves the quality of feature extraction and constitutes a smoother embedding space. To this end, we develop the deterministic conditional autoencoder (**dCAE**), and the corresponding loss function can be expressed as","\mathcal{L}(\mathbf{\phi},\mathbf{\psi})=(H_{t}-f_{\psi}^{H}(f_{\phi}(s_{t}|t)|t ))^{2}+\lambda_{rcon}||s_{t}-f_{\psi}^{s}(f_{\phi}(s_{t}|t)|t)||_{2}^{2},",where \(f_{\psi}^{H}\) predicts the highest return; \(f_{\psi}^{s}\) reconstructs \(s_{t}\); \(\lambda_{rcon}\) is a scale factor,"\mathcal{L}_{dCAE}(\mathbf{\phi},\mathbf{\psi},\mathbf{\theta})=(H_{t}-f_{\psi}(f_{\phi}(s_{t})))^{2} + \beta||s_{t}-f_{\theta}(f_{\phi}(s_{t}),t)||_{2}^{2},","where the first term ensures accurate prediction of the highest return, the second term reconstructs the state conditioned on the timestep, and $\beta$ is a weighting coefficient balancing the two objectives."
ICLR_2024_oral_16,6,"or. Here, \(f_{\psi}^{H}\) and \(f_{\psi}^{s}\) share the lower part of networks as illustrated in Figure 1(c). Appendix C.1 presents the details of network structure of \(f_{\phi}\) and \(f_{\psi}\), and Algorithm 1 in Appendix C.1 presents the learning framework for \(f_{\phi}\) and \(f_{\psi}\). This training is conducted periodically in parallel to the RL policy learning on \(Q_{tot}(\cdot;\theta)\).

Figure 2 illustrates the result of t-SNE (Van der Maaten and Hinton, 2008) of 50K samples of \(x\in\mathcal{D}_{E}\) out of 1M memory data in training for 3s_vs_5z task of SMAC. Unlike supervised learning with label data, there is no label for each \(x_{t}\). Thus, we mark \(x_{t}\) with its pair of the highest return \(H_{t}\). Compared to a random projection in Figure 2(a), \(x_{t}\) via \(f_{\phi}\) is well-clustered, according to the similarity of the embedded state and its return. This clustering of \(x_{t}\) enables us to safely select

Figure 1: Overview of EMU framework.

episodic memories around the key state \(s_{t}\), which constitutes efficient memory utilization. This memory utilization expedites learning speed as well as encourages exploration to a more promising state \(\hat{s}_{t}\) near \(s_{t}\). Appendix F illustrates how to determine \(\delta\) of Eq. 1 in a memory-efficient way.

### Episodic Incentive

With the learnable memory embedding for an efficient memory recall, how to use the selected memories still remains a challenge because a naive utilization of episodic memory is prone to converge on local minima. To solve this issue, we propose a new reward structure called **episodic incentive**\(r^{p}\) by leveraging the desirability \(\xi\) of states in \(\mathcal{D}_{E}\). Before deriving the episodic incentive \(r^{p}\), we first need to understand the characteristics of episodic control. In this section, we denote the joint Q-function \(Q_{tot}(\cdot;\theta)\) simply as \(Q_{\theta}\) for conciseness.

**Theorem 1**.: _Given a transition \((s,\mathbf{a},r,s^{\prime})\) and \(H(x^{\prime})\), let \(L_{\theta}\) be the Q-learning loss with additional transition reward, i.e., \(L_{\theta}:=(y(s,\mathbf{a})+r^{EC}(s,\mathbf{a},s^{\prime})-Q_{tot}(s,\mathbf{a};\theta)) ^{2}\) where \(r^{EC}(s,\mathbf{a},s^{\prime}):=\lambda(r(s,\mathbf{a})+\gamma H(x^{\prime})-Q_{ \theta}(s,\mathbf{a}))\), then \(\nabla_{\theta}L_{\theta}=\nabla_{\theta}L_{\theta}^{EC}\). (Proof in Appendix B.1)_

As Theorem 1 suggests, we can generate the same gradient signal as the episodic control by leveraging the additional transition reward \(r^{EC}(s,\mathbf{a},s^{\prime})\). However, \(r^{EC}(s,\mathbf{a},s^{\prime})\) accompanies a risk of local convergence as discussed in Section 2.3. Therefore, instead of applying \(r^{EC}(s,\mathbf{a},s^{\prime})\), we propose the episodic incentive \(r^{p}:=\gamma\hat{\eta}(s^{\prime})\) that provides an additional reward for the desirable transition \((s,\mathbf{a},r,s^{\prime})\), such that \(\xi(s^{\prime})=1\). Here, \(\hat{\eta}(s^{\prime})\) estimates \(\eta^{*}(s^{\prime})\), which represents the difference between the true value \(V^{*}(s^{\prime})\) of \(s^{\prime}\) and the predicted value via target network \(\max_{\mathbf{a}^{\prime}}\!Q_{\theta^{-}}(s^{\prime},\mathbf{a}^{\prime})\), defined as","\eta^{*}(s^{\prime}):=V^{*}(s^{\prime})-\max_{\mathbf{a}^{\prime}}\!Q_{\theta^{-}} (s^{\prime},\mathbf{a}^{\prime}).",,"\eta^{*}(s^{\prime})=V^{*}(s^{\prime})-\max_{\mathbf{a}^{\prime}}\!Q_{\theta^{-}}(s^{\prime},\mathbf{a}^{\prime}).",where $V^{*}(s^{\prime})$ is the optimal value function and $Q_{\theta^{-}}$ is the target network
ICLR_2024_oral_16,7,"Note that we do not know \(V^{*}(s^{\prime})\) and subsequently \(\eta^{*}(s^{\prime})\). To accurately estimate \(\eta^{*}(s^{\prime})\) with \(\hat{\eta}(s^{\prime})\), we use the expected value considering the current policy \(\pi_{\theta}\) as \(\hat{\eta}(s^{\prime}):=\mathbb{E}_{\pi_{\theta}}[\eta(s^{\prime})]\) where \(\eta\in[0,\eta_{\max}(s^{\prime})]\) for \(s^{\prime}\sim P(s^{\prime}|s,\mathbf{a}\sim\pi_{\theta})\). Here, \(\eta_{\max}(s^{\prime})\) can be reasonably approximated by using \(H(f_{\phi}(s^{\prime}))\) in \(\mathcal{D}_{E}\). Then, with the count-based estimation \(\hat{\eta}(s^{\prime})\), episodic incentive \(r^{p}\) can be expressed as","r^{p}=\gamma\hat{\eta}(s^{\prime})=\gamma\mathbb{E}_{\pi_{\theta}}[\eta(s^{ \prime})]\simeq\gamma\frac{N_{\xi}(s^{\prime})}{N_{call}(s^{\prime})}\eta_{ \max}(s^{\prime})=\gamma\frac{N_{\xi}(s^{\prime})}{N_{call}(s^{\prime})}(H(f_ {\phi}(s^{\prime}))-\max_{a^{\prime}}\!Q_{\theta^{-}}(s^{\prime},a^{\prime})),",where \(N_{call}(s^{\prime})\) is the number of visits on \(\hat{x}^{\prime}=\mathrm{NN}(f_{\phi}(s^{\prime}))\in\mathcal{D}_{E}\); and \(N_{\xi}\) is the number of desirable transition from \(\hat{x}^{\prime}\),"r^{p}(s,\mathbf{a},s^{\prime})=\gamma\cdot\hat{\eta}(s^{\prime})\cdot\mathbb{I}[\xi(s^{\prime})=1],","where $\hat{\eta}(s^{\prime})$ is the estimated advantage of $s^{\prime}$, $\xi(s^{\prime})$ indicates desirability of $s^{\prime}$, and $\mathbb{I}[\cdot]$ is the indicator function."
ICLR_2024_oral_16,8,"\). Here, \(\mathrm{NN}(\cdot)\) represents a function for selecting the nearest neighbor. From Theorem 1, the loss function adopting episodic control with an alternative transition reward \(r^{p}\) instead of \(r^{EC}\) can be expressed as","L_{\theta}^{p}=(r(s,\mathbf{a})+r^{p}+\gamma\!\max_{\mathbf{a}^{\prime}}\!Q_{\theta^{-} }(s^{\prime},\mathbf{a}^{\prime})-Q_{\theta}(s,\mathbf{a}))^{2}.",,"L_{\theta}^{EI}=(y(s,\mathbf{a})+r^{p}(s,\mathbf{a},s^{\prime})-Q_{tot}(s,\mathbf{a};\theta))^{2},","where $L_{\theta}^{EI}$ denotes the loss function incorporating episodic incentive $r^{p}$ as an alternative transition reward in place of $r^{EC}$, aligning with the gradient signal while promoting desirable transitions and avoiding local convergence."
ICLR_2024_oral_16,9,"Then, the gradient signal of the one-step TD inference loss \(\nabla_{\theta}L_{\theta}^{p}\) with the episodic reward \(r^{p}=\gamma\hat{\eta}(s^{\prime})\) can be written as","\nabla_{\theta}L_{\theta}^{p}=-2\nabla_{\theta}Q_{\theta}(s,a)(\Delta\varepsilon _{TD}+r^{p})=-2\nabla_{\theta}Q_{\theta}(s,a)(\Delta\varepsilon_{TD}+\gamma \frac{N_{\xi}(s^{\prime})}{N_{call}(s^{\prime})}\eta_{\max}(s^{\prime})),",,"\nabla_{\theta}L_{\theta}^{p} = 2\left(r(s,\mathbf{a}) + r^{p} + \gamma\!\max_{\mathbf{a}^{\prime}}\!Q_{\theta^{-}}(s^{\prime},\mathbf{a}^{\prime}) - Q_{\theta}(s,\mathbf{a})\right)\nabla_{\theta}Q_{\theta}(s,\mathbf{a}),","where the gradient signal is derived from the episodic incentive-augmented TD loss, encouraging policy updates that favor transitions leading to desirable states with high estimated returns."
ICLR_2024_oral_16,10,"Figure 2: t-SNE of sampled embedding \(x\in\mathcal{D}_{E}\). Colors from red to purple (rainbow) represent from low return to high return.

where \(\Delta\varepsilon_{TD}=r(s,a)+\gamma\mathrm{max}_{a^{\prime}}Q_{\theta^{-}}(s^{ \prime},a^{\prime})-Q_{\theta}(s,a)\) is one-step inference TD error. Here, the gradient signal \(\nabla_{\theta}L_{\theta}^{p}\) with the proposed episodic reward \(r^{p}\) can accurately estimate the optimal gradient signal as follows.

**Theorem 2**.: _Let \(\nabla_{\theta}L_{\theta}^{*}=-2\nabla_{\theta}Q_{\theta}(s,a)(\Delta \varepsilon_{TD}^{*})\) be the optimal gradient signal with the true one step TD error \(\Delta\varepsilon_{TD}^{*}=r(s,a)+\gamma V^{*}(s^{\prime})-Q_{\theta}(s,a)\). Then, the gradient signal \(\nabla_{\theta}L_{\theta}^{p}\) with the episodic incentive \(r^{p}\) converges to the optimal gradient signal as the policy converges to the optimal policy \(\pi_{\theta}^{*}\), i.e., \(\nabla_{\theta}L_{\theta}^{p}\rightarrow\nabla_{\theta}L_{\theta}^{*}\) as \(\pi_{\theta}\rightarrow\pi_{\theta}^{*}\). (Proof in Appendix B.2)_

Theorem 2 also implies that there exists a certain bias in \(\nabla_{\theta}L_{\theta}^{EC}\) as described in Appendix B.2. Besides the property of convergence to the optimal gradient signal presented in Theorem 2, the episodic incentive has the following additional characteristics. (1) The episodic incentive is only applied to the desirable transition. We can simply see that \(r^{p}=\gamma\bar{\eta}=\gamma\bar{\mathbb{E}}_{\pi_{\theta}}[\eta]\simeq \gamma\eta_{max}N_{\xi}/N_{cail}\) and if \(\xi(s^{\prime})=0\) then \(N_{\xi}=0\), yielding \(r^{p}\to 0\). Subsequently, (2) there is no need to adjust a scale factor by the task complexity. (3) The episodic incentive can reduce the risk of overestimation by considering the expected value of \(\bar{\mathbb{E}}_{\pi_{\theta}}[\eta]\). Instead of considering the optimistic \(\eta_{max}\), the count-based estimation \(r^{p}=\gamma\hat{\eta}=\gamma\bar{\mathbb{E}}_{\pi_{\theta}}[\eta]\) can consider the randomness of the policy \(\pi_{\theta}\). Figure 3 illustrates how the episodic incentive works with the desirability stored in \(\mathcal{D}_{E}\) constructed by Algorithm 2 presented in Appendix E.1. In Figure 3 as we intended, high-value states (at small timesteps) are clustered close to the purple zone, while low-value states (at large timesteps) are located in the red zone.

### Overall Learning Objective

To construct the joint Q-function \(Q_{tot}\) from individual \(Q_{i}\) of the agent \(i\), any form of mixer can be used. In this paper, we mainly adopt the mixer presented in QPLEX (Wang et al., 2020) similar to Zheng et al. (2021), which guarantees the complete Individual-Global-Max (IGM) condition (Son et al., 2019; Wang et al., 2020). Considering any intrinsic reward \(r^{c}\) encouraging an exploration (Zheng et al., 2021) or diversity (Chenghao et al., 2021), the final loss function for the action policy learning from Eq. 8 can be extended as","\mathcal{L}_{\theta}^{p}=(r(s,\mathbf{a})+r^{p}+\beta_{c}r^{c}+\gamma\mathrm{ max}_{a^{\prime}}Q_{tot}(s^{\prime},\mathbf{a}^{\prime};\theta^{-})-Q_{tot}(s,\mathbf{a}; \theta))^{2},",where \(\beta_{c}\) is a scale factor,"L_{\theta}^{final}=L_{\theta}^{p}+\lambda_{c}(Q_{c}(s,\mathbf{a})-Q_{tot}(s,\mathbf{a};\theta))^{2},","where $L_{\theta}^{p}$ is the loss with episodic incentive, $Q_{c}(s,\mathbf{a})$ is the intrinsic reward module, and $\lambda_{c}$ is a scaling factor for the intrinsic reward loss."
ICLR_2024_oral_21,1,"#### 3.3.2 Building an ensemble graph classifier

Given a testing graph \(G\), we can use structure (or feature or structure-feature) division to divide it into \(N\) sub-graphs, where \(N=T_{s}\) (or \(N=T_{f}\) or \(N=T_{s}\cdot T_{f}\)). Given the \(N\) sub-graphs \(\{\mathcal{G}_{t}\}\) and a base graph classifier \(f\), we can use \(f\) to predict a label for each sub-graph. Note that we remove the node in the sub-graph if it simultaneously satisfies the following conditions: 1) it does not have any edge with all other nodes in the sub-graph, and 2) its feature vector is set to be a special value. The reason is that this kind of node does not provide any information for graph classification since it is not connected with any other nodes and its node feature is a special value. Suppose the set of all the possible classes is \(\{1,2,\cdots,C\}\). We use \(N_{c}\) to denote the number of sub-graphs that are predicted as the class \(c\) by the base graph classifier \(f\), i.e., \(N_{c}=\sum_{t=1}^{N}\mathbb{I}(f(\mathcal{G}_{t})=c)\), where \(c=1,2,\cdots,C\) and \(\mathbb{I}\) is the indicator function. Then, we define our ensemble graph classifier \(g\) as follows:","g(G)=\operatorname*{arg\,max}_{c\in\{1,2,\cdots,C\}}N_{c},",where a label with a smaller index is taken by our ensemble classifier when there are ties,"g(G) = \arg\max_{c \in \{1,2,\cdots,C\}} N_c",The ensemble graph classifier $g$ predicts the class $c$ that has the highest count $N_c$ among the predictions of the base classifier $f$ on the sub-graphs of $G$.
ICLR_2024_oral_21,2,"es. We denote \(l=g(G)\) as the predicted label. Then, \(g\) can provably predict the same label for \(G\) when the number of perturbed edges and/or number of nodes with perturbed features are bounded, as shown below.

#### 3.3.3 Deriving the provable robustness guarantees

Suppose we have an adversarially perturbed graph \(G^{p}\). Similarly, we use \(\mathcal{G}_{1}^{p},\mathcal{G}_{2}^{p},\cdots,\mathcal{G}_{N}^{p}\) to denote the \(N\) sub-graphs created from \(G^{p}\). Moreover, we denote by \(N_{c}^{p}=\sum_{t=1}^{N}\mathbb{I}(f(\mathcal{G}_{t}^{p})=c),\forall c\in\{1,2,\cdots,C\}\), i.e., \(N_{c}^{p}\) measures the number of sub-graphs created from \(G^{p}\) that are predicted as the class \(c\) by the base graph classifier \(f\). Due to the perturbed node features or graph edges, some sub-graphs would be corrupted. For simplicity, we use \(M\) to denote the total number of sub-graphsin \(\mathcal{G}_{1}^{p},\mathcal{G}_{2}^{p},\cdots,\mathcal{G}_{N}^{p}\) that are corrupted by the perturbed node features or graph edges. Then, we can derive the following lower or upper bounds:","N_{c}-M\leq N_{c}^{p}\leq N_{c}+M,\,\forall c\in\{1,2,\cdots,C\}.",,"N_{l}^{p} \geq N_{l} - M,",The number of sub-graphs predicted as the true class $l$ in the perturbed graph $G^p$ is at least the number of such sub-graphs in the original graph $G$ minus the number of corrupted sub-graphs $M$.
ICLR_2024_oral_21,3,"Our ensemble graph classifier \(g\) still predicts the label \(l\) for the perturbed graph \(G^{p}\) if \(N_{l}^{p}>\max_{c\in\{1,2,\cdots,C\}\setminus\{l\}}(N_{c}^{p}-\mathbb{I}(l<c))\), where the term \(\mathbb{I}(l<c)\) stems from our tie breaking mechanism (i.e., we take a label with a smaller index where there are ties). Then we have the following theorem:

**Theorem 1**.: _Given an arbitrary base graph classifier \(f\), our ensemble graph classifier \(g\) is as defined in Equation 1. Given a testing graph \(G\), we can use our structure (or feature or structure-feature) division to divide it into \(N\) sub-graphs. Suppose \(N_{c}\) is the number of sub-graphs predicted as the label \(c\) by the given base graph classifier \(f\), where \(c=1,2,\cdots,C\). Moreover, we assume \(M\) is the total number of corrupted sub-graphs created from a perturbed graph \(G^{p}\). Then, we have \(g(G)=g(G^{p})=l\) when the following condition is satisfied:_","M\leq M^{p}=\lfloor\frac{N_{l}-\max_{c\in\{1,2,\cdots,C\}\setminus\{l\}}(N_{c }-\mathbb{I}(l<c))}{2}\rfloor.",,"g(G^{p})=l\,\text{if}\,N_{l}-M>\max_{c\in\{1,2,\cdots,C\}\setminus\{l\}}(N_{c}+M-\mathbb{I}(l<c)),",Condition under which the ensemble graph classifier $g$ predicts the same label $l$ for the perturbed graph $G^{p}$ based on the number of corrupted sub-graphs $M$.
ICLR_2024_oral_23,1,"### Diffusion models and denoising

Let \(y=x+z\) where \(z\sim\mathcal{N}(0,\sigma^{2}\mathrm{Id})\). The density \(p_{\sigma}(y)\) of noisy images is then related to \(p(x)\) through marginalization over \(x\):","p_{\sigma}(y)=\int p(y|x)\,p(x)\,\mathrm{d}x=\int g_{\sigma}(y-x)\,p(x)\, \mathrm{d}x,",where \(g_{\sigma}(z)\) is the density of \(z\),"p_{\sigma}(y) = \int p(x) \mathcal{N}(y; x, \sigma^2 \mathrm{Id}) \, dx",Density of noisy images obtained by marginalizing over the original image distribution and the noise distribution.
ICLR_2024_oral_23,2,"\). Hence, \(p_{\sigma}(y)\) is obtained by convolving \(p(x)\) with a Gaussian with standard deviation \(\sigma\). The family of densities \(\{p_{\sigma}(y);\sigma\geq 0\}\) forms a scale-space representation of \(p(x)\), analogous to the temporal evolution of a diffusion process.

Diffusion models learn an approximation \(s_{\theta}(y)\) (dropping the \(\sigma\) dependence for simplicity) of the scores \(\nabla\log p_{\sigma}(y)\) of the blurred densities \(p_{\sigma}(y)\) at all noise levels \(\sigma\). The collection of these score models implicitly defines a model \(p_{\theta}(x)\) of the density of clean images \(p(x)\) through a reverse diffusion process. The error of the generative model, as measured by the KL divergence between \(p(x)\) and \(p_{\theta}(x)\), is then controlled by the integrated score error across all noise levels (Song et al., 2021):","D_{\mathrm{KL}}(p(x)\,\|\,p_{\theta}(x))\leq\int_{0}^{\infty}\mathbb{E}_{y} \big{[}\|\nabla\log p_{\sigma}(y)-s_{\theta}(y)\|^{2}\big{]}\,\sigma\,\mathrm{ d}\sigma.",,\mathrm{KL}\left(p(x)\|p_{\theta}(x)\right)=\frac{1}{2}\int_{0}^{\infty}\mathbb{E}_{p_{\sigma}(y)}\left[\left\Vert s_{\theta}(y)-\nabla\log p_{\sigma}(y)\right\Vert^{2}\right]\mathrm{d}\sigma.,The KL divergence between the true data distribution and the model distribution is expressed as an integral of the expected squared error between the learned scores and the true scores across all noise levels.
ICLR_2024_oral_23,3,The key to learning the scores is an equation due to Robbins (1956) and Miyasawa (1961) (proved in Appendix D.1 for completeness) that relates them to the mean of the corresponding posteriors:,"\nabla\log p_{\sigma}(y)=(\mathop{\mathbb{E}}_{x}[x\,|\,y]-y)/\sigma^{2}.",,\nabla\log p_{\sigma}(y) = \frac{1}{\sigma^{2}}\left(\mathbb{E}[x|y] - y\right).,This equation expresses the score function in terms of the posterior mean of the clean image given the noisy observation.
ICLR_2024_oral_23,4,"The score is learned by training a denoiser \(f_{\theta}(y)\) to minimize the mean squared error (MSE) (Raphan & Simoncelli, 2011; Vincent, 2011):","\mathrm{MSE}(f_{\theta},\sigma^{2})=\mathop{\mathbb{E}}_{x,y}\Bigl{[}\|x-f_{ \theta}(y)\|^{2}\Bigr{]},",,"\mathbb{E}_{x,z}\left[\left\|x - f_{\theta}(y)\right\|^{2}\right],",where the expectation is taken over the clean data $x$ and the noise $z$.
ICLR_2024_oral_23,5,"so that \(f_{\theta}(y)\approx\mathop{\mathbb{E}}_{x}[x\,|\,y]\). This estimated conditional mean is used to recover the estimated score using eq. (3): \(s_{\theta}(y)=(f_{\theta}(y)-y)/\sigma^{2}\). As we show in Appendix D.2, the error in estimating the density \(p(x)\) is bounded by the integrated optimality gap of the denoiser across noise levels:","D_{\mathrm{KL}}(p(x)\,||\,p_{\theta}(x))\leq\int_{0}^{\infty}\Bigl{(}\mathrm{ MSE}(f_{\theta},\sigma^{2})-\mathrm{MSE}(f^{\star},\sigma^{2})\Bigr{)}\, \sigma^{-3}\,\mathrm{d}\sigma,","where \(f^{\star}(y)=\mathop{\mathbb{E}}_{x}[x\,|\,y]\) is the optimal denoiser","D_{\mathrm{KL}}(p(x)\,\|\,p_{\theta}(x))\leq\int_{0}^{\infty}\Bigl{(} \mathop{\mathbb{E}}_{x,y}\bigl{[}\|x-f_{\theta}(y)\|^{2}\bigr{]}-\mathrm{MMSE}(\sigma^{ 2})\Bigr{)}\,\frac{1}{\sigma^{2}}\,\mathrm{d}\sigma,",This equation bounds the KL divergence between the true data density and the model density in terms of the integrated gap between the denoiser's MSE and the optimal MMSE across noise levels.
ICLR_2024_oral_23,6,"er. Thus, learning the true density model is equivalent to performing optimal denoising at all noise levels. Conversely, a suboptimal denoiser introduces a score approximation error, which in turn can result in an error in the modeled density.

Generally, the optimal denoising function \(f^{\star}\) (as well as the ""true"" distribution, \(p(x)\)) is unknown for photographic images, which makes numerical evaluation of sub-optimality challenging. We can however separate deviations from optimality arising from model bias and model variance. Model variance measures the size of the approximation class, and hence the strength (or restrictiveness) of the inductive biases. It can be evaluated without knowledge of \(f^{\star}\). Here, we define generalization as near-zero model variance (i.e., an absence of overfitting), which is agnostic to model bias. This is the subject of Section 2.2. Model bias measures the distance of the true score to the approximation class, and thus the alignment between the inductive biases and the data distribution. In the context of photographic images, visual quality of generated samples can be a qualitative indicator of the model bias, although high visual quality does not necessarily guarantee low model bias. We evaluate model bias in Section 3.2 by considering synthetic image classes for which \(f^{\star}\) is approximately known.

### Transition from memorization to generalization

DNNs are susceptible to overfitting, because the number of training examples is typically small relative to the model capacity. Since density estimation, in particular, suffers from the curse of dimensionality, overfitting is of more concern in the context of generative models. An overfitted denoiser performs well on training images but fails to generalize to test images, resulting in low-diversity generated images. Consistent with this, several papers have reported that diffusion models can memorize their training data (Sompalli et al., 2023; Carlini et al., 2023; Dar et al., 2023; Zhang et al., 2023). To directly assess this, we compared denoising performance on training and test data for different training set sizes \(N\). We trained denoisers on subsets of the (downsampled) CelebA dataset (Liu et al., 2015) of size \(N=10^{0},10^{1},10^{2},10^{3},10^{4},10^{5}\). We used a UNet architecture (Ronneberger et al., 2015), which is composed of 3 convolutional encoder and decoder blocks with rectifying non-linearities. These denoisers are universal and blind: they operate on all noise levels without having noise level as an input Mohan* et al. (2020). Networks are trained to minimize mean squared error (4). See Appendix A for architecture and training details.

Results are shown in Figure 1. When \(N=1\), the denoiser essentially memorizes the single training image, leading to a high test error. Increasing \(N\) substantially increases the performance on the test set while worsening performance on the training set, as the network transitions from memorization to generalization. At \(N=10^{5}\), empirical test and train error are matched for all noise levels.

To investigate this generalization further, we train denoisers on _non-overlapping_ subsets of CelebA of various size \(N\). We then generate samples using the scores learned by each denoiser, through the reverse diffusion algorithm of Kadkhodaie & Simoncelli (2020)--see Appendix A for details. Figure 2 shows samples generated by these denoisers, initialized from the same noise sample. For small \(N\), the networks memorize their respective training images. However, for large \(N\), the networks converge to the same score function (and thus sample from the same model density), generating nearly identical samples. This surprising behavior provides a much stronger demonstration of convergence than comparison of average train and test performance.

## 3 Inductive biases

The number of samples needed for estimation of an arbitrary probability density grows exponentially with dimensionality (the ""curse of dimensionality""). As a result, estimating high-dimensional distributions is only feasible if one imposes strong constraints or priors over the hypothesis space. In a diffusion model, these arise from the network architecture and the optimization algorithm, and are referred to as the inductive biases of the network (Wilson & Izmailov, 2020; Goyal & Bengio, 2022; Griffiths et al., 2023). In Section 2.2, we demonstrated that DNN denoisers can learn scores (and thus a density) from relatively small training sets. This generalization result, combined with the high quality of sampled images, is evidence that the inductive biases are well-matched to the ""true"" distribution of images, allowing the model to rapidly converge to a good solution through learning. On the contrary, when inductive biases are not aligned with the true distribution, the model will arrive at a poor solution with high model bias.

For diffusion methods, learning the right density model is equivalent to performing optimal denoising at all noise levels (see Section 2.1). The inductive biases on the density model thus arise directly from inductive biases in the denoiser. This connection offers a means of evaluating the accuracy of the learned probability models, which is generally difficult in high-dimensions.

### Denoising as shrinkage in an adaptive basis

The inductive biases of the DNN denoiser can be studied through an eigendecomposition of its Jacobian. We describe the general properties that are expected for an optimal denoiser, and examine several specific cases for which the optimal solution is partially known.

Jacobian eigenvectors as an adaptive basis.To analyze inductive biases, we perform a local analysis of a denoising estimator \(\hat{x}(y)=f(y)\) by looking at its Jacobian \(\nabla f(y)\). For simplicity, we assume that the Jacobian is symmetric and non-negative (we show below that this holds for the optimal denoiser, and it is approximately true of the network Jacobian (Mohan* et al., 2020)). We can then diagonalize it to obtain eigenvalues \((\lambda_{k}(y))_{1\leq k\leq d}\) and eigenvectors \((e_{k}(y))_{1\leq k\leq d}\).

If \(f(y)\) is computed with a DNN denoiser with no additive ""bias"" parameters, its input-output mapping is piecewise linear, as opposed to piecewise affine (Mohan* et al., 2020; Romano et al., 2017). It

Figure 1: Transition from memorization to generalization, for a UNet denoiser trained on face images. Each curve shows the denoising error (output PSNR, ten times log10 ratio of squared dynamic range to MSE) as a function of noise level (input PSNR), for a training set of size \(N\). As \(N\) increases, performance on the training set generally worsens (left), while performance on the test set improves (right). For \(N=1\) and \(N=10\), the train PSNR improves with unit slope, while test PSNR is poor, independent of noise level, a sign of memorization. The increase in test performance on small noise levels at \(N=1000\) is indicative of the transition phase from memorization to generalization. At \(N=10^{5}\), test and train PSNR are essentially identical, and the model is no longer overfitting the training data.

follows that the denoiser mapping can be rewritten in terms of the Jacobian eigendecomposition as","f(y)=\nabla f(y)\,y=\sum_{k}\lambda_{k}(y)\langle y,e_{k}(y)\rangle e_{ k}(y).",,"f(y)=\sum_{k=1}^{d}\lambda_{k}(y)\,\langle e_{k}(y),y\rangle\,e_{k}(y).",
ICLR_2024_oral_23,7,"The denoiser can thus be interpreted as performing shrinkage with factors \(\lambda_{k}(y)\) along axes of a basis specified by \(e_{k}(y)\). Note that both the eigenvalues and eigenvectors depend on the noisy image \(y\) (i.e., both the basis and shrinkage factors are _adaptive_(Milanfar, 2012)).

Even if the denoiser is not bias-free, small eigenvalues \(\lambda_{k}(y)\) reveal local invariances of the denoising function: small perturbations in the noisy input along the corresponding eigenvectors \(e_{k}(y)\) do not affect the denoised output. Intuitively, such invariances are a desirable property for a denoiser, and they are naturally enforced by minimizing mean squared error (MSE) as expressed with Stein's unbiased risk estimate (SURE, proved in Appendix D.3 for completeness):","\mathrm{MSE}(f,\sigma^{2})=\mathop{\mathbb{E}}_{y}\![2\sigma^{2}\,\mathrm{ tr}\,\nabla f(y)+\|y-f(y)\|^{2}-\sigma^{2}d].",,"\mathrm{MSE}(f_{\theta},\sigma^{2})=\mathop{\mathbb{E}}_{y}\Bigl{[}\|x-f_{ \theta}(y)\|^{2}\Bigr{]}=\mathop{\mathbb{E}}_{y}\Bigl{[}\|y-f_{\theta}(y)\|^{2} +2\sigma^{2}\nabla\cdot f_{\theta}(y)-d\sigma^{2}\Bigr{]},","where the divergence term $\nabla\cdot f_{\theta}(y)$ captures the sensitivity of the denoiser to input perturbations, and $d$ is the dimensionality of the data."
ICLR_2024_oral_23,8,"To minimize MSE, the denoiser must trade off the approximate ""rank"" of the Jacobian (the trace is the sum of the eigenvalues) against an estimate of the denoising error: \(\left\|y-f(y)\right\|^{2}-\sigma^{2}d\). The denoiser thus locally behaves as a (soft) projection on a subspace whose dimensionality corresponds to the

Figure 2: Convergence of model variance. Diffusion models are trained on non-overlapping subsets \(S_{1}\) and \(S_{2}\) of a face dataset (filtered for duplicates). The subset size \(N\) varies from \(1\) to \(10^{5}\). We then generate a sample from each model with a reverse diffusion algorithm, initialized from the same noise image. **Top.** For training sets of size \(N=1\) to \(N=100\), the networks memorize, producing samples nearly identical to examples from the training set. For \(N=1000\), generated samples are similar to a training example, but show distortions in some regions. This transitional regime corresponds to a qualitative change in the shape of the PSNR curve (Figure 1). For \(N=10^{5}\), the two networks generate nearly identical samples, which no longer resemble images in their corresponding training sets. **Bottom.** The distribution of cosine similarity (normalized inner product) between pairs of images generated by the two networks (blue) shifts from left to right with increasing \(N\), showing vanishing model variance. Conversely, the distribution of cosine similarity between generated samples and the most similar image in their corresponding training set (orange) shifts from right to left. For comparison, Appendix B shows the distribution of cosine similarities of closest pairs between the two training subsets, and additional results on the LSUN bedroom dataset (Yu et al., 2015) and for the BF-CNN architecture (Mohan* et al., 2020).

rank of the Jacobian. As we now explain, this subspace approximates the support of the posterior distribution \(p(x|y)\), and thus gives a local approximation of the support of \(p(x)\).

It is shown in Appendix D.1 that the optimal minimum MSE denoiser and its Jacobian are given by","f^{\star}(y) =y+\sigma^{2}\nabla\log p_{\sigma}(y)=\operatorname*{\mathbb{E}}_{ x}[x|y], || \nabla f^{\star}(y) =\operatorname{Id}+\sigma^{2}\nabla^{2}\log p_{\sigma}(y)=\sigma^ {-2}\mathrm{Cov}[x\,|\,y].",,"f^{\star}(y)=\mathop{\mathbb{E}}_{x}[x\,|\,y],\quad\nabla f^{\star}(y)=\frac{1}{\sigma^{2}}\mathop{\mathrm{Cov}}_{x}[x\,|\,y]+\mathrm{Id},","where the Jacobian of the optimal denoiser incorporates the posterior covariance and identity matrix, reflecting the local geometry of the posterior distribution $p(x|y)$."
ICLR_2024_oral_23,9,"That is, the Jacobian of the optimal denoiser is proportional to the posterior covariance matrix, which is symmetric and non-negative. This gives us another interpretation of the adaptive eigenvector basis as providing an optimal approximation of the unknown clean image \(x\) given the noisy observation \(y\). Further, the optimal denoising error is then given by (see Appendix D.1 for the first equality)","\mathrm{MSE}(f^{\star},\sigma^{2})=\operatorname*{\mathbb{E}}_{y}[\mathrm{tr }\,\mathrm{Cov}[x\,|\,y]]=\sigma^{2}\operatorname*{\mathbb{E}}_{y}\bigl{[} \mathrm{tr}\,\nabla f^{\star}(y)\bigr{]}=\sigma^{2}\operatorname*{\mathbb{E}} _{y}\biggl{[}\sum_{k}\lambda_{k}^{\star}(y)\biggr{]}.",,"\mathrm{MSE}(f^{\star},\sigma^{2})=\mathop{\mathbb{E}}_{y}\bigl{[}\|\nabla f^{\star}(y)\|_{F}^{2}\,\sigma^{4}-\sigma^{2}d\bigr{]},","The optimal denoising error is expressed in terms of the Frobenius norm of the Jacobian of the optimal denoiser, scaled by the noise level, minus a term proportional to the dimensionality."
ICLR_2024_oral_23,10,"A small denoising error thus implies an approximately low-rank Jacobian (with many small eigenvalues) and thus an efficient approximation of \(x\) given \(y\).

In most cases, the optimal adaptive basis \((e_{k}^{\star}(y))_{1\leq k\leq d}\) is not known. Rather than aiming for exact optimality, classical analyses (Donoho, 1995) thus focus on the asymptotic decay of the denoising error as the noise level \(\sigma^{2}\) falls, up to multiplicative constants. This corresponds to finding a basis \((e_{k}(y))_{1\leq k\leq d}\) which captures the asymptotic slope of the PSNR plots in Figure 1 but not necessarily the intercept. This weaker notion of optimality is obtained by showing matching upper and lower-bounds on the asymptotic behavior of the denoising error. To provide intuition, we first consider a fixed orthonormal basis \(e_{k}(y)=e_{k}\), and then consider the more general case of best bases selected from a fixed dictionary.

Denoising in a fixed basis.Consider a denoising algorithm that is restricted to operate in a fixed basis \(e_{k}\) but can adapt its shrinkage factors \(\lambda_{k}(y)\). An unreachable lower-bound on the denoising error--and thus an upper-bound on the PSNR slope--is obtained by evaluating the performance of an ""oracle"" denoiser where the shrinkage factors \(\lambda_{k}\) depend on the unknown clean image \(x\) rather than the noisy observation \(y\)(Mallat, 2008). Appendix D.4 shows that the denoising error of this oracle is","\operatorname*{\mathbb{E}}_{x}\biggl{[}\sum_{k}\Bigl{(}(1-\lambda_{k}(x))^{2} \langle x,e_{k}\rangle^{2}+\lambda_{k}(x)^{2}\sigma^{2}\Bigr{)}\biggr{]},",,"\mathrm{MSE}_{\text{oracle}}=\sum_{k=1}^{d}\min(\sigma^{2},\mathbb{E}_{x}[x_{k}^{2}]).",This represents the minimum possible denoising error when shrinkage factors are chosen with knowledge of the clean image $x$ in a fixed basis.
ICLR_2024_oral_23,11,"which is minimized when \(\lambda_{k}(x)=\frac{\langle x,e_{k}\rangle^{2}}{\langle x,e_{k}\rangle^{2}+ \sigma^{2}}\). The coefficient \(\lambda_{k}(x)\) thus acts as a soft threshold: \(\lambda_{k}(x)\approx 1\) when the signal dominates the noise and \(\lambda_{k}(x)\approx 0\) when the signal is weaker than the noise. Appendix D.4 then shows that the oracle denoising error is the expected value of","\sigma^{2}{\sum_{k}\lambda_{k}(x)}=\sum_{k}\frac{\sigma^{2}\langle x,e_{k} \rangle^{2}}{\langle x,e_{k}\rangle^{2}+\sigma^{2}}\sim\sum_{k}\min(\langle x,e_{k}\rangle^{2},\sigma^{2})=M\sigma^{2}+\|x-x_{M}\|^{2},","where \(x_{M}=\sum_{\langle x,e_{k}\rangle^{2}>\sigma^{2}}\langle x,e_{k}\rangle\,e_{k}\) is the \(M\)-term approximation of \(x\) with the \(M\) basis coefficients \(\langle x,e_{k}\rangle\) above the noise level, and \(\sim\) means that the two terms are of the same order up to multiplicative constants (here smaller than 2)","\sum_{k}\frac{\langle x,e_{k}\rangle^{2}\sigma^{2}}{\langle x,e_{k}\rangle^{2}+\sigma^{2}}.","This expression quantifies the minimum achievable denoising error in a fixed basis, where shrinkage factors depend on the clean image $x$ and the noise level $\sigma^2$."
ICLR_2024_oral_23,12,"2). The denoising error is small if \(x\) has a sparse representation in the basis, so that both \(M\) and the approximation error \(\|x-x_{M}\|^{2}\) are small. For example, if the coefficients decay as \(\langle x,e_{k}\rangle^{2}\sim k^{-(\alpha+1)}\) (up to reordering), Appendix D.4 shows that","M\sigma^{2}+\|x-x_{M}\|^{2}\sim\sigma^{2\alpha/(\alpha+1)},",,"\|x-x_{M}\|^{2}\sim\int_{M}^{\infty}k^{-(\alpha+1)}\,\mathrm{d}k\sim M^{-\alpha}.",The approximation error decays as $M^{-\alpha}$ when the basis coefficients of $x$ decay as $k^{-(\alpha+1)}$.
ICLR_2024_oral_24,1,"### Dynamical Generative Modeling

The generative modeling approaches rooted in dynamical systems, including ODE and SDE, have garnered significant attention. Here, we present three noteworthy dynamical generative models: Diffusion Model (DM), Flow Matching (FM) and Bridge Matching (BM).

Figure 1: The pixel-wise trajectories comparison with CLD(Dockhorn et al., 2021). Left figures correspond to the trajectories over time w.r.t random sampled 16 pixels, for position and velocity. Our model is able to learn straighter trajectories which is beneficial for reducing sampling complexity.

**Diffusion Model:** In the framework of DM, given \(\mathbf{x}_{0}\) drawn from a data distribution \(p_{\mathrm{data}}\), the model proceeds to construct a SDE,",\mathrm{d}\mathbf{x}_{t}=f_{t}(\mathbf{x}_{t})\mathrm{d}t+g(t)\mathrm{d}\mathbf{ w}_{t}\quad\mathbf{x}_{0}\sim p_{\mathrm{data}}(\mathbf{x}),,"\mathrm{d}\mathbf{x} = -\frac{\beta(t)}{2} \mathbf{x} \, \mathrm{d}t + \sqrt{\beta(t)} \, \mathrm{d}\mathbf{w}","The stochastic differential equation governing the diffusion process in Diffusion Models, where β(t) is the time-dependent diffusion coefficient and w represents Brownian motion."
ICLR_2024_oral_24,2,"whose terminal distributions at \(t=1\) approach an approximate Gaussian, i.e. \(\mathbf{x}_{1}\sim\mathcal{N}(\mathbf{0},\mathbf{I}_{d})\). This accomplishment is realized through the careful selection of the diffusion coefficient \(g_{t}\) and the base drift \(f_{t}(\mathbf{x}_{t})\). It is noteworthy that the time-reversal (Anderson, 1982) of (1) results in another SDE:","\mathrm{d}\mathbf{x}_{t}=[f_{t}(\mathbf{x}_{t})-g_{t}^{2}\nabla_{\mathbf{ x}}\log p(\mathbf{x}_{t},t)]\mathrm{d}t+g(t)\mathrm{d}\mathbf{w}_{t}, \quad\mathbf{x}_{1}\sim\mathcal{N}(\mathbf{0},\mathbf{I}_{d})","where \(p(\cdot,t)\) is the marginal density of (1) at time \(t\) and \(\nabla_{\mathbf{x}}\log p_{t}\) is known as the score function",\mathrm{d}\mathbf{x}_{t}=\left[-f_{t}(\mathbf{x}_{t})+g(t)^{2}\nabla_{\mathbf{x}_{t}}\log p_{t}(\mathbf{x}_{t})\right]\mathrm{d}t+g(t)\mathrm{d}\mathbf{w}_{t},"The time-reversed SDE derived from Equation 1, where the drift term incorporates both the negative original drift and a score function term $"
ICLR_2024_oral_24,3,"on. SDE (2) can be regarded as the time-reversal of (1) in such a manner that the path-wise measure is almost surely equivalent to the one induced by (1). As a consequence, these two SDEs share identical marginal over time. In practice, it is feasible to analytically sample \(\mathbf{x}_{t}\) given \(t\) and \(\mathbf{x}_{0}\). Additionally, we can leverage a neural network to learn the score function by regressing scaled Stein Score \(\mathbb{E}_{\mathbf{x}_{t},t}\|\mathbf{s}_{t}^{\theta}(\mathbf{x}_{t},t;\theta )-\nabla_{\mathbf{x}}\log p(\mathbf{x}_{t},t|\mathbf{x}_{0})\|_{2}^{2}\) for the purpose of propagating (2). This learned score can then be integrated into the solution of the aforementioned SDE(2) to simulate the generation of data that adheres to the target data distribution from the prior distribution. Meanwhile, (2) also corresponds to an ODE which shares the same path-wise measure:","\mathrm{d}\mathbf{x}_{t}=[f_{t}(\mathbf{x}_{t})-\frac{1}{2}g_{t}^{2} \nabla_{\mathbf{x}}\log p(\mathbf{x}_{t},t)]\mathrm{d}t,\quad\mathbf{ x}_{1}\sim\mathcal{N}(\mathbf{0},\mathbf{I}_{d})",,"\frac{\mathrm{d}\mathbf{x}_{t}}{\mathrm{d}t}=f_{t}(\mathbf{x}_{t})-g_{t}^{2}\nabla_{\mathbf{x}}\log p(\mathbf{x}_{t},t)","Ordinary Differential Equation (ODE) formulation corresponding to the time-reversed SDE (2), sharing the same path-wise measure for generative modeling."
ICLR_2024_oral_24,4,"which motivates the popular sampler introduced in (Zhang and Chen, 2022; Zhang et al., 2022; Bao et al., 2022) to solve the ODE (2) efficiently.

**Bridge Matching and Flow Matching:** An alternative approach to exploring the time-reversal of a forward noising process involves the concept of 'building bridges' between two distinct distributions \(p_{0}(\cdot)\) and \(p_{1}(\cdot)\). This method entails the learning of a mimicking diffusion process, commonly referred to as bridge matching, as elucidated in previous works (Peluchetti, 2021; Shi et al., 2022). Here we consider the SDE in the form of:","\mathrm{d}\mathbf{x}_{t}=\mathbf{v}_{t}(\mathbf{x},t)\mathrm{d}t+g_{t} \mathrm{d}\mathbf{w}_{t}\quad s.t.\quad(x_{0},x_{1})\sim\Pi_{0,1}(\mathbf{x}_ {0},\mathbf{x}_{1}):=p_{0}\times p_{1}",,\mathrm{d}\mathbf{x}_{t}=\mu_{t}(\mathbf{x}_{t})\mathrm{d}t+\sigma_{t}(\mathbf{x}_{t})\mathrm{d}\mathbf{w}_{t},"where $\mu_{t}(\cdot)$ and $\sigma_{t}(\cdot)$ denote the drift and diffusion coefficients of the mimicking process, respectively, which are learned to match the conditional dynamics of the reference bridge distribution."
ICLR_2024_oral_24,5,"which is pinned down at an initial and terminal point \(x_{0},x_{1}\) which are independently samples from predefined \(p_{0}\) and \(p_{1}\). This is commonly known as the reciprocal projection of \(x_{0}\) and \(x_{1}\) in the literature (Shi et al., 2023; Peluchetti, 2023; Liu et al., 2022; Leonard et al., 2014). The construction of such SDE is accomplished by meticulous design of \(\mathbf{v}_{t}\). A widely adopted choice for \(\mathbf{v}_{t}\) is \(\mathbf{v}_{t}:=(\mathbf{x}_{1}-\mathbf{x}_{t})/(1-t)\), which induces the well-known Brownian Bridge (Liu et al., 2023; Somnath et al., 2023). Similar to the approach in DM and owing to the linear structure of the dynamics, one can efficiently estimate this drift by employing a neural network parameterized by weights \(\theta\) for regression on: \(\mathbb{E}_{\mathbf{x}_{t},t}\|\mathbf{v}_{t}^{\theta}(\mathbf{x}_{t},t;\theta )-\mathbf{v}_{t}(\mathbf{x}_{t},t)\|_{2}^{2}\) given \(\mathbf{x}_{1}\) and \(t\). As extensively discussed in previous studies (Liu et al., 2023; Shi et al., 2022), this bridge matching framework takes on the characteristics of FM (Lipman et al., 2022) when the diffusion coefficient \(g_{t}\) tends to zero.

**Remark 1**.: _The practice of constraining a stochastic process to specific initial and terminal conditions is a well-established setup in SOC. For a gentle introduction of it's connection with Brownian Bridge, Schrodinger Bridge please see Appendix C. From this perspective, one can derive Brownian Bridge, as elaborated in Appendix.D.1 for comprehensive elucidation. It is imperative to note that the SOC framework will serve as the fundamental basis upon which we will develop our algorithm._

\begin{table}
\begin{tabular}{c c c c} \hline \hline Models & DM/FM & CLD & AGM(ours) \\ \hline \(p_{0}(\cdot)\) & \(p_{\mathrm{data}}(x)\) & \(p_{\mathrm{data}}(x)\times\mathcal{N}(\mathbf{0},\mathbf{I}_{d})\) & \(\mathcal{N}(\mathbf{0},\mathbf{\Sigma}_{0}\times\mathbf{I}_{2d})\) \\ \(p_{1}(\cdot)\) & \(\mathcal{N}(\mathbf{0},\mathbf{I}_{d})\) & \(\mathcal{N}(\mathbf{0},\mathbf{I}_{d})\times\mathcal{N}(\mathbf{0},\mathbf{I}_{d})\) & \(p_{\mathrm{data}}(x)\times p_{\mathrm{data}}(x)*\mathcal{N}(\mathbf{0},\mathbf{ \Sigma}_{1}\otimes\mathbf{I}_{2d})\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison between models in terms of boundary distributions \(p_{0}\) and \(p_{1}\). Our AGM generalizes DM beyond Gaussian priors to phase space, similar to CLD. However, unlike CLD, AGM does not need to converge to the Gaussian at equilibrium which causes curved trajectory(see Fig.1), instead, velocity distribution will be the convolution of data distribution with Gaussian.

## 3 Acceleration Generative Model

We apply SOC to characterize the twisted trajectory of momentum dynamics induced by CLD(Dockhorn et al., 2021). It becomes evident that the mechanisms encompassing flow matching, diffusion modeling, and Bridge matching collectively facilitate the construction of an estimated target data point, denoted as \(\mathbf{x}_{1}\), by utilizing the intermediate state of the dynamics, \(\mathbf{x}_{t}\). Our additional objective is to expedite the estimation of a plausible \(\mathbf{x}_{1}\) by incorporating additional dynamics-related information, such as velocity, thereby curtailing the requisite time integration.

In this section, we introduce the proposed method, termed as the Acceleration Generative Model (AGM), rooted in SOC theory. Building upon (Chen and Georgiou, 2015), we extend the framework by incorporating a time-varying diffusion coefficient and accommodating arbitrary boundary conditions, ultimately arriving at an analytical solution suited for the generative modeling. We demonstrate its efficacy in rectifying the trajectory of CLD, concurrently showcasing its aptitude for accurately estimating the target data at an early timestep \(t_{i}\), thereby enabling expeditious sampling.

As suggested by BM approach, there is a necessity to formulate a trajectory that bridges the two data points sampled from \(p_{0}\) and \(p_{1}\) respectively. Desirably, the intermediate trajectory should exhibit optimal characteristics that facilitate smoothness and linearity. This is essential for the ease of simulating the dynamics system to obtain the solution. In our endeavor to tackle this challenge and enhance the estimation of the data point \(\mathbf{x}_{1}\) by incorporating velocity components, we encapsulate the problem within a SOC framework, specifically formulated in the phase space which reads:

**Definition 2** (Stochastic Bridge problem of linear momentum system (Chen and Georgiou, 2015)).:","\begin{split}\min_{\mathbf{a}_{t}}\int_{\tau}^{1}\lVert \mathbf{a}_{t}\rVert_{2}^{2}\text{d}t+(\mathbf{m}_{1}-m_{1})^{\text{T}}\mathbf{ R}(\mathbf{m}_{1}-m_{1})& s.t\underbrace{\begin{bmatrix}\text{d}\mathbf{x}_{t}\\ \text{d}\mathbf{v}_{t}\end{bmatrix}}_{\text{d}\mathbf{m}_{t}}&=\begin{bmatrix} \mathbf{v}_{t}\\ \mathbf{a}_{t}(\mathbf{x}_{t},\mathbf{v}_{t},t)\end{bmatrix}\text{d}t+ \underbrace{\begin{bmatrix}\mathbf{0}&\mathbf{0}\\ \mathbf{0}&g_{t}\end{bmatrix}}_{\mathbf{g}_{t}}\text{d}\mathbf{w}_{t},\\ \mathbf{m}_{\tau}:=\begin{bmatrix}\mathbf{x}_{\tau}\\ \mathbf{v}_{\tau}\end{bmatrix}&=\begin{bmatrix}x_{\tau}\\ v_{\tau}\end{bmatrix},&\mathbf{R}=\begin{bmatrix}\mathbf{r}&\mathbf{0}\\ \mathbf{0}&\mathbf{r}\end{bmatrix}\otimes\mathbf{I}_{d},&x_{1}\sim p_{\text{data}}. \end{split}",,"\min_{\mathbf{v}_{t}}\mathbb{E}\left[\int_{0}^{1}\frac{1}{2\sigma_{t}^{2}}\|\mathbf{v}_{t}(\mathbf{x}_{t},t)\|^{2}\mathrm{d}t\right]\quad\text{s.t.}\quad\mathrm{d}\mathbf{x}_{t}=\mathbf{v}_{t}\mathrm{d}t+\sigma_{t}\mathrm{d}\mathbf{w}_{t},\quad(\mathbf{x}_{0},\mathbf{x}_{1})\sim\Pi_{0,1}",The stochastic bridge problem in phase space minimizes the expected kinetic energy of the drift subject to boundary conditions and stochastic dynamics.
ICLR_2024_oral_24,6,"(5)

In this context, the matrix \(\mathbf{R}\) is recognized as the terminal cost matrix, serving to assess the proximity between the propagated \(\mathbf{m}_{1}\) and the ground truth \(m_{1}\) at the terminal time \(t=1\). As the parameter \(\mathbf{r}\) approaches positive infinity, the trajectory converges toward the state \(x_{1}\), prompting a transition to constrained dynamics wherein the system becomes constrained by two predetermined boundaries, namely \(m_{0}\) and \(m_{1}\). This configuration aligns seamlessly with the principles of constructing a feasible bridge, as advocated by the tenets of BM. It is worth noting that this interpolation approach essentially represents a natural extension (Chen and Georgiou, 2015) of the well-established concept of the Brownian Bridge (Revuz and Yor, 2013), which has been employed in trajectory inference (Somnath et al., 2023; Tong et al., 2023) and image inpainting tasks (Liu et al., 2023) and its connection with Diffusion has been discussed in Liu et al. (2023). Indeed, it is evident that the target velocity lacks a precise definition within this problem, allowing for flexibility in the design space for our approach. To address this, we opt for the linear interpolation of the intermediate point and the target point, represented as \(\mathbf{v}_{1}=(\mathbf{x}_{1}-\mathbf{x}_{t})/(1-t)\), as the chosen terminal velocity, which also is the optimal control in the original space (see Appendix..D.1). This choice is made due to its ability to construct a trajectory characterized by straightness. Conceptually, the acceleration \(\mathbf{a}_{t}\) continually guides the dynamics towards the linear interpolation of the two data points, serving to mitigate the impact of introduced stochasticity. In contrast to previous bridge matching frameworks, the velocity's boundary condition in our approach _varies over time_ since it depends on the state \(\mathbf{x}_{t}\) and \(t\). The velocity variable serves solely as an auxiliary component aimed at straightening the trajectories. Regarding this SOC problem formulation, the solution is,

**Proposition 3** (Phase Space Brownian Bridge).: _When \(\mathbf{r}\to+\infty\), The solution w.r.t optimization problem 5 is,_","\mathbf{a}^{*}(\mathbf{m}_{t},t)=g_{t}^{2}P_{11}(\frac{\mathbf{x}_{1}- \mathbf{x}_{t}}{1-t}-\mathbf{v}_{t})\quad\text{where}:\quad P_{11}= \frac{-4}{g_{t}^{2}(t-1)}.",,"\begin{split}\mathrm{d}\begin{bmatrix}\mathbf{x}_{t}\\ \mathbf{v}_{t}\end{bmatrix}=\begin{bmatrix}\mathbf{v}_{t}\\ -\frac{2}{1-t}\mathbf{v}_{t}-\frac{1}{(1-t)^{2}}\mathbf{x}_{t}+\frac{1}{1-t}\mathbf{x}_{1}\end{bmatrix}\mathrm{d}t+\begin{bmatrix}\mathbf{0}\\ g_{t}\mathrm{d}\mathbf{w}_{t}\end{bmatrix},\quad\mathbf{x}_{1}\sim p_{\text{data}}(\mathbf{x}),\quad\mathbf{x}_{t_{i}}=\mathbf{x}_{i}.\end{split}","The Phase Space Brownian Bridge formulation describes the dynamics of position and velocity in the proposed Acceleration Generative Model, where the system is driven towards a linear interpolation between the current state and the target data point."
ICLR_2024_oral_24,7,"Proof.: Please see Appendix.D.2.

**Remark 4**.: \(P_{11}\) _denotes the second diagonal component in the matrix \(P_{t}\), a solution derived from the Lyapunov equation (see Lemma.9), serving as an implicit representation of the optimality of the control. This value is dependent upon the uncontrolled dynamics, where \(\mathbf{a}_{t}\) is set to the zero vector in (5), and will vary accordingly when uncontrolled dynamics change._

### Training

By plugging the optimal control (6) back to the dynamics (5), we can obtain the desired SDE. As been suggested by (Song et al., 2020; Dockhorn et al., 2021), such SDE has a corresponding probabilistic ODE which shares the same marginal over time in which the drift term will have an additional score term \(\nabla_{\mathbf{v}}\log p(\mathbf{m}_{t},t)\). Here we summarize the force term for SDE and ODE as:","\begin{bmatrix}\mathrm{d}\mathbf{x}_{t}\\ \mathrm{d}\mathbf{v}_{t}\end{bmatrix}=\begin{bmatrix}\mathbf{v}_{t}\\ \mathbf{F}_{t}\end{bmatrix}\mathrm{d}t+\begin{bmatrix}\mathbf{0}&\mathbf{0} \\ \mathbf{0}&h_{t}\end{bmatrix}\mathrm{d}\mathbf{w}_{t}\quad\text{s.t}\quad \mathbf{m}_{0}:=\begin{bmatrix}\mathbf{x}_{0}\\ \mathbf{v}_{0}\end{bmatrix}\sim\mathcal{N}(\boldsymbol{\mu}_{0},\boldsymbol{ \Sigma}_{0}), || \text{Bridge Matching SDE}:\mathbf{F}_{t}:=\mathbf{F}_{t}^{b}( \mathbf{m}_{t},t)\equiv\mathbf{a}_{t}^{*}(\mathbf{m}_{t},t), h(t):=g(t), || \text{Probabilistic ODE}:\mathbf{F}_{t}:=\mathbf{F}_{t}^{p}( \mathbf{m}_{t},t)\equiv\mathbf{a}_{t}^{*}(\mathbf{m}_{t},t)-\frac{1}{2}g_{t}^{2 }\nabla_{\mathbf{v}}\log p(\mathbf{m},t), h(t):=0.",,"\mathbf{a}^{\theta}(\mathbf{m}_{t},t)=g_{t}^{2}P_{11}\left(\frac{ \mathbf{x}_{1}-\mathbf{x}_{t}}{1-t}-\mathbf{v}_{t}\right)+g_{t}^{2} \nabla_{\mathbf{v}}\log p(\mathbf{m}_{t},t)","The acceleration term in the SDE and ODE formulation, combining the optimal control from Proposition 3 with the score function of the velocity."
ICLR_2024_oral_24,8,"Henceforth, we refer to the dynamics associated with the Bridge Matching SDE as AGM-SDE, and its corresponding ODE counterpart as AGM-ODE. Meanwhile, the linearity of the system implies the intermediate state \(\mathbf{m}_{t}\) and the close form solution of score term are analytically available. In particular, the mean \(\boldsymbol{\mu}_{t}\) and covariance matrix \(\boldsymbol{\Sigma}_{t}\) of the intermediate marginal \(p_{t}(\mathbf{m}_{t}|\mathbf{x}_{1})=\mathcal{N}(\boldsymbol{\mu}_{t}, \boldsymbol{\Sigma}_{t})\) of such a system can be analytically computed with \(\boldsymbol{\Sigma}_{t}=\begin{bmatrix}\Sigma_{t}^{xx}&\Sigma_{t}^{xx}\\ \Sigma_{t}^{xx}&\Sigma_{t}^{xx}\end{bmatrix}\otimes\boldsymbol{I}_{d}\), and \(\boldsymbol{\mu}_{t}=\begin{bmatrix}\mu_{t}^{x}\\ \mu_{t}^{x}\end{bmatrix}\), provided we have the boundary conditions \(\boldsymbol{\mu}_{0}\) and \(\boldsymbol{\Sigma}_{0}\) in place, as outlined in Sarkka and Solin (2019). Please see Appendix.D.3 for detail. In order to sample from such multi-variant Gaussian, one need to decompose the covariance matrix by Cholesky decomposition, and \(\mathbf{m}_{t}\) is reparametrized as:","\mathbf{m}_{t}=\boldsymbol{\mu}_{t}+\mathbf{L}_{t}\boldsymbol{\epsilon}= \boldsymbol{\mu}_{t}+\begin{bmatrix}L_{t}^{xx}\boldsymbol{\epsilon}_{0}\\ L_{t}^{xv}\boldsymbol{\epsilon}_{0}+L_{t}^{vv}\boldsymbol{\epsilon}_{1}\end{bmatrix},\nabla_{\mathbf{v}}\log p_{t}:=-\ell_{t}\boldsymbol{\epsilon}_{1}","where \(\boldsymbol{\Sigma}_{t}=\mathbf{L}_{t}\mathbf{L}_{t}^{\mathsf{T}}\), \(\epsilon=\begin{bmatrix}\boldsymbol{\epsilon}_{0}\\ \boldsymbol{\epsilon}_{1}\end{bmatrix}\sim\mathcal{N}(\mathbf{0},\mathbf{I}_{2d})\) and \(\ell_{t}=\sqrt{\frac{\Sigma_{t}^{xx}}{\Sigma_{t}^{xx}\Sigma_{t}^{xx}-(\Sigma _{t}^{xx})^{2}}}\)","\mathbf{m}_{t}=\boldsymbol{\mu}_{t}+\boldsymbol{\Sigma}_{t}^{1/2} \boldsymbol{\epsilon},\quad\boldsymbol{\epsilon}\sim\mathcal{N}(\mathbf{0}, \mathbf{I}_{2d})","Reparameterization of the intermediate state $\mathbf{m}_{t}$ for sampling from the multivariate Gaussian distribution using the mean $\boldsymbol{\mu}_{t}$ and the square root of the covariance matrix $\boldsymbol{\Sigma}_{t}$, with $\boldsymbol{\epsilon}$ being standard normal noise."
ICLR_2024_oral_24,9,"\).

Figure 2: Data estimation comparison with EDM (Karras et al., 2022). When the network is endowed with supplementary velocity, AGM gains the capacity to estimate the target data point during the early stages of the trajectory. One can use estimated image \(\tilde{\mathbf{x}}_{1}\) at \(t_{i}<t_{N}\) as generated results and allocated more NFE between time \([0,t_{i}]\) which results to smaller discretization error.

**Parameterization**: The Force term can be represented as a composite of the data point and Gaussian noise. Specifically,","\mathbf{a}^{*}(\mathbf{m}_{t},t)=4\mathbf{x}_{1}(1-t)^{2}-g_{t}^{2}P_{11}[ (\frac{L_{t}^{xx}}{1-t}+L_{t}^{xv})\mathbf{\epsilon}_{0}+L_{t}^{vv}\mathbf{ \epsilon}_{1}].",,"\mathbf{F}_{t}^{\theta}(\mathbf{m}_{t},t)=\mathbf{a}_{t}^{*}(\mathbf{m}_{t},t)-\frac{1}{2}g_{t}^{2}\nabla_{\mathbf{v}}\log p(\mathbf{m}_{t},t)=\mathbf{a}_{t}^{*}(\mathbf{m}_{t},t)+\frac{1}{2}g_{t}^{2}\ell_{t}\boldsymbol{\epsilon}_{1}.",The force term is parameterized as the sum of the optimal control and a score correction term involving the noise coefficient and the scaled Stein score.
ICLR_2024_oral_24,10,"We express the force term as \(\mathbf{F}_{t}^{\theta}=\mathbf{s}_{t}^{\theta}\cdot\mathbf{z}_{t}\). Here, \(\mathbf{z}_{t}\) assumes the role of regulating the output of the network \(\mathbf{s}_{t}^{\theta}\), ensuring that the variance of the network output is normalized to unity. For the detailed formulation of the normalizer \(\mathbf{z}_{t}\), please refer to Appendix.D.8. In a manner similar to the BM approach, one can formulate the objective function for regressing the force term as follows:","\min_{\theta}\mathbb{E}_{t\in[0,1]}\mathbb{E}_{\mathbf{x}_{1}\sim p_{\text{ data}}}\mathbb{E}_{\mathbf{m}_{t}\sim p_{t}(\mathbf{m}_{t}|\mathbf{x}_{1})} \lambda(t)[\|\mathbf{F}_{t}^{\theta}(\mathbf{m}_{t},t;\theta)-\mathbf{F} _{t}(\mathbf{m}_{t},t)\|_{2}^{2}]",Where \(\lambda(t)\) is known as the reweight of the objective function across the time horizon,"\mathcal{L}_{t}^{\text{AGM}}=\mathbb{E}_{\mathbf{m}_{t},t}\left\|\mathbf{s}_{t}^{\theta}(\mathbf{m}_{t},t;\theta)-\nabla_{\mathbf{v}}\log p(\mathbf{m}_{t},t)\right\|_{2}^{2},","The loss function $\mathcal{L}_{t}^{\text{AGM}}$ is defined for training the neural network to estimate the force term by minimizing the mean squared error between the predicted score function $\mathbf{s}_{t}^{\theta}$ and the true score $\nabla_{\mathbf{v}}\log p(\mathbf{m}_{t},t)$."
ICLR_2024_oral_24,11,"on. We defer the derivation of \(\ell_{t}\) and the presentation of \(\mathbf{L}_{t}\), \(\lambda(t)\) and \(\mathbf{a}_{t}\) in Appendix.D.

### Sampling from AGM

Once the paramterized force term \(\mathbf{F}_{t}^{\theta}\) is trained, we are ready to simulate the dynamics to generate the samples by plugging it back to the dynamics (7). One can use any type of SDE or ODE sampler to propagate the learnt system. Here we list our choice of sampler for AGM-SDE and AGM-ODE.

**Stochastic Sampler:** To simulate the SDE, prior works are majorly relying on Euler-Maruyama(EM) (Kloeden et al., 1992) and related methods. We adopt the Symmetric Splitting Sampler(SSS) from Dockhorn et al. (2021) in our AGM-SDE. This selection is based on the compelling performance it offers when dealing with momentum systems.

**Deterministic Sampler:** It is imperative to acknowledge that this system is inherently underactuated because the force term is exclusively injected into the velocity component, while velocity serves as the driving factor for the position--a variable of primary interest in generative modeling context. More specifically, at time step \(t_{i}\), the impact of force does not immediately manifest in the position but rather takes effect at a subsequent time step, denoted as \(t_{i+1}\) after discretizing time horizon. At time \(t_{0}\), it becomes undesirable to propagate the state \(\mathbf{x}_{0}\) using an initially uncontrolled velocity over an extended time interval \(\delta_{0}\). The presence of this delay phenomenon can also exert an influence when the time interval \(\delta_{t}\) is large, thereby impeding our ability to reduce the NFE during sampling. We propose the adoption of an Exponential Integrator (EI) approach, as elaborated in Zhang and Chen (2022). Empirical evidence suggests that this method aligns well with our model. We provide an illustrative example of how the AGM-ODE, in conjunction with the EI technique, can be employed to inject the learnt network into both velocity and position channels simultaneously:","\begin{bmatrix}\mathbf{x}_{t_{i+1}}\\ \mathbf{v}_{t_{i+1}}\end{bmatrix}=\Phi(t_{i+1},t_{i})\begin{bmatrix}\mathbf{x }_{t}\\ \mathbf{v}_{t}\end{bmatrix}+\sum_{j=0}^{w}\begin{bmatrix}\int_{t_{i}}^{t_{i+1} }(t_{i+1}-\tau)\mathbf{z}_{\tau}\cdot\mathbf{M}_{i,j}(\tau)\text{ d}\tau\ \mathbf{s}_{t}^{\theta}(\mathbf{m}_{t_{i-j}},t_{i-j}))\\ \int_{t_{i}}^{t_{i+1}}\mathbf{z}_{\tau}\cdot\mathbf{M}_{i,j}(\tau)\text{ d}\tau\cdot\mathbf{s}_{t}^{\theta}(\mathbf{m}_{t_{i-j}},t_{i-j})\end{bmatrix}",,\begin{bmatrix}\mathbf{x}_{t+\delta}\\ \mathbf{v}_{t+\delta}\end{bmatrix}=\begin{bmatrix}\mathbf{x}_{t}+\delta \mathbf{v}_{t}+\frac{\delta^{2}}{2}\mathbf{F}_{t}^{\theta}\\ \mathbf{v}_{t}+\delta\mathbf{F}_{t}^{\theta}\end{bmatrix}.\quad\text{(Exponential Integrator for AGM-ODE)},"Exponential Integrator update rule for AGM-ODE, enabling simultaneous injection of learned force into both position and velocity channels for improved sampling efficiency."
ICLR_2024_oral_24,12,"In Eq.11, \(\Phi(s,t)\) denotes the transition matrix for our system, while \(\mathbf{M}_{i,j}(\tau)\) represents the \(w-\)order multistep coefficient (Hochbruck and Ostermann, 2010). For a comprehensive derivation of these terms, please refer to Appendix.D.9. It is worth noting that the mapping of \(\mathbf{s}_{\theta}\) into both the position and velocity channels significantly emulates the errors introduced by discretization delays.

**Sampling-hop:** In the context of CLD (Dockhorn et al., 2021), their focus is on estimating the score function w.r.t. velocity, which essentially corresponds to estimating scaled \(\mathbf{\epsilon}_{1}\) in our notation. However, relying solely on the aforementioned information is not sufficient for estimating the data point \(\mathbf{x}_{1}\). Additional knowledge regarding \(\mathbf{\epsilon}_{0}\) is also required in order to perform such estimation. In our case, the training objective implicitly includes both \(\mathbf{\epsilon}_{0}\) and \(\mathbf{\epsilon}_{1}\) (see eq.9), hence one can manage to recover \(\mathbf{x}_{1}\) by Proposition.5. Remarkably, our observations have unveiled that when the network is equipped with additional velocity information, it acquires the capability to estimate the target data point during the early stages of the trajectory, as illustrated in fig.2. This estimation can be seamlessly integrated into AGM-SDE and AGM-ODE and we name it sampling-hop. Specifically,

**Proposition 5** (Sampling-Hop).: _Given the state, velocity and trained force term \(\mathbf{F}_{t}^{\theta}\) at time step \(t\) in sampling phase, The estimated data point \(\tilde{\mathbf{x}}_{1}\) can be represented as_","\tilde{\mathbf{x}}_{1}^{SDE}=\frac{(1-t)(\mathbf{F}_{t}^{\theta}+\mathbf{v}_{t })}{g_{t}^{2}P_{11}}+\mathbf{x}_{t},\text{ or }\quad\tilde{\mathbf{x}}_{1}^{ODE}=\frac{\mathbf{F}_{t}^{\theta}+g_{t}^{2}P_{ 11}(\alpha_{t}\mathbf{x}_{t}+\beta_{t}\mathbf{v}_{t})}{4(t-1)^{2}+g_{t}^{2}P_{ 11}(\alpha_{t}\mu_{t}^{x}+\beta_{t}\mu_{t}^{v})}",,"\tilde{\mathbf{x}}_{1}=\mathbf{x}_{t}+(1-t)\mathbf{v}_{t}+\frac{(1-t)^{2}}{4}\mathbf{F}_{t}^{\theta}(\mathbf{m}_{t},t).","Sampling-hop provides a closed-form expression to estimate the target data point $\mathbf{x}_1$ using the current state, velocity, and force term during sampling, enabling early estimation and reducing required integration steps."
ICLR_2024_oral_25,1,"### Open Surface Living on Watertight Surface

We start with the following simple observation on a category of open surfaces, which guides our insight to parameterize general 3D shapes with open surfaces that live on a watertight surface.

Any smooth and simply-connected open surface can be smoothly deformed to be a subset of a sphere.

This is a direct consequence of classical topological theories on surfaces, of which more mathematical details are given in Appendix A. Indeed, a large number of surfaces (_e.g._, plain T-shirts) can be completed to a watertight surface by first contracting the holes and later deforming it into a sphere. Inspired by this observation, we define a continuous and differentiable mapping \(\nu:\mathcal{M}\rightarrow\mathbb{R}\) on the template sphere \(\mathcal{M}\) to characterize if a point belongs to the open surface \(\mathcal{M}_{o}\):","\underbrace{\nu(x)>0,\ \ \forall x\in\mathrm{Interior}(\mathcal{M}_{o})}_{ \text{Case 1: inside the open surface}},\quad\underbrace{\nu(x)=0,\ \ \forall x\in\partial\mathcal{M}_{o}}_{\text{Case 2: on the surface boundary}},\quad\underbrace{\nu(x)<0,\ \ \text{Otherwise}}_{\text{ Case 3: outside the open surface}},",where \(\nu\) can be instantiated as the signed geodesic distance to the open surface boundary living on the watertight template,"\begin{equation}
    \mathcal{M}_{o} = \{ \mathbf{x} \in \mathcal{M} \mid \nu(\mathbf{x}) > 0 \}.
\end{equation}","A continuous and differentiable mapping $\nu:\mathcal{M}\rightarrow\mathbb{R}$ defines the subset of the template sphere $\mathcal{M}$ that corresponds to the open surface $\mathcal{M}_{o}$, where points with $\nu(\mathbf{x}) > 0$ belong to $\mathcal{M}_{o}$."
ICLR_2024_oral_25,2,"te. While the number of choices of \(\nu\) given some \(\mathcal{M}\) and \(\mathcal{M}_{o}\) can be infinite, without loss of generality we call the field of \(\nu\) manifold signed distance field (mSDF), since it is defined on a manifold surface and characterizes the boundary in a way that is similar to SDF.

The problem now effectively reduces to learning a ""2D mesh"" defined by the zero isoline of \(\nu\). Just as 3D meshes on the zero isosurface can be parameterized by 3D cubic grids in deep marching cubes, ""2D meshes"" (polyo-nal curve) can also be parameterized by a ""2D grid"", _i.e._, a mesh for the (deformed) sphere: simply to learn a \(\nu\) value on each of the sphere mesh vertices from which we extract non-watertight meshes. This process is illustrated in Figure 3. Intuitively speaking, an open surface can be viewed as the remaining essence after cutting out the hollow vacuum on a 3D shell, and hence we name the proposed 3D representation Ghost-on-the-Shell2.

Footnote 2: Inspiration drawn from the manga series _Ghost in the Shell_

Such a naive construction, however, poses modeling challenges when applied to general objects. First, it cannot capture watertight surfaces that are not homeomorphic to spheres (_e.g._, donuts). Furthermore, some naive deformation of a surface in 3D may result in self-intersection and therefore addressing this requires additional regularization and/or modeling techniques.

Figure 3: Illustration of non-watertight mesh extraction from some watertight triangular mesh. \(p_{1},p_{2}\) are the positions of (watertight) mesh vertices. \(\Delta p\!=\!\|p_{1}-p_{2}\|\) and \(\nu_{1}\!>\!0\!>\!\nu_{2}\) are the corresponding mSDF values. The orange triangle is extracted and the blue polygon is discarded.

Figure 2: Look-up table for Marching Tetrahedra (up to rotation symmetry). Grid vertices with and without a red dot possess SDF values of opposite signs.

Instead, we propose to jointly learn a general watertight mesh template, parameterized by a 3D grid of SDF values, in order to capture a larger set of meshes3. As we are not able to define \(\nu\) with mesh topology changing over time, we instead define \(\nu\) in the 3D space. Specifically, we store the discretized values of \(\nu\) in a 3D grid. The mSDF value of any point in a grid cell can therefore be computed by a barycentric interpolation of the \(\nu\) values on the grid cell vertices. We note that **G-Shell** reduces to a typical watertight surface representation if all mSDF values on the grid are set to positive values (_i.e._, no valid topological hole is defined on the manifold).

Footnote 3: Indeed, any orientable open surface without self-intersection can be modeled thereby.

### Efficient Mesh Extraction with **G-Shell**

With SDF and mSDF values stored in the same 3D grid, we obtain for **G-Shell** an efficient Marching-Cubes-like algorithm which reuses the interpolation coefficient (_rf_. Eqn. 3) for the mSDF sign computation. Specifically, with an edge \((p_{i},p_{j})\), the corresponding SDF values \(s_{i}<0<s_{j}\) and mSDF values \(\nu_{i},\nu_{j}\), we can compute the mSDF value on the extracted mesh vertex as \(\nu^{\prime}=(s_{i}\nu_{j}-s_{j}\nu_{i})/(s_{i}-s_{j})\). We give an example of the look-up table for tetrahedral grids in Figure 4, which enumerates all possible cases of SDF signs (on grid vertices) and mSDF signs (on watertight mesh vertices). Despite using tetrahedral grids as an example, we note that **G-Shell** is generally applicable to other grid structures and not limited to tetrahedral grids.

## 5 Applications of **G-Shell**

### Mesh Reconstruction from Multiview Images

With **G-Shell**, existing differentiable rasterization-based rendering methods (_e.g._, [17, 42]) can be seamlessly applied for end-to-end reconstruction of both 3D watertight and non-watertight meshes from multiview RGB and binary mask images. Reconstruction with rasterization not only allows the final geometry to be explicitly optimized without pose-processing, but also saves memory and time compared to volumetric rendering with UDFs [35, 38] - there is no need to evaluate densities on a number of sample points per ray anymore. Moreover, with physics-based mesh rendering, one can jointly optimize geometry, material and lighting in a single stage.

We note, however, some particular difficulties in non-watertight mesh reconstruction from images. Unlike watertight meshes where we never get to see the inside surface, non-watertight surfaces have two sides and one side may be more visible than the other. For example, the inside of a long dress may not be fully observed and, when actually seen, it is also likely to be poorly illuminated. Similarly, indirect illumination has to be considered along with realistic materials (especially highly specular ones) and potentially complex geometry. To simplify the problem while still being able to demonstrate the effectiveness of the **G-Shell** representation, we use Nvdiffrecmc [17], an occlusion-aware differentiable renderer that ignores indirect illumination but considers shadow rays.

Figure 4: **G-Shell** look-up table (up to rotational symmetry) for tetrahedral grids. Grid vertices with and without a red dot possess SDF values of opposite signs, and green dots on watertight mesh vertices indicates negative mSDF values. The pink regions represent the final extracted faces while the blue ones are the discarded regions on the watertight template mesh. Colored polygons other than triangles are cut along dashed lines.

Another technical challenge is how to identify the existence and location of topological holes with only 2D images, particularly when only a limited number of views are available. We therefore propose to regularize the mSDF values of the reconstructed mesh by introducing a ""hole-opening"" loss (the mSDF is parameterized by a function with some parameter set \(\theta_{\text{mSDF}}\)):",L_{\text{mSDF-reg}}(\theta_{\text{mSDF}})=\underbrace{\sum_{u:v_{\theta_{\text{ ndSF}}}(u)\geq 0}L_{\text{nuber}}(v_{\theta_{\text{noster}}}(u))}_{\text{ Encourage hole opening}}+\underbrace{\tau\cdot\sum_{\begin{subarray}{c}u^{\prime}:v_{\theta_{\text{ ndSF}}}(u^{\prime})=0\\ u^{\prime}\text{ visible from some }q\in Q\end{subarray}}L_{\text{nuber}}(v_{\theta_{\text{n }\text{ober}}}(u^{\prime})-\epsilon)}_{\text{Regularize holes from being too large}},,"\mathcal{L}_{\text{hole}} = \sum_{x\in\mathcal{X}}\max(0, -\nu(x)),","where the ""hole-opening"" loss penalizes negative mSDF values to encourage the formation of topological holes in the reconstructed mesh."
ICLR_2024_oral_25,3,"in which \(Q\) is the set of training camera poses, \(\tau\) and \(\epsilon\) are some positive scalars, \(L_{\text{nuber}}\) is the Huber loss function. We introduce the second regularization term to discourage topological holes from being too large, especially during the early stage of the optimization process. We provide all the details regarding the remaining regularization losses and other training settings in Appendix B.

### G-MeshDiffusion: Generative Modeling of Geometry

With the regular grid structure of the **G-Shell** parameterization, it is straightforward to train generative models to produce the grid attributes (SDF, mSDF and potentially grid deformation) to enable non-watertight mesh generation. Indeed, **G-Shell** enables generative modeling with diffusion models [19] in which a regular input structure is necessary.

To demonstrate the generative modeling of **G-Shell**, we consider MeshDiffusion [36], which generates watertight meshes by sampling SDF and grid deformation in a 3D tetrahedral grid, to generate non-watertight meshes. Although it is possible to simply introduce the additional dimension of mSDF on the grid vertex attributes, the generated shapes can be noisy as pointed out in [36]. Specifically, the boundary vertices of a generated non-watertight mesh are computed via an interpolation with mSDF:","u^{\prime}=\frac{|\nu_{1}|}{|\nu_{1}-\nu_{2}|}\cdot u_{2}-\frac{|\nu_{2}|}{| \nu_{1}-\nu_{2}|}\cdot u_{1},\quad\nu_{1}<0<\nu_{2},",,\nu^{\prime}=(s_{i}\nu_{j}-s_{j}\nu_{i})/(s_{i}-s_{j}),"Interpolated mSDF value $\nu^{\prime}$ computed from SDF values $s_i, s_j$ and mSDF values $\nu_i, \nu_j$ at grid vertices along an edge"
ICLR_2024_oral_26,1,"### MLPs as graphs

We first outline the procedure for constructing a neural graph \(\mathcal{G}=(\mathbf{V},\mathbf{E})\) with node features \(\mathbf{V}\in\mathbb{R}^{n\times d\mathbf{V}}\) and edge features \(\mathbf{E}\in\mathbb{R}^{n\times n\times d\mathbf{E}}\). Here, \(n\) denotes the total number of nodes in the graph, and \(d_{\mathbf{V}},d_{\mathbf{E}}\) denote the number of node and edge features, respectively. Consider an MLP with \(L\) fully connected layers. The weight matrices for this MLP are \(\big{\{}\mathbf{W}^{(1)},\dots,\mathbf{W}^{(L)}\big{\}}\), and the biases are \(\big{\{}\mathbf{b}^{(1)},\dots,\mathbf{b}^{(L)}\big{\}}\). Each weight matrix \(\mathbf{W}^{(l)}\) has dimensions \(d_{l}\times d_{l-1}\) and each bias \(\mathbf{b}^{(l)}\) has dimensions \(d_{l}\). The total number of nodes is then given by \(n=\sum_{l=0}^{L}d_{l}\), where \(d_{0}\) is the dimension of the input. We define the edge and node feature matrices containing the weights and biases as

Figure 1: Representing a neural network and its weights as a _neural graph_. We assign neural network parameters to graph features by treating biases \(\mathrm{b}_{i}\) as corresponding node features \(V_{i}\), and weights \(\mathrm{W}_{ij}\) as edge features \(\mathbf{E}_{ij}\) connecting the nodes in adjacent layers.

follows:",\mathbf{E}=(\begin{array}{cccc}\mathbf{\mathrm{W}}^{(l)\top}&\mathbf{\mathrm{W}}^{(2) \top}&\mathbf{\mathrm{W}}^{(l)\top}&\mathbf{\mathrm{W}}^{(l)\top}\\ \hline\ddots&\mathbf{\mathrm{W}}^{(2)\top}&\mathbf{\mathrm{W}}^{(2)\top}&\mathbf{\mathrm{W}} ^{(l)\top}\\ \end{array}),where \(\mathbf{\mathrm{W}}^{(l)\top}\) denotes the transposed weight matrix,"\mathbf{V} = \text{diag}(\mathbf{b}^{(1)}, \dots, \mathbf{b}^{(L)}), \quad \mathbf{E} = \bigoplus_{l=1}^{L} \mathbf{W}^{(l)},","Node features are formed by the biases of each layer, and edge features are formed by the weight matrices connecting nodes across layers."
ICLR_2024_oral_26,2,"ix. The edge features form a sparse block matrix where the sparsity pattern depends on the number of nodes per layer: the first diagonal block has size \(d_{0}\), the second \(d_{1}\), and so on. We can verify that putting \(\mathbf{\mathrm{W}}^{(l)}\) as the first off-diagonal blocks means that they have the expected size of \(d_{l}\times d_{l-1}\). The first \(d_{0}\) node features in \(\mathbf{V}\) are set to \(0\) to reflect the fact that there are no biases at the input nodes. We can show that our neural graph has a one-to-one correspondence to the neural network's computation graph. This ensures that distinct MLPs get mapped to distinct neural graphs.

Similar to the example above, many applications of neural networks in parameter space only include the neural network parameters as input. For such applications, an MLP has scalar weights \(\mathbf{\mathrm{W}}^{(l)}_{ij}\in\mathbb{R}\) and biases \(\mathbf{\mathrm{b}}^{(l)}_{i}\in\mathbb{R}\) comprising the elements of \(\mathbf{E},\mathbf{V}\), resulting in one-dimensional features, _i.e._\(d_{\mathbf{V}}=d_{\mathbf{E}}=1\). Depending on the task at hand, however, we have the flexibility to incorporate additional edge and node features. We explore some examples of this in Section 2.4.

### CNNs as graphs

So far, we have only described how to encode basic MLPs as graphs. We now address how to generalize the graph representation to alternative network architectures, namely convolutional networks. To make the exposition of our method clear, we will use the following interpretation of convolutional layers and CNNs. Convolutional layers take as input a multi-channel input image (_e.g._ an RGB image has \(C=3\) channels) or a multi-channel feature map, and process it with a filter bank, _i.e._ a collection of convolutional kernels - often termed filters. Each filter results in a single-channel feature map; applying all filters in the filter bank results in a collection of feature maps, which we concatenate together in a multi-channel feature map. CNNs, in their simplest form, are a stack of convolutional layers mixed with non-linearities in between.

Permutation symmetries in a CNN work similarly to an MLP; permuting the filters in a layer while simultaneously permuting the channels of each filter in the subsequent layer effectively cancels out the permutations, shown visually in Figure 5 in Appendix C.2. Under the aforementioned interpretation, single-channel slices of a multi-channel feature map (or single-channel slices of the input) correspond to nodes in our neural graph. Each node is connected via edges incoming from a particular convolutional kernel from the filter bank. By treating each channel as a node, our CNN neural graph respects the permutation symmetries of the CNN.

We now describe the neural graph representation for each component in a CNN (with more details in Appendix C.2). As a working example, let us consider a CNN with \(L\) convolutional layers. It consists of filters \(\left\{\mathbf{\mathrm{W}}^{(l)}\right\}\) and biases \(\left\{\mathbf{\mathrm{b}}^{(l)}\right\}\) for layers \(l\in\{1,\dots,L\}\), where \(\mathbf{\mathrm{W}}^{(l)}\in\mathbb{R}^{d_{l}\times d_{l-1}\times w_{l}\times h_{l }},\mathbf{\mathrm{b}}^{(l)}\in\mathbb{R}^{d_{l}}\), and \(w_{l},h_{l}\) denote the width and the height of kernels at layer \(l\).

**Convolutional layers.** Convolutional layers are the core operation in a convolutional network. Since channels in a CNN correspond to nodes in the neural graph, we can treat the biases the same way as in an MLP, namely as node features - see Equation (1). The kernels, however, cannot be treated identically due to their spatial dimensions, which do not exist for MLP weights. To resolve this, we represent the kernels by flattening their spatial dimensions to a vector. To ensure spatial self-consistency across kernels of different sizes, we first zero-pad all kernels to a maximum size \(s=(w_{\text{max}},h_{\text{max}})\), and then flatten them. This operation allows for a unified representation across different kernel sizes; we can process all kernels with the same network. The maximum kernel size is chosen as a hyperparameter per experiment; this operation is visualized in Figure 6 in Appendix C.2. After this operation, the kernels can be treated as a multi-dimensional equivalent of linear layer weights. We construct the edge features matrix similarly to Equation (1); the only difference is that this matrix no longer has scalar features. Instead, we have \(\mathbf{E}\in\mathbb{R}^{n\times n\times d_{\mathbf{E}}}\), with \(d_{\mathbf{E}}=w_{\text{max}}\cdot h_{\text{max}}\).

**Flattening layer.** CNNs are often tasked with predicting a single feature vector per image. In such cases, the feature maps have to be converted to a single feature vector. Modern CNN architectures (He et al., 2016) perform adaptive pooling after the last convolutional layer, which pools the whole feature map in a single feature vector, while traditional CNNs (Simonyan and Zisserman, 2015) achieved that by flattening the spatial dimensions of the feature maps. The downside of the latter approach is that CNNs are bound to a specific input resolution and cannot process arbitrary images. Our neural graph is not bound to any spatial resolution, and as such, its construction does not require any modifications to integrate adaptive pooling. While the CNNs in all our experiments employ adaptive pooling, we also propose two mechanisms to address traditional flattening, which we discuss in Appendix C.2.

**Linear layers.** Linear layers are often applied after flattening or adaptive pooling to produce the final feature vector representation for an image. The most straightforward way to treat linear layers in a CNN is in the exact same fashion as in an MLP. The downside of this approach is that linear layers and convolutional layers have separate representations, as their \(d_{\mathbf{E}}\) will typically differ. An alternative is to treat linear layers as \(1\times 1\) convolutions, which allows for a unified representation between linear and convolutional layers. When treated as convolutions, the linear layers are padded to the maximum kernel size and flattened. In our experiments, we explore both options, and choose the most suitable via hyperparameter search.

### Modelling heterogeneous architectures

One of the primary benefits of the neural graph representation is that it becomes straightforward to represent varying network architectures that can all be processed by the same graph neural network. Notably, we do not require any changes to accommodate a varying number of layers, number of dimensions per layer, or even completely different architectures and connectivities between layers. When dealing with a single architecture, we can opt to ignore certain architectural components that are shared across instances, as our method - and related methods - can learn to account for them during training. These include activation functions and residual connections. Thus, we will now describe how we can incorporate varying non-linearities and residual connections in heterogeneous architectures of CNNs or MLPs.

**Non-linearities.** Non-linearities are functions applied elementwise to each neuron, and can thus be encoded as node features. We create embeddings for a list of common activation functions and add them to the node features.

**Residual connections.** Residual connections are an integral component of modern CNN architectures. A residual connection directly connects the input of a layer to its output as \(\mathbf{y}=f(\mathbf{x})+\mathbf{x}\). Incorporating residual connections in our neural graph architecture is straightforward, since we can include edges from each sender node in \(\mathbf{x}\) to the respective receiving node in \(\mathbf{y}\). Since residual connections can be rewritten as \(\mathbf{y}=f(\mathbf{x})+\mathbf{I}\mathbf{x}\), where \(\mathbf{I}\) is the identity matrix, the edge features have a value of 1 for each neuron connected.

Transformers.The feedforward component of transformers (Vaswani et al., 2017) can be treated as an MLP and its neural graph follows a similar schema as previously outlined. We explain the conversion rules for normalization layers in Appendix C.3 and the conversion rules for multi-head self-attention in Appendix C.4. After individually converting the parts, we can compose them into a neural graph for transformers.

### Node and Edge Representation

Our neural graph representation gives us the flexibility to choose what kinds of data serve as node and edge features. Though we mainly focus on weights and biases, there are also other options that we can use, for example we use the gradients in a learning to optimize setting.

**Edge direction.** Our basic encoding only considers the forward pass computations of the neural network, yielding a directed acyclic graph. To facilitate information flow from later layers back to earlier layers we can add reversed edges to the neural graph. Specifically, we include \(\mathbf{E}^{\top}\) as additional edge features. Similarly, we can also include \(\mathbf{E}+\mathbf{E}^{\top}\) as extra features representing undirected features.

**Probe features.** Humans tend to interpret complicated functions by probing the function with a few input samples and inspecting the resulting output. We give the graph neural network a similar ability by adding extra features to every node that correspond to specific inputs. In particular, we learn a set of sample input values that we pass through the input neural network and retain the values for all the intermediate activations and the output. For example, consider the simple input neural network \(f(\mathbf{x})=\mathbf{W}^{(2)}\alpha\big{(}\mathbf{W}^{(1)}\mathbf{x}+\mathbf{b}^{(1) }\big{)}+\mathbf{b}^{(2)}\) for which we acquire an extra node feature:","\mathbf{V}_{\text{probe}}=\Big{(}\mathbf{x},\alpha\Big{(}\mathbf{W}^{(1)}\mathbf{x}+ \mathbf{b}^{(1)}\Big{)},f(\mathbf{x})\Big{)}^{\top},",,\mathbf{V}_{\text{probe}} = \alpha\left(\mathbf{W}^{(1)}\mathbf{x}_{\text{probe}} + \mathbf{b}^{(1)}\right),Extra node feature corresponding to intermediate activations from probe inputs passed through the neural network.
ICLR_2024_oral_26,3,"per \(\mathbf{x}\in\{\mathbf{x}_{m}\}_{m=1,\dots,M}\) where \(\{\mathbf{x}_{m}\}_{m=1,\dots,M}\) is a set of learned input values. For simplicity, we use the same set for all neural graphs. The features in Equation 2 are then included as additional node features. Notably, probe features are invariant to _all_ augmentations on the input neural network's parameters as long as it maintains the exact same function for the output and hidden layers.

**Normalization.** Existing works on parameter space networks (Navon et al., 2023; Zhou et al., 2023) perform feature normalization by computing the mean and standard deviation separately for each neuron in the training set. This operation, however, violates the neuron symmetries; since neurons can be permuted, it becomes essentially arbitrary to normalize neurons across neural networks. We propose a simple alternative that respects permutation equivariance: we compute a single mean and standard deviation for each layer (separately for weights and biases), and use them to standardize our neural graph, _i.e._\(\hat{\mathbf{W}}^{(l)}=\Big{(}\mathbf{W}^{(l)}-\mu_{W}^{(l)}\Big{)}/\sigma_{W }^{(l)},\hat{\mathbf{b}}^{(l)}=\Big{(}\mathbf{b}^{(l)}-\mu_{b}^{(l)}\Big{)}/ \sigma_{b}^{(l)},l\in\{1,\dots,L\}\).

**Positional embeddings.** Before processing the neural graph, we augment each node with learned positional embeddings. To maintain the permutation symmetry in the hidden layers, nodes corresponding to the same intermediate layer share the same positional embedding. Although this layer information is implicitly available in the adjacency matrix, using positional embeddings allows immediate identification, eliminating the need for multiple local message-passing steps. However, we distinguish between the symmetries of input and output nodes and those of hidden nodes. In a neural network, rearranging the input or output nodes generally alters the network's underlying function. In contrast, the graph representation is indifferent to the order of input and output nodes. To address this discrepancy, we introduce unique positional embeddings for each input and output node, thereby breaking the symmetry between and enabling GNNs and transformers to differentiate between them.

## 3 Learning with Neural Graphs

Graph neural networks (GNNs) and transformers are equivariant with respect to the permutation symmetries of graphs. We present one variant of each and adapt them for processing neural graphs.

**GNN.** Graph neural networks (Scarselli et al., 2008; Kipf and Welling, 2017) in the form of message passing neural networks (Gilmer et al., 2017) apply the same local message passing function at every node. While various GNN variants exist, only few of them are designed to accommodate edge features, and even fewer update these edge features in hidden layers. Updating edge features is important in our setting since our primary features reside on the edges; when per-weight outputs are required, they become imperative. We choose PNA (Corso et al., 2020) as our backbone, a state-of-the-art graph network that incorporates edge features. However, PNA does not update its edge features. To address this gap, we apply a common extension to it by updating the edge features at each layer using a lightweight neural network \(\phi_{e}\):","\mathbf{e}_{ij}^{(k+1)}=\phi_{e}^{(k+1)}\Big{(}\Big{[}\mathbf{v}_{i}^{(k)}, \mathbf{e}_{ij}^{(k)},\mathbf{v}_{j}^{(k)}\Big{]}\Big{)},",where \(k\) is the layer index in our network,"\mathbf{E}^{(k+1)} = \phi_{e}\left(\mathbf{E}^{(k)}, \mathbf{V}_{i}^{(k)}, \mathbf{V}_{j}^{(k)}\right),",Updating edge features using a lightweight neural network $\phi_e$ at each layer to improve the model's capacity to capture complex relationships between nodes and edges in the neural graph.
ICLR_2024_oral_26,4,"rk. This ensures that the edge features are updated per layer based on incident node features and produce a representation that depends on the graph structure.

Navon et al. (2023) suggest that the ability to approximate the forward pass of the input neural network can be indicative of expressive power. Part of the forward pass consists of the same operation for each edge: multiply the weight with the incoming activation. Motivated by the insights on algorithmic alignment by Xu et al. (2020), we adapt the message-passing step to include this multiplicative interaction between the node and edge features. In particular, we apply FiLM to the message passing step (Perez et al., 2018; Brockschmidt, 2020):","\mathbf{m}_{ij}=\phi_{\text{scale}}(\mathbf{e}_{ij})\odot\phi_{m}([\mathbf{v}_ {i},\mathbf{v}_{j}])+\phi_{\text{shift}}(\mathbf{e}_{ij}).",,"\mathbf{e}_{ij}^{(k+1)}=\phi_{e}^{(k+1)}\Big{(}\gamma\Big{(}\mathbf{v}_{i}^{(k)}\Big{)}\odot \mathbf{e}_{ij}^{(k)}+\beta\Big{(}\mathbf{v}_{j}^{(k)}\Big{)}\Big{)},",where the FiLM layer applies a multiplicative and additive transformation to the edge features based on the node features
ICLR_2024_oral_26,5,"Note that this differs from the FiLM-GNN (Brockschmidt, 2020) in that we compute the scaling factors based on the edge features and not based on the adjacent node's features.

**Transformer.** The transformer encoder (Vaswani et al., 2017) can be seen as a graph neural network that operates on the fully connected graph. Similar to GNNs, the original transformer encoder and common variants do not accommodate edge features. We use the transformer variant with relational attention (Diao and Loynd, 2023) that adds edge features to the self-attention computation. The relational transformer is already equipped with updates to the edge features. As per GNNs, we further augment the transformer with modulation to enable multiplicative interactions between the node and edge features. In particular, we change the update to the value matrix in the self-attention module:",\mathbf{v}_{ij}=(\mathbf{W}_{\text{scale}}^{\text{value}}\mathbf{e}_{ij} )\odot(\mathbf{W}_{n}^{\text{value}}\mathbf{v}_{j})+\mathbf{ W}_{\text{shift}}^{\text{value}}\mathbf{e}_{ij}.,,"\mathbf{V}^{(k+1)} = \phi_{\text{mod}}(\mathbf{Q}^{(k)}, \mathbf{K}^{(k)}, \mathbf{E}^{(k)}) \odot \mathbf{V}^{(k)},",where the value matrix is modulated by the node features through a learnable function
ICLR_2024_oral_27,1,"### The \(\int\)-noise representation

A discrete 2D Gaussian noise of dimension \(D\times D\) is represented by the function \(G:(i,j)\in\{1,\ldots,D\}^{2}\to X_{i,j}\) that maps a pixel coordinate \((i,j)\) to a random variable \(X_{i,j}\). Diffusion models employ this discrete formulation, and random variables are assumed to be independently and identically distributed (i.i.d.) Gaussian samples \(X_{i,j}\sim\mathcal{N}(0,1)\). At the core of our work is the reinterpretation of this discrete 2D Gaussian noise as the integral of an underlying infinite noise field.

An infinite-resolution noise field is represented by a 2D white Gaussian noise signal. To construct that, we start by endowing the domain \(E=[0,D]\times[0,D]\) with the usual Borel \(\sigma\)-algebra \(\mathcal{E}=\mathcal{B}(E)\) and the standard Lebesgue measure \(\nu\). Then, the white Gaussian noise on the \(\sigma\)-finite measure space \((E,\mathcal{E},\nu)\) is defined as a function \(W:A\in\mathcal{E}\to W(A)\sim\mathcal{N}\left(0,\nu(A)\right)\) that maps \(A\rightarrow\) a subset of the domain \(E\) -- to a Gaussian-distributed variable, with variance \(\nu(A)\)(Walsh, 2006). There are many valid ways to subdivide the domain representing the continuous noise. The standard discrete setting partitions the domain \(E\) into \(D\times D\) regularly spaced non-overlapping square subsets. We denote this partition as \(\mathbb{A}^{0}\subseteq\mathcal{E}\) (level 0 in the inset image). Another way of partitioning the domain is to further refine \(E\) into a higher resolution set \(\mathbb{A}^{k}\subseteq\mathcal{E}\) (levels \(k=1,2,...,\infty\) in the inset image, where each \(k\)-th level below subdivides each pixel in level \(0\) into \(N_{k}=2^{k}\times 2^{k}\) sub-pixels.) Due to the properties of white Gaussian noise, integrating sub-pixels of the noise defined on \(\mathbb{A}^{k}\) maintains the properties of noise defined on \(\mathbb{A}^{0}\). If we assume there is only one single pixel sample in the domain (\(D=1\)) \(A^{0}=[0,1]\times[0,1]\), with \(\mathbb{A}^{k}=\{A_{1}^{k},\ldots,A_{N_{k}}^{k}\}\) representing the \(N_{k}\) sub-pixels at a finer resolution \(k\), the following holds:",\sum_{i=1}^{N_{k}}W(A_{i}^{k})=W(\bigcup_{i=1}^{N_{k}}A_{i}^{k})=W(A^{0}).,,\sum_{i=1}^{N_{k}} W\left(A_{i}^{k}\right) = W\left(\bigcup_{i=1}^{N_{k}} A_{i}^{k}\right) = W\left(A^{0}\right).,"This equation expresses the additive property of white Gaussian noise under partitioning, showing that the sum of noise values over refined sub-pixel regions equals the noise value over the original unpartitioned region."
ICLR_2024_oral_27,2,"The proposed \(\int\)-noise refers to the idea that instead of representing a noise value in a discrete point of domain, we rather represent the white noise integral over a pre-specified area. This property is represented in Equation (1) and a more thorough explanation is presented in Appendix B.1. Since we assume that each pixel on the coarsest level \(\mathbb{A}^{0}\) has unit area, the noise variance \(\nu_{k}=\nu(A_{i}^{k})\) at each level is implicitly scaled by the sub-pixel area as \(\nu_{k}=1/N_{k}\). While it is impossible to sample the noise \(\mathbb{A}^{\infty}\) in the infinite setting, we show in Section 2.2 that approximating it with a higher-resolution grid is sufficient for a temporally coherent noise transport.

**Conditional white noise sampling.** In practice, after obtaining an _a priori_ noise, e.g. from noise inversion techniques in diffusion models, one important aspect is how to construct the \(\int\)-noise at \(\mathbb{A}^{k}\) from samples defined at \(\mathbb{A}^{0}\). This is fundamentally a conditional probability question: given the value of an entire pixel, what is the distribution of its sub-pixels values? Let \(W(\mathbb{A}^{k})=\left(W(A_{1}^{k}),\ldots,W(A_{N_{k}}^{k})\right)^{\top} \sim\mathcal{N}(\mathbf{0},\nu_{k}\mathbf{I})\) be the \(N_{k}\)-dimensional Gaussian random variable representing sub-pixels of a single pixel. Then, the conditional distribution \(\left(W(\mathbb{A}^{k})|W(A^{0})=x\right)\) is","(W(\mathbb{A}^{k})|W(A^{0})=x)\sim\mathcal{N}(\mathbf{\bar{\mu}}, \mathbf{\bar{\Sigma}}),\quad\text{with }\mathbf{\bar{\mu}}=\frac{x}{N_{k}}\mathbf{u},\mathbf{\bar{\Sigma}}=\frac{1}{N_{k}} (\mathbf{I}_{N_{k}}-\frac{1}{N_{k}}\mathbf{u}\mathbf{u}^{\top}),","where \(\mathbf{u}=(1,\ldots,1)^{\top}\)","\mathcal{N}\left(\frac{x}{N_{k}}\mathbf{1},\ \nu_{k}\left(\mathbf{I}-\frac{1}{N_{k}}\mathbf{1}\mathbf{1}^{\top}\right)\right),","The conditional distribution of sub-pixel noise values given the total pixel value, characterized by a multivariate Gaussian with mean proportional to the pixel value and a covariance matrix that enforces consistency with the total integral."
ICLR_2024_oral_27,3,"\). By setting \(\mathbf{U}=\sqrt{N_{k}}\mathbf{\bar{\Sigma}}\), the reparameterization trick gives us a simple way to sample \(W(\mathbb{A}^{k})\) as","(W(\mathbb{A}^{k})|W(A^{0})=x)=\mathbf{\bar{\mu}}+\mathbf{U}Z=\frac{x}{N_{k}}\mathbf{ u}+\frac{1}{\sqrt{N_{k}}}(Z-\langle Z\rangle\mathbf{u}),\quad\text{with }Z\sim(\mathbf{0},\mathbf{I}),",where \(\langle Z\rangle\) is the mean of \(Z\),"W(\mathbb{A}^{k}) = \mathbf{\bar{\mu}} + \mathbf{U} \boldsymbol{\epsilon}, \quad \text{with } \boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}),",where $\mathbf{U}$ is the scaled covariance matrix and $\boldsymbol{\epsilon}$ is a standard Gaussian noise vector used for reparameterization.
ICLR_2024_oral_27,4,"\). Intuitively, in order to conditionally sample the white noise under a pixel of value \(x\) at level \(k\), one can 1) unconditionally sample a discrete \(N_{k}=2^{k}\times 2^{k}\) Gaussian sample, 2) remove its mean from it, and 3) add the pixel value \(x\) (up to a scaling factor). The full derivation of Equations (2) and (3) and the corresponding python code are included in Appendix B.

### Temporally-correlated distribution-preserving noise transport

In this section, we first introduce our proposed _noise transport equation_, which offers a theoretical way of warping a continuously defined white noise while preserving its characteristics. Then, we present our practical implementation and show that it still retains many theoretical guarantees from the infinite setting. Lastly, we provide a simple 1-D example to support the analysis of how the proposed \(\int\)-noise balances between satisfying seemingly opposite objectives such as preserving the correlation imposed by interpolation and maintaining the original noise distribution.

**White noise transport.** We first assume that the noise is transported with a diffeomorphic deformation field \(\mathcal{T}:E\to E\). This mapping could be represented by an optical flow field between two frames, or a deformation field for image editing. Our goal is to transport a continuous white noise \(W\) with \(\mathcal{T}\) in a distribution-preserving manner. The resulting noise \(\mathcal{T}(W)\) can be expressed as an Ito integral through our _noise transport equation_ for any subset \(A\subseteq E\) as","\mathcal{T}(W)(A)=\int_{\mathbf{x}\in A}\frac{1}{|\nabla\mathcal{T}( \mathcal{T}^{-1}(\mathbf{x}))|^{\frac{1}{2}}}W(\mathcal{T}^{-1}( \mathbf{x}))\ d\mathbf{x},",where \(\left|\nabla\mathcal{T}\right|\) is the determinant of the Jacobian of \(\mathcal{T}\),"\mathcal{T}(W)(A) = W(\mathcal{T}^{-1}(A)),",where the transported noise $\mathcal{T}(W)$ evaluated on subset $A$ is equal to the original noise $W$ evaluated on the pre-image $\mathcal{T}^{-1}(A)$.
ICLR_2024_oral_27,5,"\). Intuitively, Equation (4) warps a non-empty set of the domain with the deformation field \(\mathcal{T}^{-1}\), fetching values from the original white noise at the warped domain coordinates. The determinant of the Jacobian is necessary to rescale the samples according to the amount of local stretching that the deformation induces, while also accounting for the variance change required by the white noise definition. A detailed derivation of Equation (4) can be found in the Appendix C.2. In practice, optical flow maps can be non-diffeomorphic due to discontinuities and disocclusions. Appendix C.4 explains our treatment of these cases.

**Discrete Warping.** The noise transport equation (Equation (4)) cannot be solved in practice due the infinite nature of the white noise. Thus, we first compute the higher-resolution discrete \(\int\)-noise \(W(\mathbb{A}^{k})\), possibly from an _a priori_ sample (Equation (3)). Since the pixel area can undergo a non-linear warping deformation, we subdivide its contour into \(s\) smaller segments which are mapped backwards via the reverse deformation field. The warped segments define a polygonal shape that is then triangulated and rasterized over the high-resolution domain \(\mathbb{A}^{k}\). Lastly, the sub-pixels covered by the warped polygon are summed together and normalized, which yields the discrete noise transport for the noise pixel at position \(\mathbf{p}\)","G(\mathbf{p})=\frac{1}{\sqrt{|\Omega_{\mathbf{p}}|}}\sum_{A_{s}^{k}\in\Omega_ {\mathbf{p}}}W_{k}(A_{s}^{k})\,",,"\mathcal{T}(W)_{\mathbf{p}}=\frac{1}{\sqrt{|\nabla\mathcal{T}(\mathbf{p})|}}\sum_{A_{i}^{k}\in\mathcal{T}^{-1}(\mathbf{p})}W(A_{i}^{k}),","where $|\nabla\mathcal{T}(\mathbf{p})|$ is the determinant of the Jacobian at position $\mathbf{p}$, and the summation aggregates sub-pixel integrals from the pre-image of $\mathbf{p}$ under the inverse deformation."
ICLR_2024_oral_27,6,"Figure 2: (a) **The discrete noise transport equation pipeline.** A subdivided pixel contour (top right) is triangulated and traced backwards from frame \(T\) to frame \(0\) (top left). Then the warped triangulated shape is rasterized into a higher resolution approximation of the white noise (bottom). The sub-pixel values are added together, and properly scaled by Equation (5). (b) **1-D toy example.** A pixel slides between two existing pixels whose values \(x_{0}\), \(x_{1}\) are sampled from a Gaussian distribution. Bilinear interpolation creates a sample of lower variance (straight line), whereas \(\int\)-noise would create samples that follow a Brownian bridge between \(x_{0}\) and \(x_{1}\), maintaining a unit variance.

where \(W_{k}=\sqrt{N_{k}}\cdot W\) is the white noise scaled to unit variance at level \(k\), and \(\Omega_{\mathbf{p}}\subseteq\mathbb{A}^{k}\) contains all subpixels at level \(k\) that are covered by the warped pixel polygon, with \(|\Omega_{\mathbf{p}}|\) representing the cardinality of the set. A detailed algorithm is outlined in Appendix C.4, and illustrated in Figure 1(a). Note that the discrete implementation will still preserve independence between neighboring pixels in the warped result. This is because the warped polygons still form a partition of the space, so each sub-pixel in \(\mathbb{A}^{k}\) will only belong to a single warped polygon. More details on the discretization of Equation (4) into Equation (5) can be found in Appendix C.3.

**Toy example in 1-D.** We will demonstrate the properties of our \(\int\)-noise sampling in a simpler one dimensional setting. Consider a 1-D set of _i.i.d._ random variables indexed by \(\mathcal{I}=\{0,\ldots,n\}\) with values represented by \(\{x_{0},x_{1},\ldots,x_{n}\}\sim\mathcal{N}(0,1)\), and a mapping function that translates the discrete locations by a constant \(\mathcal{T}^{1}_{1D}(i)=i-\alpha\), where \(i\in\mathcal{I}\) and \(\alpha\in[0,1]\). Using a simple linear interpolation to compute the transported values \(z_{i}\) yields","z_{i}=\alpha x_{i-1}+(1-\alpha)x_{i},\quad z_{i}\sim\mathcal{N}(0,\,\sigma_{z }^{2}),\quad\text{with }\sigma_{z}^{2}=\alpha^{2}+(1-\alpha)^{2}.",,"z_{i}=(1-\alpha)x_{i}+\alpha x_{i+1},",which results in a linear interpolation between adjacent samples $x_i$ and $x_{i+1}$
ICLR_2024_oral_27,7,"The equation above means that the variance of \(z_{i}\) is a quadratic function of \(\alpha\) such that \(\sigma_{z}^{2}=1\) for \(\alpha\in\{0,1\}\) and \(\sigma_{z}^{2}<1\) for \(\alpha\in(0,1)\). This shows that the linear interpolation does not preserve the original distribution of the input variables.

However, if we obtain \(x_{i-1}\) and \(x_{i}\) from the integral over an underlying high-resolution white noise, the original distribution can be preserved. By employing the \(\int\)-noise, the pixel's value that is sampled between \(x_{i-1}\) and \(x_{i}\) is no longer deterministic. Figure 1(b) shows the value of \(z\) for different higher-resolution samples. Mathematically, the value of \(z\) is now a _Brownian bridge_ between neighboring \(x\) values. In Appendix C.5, we show that the value of \(z\) is a conditional probability distribution given by","z_{i}|x_{i},x_{i-1}\sim\mathcal{N}(\mu_{\infty},\sigma_{\infty}^{2}),\quad \text{with }\{\begin{array}{l}\mu_{\infty}=\alpha x_{i-1}+(1-\alpha)x_{i}\\ \sigma_{\infty}^{2}=1-(\alpha^{2}+(1-\alpha)^{2})=1-\sigma_{z}^{2}\end{array}.",,"z\sim\mathcal{N}(\alpha x_{i-1}+(1-\alpha)x_{i},\,\alpha(1-\alpha)).",where $z$ follows a Gaussian distribution conditioned on the neighboring values $x_{i-1}$ and $x_{i}$ with a variance that depends on the interpolation parameter $\alpha$
ICLR_2024_oral_28,1,"## 1 Introduction

In the active linear regression problem, we are given a data matrix \(\mathbf{A}\in\mathbb{R}^{n\times d}\) with \(n\gg d\) rows and query access to a target vector \(\mathbf{b}\in\mathbb{R}^{n}\). The goal is to learn parameters \(\mathbf{x}\in\mathbb{R}^{d}\) such that \(\mathbf{A}\mathbf{x}\approx\mathbf{b}\) while observing as few entries in \(\mathbf{b}\) as possible. We study this problem in the challenging agnostic learning or ""adversarial noise"" setting, where we do not assume any underlying relationship between \(\mathbf{A}\) and \(\mathbf{b}\). Instead, our goal is to find parameters competitive with the best possible fit, good or bad. Specifically, considering \(\ell_{2}\) loss, let \(\mathbf{x}^{\star}=\operatorname*{arg\,min}_{\mathbf{x}}\lVert\mathbf{A} \mathbf{x}-\mathbf{b}\rVert_{2}^{2}\) be optimal model parameters. We want to find \(\hat{\mathbf{x}}^{\star}\) using a small number of queried target values in \(\mathbf{b}\) such that","\lVert\mathbf{A}\hat{\mathbf{x}}^{\star}-\mathbf{b}\rVert_{2}^{2}\leq(1+ \epsilon)\lVert\mathbf{A}\mathbf{x}^{\star}-\mathbf{b}\rVert_{2}^{2},",,"\lVert\mathbf{A}\hat{\mathbf{x}}^{\star} - \mathbf{b}\rVert_2^2 \leq \alpha \cdot \lVert\mathbf{A}\mathbf{x}^{\star} - \mathbf{b}\rVert_2^2 + \epsilon,","Equation 1 defines the objective of the active linear regression problem in the agnostic learning setting, where the goal is to find an approximate solution $\hat{\mathbf{x}}^{\star}$ such that the $\ell_2$ loss is within a multiplicative factor $\alpha$ and an additive error $\epsilon$ of the optimal loss achieved by $\mathbf{x}^{\star}$."
ICLR_2024_oral_28,2,"for some error parameter \(\epsilon>0\). Beyond being a fundamental learning problem, active regression has emerged as a fundamental tool in learning based methods for the solution and uncertainty analysis of parametric partial differential equations (PDEs) (Chkifa et al., 2015; Guo et al., 2020). For such applications, the agnostic setting is crucial, as a potentially complex quantity of interest is approximated by a simple surrogate model (e.g. polynomials, sparse polynomials, single layer neural networks, etc.) (Luthen et al., 2021; Hokanson and Constantine, 2018). Additionally, reducing the number of labels used for learning is crucial, as each label usually requires the computationally expensive numerical solution of a PDE for a new set of parameters (Cohen and DeVore, 2015).

### Leverage Score Sampling

Active linear regression has been studied for decades in the statistical model where \(\mathbf{b}\) is assumed to equal \(\mathbf{A}\mathbf{x}^{\star}\) plus i.i.d. random noise. In this case, the problem can be addressed using tools from optimal experimental design (Pukelsheim, 2006). In the agnostic case, near-optimal sample complexity results were only obtained relatively recently using tools from non-asymptotic matrix concentration (Tropp, 2012). In particular, it was shown independently in several papers that collecting entries from \(\mathbf{b}\)_randomly_ with probability proportional to the _statistical leverage scores_ of rows in \(\mathbf{A}\) can achieve (1.1) with \(O(d\log d+d/\epsilon)\) samples (Sarlos, 2006; Rauhut and Ward, 2012; Hampton and Doostan, 2015; Cohen and Migliorati, 2017). The leverage scores are defined as follows:

**Definition 1.1** (Leverage Score).: _Let \(\mathbf{U}\in\mathbb{R}^{n\times r}\) be any orthogonal basis for the column span of a matrix \(\mathbf{A}\in\mathbb{R}^{n\times d}\). Let \(\mathbf{a}_{i}\) and \(\mathbf{u}_{i}\) be the \(i\)-th rows of \(\mathbf{A}\) and \(\mathbf{U}\), respectively. The leverage score \(\tau_{i}\) of the \(i\)-th row in \(\mathbf{A}\) can be equivalently written as:_",\tau_{i}=\|\mathbf{u}_{i}\|_{2}^{2}=\mathbf{a}_{i}^{T}(\mathbf{A}^{T}\mathbf{ A})^{-1}\mathbf{a}_{i}=\max_{\mathbf{x}\in\mathbb{R}^{d}}(\mathbf{a}_{i}^{T} \mathbf{x})^{2}/\|\mathbf{A}\mathbf{x}\|_{2}^{2}.,,\tau_{i}(\mathbf{A}) = \lVert\mathbf{u}_{i}\rVert_{2}^{2} = \max_{\mathbf{x}\in\mathbb{R}^{d}\setminus\{0\}}\frac{|\mathbf{a}_{i}\mathbf{x}|^{2}}{\lVert\mathbf{A}\mathbf{x}\rVert_{2}^{2}}.,"The leverage score of the i-th row of A measures the influence of that row in the column space of A, defined either through the squared norm of the corresponding row in an orthogonal basis or through the maximum ratio of the squared inner product with any non-zero vector x over the squared norm of A times x."
ICLR_2024_oral_28,3,"_Notice that \(\tau_{i}=\|\mathbf{u}_{i}\|_{2}^{2}\), \(\sum_{i=1}^{n}\tau_{i}=d\) when \(\mathbf{A}\) is full-rank and thus \(\mathbf{U}\) has \(r=d\) columns._

See (Avron et al., 2017) for a short proof of the final equality in (1.2). This last definition, based on a maximization problem, gives an intuitive understanding of the leverage scores. The score of row \(\mathbf{a}_{i}\) is higher if it is more ""exceptional"", meaning that we can find a vector \(\mathbf{x}\) that has large inner product with \(\mathbf{a}_{i}\) relative to its average inner product (captured by \(\|\mathbf{A}\mathbf{x}\|_{2}^{2}\)) with all other rows in the matrix. Based on leverage score, rows that are more exceptional are sampled with higher probability.

Prior work considers independent leverage score sampling, either with or without replacement. The typical approach for sampling without replacement, which we call ""Bernoulli sampling"" is as follows: Each row \(\mathbf{a}_{i}\) is assigned a probability \(p_{i}=\min(1,c\cdot\tau_{i})\) for an oversampling parameter \(c\geq 1\). Then each row is sampled independently with probability \(p_{i}\). We construct a subsampled data matrix \(\tilde{\mathbf{A}}\) and subsampled target vector \(\tilde{\mathbf{b}}\) by adding \(\mathbf{a}_{i}/\sqrt{p_{i}}\) to \(\tilde{\mathbf{A}}\) and \(b_{i}/\sqrt{p_{i}}\) to \(\tilde{\mathbf{b}}\) for any index \(i\) that is sampled. To solve the active regression problem, we return \(\tilde{\mathbf{x}}^{*}=\arg\min_{\mathbf{x}}\|\tilde{\mathbf{A}}\mathbf{x}- \tilde{\mathbf{b}}\|_{2}\).

### Our Contributions

In applications to PDEs, the goal is often to approximate a function over a low dimensional distribution \(\mathcal{X}\). E.g. \(\mathcal{X}\) might be uniform over an interval \([-1,1]\subset\mathbb{R}\) or over a box \([-1,1]\times\ldots\times[-1,1]\subset\mathbb{R}^{q}\). In this setting, the length \(d\) rows of \(\mathbf{A}\) correspond to feature transformations of samples from \(\mathcal{X}\). For example, in the ubiquitous task of polynomial regression, we start with \(\mathbf{x}\sim\mathcal{X}\) and add to \(\mathbf{A}\) a row containing all combinations of entries in \(\mathbf{x}\) with total degree \(p\), i.e., \(x_{1}^{\ell_{1}}x_{2}^{\ell_{2}}\ldots x_{q}^{\ell_{q}}\) for all non-negative integers \(\ell_{1},\ldots,\ell_{q}\) such that \(\sum_{i=1}^{q}\ell_{i}\leq p\). For such problems, ""grid"" based interpolation is often used in place of randomized methods like leverage scores sampling, i.e., the target \(\mathbf{b}\) is queried on a deterministic grid tailored to \(\mathcal{X}\). For example, when \(\mathcal{X}\) is uniform on a box, the standard approach is to use a grid based on the Chebyshev nodes (Xiu, 2016). Pictured in Figure 2, the Chebyshev grid concentrates samples near the boundaries of the box, avoiding the well known issue of Runge's phenomenon for uniform grids. Leverage score sampling does the same. In fact, the methods are closely related: in the high degree limit, the leverage scores for polynomial regression over the box match the asymptotic density of the Chebyshev nodes (Luthen et al., 2021).

So how do the deterministic and randomized methods compare? The advantage of randomized methods based on leverage score sampling is that they yield strong provable approximation guarantees, and easily generalize to any distribution \(\mathcal{X}\).1 Deterministic methods are less flexible on the

Figure 1: Polynomial approximations to the maximum displacement of a damped harmonic oscillator, as a function of driving frequency and spring constant. (a) is the target value, and samples can be obtained through the numerical solution of a differential equation governing the oscillator. Both (b) and (c) draw \(250\) samples using leverage score sampling and perform polynomial regression of degree \(20\). (b) uses Bernoulli sampling while (c) uses our pivotal sampling method. Our method gives a better approximation, avoiding artifacts that result from gaps between the Bernoulli samples.

other hand, and do not yield provable guarantees. However, the advantage of grid based methods is that they more ""evenly"" distribute samples over the original data domain, which can lead to better performance in practice. Randomized methods are prone to ""missing"" larger regions of \(\mathcal{X}\)'s support, as shown in Figure 2. The driving question behind our work is:

_Is it possible to obtain the ""best of both worlds"" for fitting functions over low-dimensional domains? Le., can we match or improve on the strong theoretical guarantees of leverage score sampling with a method that produces spatially well-distributed samples?_

We answer this question in the affirmative. Instead of sampling rows from \(\mathbf{A}\)_independently_ with probability proportional to the leverage scores, we adopt a tool from survey sampling known as pivotal sampling (Deville and Tille, 1998). Our specific version of pivotal sampling is _spatially-aware_, meaning that it covers the domain in a well-balanced manner, while the marginal probabilities remain proportional to the leverage score. At a high-level, the pivotal method is a ""competition"" based sampling approach, where candidate rows compete in a binary tree tournament. By structuring the tournament so that spatially close points compete at lower levels (we use a novel recursive PCA procedure to build the tree), we ensure better spatial spread than Bernoulli leverage score sampling.

We show that our pivotal sampling method matches or beats the complexity of independent leverage score sampling in theory, while performing significantly better in practice. On the practice side, we offer Figure 1 as an example from a PDE test problem. In comparison to independent sampling, our spatially-aware method obtains a much better approximation to the target for a fixed number of samples (more details in Section 4). On the theory side, we prove two results. The first is general: we show that, as long as it samples rows from \(\mathbf{A}\) with _marginal probabilities_ proportional to the leverage scores, _any_ sampling strategy that obeys a weak ""one-sided \(\ell_{\infty}\) independence"" condition (which includes pivotal sampling) matches the complexity of independent leverage score sampling:

**Theorem 1.1**.: _Let \(\mathbf{A}\in\mathbb{R}^{n\times d}\) be a data matrix and \(\mathbf{b}\in\mathbb{R}^{n}\) be a target vector. Consider any algorithm which samples exactly \(k\) rows from \(\mathbf{A}\) (and observes the corresponding entries in \(\mathbf{b}\)) from a distribution that 1) satisfies one-sided \(\ell_{\infty}\) independence (Defn. 3.1) with parameter \(D_{\text{inf}}\) and 2) the marginal probability of sampling any row \(\mathbf{a}_{i}\) is proportional to \(\tau_{i}\). 2 Let \(\tilde{\mathbf{A}}\) and \(\tilde{\mathbf{b}}\) be the scaled sampled data and target, as defined in Section 1.1, and let \(\tilde{\mathbf{x}}^{*}=\arg\min_{\mathbf{x}\in\mathbb{R}^{d}}\|\tilde{\mathbf{ A}}\mathbf{x}-\tilde{\mathbf{b}}\|_{2}^{2}\). As long as \(k\geq c\cdot\left(d\log d\cdot D_{\text{inf}}^{2}+\frac{d}{\epsilon}\cdot D_ {\text{inf}}\right)\) for a fixed positive constant \(c\), then with probability \(99/100\),_

Footnote 2: Formally, we assume that the marginal probability of sampling \(\mathbf{a}_{i}\) equals \(\min(1,c\tau_{i})\) for a fixed constant \(c\geq 1\). Our proof easily generalizes to the case when some probabilities exceed this bound (since sampling more never hurts) although the total sample complexity will depend on the sum of the marginal probabilities.",\|\mathbf{A}\tilde{\mathbf{x}}^{*}-\mathbf{b}\|_{2}^{2}\leq(1+\epsilon)\| \mathbf{A}\mathbf{x}^{*}-\mathbf{b}\|_{2}^{2},,\lVert\mathbf{A}\tilde{\mathbf{x}}^{\star}-\mathbf{b}\rVert_{2}^{2}\leq(1+ \epsilon)\lVert\mathbf{A}\mathbf{x}^{\star}-\mathbf{b}\rVert_{2}^{2}.,"This equation states the main guarantee of our pivotal sampling method, showing that the learned parameters $\tilde{\mathbf{x}}^{\star}$ achieve a $(1+\epsilon)$-approximation to the optimal regression error, matching the bound in Equation 1 but under a spatially-aware sampling strategy that improves practical performance."
ICLR_2024_oral_28,4,"One-sided \(\ell_{\infty}\) independence was introduced in a recent paper by Kaufman et al. (2022) on matrix Chernoff bounds (we provide a formal definition in Sec. 3). It is the _weakest_ condition under which a tight matrix Chernoff bound is known to hold. For example, the condition is implied with constant \(D_{\text{inf}}=O(1)\) by almost all existing notions of negative dependence between random variables, including conditional negative association (CNA) and the strongly Rayleigh property (Pemantle, 2000). As discussed in Sec. 3, a tight matrix Chernoff bound is a prerequisite for proving relative

Figure 2: The results of three different active learning methods used to collect samples to fit a polynomial over \([-1,1]\times[-1,1]\). The image on the left was obtained by collecting points independently at random with probability according to their statistical leverage scores. The image on the right was obtained by collecting samples at the 2-dimensional Chebyshev nodes. The image in the middle shows our method, which collects samples according to leverage scores, but using a non-independent pivotal sampling strategy that ensures samples are evenly spread in spatially.

error active learning results like Equation (1.3), however does not imply such a result alone. Our proof of Theorem 1.1 requires adapting an approximate matrix-multiplication method from (Drineas et al., 2006) to non-independent sampling. It can be viewed as extending the work of Kaufman et al. (2022) to show that essentially all sampling distributions known to yield tight matrix Chernoff bounds also yield near optimal active regression bounds in the agnostic setting. Importantly, this includes binary-tree-based pivotal sampling methods like those introduced in this work. Such methods are known to satisfy the strongly Rayleigh property (Branden and Jonasson, 2012), and thus one-sided \(\ell_{\infty}\) independence with \(D_{\inf}=O(1)\). So, as a corollary of Theorem 1.1, we obtain:

**Corollary 1.1**.: _The spatially-aware pivotal sampling methods introduced in Section 2 (which use a fixed binary tree) return with probability \(99/100\) a vector \(\tilde{\mathbf{x}}^{*}\) satisfying \(\|\mathbf{A}\tilde{\mathbf{x}}^{*}-\mathbf{b}\|_{2}^{2}\leq(1+\epsilon)\| \mathbf{A}\mathbf{x}^{*}-\mathbf{b}\|_{2}^{2}\) while only observing \(O\left(d\log d+\frac{\mathrm{d}}{\epsilon}\right)\) entries in \(\mathbf{b}\)._

We hope that Theorem 1.1 will be valuable in obtaining similar results for other sampling methods beyond our own. However, the result falls short of justifying why pivotal sampling performs _better_ than independent leverage score sampling in experiments. Towards that end, we prove a second result specific to pivotal sampling, which shows that the method actually improves on the complexity of independent sampling by a log factor in the important special case of polynomial regression:

**Theorem 1.2**.: _Consider any function \(b:[\ell,u]\to\mathbb{R}\) defined on an interval \([\ell,u]\subset\mathbb{R}\), and consider fitting \(b\) with a degree \(d\) polynomial based on evaluations of the function at \(x_{1},\ldots,x_{k}\in[\ell,u]\). If \(x_{1},\ldots,x_{k}\) are collected via pivotal sampling with leverage score marginals (see Appendix C for details), then as long as \(k\geq c\cdot(d+\frac{d}{\epsilon})\) for a fixed positive constant \(c\), there is a procedure that uses these samples to construct a degree \(d\) polynomial \(\tilde{p}\) which, with probability \(99/100\), satisfies:_",\|\tilde{p}-b\|_{2}^{2}\leq(1+\epsilon)\underset{\text{degree }d\text{ polynomial }p}{\min}\|p-b\|_{2}^{2}.,,\|\tilde{p}-b\|_{2}^{2}\leq(1+\epsilon)\min_{p:\deg(p)=d}\|p-b\|_{2}^{2}.,"Equation 4 represents the error bound for polynomial regression using pivotal sampling, showing that the constructed polynomial $\tilde{p}$ approximates the target function $b$ within a $(1+\epsilon)$ factor of the best degree $d$ polynomial fit, with a sample complexity improved by a logarithmic factor compared to independent leverage score sampling."
ICLR_2024_oral_28,5,"_Here \(\|f\|_{2}^{2}\) denotes the average squared magnitude \(\int_{\ell}^{u}f(x)^{2}dx\) of a function \(f\)._

The problem of finding a polynomial approximation to a real-valued function that minimizes the average square error \(\|p-b\|_{2}^{2}\) can be modeled as an active regression problem involving a matrix \(\mathbf{A}\) with \(d+1\) columns and an infinite number of rows. In fact, polynomial approximation is one of the primary applications of prior work on leverage score-based active learning methods (Cohen and Migliorati, 2017; Avron et al., 2019). Such methods require \(O\left(d\log d+\frac{d}{\epsilon}\right)\) samples, so Theorem 1.2 is better by a \(\log d\) factor. Since polynomial approximation is a representative problem where spatially-distributed samples are important, Theorem 1.2 provides theoretical justification for the strong performance of pivotal sampling in experiments. Our proof is inspired by a result of Kane et al. (2017), and relies on showing a tight relation between the leverage scores of the polynomial regression problem and the orthogonality measure of the Chebyshev polynomials on \([\ell,u]\).

### Related Work

The application of leverage score sampling to the agnostic active regression problem has received significant recent attention. Beyond the results discussed above, extensions of leverage score sampling have been studied for norms beyond \(\ell_{2}\)(Chen and Derezinski, 2021; Musco et al., 2023; Parulekar et al., 2021), in the context where the sample space is infinite (i.e. \(\mathbf{A}\) is an operator with infinite rows) (Erdelyi et al., 2020; Avron et al., 2019), and for functions that involve non-linear transformations (Gajjar et al., 2023; Mai et al., 2021; Munteanu et al., 2018).

Theoretical improvements on leverage score sampling have also been studied. Notable is a recent result that improves on the \(O(d\log d+d/\epsilon)\) bound by a log \(d\) factor, showing that the active least squares regression problem can be solved with \(O(d/\epsilon)\) samples (Chen and Price, 2019). This is provably optimal. However, the algorithm in (Chen and Price, 2019) is complex, and appears to involve large constant factors: in our initial experiments, it did not empirically improve on independent leverage score sampling. In contrast, by Theorem 1.2, our pivotal sampling method matches the theoretical sample complexity of (Chen and Price, 2019) for the special case of polynomial regression, but performs well in experiments (significantly better than independent leverage score sampling). There have been a few other efforts to develop practical improvements on leverage score sampling. Similar to our work, Derezinski et al. (2018) study a variant of volume sampling that matches the theoretical guarantees of leverage score sampling, but performs better experimentally. However, this method does not explicitly take into account spatial-structure in the underlying regression problem. While the method from (Derezinski et al., 2018) does not quite fit our Theorem 1.1(e.g., it samples indices _with_ replacement) we expect similar methods could be analyzed as a special case of our result, as volume sampling induces a strongly Rayleigh distribution.

While pivotal sampling has not been studied in the context of agnostic active regression, it is widely used in other applications, and its negative dependence properties have been studied extensively. (Dubhashi et al., 2007) proves that pivotal sampling satisfies the negative association (NA) property. (Borcea et al., 2009) introduced the notion of a strongly Rayleigh distribution and proved that it implies a stronger notion of conditional negative association (CNA), and (Branden and Jonasson, 2012) showed that pivotal sampling run with an arbitrary binary tree is strongly Rayleigh. It follows that the method satisfies CNA. (Greene et al., 2022) discusses an efficient algorithm for pivotal sampling by parallelization and careful manipulation of inclusion probabilities. Another variant of pivotal sampling that is _spatially-aware_ is proposed in (Grafstrom et al., 2011). Though their approach is out of the scope of our analysis as it involves randomness in the competition order used during sampling, our method is inspired by their work.

### Notation and Preliminaries

**Notation.** We let \([n]\) denote \(\{1,\cdots,n\}\). \(\mathbb{E}[X]\) denotes the expectation of a random variable \(X\). We use bold lower-case letters for vectors and bold upper-case letters for matrices. For a vector \(\mathbf{z}\in\mathbb{R}^{n}\) with entries \(z_{1},\cdots,z_{n}\), \(\|\mathbf{z}\|_{2}=(\sum_{i=1}^{n}z_{i}^{2})^{1/2}\) denotes the Euclidean norm of \(\mathbf{z}\). Given a matrix \(\mathbf{A}\in\mathbb{R}^{n\times d}\), we let \(\mathbf{a}_{i}\) denote the \(i\)-th row, and \(a_{ij}\) denote the entry in the \(i\)-th row and \(j\)-th column.

**Importance sampling.** All of the methods studied in this paper solve the active regression problem by collecting a single random sample of rows in \(\mathbf{A}\) and corresponding entries in \(\mathbf{b}\). We introduce a vector of binary random variables \(\boldsymbol{\xi}=\{\xi_{1},\cdots,\xi_{n}\}\), where \(\xi_{i}\) is \(1\) if \(\mathbf{a}_{i}\) (and thus \(b_{i}\)) is selected, and \(0\) otherwise. \(\xi_{1},\cdots,\xi_{n}\) will not necessarily be independent depending on our sampling method. Given a sampling method, let \(p_{i}=\mathbb{E}[\xi_{i}]\) denote the marginal probability that row \(i\) is selected. We return an approximate regression solution as follows: let \(\tilde{\mathbf{A}}\in\mathbb{R}^{k\times d}\) contain \(\mathbf{a}_{i}/\sqrt{p_{i}}\) for all \(i\) such that \(\xi_{i}=1\), and similarly let \(\tilde{\mathbf{b}}\in\mathbb{R}^{k}\) contain \(b_{i}/\sqrt{p_{i}}\) for the same values of \(i\). This scaling ensures that, for any fixed \(\mathbf{x}\), \(\mathbb{E}\|\tilde{\mathbf{A}}\mathbf{x}-\tilde{\mathbf{b}}\|_{2}^{2}=\| \mathbf{A}\mathbf{x}-\mathbf{b}\|_{2}^{2}\). To solve the active regression problem, we return \(\tilde{\mathbf{x}}^{*}=\arg\min_{\mathbf{x}\in\mathbb{R}^{d}}\|\tilde{\mathbf{ A}}\mathbf{x}-\tilde{\mathbf{b}}\|_{2}^{2}\). Computing \(\tilde{\mathbf{x}}^{*}\) only requires querying \(k\) target values in \(\mathbf{b}\).

**Leverage Score Sampling.** We consider methods that choose the marginal probabilities proportional to \(\mathbf{A}\)'s leverage scores. Specifically, our methods sample row \(\mathbf{a}_{i}\) with marginal probability \(\tilde{p}_{i}=\min(1,c_{k}\cdot\tau_{i})\), where \(c_{k}\) is chosen so that \(\sum_{i=1}^{n}\tilde{p}_{i}=k\). Details of how to find \(c_{k}\) are discussed in Appendix A. We note that we always have \(c_{k}\geq k/d\) since \(\sum_{i=1}^{n}p_{i}\leq\sum_{i=1}^{n}\frac{k}{d}\cdot\tau_{i}\leq\frac{k}{d} \cdot d=k\).

## 2 Our Methods

In this section, we present our sampling scheme which consists of two steps; deterministically constructing a binary tree, and choosing samples by running the pivotal method on this tree. The pivotal method is described in Algorithm 1. It takes as input a binary tree with \(n\) leaf nodes, each corresponding to a single index \(i\) to be sampled. For each index, we also have an associated probability \(\tilde{p}_{i}\). The algorithm collects a set of exactly \(k\) samples \(\mathcal{S}\) where \(k=\sum_{i=1}^{n}\tilde{p}_{i}\). It does so by percolating up the tree and performing repeated head-to-head comparisons of the indices at sibling nodes in the tree. After each comparison, one node promotes to the parent node with updated inclusion probability, and the other node is determined to be sampled or not to be sampled.

It can be checked that, after running Algorithm 1, index \(i\) is always sampled with probability \(\tilde{p}_{i}\), regardless of the choice of \(T\). However, the samples collected by the pivotal method are not independent, but rather negatively correlated: siblings in \(T\) are unlikely to both be sampled, and in general, the events that close neighbors in the tree are both sampled are negatively correlated. In particular, if index \(i\) could at some point compete with an index \(j\) in the pivotal process, the chance of selecting \(j\) decreases if we condition on \(i\) being selected. We take advantage of this property to generate spatially distributed samples by constructing a binary tree that matches the underlying geometry of our data. In particular, assume we are given a set of points \(\mathbf{X}\in\mathbb{R}^{n\times d^{\prime}}\). \(\mathbf{X}\) will eventually be used to construct a regression matrix \(\mathbf{A}\in\mathbb{R}^{n\times d}\) via feature transformation (e.g. by adding polynomial features). However, we construct the sampling tree using \(\mathbf{X}\) alone.

```
0: Depth \(t\) full binary tree \(T\) with \(n\) leaves, inclusion probabilities \(\{\tilde{p}_{1},\cdots,\tilde{p}_{n}\}\) for each leaf.
0: Set of \(k\) sampled indices \(\mathcal{S}\).
1: Initialize \(\mathcal{S}=\emptyset\).
2:while\(T\) has at least two remaining children nodes do
3: Select any pair of sibling nodes \(S_{1},S_{2}\) with parent \(P\). Let \(i,j\) be the indices stored at \(S_{1},S_{2}\).
4:if\(\tilde{p}_{i}+\tilde{p}_{j}\leq 1\)then
5: With probability \(\frac{\tilde{p}_{i}}{\tilde{p}_{i}+\tilde{p}_{j}}\), set \(\tilde{p}_{i}+\tilde{p}_{i}+\tilde{p}_{j}\), \(\tilde{p}_{j}\gets 0\). Store \(i\) at \(P\).
6: Otherwise, set \(\tilde{p}_{i}\leftarrow\tilde{p}_{i}+\tilde{p}_{j}\), \(\tilde{p}_{i}\gets 0\). Store \(j\) at \(P\).
7:elseif\(\tilde{p}_{i}+\tilde{p}_{j}>1\)then
8: With probability \(\frac{1-\tilde{p}_{i}}{\tilde{p}_{i}-\tilde{p}_{j}}\), set \(\tilde{p}_{i}\leftarrow\tilde{p}_{i}+\tilde{p}_{j}-1\), \(\tilde{p}_{j}=1\). Store \(i\) at \(P\) and set \(\mathcal{S}\leftarrow\mathcal{S}\cup\{j\}\).
9: Otherwise, set \(\tilde{p}_{j}\leftarrow\tilde{p}_{i}+\tilde{p}_{j}-1\), \(\tilde{p}_{i}\gets 1\). Store \(j\) at \(P\) and set \(\mathcal{S}\leftarrow\mathcal{S}\cup\{i\}\).
10: Remove \(S_{1},S_{2}\) from \(T\).
11:return\(\mathcal{S}\)
```

**Algorithm 1** Binary Tree Based Pivotal Sampling (Deville and Tille, 1998)

```
0: Matrix \(\mathbf{X}\in\mathbb{R}^{n\times d^{\prime}}\), split method \(\in\{\)PCA, coordinate\(\}\), inclusion probabilities \(\tilde{p}_{1},\cdots,\tilde{p}_{n}\).
0: Binary tree \(\mathcal{T}\) where each leaf corresponds to a row in \(\mathbf{X}\).
1: Create a tree \(T\) with a single root node. Assign set \(\mathcal{R}\) to the root where \(\mathcal{R}=\{i\in[n];\tilde{p}_{i}<1\}\).
2:while There exists a node in \(T\) that holds set \(\mathcal{K}\) such that \(|\mathcal{K}|>1\)do
3: Select any such node \(N\) and let \(t\) be its level in the tree. Construct \(\mathbf{X}_{(\mathcal{K})}\in\mathbb{R}^{|\mathcal{K}|\times d^{\prime}}\).
4:if split method \(=\) PCA then
5: Sort \(\mathbf{X}_{(\mathcal{K})}\) according to the direction of the maximum variance.
6:elseif split method \(=\) coordinate then
7: Sort \(\mathbf{X}_{(\mathcal{K})}\) according to values in its \(((t\mod d^{\prime})+1)\)-th column.
8: Create a left child of \(N\). Assign to it all indices associated with the first \(\lfloor\frac{|\mathcal{K}|}{2}\rfloor\) rows of \(\mathbf{X}_{(\mathcal{K})}\).
9: Create a right child of \(N\). Assign to it the all remaining indices in \(\mathbf{X}_{(\mathcal{K})}\). Delete \(\mathcal{K}\) from \(N\).
10:return\(T\)
```

**Algorithm 2** Binary Tree Construction by Coordinate or PCA Splitting

Our tree construction method is given as Algorithm 2. \(\mathbf{X}_{\mathcal{K}}\) denotes the subset of rows of \(\mathbf{X}\) with indices in the set \(\mathcal{K}\). First, the algorithm eliminates all data points with inclusion probability \(\tilde{p}_{i}=1\). Next, it recursively partitions the remaining data points into two subgroups of the same size until all the subgroups have only one data point. Our two methods, PCA-based and coordinate-wise, only differ in how to partition. The PCA-based method performs principal component analysis to find the direction of the maximum variance and splits the space by a hyperplane orthogonal to the direction so that the numbers of data points on both sides are equal. The coordinate-wise version takes a coordinate (corresponding to a column in \(\mathbf{X}\)) in cyclic order and divides the space by a hyperplane orthogonal to the chosen coordinate. An illustration of the PCA-based binary tree construction run on a fine uniform grid of data points in \(\mathbb{R}^{2}\) is shown in Figure 3. Note that our tree construction method ensures the the number of indices assigned to each subgroup (color) at each level is equal to with \(\pm 1\) point. As such, we end of with an even partition of data points into spatially correlated sets. Two indices will be more negatively correlated if they lie in the same set at a higher depth number.

## 3 Theoretical Analysis

As will be shown experimentally, when using probabilities \(\tilde{p}_{1},\ldots,\tilde{p}_{n}\) proportional to the statistical leverage scores of \(\mathbf{A}\), our tree-based pivotal methods significantly outperform its Bernoulli counterpart for active regression. We provide two results theoretically justifying this operation. We first show that, no matter what our original data matrix \(\mathbf{X}\) is, and what feature transformation is used to construct \(\mathbf{A}\), our methods never perform _worse_ than Bernoulli sampling. In particular, they match the \(O(d\log d+d/\epsilon)\) sample complexity of independent leverage score sampling. This result is stated as Theorem 1.1. Its proof is given to Appendix B, but we outline our main approach here.

Following existing proofs for independent random sampling (e.g. (Woodruff, 2014) Theorem 1.1 requires two main ingredients: a subspace embedding result, and an approximate matrix-vector multiplication result. In particular, let \(\mathbf{U}\in\mathbb{R}^{n\times d}\) be any orthogonal span for the columns of \(\mathbf{A}\). Let \(\mathbf{S}\in\mathbb{R}^{k\times n}\) be a subsampling matrix that contains a row for every index \(i\) selected by our sampling scheme, which has value \(1/\sqrt{p_{i}}\) at entry \(i\), and is \(0\) everywhere else. So, in the notation of Theorem 1.1, \(\tilde{\mathbf{A}}=\mathbf{S}\mathbf{A}\) and \(\tilde{\mathbf{b}}=\mathbf{S}\mathbf{b}\). To prove the theorem, it suffices to show that with high probability,

1. **Subspace Embedding:** For all \(\mathbf{x}\in\mathbb{R}^{d}\), \(\frac{1}{2}\|\mathbf{x}\|_{2}\leq\|\mathbf{S}\mathbf{U}\mathbf{x}\|_{2}\leq 1.5 \|\mathbf{x}\|_{2}\).
2. **Approximate Matrix-Vector Multiplication**: \(\|\mathbf{U}^{T}\mathbf{S}^{T}\mathbf{S}(\mathbf{b}-\mathbf{A}\mathbf{x}^{*} )\|_{2}^{2}\leq\epsilon\|\mathbf{b}-\mathbf{A}\mathbf{x}^{*}\|_{2}^{2}\).

The first property is equivalent to \(\|\mathbf{U}^{T}\mathbf{S}^{T}\mathbf{S}\mathbf{U}-\mathbf{I}\|_{2}\leq 1/2\). I.e., after subsampling, \(\mathbf{U}\) should remain nearly orthogonal. The second property requires that after subsampling with \(\mathbf{S}\), the optimal residual, \(\mathbf{b}-\mathbf{A}\mathbf{x}^{*}\), should have small product with \(\mathbf{U}\). Note that without subsampling, \(\|\mathbf{U}^{T}(\mathbf{b}-\mathbf{A}\mathbf{x}^{*})\|_{2}^{2}=0\).

We show that both of the above bounds can be established for any sampling method that 1) samples index \(i\) with marginal probability proportional to its leverage score 2) is homogeneous, meaning that it takes a fixed number of samples \(k\), and 3) produces a distribution over binary vectors satisfying the following property:

**Definition 3.1** (One-sided \(\ell_{\infty}\)-independence).: _Let \(\xi_{1},\cdots,\xi_{n}\in\{0,1\}^{n}\) be random variables with joint distribution \(\mu\). Let \(\mathcal{S}\subseteq[n]\) and let \(i,j\in[n]\backslash\mathcal{S}\). Define the one-sided influence matrix \(\mathcal{I}_{\mu}^{\mathcal{S}}\) as:_","\mathcal{I}_{\mu}^{\mathcal{S}}(i,j)=\Pr_{\mu}[\xi_{j}=1|\xi_{i}=1\wedge\xi_{ \ell}=1\forall\ell\in\mathcal{S}]-\Pr_{\mu}[\xi_{j}=1|\xi_{\ell}=1\forall\ell \in\mathcal{S}]",,"\mathcal{I}_{\mu}^{\mathcal{S}}(i,j)=\max_{\substack{\mathcal{T}\subseteq[n]\backslash(\mathcal{S}\cup\{i,j\})\\ \mathcal{T}\text{ is finite}}}\left|\Pr_{\mu}[\xi_{j}=1|\xi_{i}=1,\xi_{\mathcal{T}}=1]-\Pr_{\mu}[\xi_{j}=1|\xi_{i}=0,\xi_{\mathcal{T}}=1]\right|.",
ICLR_2024_oral_29,1,"## 2 Consistency models

Central to the formulation of consistency models is the probability flow ordinary differential equation (ODE) from Song et al. (2021). Let us denote the data distribution by \(p_{\text{data}}(\mathbf{x})\). When we add Gaussian noise with mean zero and standard deviation \(\sigma\) to this data, the resulting perturbed distribution is given by \(p_{\sigma}(\mathbf{x})=\int_{\text{data}}(\mathbf{y})\mathcal{N}(\mathbf{x} \mid\mathbf{y},\sigma^{2}\mathbf{I})\,\mathrm{d}\mathbf{y}\). The probability flow ODE, as presented in Karras et al. (2022), takes the form of","\frac{\mathrm{d}\mathbf{x}}{\mathrm{d}\sigma}=-\sigma\nabla_{\mathbf{x}}\log p _{\sigma}(\mathbf{x})\quad\sigma\in[\sigma_{\text{min}},\sigma_{\text{max}}],",where the term \(\nabla_{\mathbf{x}}\log p_{\sigma}(\mathbf{x})\) is known as the _score function_ of \(p_{\sigma}(\mathbf{x})\)(Song et al,"\frac{\mathrm{d}\mathbf{x}}{\mathrm{d}t} = \mathbf{f}(\mathbf{x}, t),","where $\mathbf{f}(\mathbf{x}, t)$ represents the drift function that governs the evolution of the data variable $\mathbf{x}$ over time $t$ according to the probability flow ordinary differential equation."
ICLR_2024_oral_29,2,"al., 2019; Song and Ermon, 2019, 2020; Song et al., 2021). Here \(\sigma_{\text{min}}\) is a small positive value such that \(p_{\sigma_{\text{min}}}(\mathbf{x})\approx p_{\text{data}}(\mathbf{x})\), introduced to avoid numerical issues in ODE solving. Meanwhile, \(\sigma_{\text{max}}\) is sufficiently large so that \(p_{\sigma}(\mathbf{x})\approx\mathcal{N}(0,\sigma_{\text{max}}^{2}\mathbf{I})\). Following Karras et al. (2022); Song et al. (2023), we adopt \(\sigma_{\text{min}}=0.002\), and \(\sigma_{\text{max}}=80\) throughout the paper. Crucially, solving the probability flow ODE from noise level \(\sigma_{1}\) to \(\sigma_{2}\) allows us to transform a sample \(\mathbf{x}_{\sigma_{1}}\sim p_{\sigma_{1}}(\mathbf{x})\) into \(\mathbf{x}_{\sigma_{2}}\sim p_{\sigma_{2}}(\mathbf{x})\).

The ODE in Eq. (1) establishes a bijective mapping between a noisy data sample \(\mathbf{x}_{\sigma}\sim p_{\sigma}(\mathbf{x})\) and \(\mathbf{x}_{\sigma_{\text{min}}}\sim p_{\sigma_{\text{min}}}(\mathbf{x})\approx p _{\text{data}}(\mathbf{x})\). This mapping, denoted as \(\mathbf{f}^{\ast}:(\mathbf{x}_{\sigma},\sigma)\mapsto\mathbf{x}_{\sigma_{\text{ min}}}\), is termed the _consistency function_. By its very definition, the consistency function satisfies the _boundary condition_\(\mathbf{f}^{\ast}(\mathbf{x},\sigma_{\text{min}})=\mathbf{x}\). A _consistency model_, which we denote by \(\mathbf{f}_{\mathbf{\theta}}(\mathbf{x},\sigma)\), is a neural network trained to approximate the consistency function \(\mathbf{f}^{\ast}(\mathbf{x},\sigma)\). To meet the boundary condition, we follow Song et al. (2023) to parameterize the consistency model as","\mathbf{f}_{\mathbf{\theta}}(\mathbf{x},\sigma)=c_{\text{skip}}(\sigma)\mathbf{x}+c_{ \text{out}}(\sigma)\mathbf{F}_{\mathbf{\theta}}(\mathbf{x},\sigma),","where \(\mathbf{F}_{\mathbf{\theta}}(\mathbf{x},\sigma)\) is a free-form neural network, while \(c_{\text{skip}}(\sigma)\) and \(c_{\text{out}}(\sigma)\) are differentiable functions such that \(c_{\text{skip}}(\sigma_{\text{min}})=1\) and \(c_{\text{out}}(\sigma_{\text{min}})=0\)","\mathbf{f}_{\mathbf{\theta}}(\mathbf{x},\sigma)=\mathbf{x}+\sigma\mathbf{u}_{\mathbf{\theta}}(\mathbf{x},\sigma),","where $\mathbf{u}_{\mathbf{\theta}}(\mathbf{x},\sigma)$ is a neural network that ensures the boundary condition $\mathbf{f}_{\mathbf{\theta}}(\mathbf{x},\sigma_{\text{min}})=\mathbf{x}$ is satisfied."
ICLR_2024_oral_29,3,"\).

To train the consistency model, we discretize the probability flow ODE using a sequence of noise levels \(\sigma_{\text{min}}=\sigma_{1}<\sigma_{2}<\dots<\sigma_{N}=\sigma_{\text{max}}\), where we follow Karras et al. (2022); Song et al. (2023) in setting \(\sigma_{i}=(\sigma_{\text{min}}^{1/\rho}+\frac{i-1}{N-1}(\sigma_{\text{max}}^{ 1/\rho}-\sigma_{\text{min}}^{1/\rho}))^{\rho}\) for \(i\in[\![1,N]\!]\), and \(\rho=7\), where \([\![a,b]\!]\) denotes the set of integers \(\{a,a+1,\cdots,b\}\). The model is trained by minimizing the following _consistency matching_ (CM) loss over \(\mathbf{\theta}\):","\mathcal{L}^{N}(\mathbf{\theta},\mathbf{\theta}^{-})=\mathbb{E}[\lambda(\sigma_{i} )d(\mathbf{f_{\theta}}(\mathbf{x}_{\sigma_{i+1}},\sigma_{i+1}),\mathbf{f_{\theta}}_{-}( \tilde{\mathbf{x}}_{\sigma_{i}},\sigma_{i}))],",where \(\tilde{\mathbf{x}}_{\sigma_{i}}=\mathbf{x}_{\sigma_{i+1}}-(\sigma_{i}-\sigma_{ i+1})\sigma_{i+1}\nabla_{\mathbf{x}}\log p_{\sigma_{i+1}}(\mathbf{x})|_{\mathbf{x}= \mathbf{x}_{\sigma_{i+1}}}\),"\mathcal{L}_{\text{CM}}(\mathbf{\theta})=\mathbb{E}_{\sigma_{i},\sigma_{j},\mathbf{x}_{\sigma_{i}}}\left[\left\Vert \mathbf{f}_{\mathbf{\theta}}(\mathbf{x}_{\sigma_{j}},\sigma_{j})-\mathbf{f}_{\mathbf{\theta}}(\mathbf{x}_{\sigma_{i}},\sigma_{i})\right\Vert ^{2}\right],","where the expectation is taken over pairs of noise levels $\sigma_i, \sigma_j$ and samples $\mathbf{x}_{\sigma_i} \sim p_{\sigma_i}(\mathbf{x})$, enforcing consistency between model predictions across different noise levels."
ICLR_2024_oral_29,4,"\). In Eq. (3), \(d(\mathbf{x},\mathbf{y})\) is a metric function comparing vectors \(\mathbf{x}\) and \(\mathbf{y}\), and \(\lambda(\sigma)>0\) is a weighting function. Typical metric functions include the squared \(\ell_{2}\) metric \(d(\mathbf{x},\mathbf{y})=\|\mathbf{x}-\mathbf{y}\|_{2}^{2}\), and the Learned Perceptual Image Patch Similarity (LPIPS) metric introduced in Zhang et al. (2018). The expectation in Eq. (3) is taken over the following sampling process: \(i\sim\mathcal{U}[1,N-1]\) where \(\mathcal{U}[1,N-1]\) represents the uniform distribution over \(\{1,2,\cdots,N-1\}\), and \(\mathbf{x}_{\sigma_{i+1}}\sim p_{\sigma_{i+1}}(\mathbf{x})\). Note that \(\tilde{\mathbf{x}}_{\sigma_{i}}\) is derived from \(\mathbf{x}_{\sigma_{i+1}}\) by solving the probability flow ODE in the reverse direction for a single step. In Eq. (3), \(\mathbf{f_{\theta}}\) and \(\mathbf{f_{\theta^{-}}}\) are referred to as the _student network_ and the _teacher network_, respectively. The teacher's parameter \(\mathbf{\theta}^{-}\) is obtained by applying Exponential Moving Average (EMA) to the student's parameter \(\mathbf{\theta}\) during the course of training as follows:","\mathbf{\theta}^{-}arrow\mathrm{stopgrad}(\mu\mathbf{\theta}^{-}+(1-\mu)\mathbf{ \theta}),",,"\mathbf{\theta}^{-}=\alpha\mathbf{\theta}^{-}+(1-\alpha)\mathbf{\theta},",where $\alpha$ is a smoothing factor that controls the update rate of the teacher network parameters $\mathbf{\theta}^{-}$ based on the student network parameters $\mathbf{\theta}$.
ICLR_2024_oral_29,5,"with \(0\leqslant\mu<1\) representing the EMA decay rate. Here we explicitly employ the \(\mathrm{stopgrad}\) operator to highlight that the teacher network remains fixed for each optimization step of the student network. However, in subsequent discussions, we will omit the stopgrad operator when its presence is clear and unambiguous. In practice, we also maintain EMA parameters for the student network to achieve better sample quality at inference time. It is clear that as \(N\) increases, the consistency model optimized using Eq. (3) approaches the true consistency function. For faster training, Song et al. (2023) propose a curriculum learning strategy where \(N\) is progressively increased and the EMA decay rate \(\mu\) is adjusted accordingly. This curriculum for \(N\) and \(\mu\) is denoted by \(N(k)\) and \(\mu(k)\), where \(k\in\mathbb{N}\) is a non-negative integer indicating the current training step.

Given that \(\tilde{\mathbf{x}}_{\sigma_{i}}\) relies on the unknown score function \(\nabla_{\mathbf{x}}\log p_{\sigma_{i+1}}(\mathbf{x})\), directly optimizing the consistency matching objective in Eq. (3) is infeasible. To circumvent this challenge, Song et al. (2023) propose two training algorithms: _consistency distillation_ (CD) and _consistency training_ (CT). For consistency distillation, we first train a diffusion model \(\mathbf{s_{\theta}}(\mathbf{x},\sigma)\) to estimate \(\nabla_{\mathbf{x}}\log p_{\sigma}(\mathbf{x})\) via score matching (Hyvarinen, 2005; Vincent, 2011; Song et al., 2019; Song and Ermon, 2019), then approximate \(\tilde{\mathbf{x}}_{\sigma_{i}}\) with \(\hat{\mathbf{x}}_{\sigma_{i}}=\mathbf{x}_{\sigma_{i+1}}-(\sigma_{i}-\sigma_{i+ 1})\sigma_{i+1}\mathbf{s_{\theta}}(\mathbf{x}_{\sigma_{i+1}},\sigma_{i+1})\). On the other hand, consistency training employs a different approximation method. Recall that \(\mathbf{x}_{\sigma_{i+1}}=\mathbf{x}+\sigma_{i+1}\mathbf{z}\) with \(\mathbf{x}\sim p_{\text{data}}(\mathbf{x})\) and \(\mathbf{z}\sim\mathcal{N}(\mathbf{0},\mathbf{I})\). Using the same \(\mathbf{x}\) and \(\mathbf{z}\), Song et al. (2023) define \(\tilde{\mathbf{x}}_{\sigma_{i}}=\mathbf{x}+\sigma_{i}\mathbf{z}\) as an approximation to \(\tilde{\mathbf{x}}_{\sigma_{i}}\), which leads to the consistency training objective below:","\mathcal{L}_{\text{CT}}^{N}(\mathbf{\theta},\mathbf{\theta}^{-})=\mathbb{E}[ \lambda(\sigma_{i})d(\mathbf{f_{\theta}}(\mathbf{x}+\sigma_{i+1}\mathbf{z},\sigma_ {i+1}),\mathbf{f_{\theta^{-}}}(\mathbf{x}+\sigma_{i}\mathbf{z},\sigma_{i}))].",,"\tilde{\mathbf{x}}_{\sigma_{i}} = \mathbf{x} + \sigma_{i}\mathbf{z},","where $\mathbf{x} \sim p_{\text{data}}(\mathbf{x})$ and $\mathbf{z} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$, used in the consistency training objective as an approximation derived from shared random variables."
ICLR_2024_oral_29,6,"As analyzed in Song et al. (2023), this objective is asymptotically equivalent to consistency matching in the limit of \(N\rightarrow\infty\). We will revisit this analysis in Section 3.2.

After training a consistency model \(\mathbf{f_{\theta}}(\mathbf{x},\sigma)\) through CD or CT, we can directly generate a sample \(\mathbf{x}\) by starting with \(\mathbf{z}\sim\mathcal{N}(\mathbf{0},\sigma_{\text{max}}^{2}\mathbf{I})\) and computing \(\mathbf{x}=\mathbf{f_{\theta}}(\mathbf{z},\sigma_{\text{max}})\). Notably, these models also enable multistep generation. For a sequence of indices \(1=i_{1}<i_{2}<\cdots<i_{K}=N\), we start by sampling \(\mathbf{x}_{K}\sim\mathcal{N}(\mathbf{0},\sigma_{\text{max}}^{2}\mathbf{I})\) and then iteratively compute \(\mathbf{x}_{k}\leftarrow\mathbf{f_{\theta}}(\mathbf{x}_{k+1},\sigma_{i_{k+1}})+ \sqrt{\sigma_{i_{k}}^{2}-\sigma_{\text{min}}^{2}\mathbf{z}_{k}}\) for \(k=K-1,K-2,\cdots,1\), where \(\mathbf{z}_{k}\sim\mathcal{N}(\mathbf{0},\mathbf{I})\). The resulting sample \(\mathbf{x}_{1}\) approximates the distribution \(p_{\text{data}}(\mathbf{x})\). In our experiments, setting \(K=3\) (two-step generation) often enhances the quality of one-step generation considerably, though increasing the number of sampling steps further provides diminishing benefits.

## 3 Improved techniques for consistency training

Below we re-examine the design choices of CT in Song et al. (2023) and pinpoint modifications that improve its performance, which we summarize in Table 1. We focus on CT without learned metric functions. For our experiments, we employ the Score SDE architecture in Song et al. (2021) and train the consistency models for 400,000 iterations on the CIFAR-10 dataset (Krizhevsky et al., 2014) without class labels. While our primary focus remains on CIFAR-10 in this section, we observe similar improvements on other datasets, including ImageNet \(64\times 64\)(Deng et al., 2009). We measure sample quality using Frechet Inception Distance (FID) (Heusel et al., 2017).

### Weighting functions, noise embeddings, and dropout

We start by exploring several hyperparameters that are known to be important for diffusion models, including the weighting function \(\lambda(\sigma)\), the embedding layer for noise levels, and dropout (Ho et al., 2020; Song et al., 2021; Dhariwal and Nichol, 2021; Karras et al., 2022). We find that proper selection of these hyperparameters greatly improve CT when using the squared \(\ell_{2}\) metric.

The default weighting function in Song et al. (2023) is uniform, _i.e._, \(\lambda(\sigma)=1\). This assigns equal weights to consistency losses at all noise levels, which we find to be suboptimal. We propose to modify the weighting function so that it reduces as noise levels increase. The rationale is that errors from minimizing consistency losses in smaller noise levels can influence larger ones and therefore should be weighted more heavily. Specifically, our weighting function (_cf._, Table 1) is defined as \(\lambda(\sigma_{i})=\frac{\sigma_{i}}{\sigma_{i+1}-\sigma_{i}}\). The default choice for \(\sigma_{i}\), given in Section 2, ensures that \(\lambda(\sigma_{i})=\frac{\sigma_{i}}{\sigma_{i+1}-\sigma_{i}}\) reduces monotonically as \(\sigma_{i}\) increases, thus assigning smaller weights to higher noise levels. As shown in Fig. 0(c), this refined weighting function notably improves the sample quality in CT with the squared \(\ell_{2}\) metric.

In Song et al. (2023), Fourier embedding layers (Tancik et al., 2020) and positional embedding layers (Vaswani et al., 2017) are used to embed noise levels for CIFAR-10 and ImageNet \(64\times 64\) respectively. It is essential that noise embeddings are sufficiently sensitive to minute differences to offer training signals, yet too much sensitivity can lead to training instability. As shown in Fig. 0(b), high sensitivity can lead to the divergence of continuous-time CT (Song et al., 2023). This is a known challenge in Song et al. (2023), which they circumvent by initializing the consistency model with parameters from a pre-trained diffusion model. In Fig. 0(b), we show continuous-time CT on CIFAR-10 converges with random initial parameters, provided we use a less sensitive noise embedding layer with a reduced Fourier scale parameter, as visualized in Fig. 0(a). For discrete-time CT, models are less affected by the sensitivity of the noise embedding layers, but as shown in Fig. 0(c), reducing the scale parameter in Fourier embedding layers from the default value of 16.0 to a smaller value of 0.02 still leads to slight improvement of FIDs on CIFAR-10. For ImageNet models, we employ the default positional embedding, as it has similar sensitivity to Fourier embedding with scale 0.02 (see Fig. 0(a)).

Previous experiments with consistency models in Song et al. (2023) always employ zero dropout, motivated by the fact that consistency models generate samples in a single step, unlike diffusion models that do so in multiple steps. Therefore, it is intuitive that consistency models, facing a more challenging task, would be less prone to overfitting and need less regularization than their diffusion counterparts. Contrary to our expectations, we discovered that using larger dropout than diffusion models improves the sample quality of consistency models. Specifically, as shown in Fig. 0(c), a dropout rate of 0.3 for consistency models on CIFAR-10 obtains better FID scores. For ImageNet \(64\times 64\), we find it beneficial to apply dropout of 0.2 to layers with resolution less than or equal to \(16\times 16\), following Hoogeboom et al. (2023). We additionally ensure that the random number

\begin{table}
\begin{tabular}{l|l|l} \hline  & **Design choice in Song et al. (2023)** & **Our modifications** \\ \hline EMA decay rate for the teacher network & \(\mu(k)=\exp(\frac{\mu_{1}\log(\mu_{1})}{N(k)})\) & \(\mu(k)=0\) \\ Metric in consistency loss & \(d(\mathbf{x},\mathbf{y})=\text{LPIPS}(\mathbf{x},\mathbf{y})\) & \(d(\mathbf{x},\mathbf{y})=\sqrt{\|\mathbf{x}-\mathbf{y}\|_{2}^{2}+c^{2}}-c\) \\ Discretization curriculum & \(N(k)=\left[\sqrt{\frac{k}{K}((s_{1}+1)^{2}-s_{0}^{2})+s_{0}^{2}}-1\right]+1\) & \(N(k)=\min(s_{0}2^{\frac{1}{K}}|,s_{1})+1\), \\ Noise schedule & \(\sigma_{i}\), where \(i\sim\mathcal{U}[1,N(k)-1]\) & \(\sigma_{i}\), where \(i\sim\mathcal{U}[i,p(i)]\)(\(x\)) \\  & & \(\text{erf}\left(\frac{\|\sigma_{i}-p_{i}\|-P_{\text{max}}}{\sqrt{2P_{\text{max}}}} \right)-\text{erf}\left(\frac{\|\sigma_{i}-P_{\text{max}}}{\sqrt{2P_{\text{max}}}}\right)\) \\ Weighting function & \(\lambda(\sigma_{i})=1\) & \(\lambda(\sigma_{i})=\frac{1}{\sigma_{\text{max}}\sigma_{i}}\) \\ \hline \hline Parameters & \(s_{0}=2,s_{1}=150,\mu_{0}=0.9\) on CIFAR-10 & \(s_{0}=10,s_{1}=1280\) \\  & \(s_{0}=2,s_{1}=200,\mu_{0}=0.95\) on ImageNet \(64\times 64\) & \(c=0.00054\sqrt{d}\), \(d\) is data dimensionality \\  & & \(P_{\text{mean}}=-1.1,P_{\text{mid}}=2.0\) \\ \cline{2-4}  & \multicolumn{2}{c}{\(k\in[0,K]\), where \(K\) is the total training iterations} \\ \cline{2-4}  & \(\sigma_{i}=(\sigma_{\text{max}}^{1/\mu}+\frac{i-1}{N(k)-1}(\sigma_{\text{max}}^ {1/\mu}-\sigma_{\text{min}}^{1/\mu}))^{\rho}\), where \(i\in[1,N(k)],\rho=7,\sigma_{\text{min}}=0.002,\sigma_{\text{max}}=80\) \\ \hline \end{tabular}
\end{table}
Table 1: Comparing the design choices for CT in Song et al. (2023) versus our modifications.

generators for dropout share the same states across the student and teacher networks when optimizing the CT objective in Eq. (5).

By choosing the appropriate weighting function, noise embedding layers, and dropout, we significantly improve the sample quality of consistency models using the squared \(\ell_{2}\) metric, closing the gap with the original CT in Song et al. (2023) that relies on LPIPS (see Fig. 0(c)). Although our modifications do not immediately improve the sample quality of CT with LPIPS, combining with additional techniques in Section 3.2 will yield significant improvements for both metrics.

### Removing EMA for the teacher network

When training consistency models, we minimize the discrepancy between models evaluated at adjacent noise levels. Recall from Section 2 that the model with the lower noise level is termed the _teacher network_, and its counterpart the _student network_. While Song et al. (2023) maintains EMA parameters for both networks with potentially varying decay rates, we present a theoretical argument indicating that the EMA decay rate for the teacher network should always be zero for CT, although it can be nonzero for CD. We revisit the theoretical analysis in Song et al. (2023) to support our assertion and provide empirical evidence that omitting EMA from the teacher network in CT notably improves the sample quality of consistency models.

To support the use of CT, Song et al. (2023) present two theoretical arguments linking the CT and CM objectives as \(N\to\infty\). The first line of reasoning, which we call Argument (i), draws upon Theorem 2 from Song et al. (2023) to show that under certain regularity conditions, \(\mathcal{L}_{\mathrm{CT}}^{N}(\mathbf{\theta},\mathbf{\theta}^{-})=\mathcal{L}^{N}( \mathbf{\theta},\mathbf{\theta}^{-})+o(\Delta\sigma)\). That is, when \(N\to\infty\), we have \(\Delta\sigma\to 0\) and hence \(\mathcal{L}_{\mathrm{CT}}^{N}(\mathbf{\theta},\mathbf{\theta}^{-})\) converges to \(\mathcal{L}^{N}(\mathbf{\theta},\mathbf{\theta}^{-})\) asymptotically. The second argument, called Argument (ii), is grounded in Theorem 6 from Song et al. (2023) which asserts that when \(\mathbf{\theta}^{-}=\mathbf{\theta}\), both \(\lim_{N\to\infty}(N-1)\nabla_{\mathbf{\theta}}\mathcal{L}^{N}(\mathbf{\theta},\mathbf{ \theta}^{-})\) and \(\lim_{N\to\infty}(N-1)\nabla_{\mathbf{\theta}}\mathcal{L}_{\mathrm{CT}}^{N}(\mathbf{ \theta},\mathbf{\theta}^{-})\) are well-defined and identical. This suggests that after scaling by \(N-1\), gradients of the CT and CM objectives match in the limit of \(N\to\infty\), leading to equivalent training dynamics. Unlike Argument (i), Argument (ii) is valid only when \(\mathbf{\theta}^{-}=\mathbf{\theta}\), which can be enforced by setting the EMA decay rate \(\mu\) for the teacher network to zero in Eq. (4).

We show this inconsistency in requirements for Argument (i) and (ii) to hold is caused by flawed theoretical analysis of the former. Specifically, Argument (i) fails if \(\lim_{N\to\infty}\mathcal{L}^{N}(\mathbf{\theta},\mathbf{\theta}^{-})\) is not a valid objective for learning consistency models, which we show can happen when \(\mathbf{\theta}^{-}\neq\mathbf{\theta}\). To give a concrete example, consider a data distribution \(p_{\text{data}}(x)=\delta(x-\xi)\), which leads to \(p_{\sigma}(x)=\mathcal{N}(x;\xi,\sigma^{2})\) and a ground truth consistency function \(f^{\mathbf{\ast}}(x,\sigma)=\frac{\sigma_{\text{data}}}{\sigma}x+\left(1-\frac{ \sigma_{\text{data}}}{\sigma}\right)\xi\). Let us define the consistency model as \(f_{\theta}(x,\sigma)=\frac{\sigma_{\text{data}}}{\sigma}x+\left(1-\frac{ \sigma_{\text{data}}}{\sigma}\right)\theta\). In addition, let \(\sigma_{i}=\sigma_{\text{min}}+\frac{i-1}{N-1}(\sigma_{\text{max}}-\sigma_{ \text{min}})\) for \(i\in\llbracket 1,N\rrbracket\) be the noise levels, where we have \(\Delta\sigma=\frac{\sigma_{\text{data}}-\sigma_{\text{min}}}{N-1}\). Given \(z\sim\mathcal{N}(0,1)\) and \(x_{\sigma_{i+1}}=\xi+\sigma_{i+1}z\), it is straightforward to show that \(\ddot{x}_{\sigma_{i}}=x_{\sigma_{i+1}}-\sigma_{i+1}(\sigma_{i}-\sigma_{i+1}) \nabla_{x}\log p_{\sigma}(x_{\sigma_{i+1}})\)

Figure 1: (a) As the Fourier scale parameter decreases, Fourier noise embeddings become less sensitive to minute noise differences. This sensitivity is closest to that of positional embeddings when the Fourier scale is set to 0.02. (b) Continuous-time CT diverges when noise embeddings are overly sensitive to minor noise differences. (c) An ablation study examines the effects of our selections for weighting function (\(\frac{1}{\sigma_{i+1}-\sigma_{i}}\)), noise embedding (Fourier scale \(=0.02\)), and dropout (\(=0.3\)) on CT using the squared \(\ell_{2}\) metric. Here baseline models for both metrics follow configurations in Song et al. (2023). All models are trained on CIFAR-10 without class labels.

simplifies to \(\tilde{x}_{\sigma_{i}}=\xi+\sigma_{i}z\). As a result, the objectives for CM and CT align perfectly in this toy example. Building on top of this analysis, the following result proves that \(\lim_{N\to\infty}\mathcal{L}^{N}(\theta,\theta^{-})\) here is not amenable for learning consistency models whenever \(\theta^{-}\neq\theta\).

**Proposition 1**.: _Given the notations introduced earlier, and using the uniform weighting function \(\lambda(\sigma)=1\) along with the squared \(\ell_{2}\) metric, we have_","\lim_{N\to\infty}\mathcal{L}^{N}(\theta,\theta^{-})=\lim_{N\to \infty}\mathcal{L}^{N}_{\text{CT}}(\theta,\theta^{-})=\mathbb{E}\Big{[}\big{(} 1-\frac{\sigma_{min}}{\sigma_{i}}\big{)}^{2}(\theta-\theta^{-})^{2}\Big{]} \quad\text{if }\theta^{-}\neq\theta || \lim_{N\to\infty}\frac{1}{\Delta\sigma}\frac{\mathrm{d}\mathcal{ L}^{N}(\theta,\theta^{-})}{\mathrm{d}\theta}=\{\begin{array}{ll} \frac{\mathrm{d}\mathcal{L}}{\mathrm{d}\theta}\mathbb{E}\Big{[}\frac{\sigma_ {min}}{\sigma_{i}^{2}}\Big{(}1-\frac{\sigma_{min}}{\sigma_{i}}\Big{)}(\theta -\xi)^{2}\Big{]},&\theta^{-}=\theta\\ +\infty,&\theta^{-}\prec\theta\\ -\infty,&\theta^{-}>\theta\end{array}.",,"\lim_{N\to\infty}\mathcal{L}^{N}(\theta,\theta^{-})=\mathbb{E}_{z}\left[\left(\frac{\sigma_{\text{data}}}{\sigma_{\text{max}}}\sigma_{\text{max}}z+\left(1-\frac{\sigma_{\text{data}}}{\sigma_{\text{max}}}\right)\theta-\left(\frac{\sigma_{\text{data}}}{\sigma_{\text{max}}}\sigma_{\text{max}}z+\left(1-\frac{\sigma_{\text{data}}}{\sigma_{\text{max}}}\right)\theta^{-}\right)\right)^{2}\right].",
ICLR_2024_oral_29,7,"Proof.: See Appendix A. 

Recall that typically \(\theta^{-}\neq\theta\) when \(\mu\neq 0\). In this case, Eq. (6) shows that the CM/CT objective is independent of \(\xi\), thus providing no signal of the data distribution and are therefore impossible to train correct consistency models. This directly refutes Argument (i). In contrast, when we set \(\mu=0\) to ensure \(\theta^{-}=\theta\), Eq. (7) indicates that the gradient of the CM/CT objective, when scaled by \(1/\Delta\sigma\), converges to the gradient of the mean squared error between \(\theta\) and \(\xi\). Optimizing this gradient consequently yields \(\theta=\xi\), accurately learning the ground truth consistency function. This analysis is consistent with Argument (ii).

As illustrated in Fig. 1(a), discarding EMA from the teacher network notably improves sample quality for CT across both LPIPS and squared \(\ell_{2}\) metrics. The curves labeled ""Improved"" correspond to CT using the improved design outlined in Section 3.1. Setting \(\mu(k)=0\) for all training iteration \(k\) effectively counters the sample quality degradation of LPIPS caused by the modifications in Section 3.1. Combining the strategies from Section 3.1 with a zero EMA for the teacher, we are able to match the sample quality of the original CT in Song et al. (2023) that necessitates LPIPS, by using simple squared \(\ell_{2}\) metrics.

### Pseudo-Huber metric functions

Using the methods from Sections 3.1 and 3.2, we are able to improve CT with squared \(\ell_{2}\) metric, matching the original CT in Song et al. (2023) that utilizes LPIPS. Yet, as shown in Fig. 1(a), LPIPS still maintains a significant advantage over traditional metric functions when the same improved techniques are in effect for all. To address this disparity, we adopt the Pseudo-Huber metric family (Charbonnier et al., 1997), defined as","d(\mathbf{x},\mathbf{y})=\sqrt{\lVert\mathbf{x}-\mathbf{y}\rVert_{2}^{2}+c^{2}}-c,",,"d_{c}(\mathbf{x},\mathbf{y})=\sqrt{\|\mathbf{x}-\mathbf{y}\|_{2}^{2}+c^{2}}-c,",which smoothly interpolates between the $\ell_1$ and $\ell_2$ metrics depending on the value of the parameter $c > 0$.
ICLR_2024_oral_29,8,"Figure 2: (a) Removing EMA in the teacher network leads to significant improvement in FIDs. (b, c) Pseudo-Huber metrics significantly improve the sample quality of squared \(\ell_{2}\) metric, and catches up with LPIPS when using overall larger \(N(k)\), where the Pseudo-Huber metric with \(c=0.03\) is the optimal. All training runs here employ the improved techniques from Sections 3.1 and 3.2.

where \(c>0\) is an adjustable constant. As depicted in Fig. 5a, Pseudo-Huber metrics smoothly bridge the \(\ell_{1}\) and squared \(\ell_{2}\) metrics, with \(c\) determining the breadth of the parabolic section. In contrast to common metrics like \(\ell_{0}\), \(\ell_{1}\), and \(\ell_{\infty}\), Pseudo-Huber metrics are continuously twice differentiable, and hence meet the theoretical requirement for CT outlined in Song et al. (2023).

Compared to the squared \(\ell_{2}\) metric, the Pseudo-Huber metric is more robust to outliers as it imposes a smaller penalty for large errors than the squared \(\ell_{2}\) metric does, yet behaves similarly for smaller errors. We posit that this added robustness can reduce variance during training. To validate this hypothesis, we examine the \(\ell_{2}\) norms of parameter updates obtained from the Adam optimizer during the course of training for both squared \(\ell_{2}\) and Pseudo-Huber metric functions, and summarize results in Fig. 5b. Our observations confirm that the Pseudo-Huber metric results in reduced variance relative to the squared \(\ell_{2}\) metric, aligning with our hypothesis.

We evaluate the effectiveness of Pseudo-Huber metrics by training several consistency models with varying \(c\) values on CIFAR-10 and comparing their sample quality with models trained using LPIPS and squared \(\ell_{2}\) metrics. We incorporate improved techniques from Sections 3.1 and 3.2 for all metrics. Fig. 2 reveals that Pseudo-Huber metrics yield notably better sample quality than the squared \(\ell_{2}\) metric. By increasing the overall size of \(N(k)\)--adjusting \(s_{0}\) and \(s_{1}\) from the standard values of 2 and 150 in Song et al. (2023) to our new values of 10 and 1280 (more in Section 3.4)--we for the first time surpass the performance of CT with LPIPS on equal footing using a traditional metric function that does not rely on learned feature representations. Furthermore, Fig. 2c indicates that \(c=0.03\) is optimal for CIFAR-10 images. We suggest that \(c\) should scale linearly with \(\|\mathbf{x}-\mathbf{y}\|_{2}\), and propose a heuristic of \(c=0.00054\sqrt{d}\) for images with \(d\) dimensions. Empirically, we find this recommendation to work well on both CIFAR-10 and ImageNet \(64\times 64\) datasets.

### Improved curriculum for total discretization steps

As mentioned in Section 3.2, CT's theoretical foundation holds asymptotically as \(N\to\infty\). In practice, we have to select a finite \(N\) for training consistency models, potentially introducing bias into the learning process. To understand the influence of \(N\) on sample quality, we train a consistency model with improved techniques from Sections 3.1 to 3.3. Unlike Song et al. (2023), we use an exponentially increasing curriculum for the total discretization steps \(N\), doubling \(N\) after a set number of training iterations. Specifically, the curriculum is described by","N(k)=\min(s_{0}2^{\lfloor\frac{1}{N}\rfloor},s_{1})+1,\quad K^{\prime}=\Big{|} \frac{K}{\log_{2}[s_{1}/s_{0}]+1}\Big{|},",,"N(k)=\min(s_{0}2^{\frac{k}{K}}|,s_{1})+1,","where $s_{0}$ and $s_{1}$ are the initial and maximum number of discretization steps, $k$ is the current training iteration, and $K$ is the total number of training iterations"
ICLR_2024_oral_3,1,"## 1 Introduction

Stochastic optimization algorithms solve optimization problems of the form","\mathbf{\theta}^{*}\in\operatorname*{arg\,min}_{\mathbf{\theta}\in\mathbb{R}^{d}}f( \mathbf{\theta}),\ \ \ \text{where }f(\mathbf{\theta})\triangleq\mathbb{E}_{X\sim\mathbf{\mu}}[F(\mathbf{\theta},X) ]=\sum_{i\in\mathcal{N}}\mu_{i}F(\mathbf{\theta},i),",,"\begin{equation}
\min_{x \in \mathcal{X}} f(x),
\end{equation}","Equation 1 represents the standard form of a stochastic optimization problem, where the objective is to minimize a function $ f(x) $ over the domain $ \mathcal{X} $."
ICLR_2024_oral_3,2,"with the objective function \(f:\mathbb{R}^{d}\to\mathbb{R}\) and \(X\) taking values in a finite state space \(\mathcal{N}\) with distribution \(\mathbf{\mu}\triangleq[\mu_{i}]_{i\in\mathcal{N}}\). Leveraging partial gradient information per iteration, these algorithms have been recognized for their scalability and efficiency with large datasets (Bottou et al., 2018; Even, 2023). For any given _noise sequence_\(\{X_{n}\}_{n\geq 0}\subset\mathcal{N}\), and step size sequence \(\{\beta_{n}\}_{n\geq 0}\subset\mathbb{R}_{+}\), most stochastic optimization algorithms can be classified as stochastic approximations (SA) of the form","\mathbf{\theta}_{n+1}=\mathbf{\theta}_{n}+\beta_{n+1}H(\mathbf{\theta}_{n},X_{n+1}),\ \ \ \forall\ n\geq 0,","where, roughly speaking, \(H(\mathbf{\theta},i)\) contains gradient information \(\nabla_{\mathbf{\theta}}F(\theta,i)\), such that \(\mathbf{\theta}^{*}\) solves \(\mathbf{h}(\mathbf{\theta})\triangleq\mathbb{E}_{X\sim\mathbf{\mu}}[H(\mathbf{\theta},X)]= \sum_{i\in\mathcal{N}}\mu_{i}H(\mathbf{\theta},i)=\mathbf{0}\)","\theta_{n+1} = \theta_{n} - \beta_{n} \nabla_{\theta} F(\theta_{n}, X_{n}).",Stochastic approximation update rule for iterative parameter estimation using noisy gradient information.
ICLR_2024_oral_3,3,"\). Such SA iterations include the well-known stochastic gradient descent (SGD), stochastic heavy ball (SHB) (Gadat et al., 2018; Li et al., 2022), and some SGD-type algorithms employing additional auxiliary variables (Barakat et al., 2021).1 These algorithms typically have the stochastic noise term \(X_{n}\) generated by _i.i.d._ random variables with probability distribution \(\mathbf{\mu}\) in each iteration. In this paper, we study a stochastic optimization algorithm where the noise sequence governing access to the gradient information is generated from general stochastic processes in place of _i.i.d._ random variables.

This is commonly the case in distributed learning, where \(\{X_{n}\}\) is a (typically Markovian) random walk, and should asymptotically be able to sample the gradients from the desired probability distribution \(\mathbf{\mu}\). This is equivalent to saying that the random walker's _empirical distribution_ converges to \(\mathbf{\mu}\) almost surely (a.s.); that is, \(\mathbf{x}_{n}\triangleq\frac{1}{n+1}\sum_{k=0}^{n}\mathbf{\delta}_{X_{k}}\ \frac{a.s.}{n\to\infty}\ \mathbf{\mu}\) for any initial \(X_{0}\in\mathcal{N}\), where \(\mathbf{\delta}_{X_{k}}\) is the delta measure whose \(X_{k}\)'th entry is one, the rest being zero. Such convergence is most commonly achieved by employing the Metropolis Hastings random walk (MHRW) which can be designed to sample from any _target_ measure \(\mathbf{\mu}\) and implemented in a scalable manner (Sun et al., 2018). Unsurprisingly, convergence characteristics of the employed Markov chain affect that of the SA sequence (2), and appear in both finite-time and asymptotic analyses. Finite-time bounds typically involve the second largest eigenvalue in modulus (SLEM) of the Markov chain's transition kernel \(\mathbf{P}\), which is critically connected to the mixing time of a Markov chain (Levin and Peres, 2017); whereas asymptotic results such as central limit theorems (CLT) involve asymptotic covariance matrices that embed information regarding the entire spectrum of \(\mathbf{P}\), i.e., all eigenvalues as well as eigenvectors (Bremaud, 2013), which are key to understanding the sampling efficiency of a Markov chain. Thus, the choice of random walker can significantly impact the performance of (2), and simply ensuring that it samples from \(\mathbf{\mu}\) asymptotically is not enough to achieve optimal algorithmic performance. In this paper, we take a closer look at the distributed stochastic optimization problem through the lens of a _non-linear_ Markov chain, known as the _Self Repellent Random Walk_ (SRRW), which was shown in Doshi et al. (2023) to achieve asymptotically minimal sampling variance for large values of \(\alpha\), a positive scalar controlling the strength of the random walker's self-repellence behaviour. Our proposed modification of (2) can be implemented within the settings of decentralized learning applications in a scalable manner, while also enjoying significant performance benefit over distributed stochastic optimization algorithms driven by vanilla Markov chains.

**Token Algorithms for Decentralized Learning.** In decentralized learning, agents like smartphones or IoT devices, each containing a subset of data, collaboratively train models on a graph \(\mathcal{G}(\mathcal{N},\mathcal{E})\) by sharing information locally without a central server (McMahan et al., 2017). In this setup, \(N\!=\!|\mathcal{N}|\) agents correspond to nodes \(i\in\mathcal{N}\), and an edge \((i,j)\!\in\!\mathcal{E}\) indicates direct communication between agents \(i\) and \(j\). This decentralized approach offers several advantages compared to the traditional centralized learning setting, promoting data privacy and security by eliminating the need for raw data to be aggregated centrally and thus reducing the risk of data breach or misuse (Bottou et al., 2018; Nedic, 2020). Additionally, decentralized approaches are more scalable and can handle vast amounts of heterogeneous data from distributed agents without overwhelming a central server, alleviating concerns about single point of failure (Vogels et al., 2021).

Among decentralized learning approaches, the class of 'Token' algorithms can be expressed as stochastic approximation iterations of the type (2), wherein the sequence \(\{X_{n}\}\) is realized as the sample path of a token that stochastically traverses the graph \(\mathcal{G}\), carrying with it the iterate \(\mathbf{\theta}_{n}\) for any time \(n\geq 0\) and allowing each visited node (agent) to incrementally update \(\mathbf{\theta}_{n}\) using locally available gradient information. Token algorithms have gained popularity in recent years (Hu et al., 2022; Tristacyn et al., 2022; Hendrikx, 2023), and are provably more communication efficient (Even, 2023) when compared to consensus-based algorithms - another popular approach for solving distributed optimization problems (Boyd et al., 2006; Morral et al., 2017; Olshevsky, 2022). The construction of token algorithms means that they do not suffer from expensive costs of synchronization and communication that are typical of consensus-based approaches, where all agents (or a subset of agents selected by a coordinator (Boyd et al., 2006; Wang et al., 2019)) on the graph are required to take simultaneous actions, such as communicating on the graph at each iteration. While decentralized Federated learning has indeed helped mitigate the communication overhead by processing multiple SGD iterations prior to each aggregation (Lalitha et al., 2018; Ye et al., 2022; Chellapandi et al., 2023), they still cannot overcome challenges such as synchronization and straggler issues.

**Self Repellent Random Walk.** As mentioned earlier, sample paths \(\{X_{n}\}\) of token algorithms are usually generated using Markov chains with \(\mathbf{\mu}\in\text{Int}(\Sigma)\) as their limiting distribution. Here, \(\Sigma\) denotes the \(N\)-dimensional probability simplex, with \(\text{Int}(\Sigma)\) representing its interior. A recent work by Doshi et al. (2023) pioneers the use of _non-linear Markov chains_ to, in some sense, improve upon _any given_ time-reversible Markov chain with transition kernel \(\mathbf{P}\) whose stationary distribution is \(\mathbf{\mu}\)They show that the non-linear transition kernel2\(\mathbf{K}[\cdot]:\text{Int}(\Sigma)\to[0,1]^{N\times N}\), given by

Footnote 2: Here, non-linearity in the transition kernel implies that \(\mathbf{K}[\mathbf{x}]\) takes probability distribution \(\mathbf{x}\) as the argument (Andrieu et al., 2007), as opposed to the kernel being a linear operator \(\mathbf{K}[\mathbf{x}]=\mathbf{P}\) for a constant stochastic matrix \(\mathbf{P}\) in a standard (linear) Markovian setting.","K_{ij}[\mathbf{x}]\triangleq\frac{P_{ij}(x_{j}/\mu_{j})^{-\alpha}}{\sum_{k\in \mathcal{N}}P_{ik}(x_{k}/\mu_{k})^{-\alpha}},\quad\quad\forall\,i,j\in\mathcal{ N},",,"\mathbf{K}[\mathbf{x}](i,j)\triangleq\frac{\mathbf{P}(i,j)}{1+\alpha(\mathbf{x}_{i}-\mathbf{x}_{j})_{+}},\ \ \ \forall\ i,j\in\mathcal{N},","where $\alpha > 0$ is a parameter controlling the strength of self-repellence, and $(\cdot)_+$ denotes the positive part of the expression."
ICLR_2024_oral_3,4,"for any \(\mathbf{x}\in\text{Int}(\Sigma)\), when simulated as a self-interacting random walk (Del Moral and Miclo, 2006; Del Moral and Doucet, 2010), can achieve smaller asymptotic variance than the base Markov chain when sampling over a graph \(\mathcal{G}\), for all \(\alpha>0\). The argument \(\mathbf{x}\) for the kernel \(\mathbf{K}[\mathbf{x}]\) is taken to be the empirical distribution \(\mathbf{x}_{n}\) at each time step \(n\geq 0\). For instance, if node \(j\) has been visited more often than other nodes so far, the entry \(x_{j}\) becomes larger (than target value \(\mu_{j}\)), resulting in a smaller transition probability from \(i\) to \(j\) under \(\mathbf{K}[\mathbf{x}]\) in (3) compared to \(P_{ij}\). This ensures that a random walker prioritizes more seldom visited nodes in the process, and is thus'self-repellent'. This effect is made more drastic by increasing \(\alpha\), and leads to asymptotically near-zero variance at a rate of \(O(1/\alpha)\). Moreover, the polynomial function \((x_{i}/\mu_{i})^{-\alpha}\) chosen to encode self-repellent behaviour is shown in Doshi et al. (2023) to be the only one that allows the SRRW to inherit the so-called'scale-invariance' property of the underlying Markov chain - a necessary component for the scalable implementation of a random walker over a large network without requiring knowledge of any graph-related global constants. Conclusively, such attributes render SRRW especially suitable for distributed optimization.3

Footnote 3: Recently, Guo et al. (2020) introduce an optimization scheme, which designs self-repellence into the perturbation of the gradient descent iterates (Jin et al., 2017, 2018, 2021) with the goal of escaping saddle points. This notion of self-repellence is distinct from the SRRW, which is a probability kernel designed specifically for a token to sample from a target distribution \(\mathbf{\mu}\) over a set of nodes on an arbitrary graph.

**Effect of Stochastic Noise - Finite time and Asymptotic Approaches.** Most contemporary token algorithms driven by Markov chains are analyzed using the finite-time bounds approach for obtaining insights into their convergence rates (Sun et al., 2018; Doan et al., 2019, 2020; Triastcyn et al., 2022; Hendrikx, 2023). However, as also explained in Even (2023), in most cases these bounds are overly dependent on mixing time properties of the specific Markov chain employed therein. This makes them largely ineffective in capturing the exact contribution of the underlying random walk in a manner which is qualitative enough to be used for algorithm design; and performance enhancements are typically achieved via application of techniques such as variance reduction (Defazio et al., 2014; Schmidt et al., 2017), momentum/Nesterov's acceleration (Gadat et al., 2018; Li et al., 2022), adaptive step size (Kingma and Ba, 2015; Reddi et al., 2018), which work by modifying the algorithm iterations themselves, and never consider potential improvements to the stochastic input itself.

Complement to finite-time approaches, asymptotic analysis using CLT has proven to be an excellent tool to approach the design of stochastic algorithms (Hu et al., 2022; Devraj and Meyn, 2017; Morral et al., 2017; Chen et al., 2020; Mou et al., 2020; Devraj and Meyn, 2021). Hu et al. (2022) shows how asymptotic analysis can be used to compare the performance of SGD algorithms for various stochastic inputs using their notion of efficiency ordering, and, as mentioned in Devraj and Meyn (2017), the asymptotic benefits from minimizing the limiting covariance matrix are known to be a good predictor of finite-time algorithmic performance, also observed empirically in Section 4.

From the perspective of both finite-time analysis as well as asymptotic analysis of token algorithms, it is now well established that employing 'better' Markov chains can enhance the performance of stochastic optimization algorithm. For instance, Markov chains with smaller SLEMs yield tighter finite-time upper bounds (Sun et al., 2018; Ayache and El Rouayheb, 2021; Even, 2023). Similarly, Markov chains with smaller asymptotic variance for MCMC sampling tasks also provide better performance, resulting in smaller covariance matrix of SGD algorithms (Hu et al., 2022). Therefore, with these breakthrough results via SRRW achieving near-zero sampling variance, it is within reason to ask: _Can we achieve near-zero variance in distributed stochastic optimization driven by SRRW-like token algorithms on any general graph?4_ In this paper, we answer in the affirmative.

Footnote 4: This near-zero sampling variance implies a significantly smaller variance than even an _i.i.d._ sampling counterpart, while adhering to graph topological constraints of token algorithms.

**SRRW Driven Algorithm and Analysis Approach.** For any ergodic time-reversible Markov chain with transition probability matrix \(\mathbf{P}\triangleq[P_{ij}]_{i,j\in\mathcal{N}}\) and stationary distribution \(\mathbf{\mu}\in\text{Int}(\Sigma)\), we consider a general step size version of the SRRW stochastic process analysed in Doshi et al. (2023) and use it to drive the noise sequence in (2). Our SA-SRRW algorithm is as follows:","\text{Draw:}\qquad X_{n+1} \sim\mathbf{K}_{X_{n},\cdot}[\mathbf{x}_{n}]",,"\mathbf{\theta}_{n+1}=\mathbf{\theta}_{n}+\beta_{n+1}H(\mathbf{\theta}_{n},X_{n+1}),\quad X_{n+1}\sim\mathbf{K}[\mathbf{x}_{n}](X_{n},\cdot),","Stochastic approximation iteration driven by the Self Repellent Random Walk (SRRW), where the next state $X_{n+1}$ is sampled from the non-linear transition kernel $\mathbf{K}[\mathbf{x}_{n}]$ evaluated at the current empirical distribution $\mathbf{x}_{n}$."
ICLR_2024_oral_3,5,(4a) Update:,"\quad\mathbf{x}_{n+1} =\mathbf{x}_{n}+\gamma_{n+1}(\boldsymbol{\delta}_{X_{n+1}}- \mathbf{x}_{n}), || \quad\boldsymbol{\theta}_{n+1} =\boldsymbol{\theta}_{n}+\beta_{n+1}H(\boldsymbol{\theta}_{n},X_{ n+1}),",,"\mathbf{\theta}_{n+1} = \mathbf{\theta}_{n} + \beta_{n+1} H(\mathbf{\theta}_{n}, X_{n+1}),",where the update step follows the standard stochastic approximation framework using the SRRW-driven noise sequence $X_{n+1}$ generated from the non-linear transition kernel $\mathbf{K}[\mathbf{x}_{n}]$.
ICLR_2024_oral_3,6,"3). Current non-asymptotic analyses require globally Lipschitz mean field function (Chen et al., 2020; Doan, 2021; Zeng et al., 2021; Even, 2023) and is thus inapplicable to SA-SRRW since the mean field function of the SRRW iterates (4b) is only locally Lipschitz (details deferred to Appendix B). Instead, we successfully obtain non-trivial results by taking an asymptotic CLT-based approach to analyze (4). This goes beyond just analyzing the asymptotic _sampling covariance5_ as in Doshi et al. (2023), the result therein forming a special case of ours by setting \(\gamma_{n}\!=\!1/(n\!+\!1)\) and considering only (4a) and (4b), that is, in the absence of optimization iteration (4c). Specifically, we capture the effect of SRRW's hyper-parameter \(\alpha\) on the asymptotic speed of convergence of the _optimization_ error term \(\boldsymbol{\theta}_{n}-\boldsymbol{\theta}^{*}\) to zero via explicit deduction of its asymptotic covariance matrix. See Figure 1 for illustration.

Footnote 5: Sampling covariance corresponds to only the empirical distribution \(\mathbf{x}_{n}\) in (4b).

**Our Contributions.**

**1.** Given any time-reversible 'base' Markov chain with transition kernel \(\mathbf{P}\) and stationary distribution \(\boldsymbol{\mu}\), we generalize first and second order convergence results of \(\mathbf{x}_{n}\) to target measure \(\boldsymbol{\mu}\) (Theorems 4.1 and 4.2 in Doshi et al., 2023) to a class of weighted empirical measures, through the use of more general step sizes \(\gamma_{n}\). This includes showing that the asymptotic _sampling_ covariance terms decrease to zero at rate \(O(1/\alpha)\), thus quantifying the effect of self-repellent on \(\mathbf{x}_{n}\). Our generalization is not for the sake thereof and is shown in Section 3 to be crucial for the design of step sizes \(\beta_{n},\gamma_{n}\).

**2.** Building upon the convergence results for iterates \(\mathbf{x}_{n}\), we analyze the algorithm (4) driven by the SRRW kernel in (3) with step sizes \(\beta_{n}\) and \(\gamma_{n}\) separated into three disjoint cases:

1. \(\beta_{n}=o(\gamma_{n})\), and we say that \(\boldsymbol{\theta}_{n}\) is on the _slower_ timescale compared to \(\mathbf{x}_{n}\);
2. \(\beta_{n}\!=\!\gamma_{n}\), and we say that \(\boldsymbol{\theta}_{n}\) and \(\mathbf{x}_{n}\) are on the same timescale;
3. \(\gamma_{n}=o(\beta_{n})\), and we say that \(\boldsymbol{\theta}_{n}\) is on the _faster_ timescale compared to \(\mathbf{x}_{n}\).

For any \(\alpha\geq 0\) and let \(k=1,2\) and \(3\) refer to the corresponding cases (i), (ii) and (iii), we show that","\boldsymbol{\theta}_{n}\xrightarrow[n\to\infty]{a.s.}\boldsymbol{\theta}^{*} \quad\text{and}\quad(\boldsymbol{\theta}_{n}-\boldsymbol{\theta}^{*})/\sqrt{ \beta_{n}}\xrightarrow[n\to\infty]{dist.}N(\boldsymbol{0},\mathbf{V}_{ \boldsymbol{\theta}}^{(k)}(\alpha)),",,"\sqrt{n}(\boldsymbol{\theta}_{n}-\boldsymbol{\theta}^{*})\overset{d}{\to} \mathcal{N}( \mathbf{0},\mathbf{\Sigma}_{k}^{(\alpha)})\quad\text{as }n\to\infty,",Asymptotic normality of the optimization error $\boldsymbol{\theta}_n - \boldsymbol{\theta}^*$ with a covariance matrix $\mathbf{\Sigma}_k^{(\alpha)}$ that depends on the timescale separation $k$ and the self-repellence parameter $\alpha$.
ICLR_2024_oral_3,7,"featuring distinct asymptotic covariance matrices \(\mathbf{V}_{\boldsymbol{\theta}}^{(1)}(\alpha),\mathbf{V}_{\boldsymbol{ \theta}}^{(2)}(\alpha)\) and \(\mathbf{V}_{\boldsymbol{\theta}}^{(3)}(\alpha)\), respectively. The three matrices coincide when \(\alpha=0\),6. Moreover, the derivation of the CLT for cases (i) and (iii), for which (4) corresponds to _two-timescale_ SA with _controlled_ Markov noise, is the first of its kind and thus a key technical contribution in this paper, as expanded upon in Section 3.

Footnote 6: The \(\alpha=0\) case is equivalent to simply running the base Markov chain, since from (3) we have \(\mathbf{K}[\cdot]=\mathbf{P}\), thus bypassing the SRRW’s effect and rendering all three cases nearly the same.

**3.** For case (i), we show that \(\mathbf{V}_{\boldsymbol{\theta}}^{(1)}(\alpha)\) decreases to zero (in the sense of Loewner ordering introduced in Section 2.1) as \(\alpha\) increases, with rate \(O(1/\alpha^{2})\). This is especially surprising, since the asymptotic performance benefit from using the SRRW kernel with \(\alpha\) in (3), to drive the noise terms \(X_{n}\), is _amplified_ in the context of distributed learning and estimating \(\boldsymbol{\theta}^{*}\); compared to the sampling case, for which the rate is \(O(1/\alpha)\) as mentioned earlier. For case (iii), we show that \(\mathbf{V}_{\boldsymbol{\theta}}^{(3)}(\alpha)\!=\!\mathbf{V}_{\boldsymbol{ \theta}}^{(3)}(0)\) for all \(\alpha\!\geq 0\), implying that using the SRRW in this case provides no asymptotic benefit than the

Figure 1: Visualization of token algorithms using SRRW versus traditional MC in distributed learning. Our CLT analysis, extended from SRRW itself to distributed stochastic approximation, leads to near-zero variance for the SA iteration \(\boldsymbol{\theta}_{n}\). Node numbers on the left denote visit counts.

original base Markov chain, and thus performs worse than case (i). In summary, we deduce that \(\mathbf{V}_{\boldsymbol{\theta}}^{(1)}(\alpha_{2})\!<_{L}\!\mathbf{V}_{\boldsymbol{ \theta}}^{(1)}(\alpha_{1})\!<_{L}\!\mathbf{V}_{\boldsymbol{\theta}}^{(1)}(0)\!= \!\mathbf{V}_{\boldsymbol{\theta}}^{(3)}(0)\!=\!\mathbf{V}_{\boldsymbol{\theta }}^{(3)}(\alpha)\) for all \(\alpha_{2}>\alpha_{1}>0\) and \(\alpha>0\).7

Footnote 7: In particular, this is the reason why we advocate for a more general step size \(\gamma_{n}=(n+1)^{-a}\) in the SRRW iterates with \(a<1\), allowing us to choose \(\beta_{n}=(n+1)^{-b}\) with \(b\in(a,1]\) to satisfy \(\beta_{n}=o(\gamma_{n})\) for case (i).

**4**. We numerically simulate our SA-SRRW algorithm on various real-world datasets, focusing on a binary classification task, to evaluate its performance across all three cases. By carefully choosing the function \(H\) in SA-SRRW, we test the SGD and algorithms driven by SRRW. Our findings consistently highlight the superiority of case (i) over cases (ii) and (iii) for diverse \(\alpha\) values, even in their finite time performance. Notably, our tests validate the variance reduction at a rate of \(O(1/\alpha^{2})\) for case (i), suggesting it as the best algorithmic choice among the three cases.

## 2 Preliminaries and Model Setup

In Section 2.1, we first standardize the notations used throughout the paper, and define key mathematical terms and quantities used in our theoretical analyses. Then, in Section 2.2, we consolidate the model assumptions of our SA-SRRW algorithm (4). We then go on to discuss our assumptions, and provide additional interpretations of our use of generalized step-sizes.

### Basic Notations and Definitions

Vectors are denoted by lower-case bold letters, e.g., \(\mathbf{v}\triangleq[v_{i}]\in\mathbb{R}^{D}\), and matrices by upper-case bold, e.g., \(\mathbf{M}\triangleq[M_{ij}]\in\mathbb{R}^{D\times D}\). \(\mathbf{M}^{-T}\) is the transpose of the matrix inverse \(\mathbf{M}^{-1}\). The diagonal matrix \(\mathbf{D}_{\mathbf{v}}\) is formed by vector \(\mathbf{v}\) with \(v_{i}\) as the \(i\)'th diagonal entry. Let \(\mathbf{1}\) and \(\mathbf{0}\) denote vectors of all ones and zeros, respectively. The identity matrix is represented by \(\mathbf{I}\), with subscripts indicating dimensions as needed. A matrix is _Hurwitz_ if all its eigenvalues possess strictly negative real parts. \(\mathds{1}_{\{\cdot\}}\) denotes an indicator function with condition in parentheses. We use \(\|\cdot\|\) to denote both the Euclidean norm of vectors and the spectral norm of matrices. Two symmetric matrices \(\mathbf{M}_{1},\mathbf{M}_{2}\) follow Loewner ordering \(\mathbf{M}_{1}\!<_{L}\mathbf{M}_{2}\) if \(\mathbf{M}_{2}\!-\!\mathbf{M}_{1}\) is positive semi-definite and \(\mathbf{M}_{1}\neq\mathbf{M}_{2}\). This slightly differs from the conventional definition with \(\leq_{L}\), which allows \(\mathbf{M}_{1}\!=\!\mathbf{M}_{2}\).

Throughout the paper, the matrix \(\mathbf{P}\triangleq[P_{i,j}]_{i,j\in\mathcal{N}}\) and vector \(\boldsymbol{\mu}\triangleq[\mu_{i}]_{i\in\mathcal{N}}\) are used exclusively to denote an \(N\times N\)-dimensional transition kernel of an ergodic Markov chain, and its stationary distribution, respectively. Without loss of generality, we assume \(P_{ij}>0\) if and only if \(a_{ij}>0\). Markov chains satisfying the _detailed balance equation_, where \(\mu_{i}P_{ij}=\mu_{j}P_{ji}\) for all \(i,j\in\mathcal{N}\), are termed _time-reversible_. For such chains, we use \((\lambda_{i},\mathbf{u}_{i})\) (resp. \((\lambda_{i},v_{i})\)) to denote the \(i\)'th left (resp. right) eigenpair where the eigenvalues are ordered: \(-1\!<\!\lambda_{1}\!\leq\!\cdots\!\leq\!\lambda_{N-1}\!<\!\lambda_{N}\!=\!1\), with \(\mathbf{u}_{N}\!=\!\boldsymbol{\mu}\) and \(\mathbf{v}_{N}\!=\!\boldsymbol{1}\) in \(\mathbb{R}^{N}\). We assume eigenvectors to be normalized such that \(\mathbf{u}_{i}^{T}\mathbf{v}_{i}\!=\!1\) for all \(i\), and we have \(\mathbf{u}_{i}\!=\!\mathbf{D}_{\boldsymbol{\mu}}\mathbf{v}_{i}\) and \(\mathbf{u}_{i}^{T}\mathbf{v}_{j}\!=\!0\) for all \(i,j\!\in\!\mathcal{N}\). We direct the reader to Aldous & Fill (2002, Chapter 3.4) for a detailed exposition on spectral properties of time-reversible Markov chains.

### SA-SRRW: Key Assumptions and Discussions

**Assumptions:** All results in our paper are proved under the following assumptions.

* The function \(H:\mathbb{R}^{D}\times\mathcal{N}\to\mathbb{R}^{D}\), is a continuous at every \(\boldsymbol{\theta}\in\mathbb{R}^{D}\), and there exists a positive constant \(L\) such that \(\|H(\boldsymbol{\theta},i)\|\leq L(1+\|\boldsymbol{\theta}\|)\) for every \(\boldsymbol{\theta}\in\mathbb{R}^{D},i\in\mathcal{N}\).
* Step sizes \(\beta_{n}\) and \(\gamma_{n}\) follow \(\beta_{n}\!=\!(n\!+\!1)^{-b}\), and \(\gamma_{n}\!=\!(n\!+\!1)^{-a}\), where \(a,b\in(0.5,1]\).
* Roots of function \(\mathbf{h}(\cdot)\) are disjoint, which comprise the globally attracting set \(\Theta\triangleq\left\{\boldsymbol{\theta}^{*}|\mathbf{h}(\boldsymbol{\theta}^ {*})\!=\!0,\nabla\mathbf{h}(\boldsymbol{\theta}^{*})+\frac{\mathbf{1}_{(n\!=\! 1)}}{2}\mathbf{I}\text{ is Hurwitz}\right\}\neq\emptyset\) of the associated ordinary differential equation (ODE) for iteration (4c), given by \(d\boldsymbol{\theta}(t)/dt\!=\!\mathbf{h}(\boldsymbol{\theta}(t))\).
* For any \((\boldsymbol{\theta}_{0},\mathbf{x}_{0},X_{0})\in\mathbb{R}^{D}\times\text{Int}( \Sigma)\times\mathcal{N}\), the iterate sequence \(\{\boldsymbol{\theta}_{n}\}_{n\geq 0}\) (resp. \(\{\mathbf{x}_{n}\}_{n\geq 0}\)) is \(\mathbb{P}_{\Theta_{0},\mathbf{x}_{0},X_{0}}\)-almost surely contained within a compact subset of \(\mathbb{R}^{D}\) (resp. \(\text{Int}(\Sigma)\)).

**Discussions on Assumptions:** Assumption A1 requires \(H\) to only be locally Lipschitz albeit with linear growth, and is less stringent than the globally Lipschitz assumption prevalent in optimization literature (Li & Wai, 2022; Hendrikx, 2023; Even, 2023).

Assumption A2 is the general umbrella assumption under which cases (i), (ii) and (iii) mentioned in Section 1 are extracted by setting: (i) \(a<b\), (ii) \(a=b\), and (iii) \(a>b\). Cases (i) and (iii) render \(\mathbf{\theta}_{n},\mathbf{x}_{n}\) on different timescales; the polynomial form of \(\beta_{n},\gamma_{n}\) widely assumed in the two-timescale SA literature (Mokkadem & Pelletier, 2006; Zeng et al., 2021; Hong et al., 2023). Case (ii) characterizes the SA-SRRW algorithm (4) as a single-timescale SA with polynomially decreasing step size, and is among the most common assumptions in the SA literature (Borkar, 2022; Fort, 2015; Li et al., 2023). In all three cases, the form of \(\gamma_{n}\) ensures \(\gamma_{n}\leq 1\) such that the SRRW iterates \(\mathbf{x}_{n}\) in (4b) is within \(\text{Int}(\Sigma)\), ensuring that \(\mathbf{K}[\mathbf{x}_{n}]\) is well-defined for all \(n\geq 0\).

In Assumption A3, limiting dynamics of SA iterations \(\{\mathbf{\theta}_{n}\}_{n\geq 0}\) closely follow trajectories \(\{\mathbf{\theta}(t)\}_{t\geq 0}\) of their associated ODE, and assuming the existence of globally stable equilibria is standard (Borkar, 2022; Fort, 2015; Li et al., 2023). In optimization problems, this is equivalent to assuming the existence of at most countably many local minima.

Assumption A4 assumes almost sure boundedness of iterates \(\mathbf{\theta}_{n}\) and \(\mathbf{x}_{n}\), which is a common assumption in SA algorithms (Kushner & Yin, 2003; Chen, 2006; Borkar, 2022; Karmakar & Bhatnagar, 2018; Li et al., 2023) for the stability of the SA iterations by ensuring the well-definiteness of all quantities involved. Stability of the weighted empirical measure \(\mathbf{x}_{n}\) of the SRRW process is practically ensured by studying (4b) with a truncation-based procedure (see Doshi et al., 2023, Remark 4.5 and Appendix E for a comprehensive explanation), while that for \(\mathbf{\theta}_{n}\) is usually ensured either as a by-product of the algorithm design, or via mechanisms such as projections onto a compact subset of \(\mathbb{R}^{D}\), depending on the application context. We now provide additional discussions regarding the step-size assumptions and their implications on the SRRW iteration (4b).

**SRRW with General Step Size:** As shown in Benaim & Cloez (2015, Remark 1.1), albeit for a completely different non-linear Markov kernel driving the algorithm therein, iterates \(\mathbf{x}_{n}\) of (4b) can also be expressed as _weighted_ empirical measures of \(\{X_{n}\}_{n\geq 0}\), in the following form:","\mathbf{x}_{n}=\frac{\sum_{i=1}^{n}\omega_{i}\mathbf{\theta}_{X_{i}}+\omega_{0} \mathbf{x}_{0}}{\sum_{i=0}^{n}\omega_{i}},\ \ \text{where}\ \ \omega_{0}=1,\ \ \text{and}\ \ \omega_{n}=\frac{\gamma_{n}}{\prod_{i=1}^{n}(1-\gamma_{i})},",,"\mathbf{x}_{n}=\sum_{k=0}^{n}\gamma_{k+1}\prod_{j=k+2}^{n}(1-\gamma_{j})\boldsymbol{\delta}_{X_{k}},","Weighted empirical measure representation of the SRRW iterate $\mathbf{x}_{n}$, where the step size sequence $\{\gamma_n\}$ governs the influence of past states $X_k$ through a product of decay factors."
ICLR_2024_oral_3,8,"for all \(n>0\). For the special case when \(\gamma_{n}=1/(n+1)\) as in Doshi et al. (2023), we have \(\omega_{n}=1\) for all \(n\geq 0\) and \(\mathbf{x}_{n}\) is the typical, unweighted empirical measure. For the additional case considered in our paper, when \(a<1\) for \(\gamma_{n}\) as in assumption A2, we can approximate \(1-\gamma_{n}\approx e^{-\gamma_{n}}\) and \(\omega_{n}\approx n^{-a}e^{n^{(1-a)}/(1-a)}\). This implies that \(\omega_{n}\) will increase at sub-exponential rate, giving more weight to recent visit counts and allowing it to quickly 'forget' the poor initial measure \(\mathbf{x}_{0}\) and shed the correlation with the initial choice of \(X_{0}\). This'speed up' effect by setting \(a<1\) is guaranteed in case (i) irrespective of the choice of \(b\) in Assumption A2, and in Section 3 we show how this can lead to further reduction in covariance of optimization error \(\mathbf{\theta}_{n}=\mathbf{\theta}^{*}\) in the asymptotic regime.

**Additional assumption for case (iii):** Before moving on to Section 3, we take another look at the case when \(\gamma_{n}=o(\beta_{n})\), and replace A3 with the following, stronger assumption only for case (iii).

* For any \(\mathbf{x}\in\text{Int}(\Sigma)\), there exists a function \(\rho:\text{Int}(\Sigma)\operatorname{\rightarrow}\mathbb{R}^{D}\) such that \(\|\rho(\mathbf{x})\|\operatorname{\leq}L_{2}(1+\|\mathbf{x}\|)\) for some \(L_{2}\operatorname{>}0\), \(\mathbb{E}_{i\operatorname{\sim}\mathbf{\pi}[\mathbf{x}]}(H(\rho(\mathbf{x}),i)] \operatorname{=}0\) and \(\mathbb{E}_{i\operatorname{\sim}\mathbf{\pi}[\mathbf{x}]}[\nabla H(\rho(\mathbf{x }),i)]+\frac{1_{\{n=1\}}}{2}\mathbf{I}\) is Hurwitz.

While Assumption A3\({}^{\prime}\) for case (iii) is much stronger than A3, it is not detrimental to the overall results of our paper, since case (i) is of far greater interest as impressed upon in Section 1. This is discussed further in Appendix C.

## 3 Asymptotic Analysis of the SA-SRRW Algorithm

In this section, we provide the main results for the SA-SRRW algorithm (4). We first present the a.s. convergence and the CLT result for SRRW with generalized step size, extending the results in Doshi et al. (2023). Building upon this, we present the a.s. convergence and the CLT result for the SA iterate \(\mathbf{\theta}_{n}\) under different settings of step sizes. We then shift our focus to the analysis of the different asymptotic covariance matrices emerging out of the CLT result, and capture the effect of \(\alpha\) and the step sizes, particularly in cases (i) and (iii), on \(\mathbf{\theta}_{n}-\mathbf{\theta}^{*}\) via performance ordering.

**Almost Sure convergence and CLT:** The following result establishes first and second order convergence of the sequence \(\{\mathbf{x}_{n}\}_{n\geq 0}\), which represents the weighted empirical measures of the SRRW process \(\{X_{n}\}_{n\geq 0}\), based on the update rule in (4b).

**Lemma 3.1**.: _Under Assumptions A1, A2 and A4, for the SRRW iterates (4b), we have_","\mathbf{x}_{n}\xrightarrow[n\to\infty]{a.s.}\boldsymbol{\mu},\quad \text{and}\quad\gamma_{n}^{-1/2}(\mathbf{x}_{n}-\boldsymbol{\mu})\xrightarrow[n \to\infty]{dist.}N(\mathbf{0},\mathbf{V}_{\mathbf{x}}(\alpha)), || \text{where}\quad\mathbf{V}_{\mathbf{x}}(\alpha)=\sum_{i=1}^{N-1 }\frac{1}{2\alpha(1+\lambda_{i})+2-\mathds{1}_{\{a=1\}}}\cdot\frac{1+\lambda_ {i}}{1-\lambda_{i}}\mathbf{u}_{i}\mathbf{u}_{i}^{T}.",,"\omega_{n} \approx n^{-a} \, e^{n^{(1-a)}/(1-a)}","Approximation of the weight sequence $\omega_n$ for $a < 1$, showing sub-exponential growth that prioritizes recent visit counts in the weighted empirical measure $\mathbf{x}_n$, enabling faster adaptation to the target distribution $\mathbf{\mu}$."
ICLR_2024_oral_3,9,"_Moreover, for all \(\alpha_{2}>\alpha_{1}>0\), we have \(\mathbf{V}_{\mathbf{x}}(\alpha_{2})<_{L}\mathbf{V}_{\mathbf{x}}(\alpha_{1})< _{L}\mathbf{V}_{\mathbf{x}}(0)\)._

Lemma 3.1 shows that the SRRW iterates \(\mathbf{x}_{n}\) converges to the target distribution \(\boldsymbol{\mu}\) a.s. even under the general step size \(\gamma_{n}=(n+1)^{-a}\) for \(a\in(0.5,1]\). We also observe that the asymptotic covariance matrix \(\mathbf{V}_{\mathbf{x}}(\alpha)\) decreases at rate \(O(1/\alpha)\). Lemma 3.1 aligns with Doshi et al. (2023, Theorem 4.2 and Corollary 4.3) for the special case of \(a=1\), and is therefore more general. Critically, it helps us establish our next result regarding the first-order convergence for the optimization iterate sequence \(\{\boldsymbol{\theta}_{n}\}_{n\geq 0}\) following update rule (4c), as well as its second-order convergence result, which follows shortly after. The proofs of Lemma 3.1 and our next result, Theorem 3.2, are deferred to Appendix D. In what follows, \(k=1,2\), and \(3\) refer to cases (i), (ii), and (iii) in Section 2.2, respectively. All subsequent results are proven under Assumptions A1 to A4, with A3\({}^{\prime}\) replacing A3 only when the step sizes \(\beta_{n},\gamma_{n}\) satisfy case (iii).

**Theorem 3.2**.: _For \(k\in\{1,2,3\}\), and any initial \((\boldsymbol{\theta}_{0},\mathbf{x}_{0},X_{0})\in\mathbb{R}^{D}\times\text{ Int}(\Sigma)\times\mathcal{N}\), we have \(\boldsymbol{\theta}_{n}\to\boldsymbol{\theta}^{*}\) as \(n\to\infty\) for some \(\boldsymbol{\theta}^{*}\in\Theta\), \(\mathbb{P}_{\boldsymbol{\theta}_{0},\mathbf{x}_{0},X_{0}}\)-almost surely._

In the stochastic optimization context, the above result ensures convergence of iterates \(\boldsymbol{\theta}_{n}\) to a local minimizer \(\boldsymbol{\theta}^{*}\). Loosely speaking, the first-order convergence of \(\mathbf{x}_{n}\) in Lemma 3.1 as well as that of \(\boldsymbol{\theta}_{n}\) are closely related to the convergence of trajectories \(\{\mathbf{z}(t)\triangleq(\boldsymbol{\theta}(t),\mathbf{x}(t))\}_{t\geq 0}\) of the (coupled) mean-field ODE, written in a matrix-vector form as",\tfrac{d}{dt}\mathbf{z}(t)=\mathbf{g}(\mathbf{z}(t))\triangleq\begin{bmatrix} \mathbf{H}(\boldsymbol{\theta}(t))^{T}\pi[\mathbf{x}(t)]\\ \pi[\mathbf{x}(t)]-\mathbf{x}(t)\end{bmatrix}\in\mathbb{R}^{D+N}.,"where matrix \(\mathbf{H}(\boldsymbol{\theta})\triangleq[H(\boldsymbol{\theta},1),\cdot,H( \boldsymbol{\theta},N)]^{T}\in\mathbb{R}^{N\times D}\) for any \(\boldsymbol{\theta}\in\mathbb{R}^{D}\)","\frac{d\mathbf{z}(t)}{dt}=\begin{bmatrix}\mathbf{h}(\boldsymbol{\theta}(t),\mathbf{x}(t))\\ \mathbf{g}(\mathbf{x}(t)) \end{bmatrix},\quad\text{where}\quad\mathbf{g}(\mathbf{x})\triangleq\sum_{i,j\in\mathcal{N}}x_{i}K_{ij}[\mathbf{x}]-\mathbf{x}.","Matrix-vector form of the mean-field ODE governing the coupled dynamics of the optimization iterate $\boldsymbol{\theta}(t)$ and the empirical distribution $\mathbf{x}(t)$, where $\mathbf{g}(\mathbf{x})$ represents the drift of the empirical distribution under the SRRW kernel."
ICLR_2024_oral_3,10,"\). Here, \(\boldsymbol{\pi}[\mathbf{x}]\in\text{Int}(\Sigma)\) is the stationary distribution of the SRRW kernel \(\mathbf{K}[\mathbf{x}]\) and is shown in Doshi et al. (2023) to be given by \(\pi_{i}[\mathbf{x}]\propto\sum_{j\in\mathcal{N}}\mu_{i}P_{ij}(x_{i}/\mu_{i})^{ -\alpha}(x_{j}/\mu_{j})^{-\alpha}\). The Jacobian matrix of (7) when evaluated at equilibria \(\mathbf{z}^{*}=(\boldsymbol{\theta}^{*},\boldsymbol{\mu})\) for \(\boldsymbol{\theta}^{*}\in\Theta\) captures the behaviour of solutions of the mean-field in their vicinity, and plays an important role in the asymptotic covariance matrices arising out of our CLT results. We evaluate this Jacobian matrix \(\mathbf{J}(\alpha)\) as a function of \(\alpha\geq 0\) to be given by",\mathbf{J}(\alpha)\!\triangleq\!\nabla g(\mathbf{z}^{*})\!=\!\begin{bmatrix} \nabla\mathbf{h}(\boldsymbol{\theta}^{*})&-\alpha\mathbf{H}(\boldsymbol{ \theta}^{*})^{T}(\mathbf{P}^{T}\!+\mathds{I})\\ \mathbf{0}_{N\!\times\!D}&2\alpha\boldsymbol{\mu}\mathds{1}^{T}\!-\!\alpha \mathds{P}^{T}\!\!-\!(\alpha\!+\!1)\mathds{I}\end{bmatrix}\!\triangleq\! \begin{bmatrix}\mathbf{J}_{11}&\mathbf{J}_{12}(\alpha)\\ \mathbf{J}_{21}&\mathbf{J}_{22}(\alpha)\end{bmatrix}.,,"\mathbf{J}(\alpha)\triangleq\frac{\partial\mathbf{g}(\mathbf{z})}{\partial\mathbf{z}}\Big|_{\mathbf{z}=\mathbf{z}^{*}}=\begin{bmatrix}\nabla\mathbf{h}(\boldsymbol{\theta}^{*}) & \mathbf{M}_{1}(\alpha)\\ \mathbf{0} & \mathbf{M}_{2}(\alpha)\end{bmatrix},","where the block entries $\mathbf{M}_{1}(\alpha)$ and $\mathbf{M}_{2}(\alpha)$ depend on the self-repellence parameter $\alpha$ and encode the sensitivity of the SRRW stationary distribution $\boldsymbol{\pi}[\mathbf{x}]$ to changes in $\mathbf{x}$, influencing the local dynamics of the coupled mean-field ODE near equilibrium."
ICLR_2024_oral_3,11,"The derivation of \(\mathbf{J}(\alpha)\) is referred to Appendix E.1.8 Here, \(\mathbf{J}_{21}\) is a zero matrix since \(\boldsymbol{\pi}[\mathbf{x}]-\mathbf{x}\) is devoid of \(\boldsymbol{\theta}\). While matrix \(\mathbf{J}_{22}(\alpha)\) is exactly of the form in Doshi et al. (2023, Lemma 3.4) to characterize the SRRW performance, our analysis includes an additional matrix \(\mathbf{J}_{12}(\alpha)\), which captures the effect of \(\mathbf{x}(t)\) on \(\boldsymbol{\theta}(t)\) in the ODE (7), which translates to the influence of our generalized SRRW empirical measure \(\mathbf{x}_{n}\) on the SA iterates \(\boldsymbol{\theta}_{n}\) in (4).

Footnote 8: The Jacobian \(\mathbf{J}(\alpha)\) is \((D+N)\times(D+N)\)– dimensional, with \(\mathbf{J}_{11}\in\mathbb{R}^{D\times D}\) and \(\mathbf{J}_{22}(\alpha)\in\mathbb{R}^{N\times N}\). Following this, all matrices written in a block form, such as matrix \(\mathbf{U}\) in (9), will inherit the same dimensional structure.

For notational simplicity, and without loss of generality, all our remaining results are stated while conditioning on the event that \(\{\boldsymbol{\theta}_{n}\to\boldsymbol{\theta}^{*}\}\), for some \(\boldsymbol{\theta}^{*}\in\Theta\). We also adopt the shorthand notation \(\mathbf{H}\) to represent \(\mathbf{H}(\boldsymbol{\theta}^{*})\). Our main CLT result is as follows, with its proof deferred to Appendix E.

**Theorem 3.3**.: _For any \(\alpha\geq 0\), we have: (a) There exists \(\mathbf{V}^{(k)}(\alpha)\) for all \(k\in\{1,2,3\}\) such that_","\begin{bmatrix}\beta_{n}^{-1/2}(\boldsymbol{\theta}_{n}-\boldsymbol{\theta}^{*} )\\ \gamma_{n}^{-1/2}(\mathbf{x}_{n}-\boldsymbol{\mu})\end{bmatrix}\xrightarrow[n\to \infty]{ait.}N(\mathbf{0},\mathbf{V}^{(k)}(\alpha)).",,"\sqrt{\beta_{n}}\left(\boldsymbol{\theta}_{n}-\boldsymbol{\theta}^{*}\right) \xrightarrow[n\to\infty]{\text{dist.}}N\left(\mathbf{0},\mathbf{V}_{ \boldsymbol{\theta}}^{(k)}(\alpha)\right),","Asymptotic normality of the optimization error $\boldsymbol{\theta}_n - \boldsymbol{\theta}^*$ under different step size regimes, with distinct covariance matrices $\mathbf{V}_{\boldsymbol{\theta}}^{(k)}(\alpha)$ for cases $k = 1, 2, 3$."
ICLR_2024_oral_3,12,"_(b) For \(k=2\), matrix \(\mathbf{V}^{(2)}(\alpha)\) solves the Lyapunov equation \(\mathbf{J}(\alpha)\mathbf{V}^{(2)}(\alpha)+\mathbf{V}^{(2)}(\alpha)\mathbf{J}( \alpha)^{T}+\mathds{1}_{\{b=1\}}\mathbf{V}^{(2)}(\alpha)=-\mathbf{U}\), where the Jacobian matrix \(\mathbf{J}(\alpha)\) is in (8), and_",\mathbf{U}\triangleq\sum_{i=1}^{N-1}\frac{1+\lambda_{i}}{1-\lambda_{i}}\cdot \begin{bmatrix}\mathbf{H}^{T}\mathbf{u}_{i}\mathbf{u}_{i}^{T}\mathbf{H}& \mathbf{H}^{T}\mathbf{u}_{i}\mathbf{u}_{i}^{T}\\ \mathbf{u}_{i}\mathbf{u}_{i}^{T}\mathbf{H}&\mathbf{u}_{i}\mathbf{u}_{i}^{T} \end{bmatrix}\triangleq\begin{bmatrix}\mathbf{U}_{11}&\mathbf{U}_{12}\\ \mathbf{U}_{21}&\mathbf{U}_{22}\end{bmatrix}.,,\mathbf{U}\triangleq\begin{bmatrix}\mathbf{H}^{T}\operatorname{diag}( \boldsymbol{\mu})\mathbf{H} & \mathbf{0}_{D\times N}\\ \mathbf{0}_{N\times D} & \mathbf{V}_{\mathbf{x}}(\alpha)\end{bmatrix}.,Matrix U is a block diagonal matrix combining the Hessian term and the asymptotic covariance of the SRRW empirical measure.
ICLR_2024_oral_3,13,"_(c) For \(k\in\{1,3\}\), \(\mathbf{V}^{(k)}(\alpha)\) becomes block diagonal, which is given by_","\mathbf{V}^{(k)}(\alpha)=\begin{bmatrix}\mathbf{V}^{(k)}_{\boldsymbol{\theta} }(\alpha)&\mathbf{0}_{D\times\!N}\\ \mathbf{0}_{N\!\times\!D}&\mathbf{V}_{\mathbf{x}}(\alpha)\end{bmatrix},",,"\mathbf{V}^{(k)}(\alpha)=\begin{bmatrix}\mathbf{V}_{\boldsymbol{\theta}}^{(k)}(\alpha) & \mathbf{0}\\ \mathbf{0} & \mathbf{V}_{\mathbf{x}}(\alpha)\end{bmatrix},","Asymptotic covariance matrix structure for cases (i) and (iii), where the covariance matrices of the optimization iterate $\boldsymbol{\theta}_n$ and the empirical distribution $\mathbf{x}_n$ decouple due to the timescale separation."
ICLR_2024_oral_31,1,"### Preliminaries

**Static Pruning.** Given a dataset \(\mathcal{D}=\{z_{i}\}|_{i=1}^{|\mathcal{D}|}=\{(x_{i},y_{i})\}|_{i=1}^{|\mathcal{ D}|}\), a score \(\mathcal{H}(z)\) can be defined for each sample. For the pruning process, samples are discarded by a pruning probability \(\mathcal{P}\) defined on top of \(\mathcal{H}\). Static pruning directly discards all samples satisfying a certain condition before training, resulting in \(\mathcal{P}(z;\mathcal{H})\in\{0,1\}\). For examples, Toneva et al. (2018) defines:","\mathcal{P}(z;\mathcal{H})=\mathds{1}(\mathcal{H}(z)<\bar{\mathcal{H}}),",where \(\bar{\mathcal{H}}\) is a threshold and \(\mathds{1}(\cdot)\) is indicator function,\mathcal{P}(z;\mathcal{H}) = \mathbb{1}\left\{ \mathcal{H}(z) \geq \tau \right\},"Static pruning probability function that deterministically retains samples with score $\mathcal{H}(z)$ exceeding threshold $\tau$, where $\mathbb{1}\{\cdot\}$ denotes the indicator function."
ICLR_2024_oral_31,2,"on. A subset \(\mathcal{S}\) is formed by pruning samples with \(\mathcal{P}(z;\mathcal{H})=1\). Fig. 1a briefly illustrates the whole process of static pruning. A more detailed theoretical analysis is provided in Appendix B.

**Dynamic Pruning.** For dynamic pruning, pruning is done across training, and the score \(\mathcal{H}_{t}\) can change along training steps, where \(t\) denotes the temporal status. The probability is step-dependent:","\mathcal{P}_{t}(z)=\mathcal{P}(z;\mathcal{H}_{t}),",,"\mathcal{P}_{t}(z;\mathcal{H}_{t})=\mathds{1}(\mathcal{H}_{t}(z)<\bar{\mathcal{H}}_{t}),",where $\mathcal{H}_{t}(z)$ is the time-dependent score function at step $t$ and $\bar{\mathcal{H}}_{t}$ is the corresponding time-dependent threshold.
ICLR_2024_oral_31,3,"and forms a dynamically pruned dataset \(\mathcal{S}_{t}\). Compared to static pruning, dynamic pruning has access to all the original data during training. Thus the gradient expectation bias should be much smaller than static pruning. However, such a scheme still has the following limitations: i). As claimed in Raju et al. (2021), low-score samples of different \(t\) during the training could easily overlap. Directly pruning them every time may still lead to a bias (see in Fig. 1a). ii). Pruning samples leads to a reduced number of gradient updates. Under the premise of saving training cost, existing dynamic pruning methods Raju et al. (2021) hardly achieve lossless results compared to training on the original dataset. iii). Scoring and sorting operations in dynamic pruning are conducted repeatedly, the overhead of which is still non-negligible on large-scale datasets.

### Overview of InfoBatch

Based on the above observation and analysis, we propose InfoBatch, a novel framework for achieving lossless training acceleration based on unbiased dynamic data pruning. As illustrated in Fig. 2, a score is maintained for each sample with its loss value during forward propagation. The mean of these values are set as the pruning threshold. A certain portion of small-score samples is accordingly pruned in each epoch. Then, to obtain the same expectation of gradient as the original dataset in each epoch, the gradients of the remaining small-score samples are scaled up. By doing this, compared to previous static and dynamic pruning methods, InfoBatch mitigates the performance differences between training on the pruned dataset and the original dataset. In order to further reduce the remaining gradient expectation bias, we train with the full dataset in the last few epochs.

Figure 2: Illustration of the proposed _InfoBatch_ framework. InfoBatch mainly consists of two operations, named soft pruning and expectation rescaling. \(\bar{\mathcal{H}}_{t}\) denotes the adaptive thresholds of scores of samples. Soft pruning randomly prunes some samples from \(\mathcal{D}_{1}\) with relatively small scores. For remaining samples from \(\mathcal{D}_{1}\), expectation rescaling scales up the losses to keep the approximately same gradient expectation as the original dataset.

### Unbiased Prune and Rescale

InfoBatch adopts the dynamic pruning process as in Eqn. 2. We first define its pruning policy \(\mathcal{P}_{t}\). Previous methods using a deterministic pruning operation could cause bias. In contrast, InfoBatch introduces randomness into the pruning process. Given a dataset \(\mathcal{D}\), in \(t\)-th epoch, a pruning probability is assigned to each sample based on its score. Such a soft pruning policy is formulated as:","\mathcal{P}_{t}(z)=\begin{cases}r,&\mathcal{H}_{t}(z)<\bar{\mathcal{H}}_{t}\\ 0,&\mathcal{H}_{t}(z)\geq\bar{\mathcal{H}}_{t}\end{cases},","where \(\bar{\mathcal{H}}_{t}\) is the mean value of all the scores \(\mathcal{H}_{t}\) and \(r\in(0,1)\) is a predefined hyper-parameter as the pruning probability","\mathcal{P}_{t}(z)=\sigma\left(\lambda\left(\mathcal{H}_{t}(z)-\bar{\mathcal{H}}_{t}\right)\right),","where $\sigma(\cdot)$ is the sigmoid function, $\lambda$ controls the steepness of the pruning probability, $\mathcal{H}_{t}(z)$ is the score of sample $z$ at epoch $t$, and $\bar{\mathcal{H}}_{t}$ is the adaptive threshold at epoch $t$"
ICLR_2024_oral_31,4,"ty. This new prune policy has the following benefits: i). Soft pruning allows each small-score sample to be utilized for training, which reduces the bias caused by hard pruning in previous dynamic pruning methods. ii). The proposed strategy is based on the comparison with \(\bar{\mathcal{H}}_{t}\), with no requirement to sort the whole training samples, which reduces the time complexity from \(O(logN)\) to \(O(1)\). It indicates that InfoBatch could be more efficient on large-scale datasets.

Then, we utilize loss values \(\mathcal{L}(z)\) of each sample as the corresponding score based on the following two reasons: i). loss values can be obtained without extra cost, ii). loss values reflect the learning status of samples (Cilimkovic, 2015). Specifically, before the \(t\)-th (\(t>0\)) epoch, we utilize the soft pruning policy to prune samples based on their scores. Then for the pruned samples, their scores remain unmodified as previous. For the remaining samples, their scores are updated by the losses in the current epoch. Mathematically, \(\mathcal{H}_{t}(z)\) would be updated by the latest losses to \(\mathcal{H}_{t+1}(z)\) by:","\mathcal{H}_{t+1}(z)=\begin{cases}\mathcal{H}_{t}(z),&z\in\mathcal{D} \backslash\mathcal{S}_{t}\\ \mathcal{L}(z),&z\in\mathcal{S}_{t}\end{cases}.",,"\mathcal{H}_{t+1}(z)=\begin{cases}\mathcal{H}_{t}(z),&z\text{ is pruned in }t\text{-th epoch}\\ \mathcal{L}_{t}(z),&z\text{ is retained in }t\text{-th epoch}\end{cases}.","where $\mathcal{H}_{t+1}(z)$ is updated based on whether the sample $z$ is pruned or retained in the $t$-th epoch, with $\mathcal{L}_{t}(z)$ denoting the loss of sample $z$ in the $t$-th epoch"
ICLR_2024_oral_31,5,"Note that, for the first epoch, we initialize the scores with \(\{1\}\) provided no previous loss.

There are several benefits of the soft pruning policy, yet it still cannot avoid the influence caused by the reduced number of gradient updates. To address this issue, we scale up the gradients of the remaining samples. Specifically, given a remaining sample with score \(\mathcal{H}(z)<\bar{\mathcal{H}}_{t}\), whose corresponding pruning probability is \(r\), its gradient is scaled to \(1/(1-r)\) times of original. For the samples with scores no less than \(\bar{\mathcal{H}}_{t}\), the loss is not modified. Thereby the gradient update expectation is approximately equal to training on the original dataset. Besides, as the rescaling is operated on small-score samples, it further refines the direction of gradient update expectation. We provide the following theoretical analysis to demonstrate the necessity and advantages of expectation rescaling.

**Theoretical Analysis.** We can interpret the training objective as minimizing empirical risk \(\mathcal{L}\). Assuming all samples \(z\) from \(\mathcal{D}\) are drawn from continuous distribution \(\rho(z)\), we can establish the training objective as:","\operatorname*{arg\,min}_{\theta\in\Theta}\mathop{\mathbb{E}}_{z\in\mathcal{D }}[\mathcal{L}(z,\theta)]=\int_{z}\mathcal{L}(z,\theta)\rho(z)dz.",,"\mathbb{E}_{z\sim\rho(z)}[\nabla\mathcal{L}(z;\theta)]=\mathbb{E}_{z\sim\rho(z)}[\nabla\mathcal{L}(z;\theta)],",representing the expectation of gradient of the loss function with respect to model parameters over the data distribution
ICLR_2024_oral_31,6,"After applying proposed pruning, we sample \(z\) according to normalized \((1-\mathcal{P}_{t}(z))\rho(z)\). In back-propagation, rescaling loss is equivalent to rescaling the gradient. By rescaling the loss of each sample \(z\) with a factor \(\gamma_{t}(z)=1/(1-\mathcal{P}_{t}(z))\) ( \(\forall z\in\mathcal{D},\mathcal{P}_{t}(z)=0\Rightarrow\gamma_{t}(z)=1\)), the training objective (expanded proof in Appendix B.1) on \(\mathcal{S}_{t}\) becomes:","\operatorname*{arg\,min}_{\theta\in\Theta}\mathop{\mathbb{E}}_{z\in\mathcal{S }_{t}}[\gamma_{t}(z)\mathcal{L}(z,\theta)]=\operatorname*{arg\,min}_{\theta \in\Theta}\frac{1}{c_{t}}\int_{z}\mathcal{L}(z,\theta)\rho(z)dz,","where \(c_{t}=\mathbb{E}_{z\sim\rho}[1-\mathcal{P}_{t}(z)]=\int_{z}\rho(z)(1-\mathcal{ P}_{t}(z))dz\), \(c_{t}\in(0,1)\) is a constant for temporal status \(t\)","\operatorname*{arg\,min}_{\theta\in\Theta}\mathop{\mathbb{E}}_{z\in\mathcal{S}_{t}}[\gamma_{t}(z)\mathcal{L}(z,\theta)]=\int_{z}\gamma_{t}(z)\mathcal{L}(z,\theta)(1-\mathcal{P}_{t}(z))\rho(z)dz.","Rescaled training objective after applying soft pruning and expectation rescaling, showing equivalence to minimizing empirical risk on the original dataset."
ICLR_2024_oral_31,7,"\). Then the objective in Eqn. 6 is a constant-rescaled version of the original objective in Eqn. 5. Therefore, training on \(\mathcal{S}_{t}\) with rescaled factor \(\gamma_{t}(z)\) could achieve a similar result as training on the original dataset. Furthermore, these operations also leverage the problem of reduced iterations. In real-world applications, considering the dataset as discrete ones, we force the sample number as:",\frac{1}{c_{t}}=\frac{|\mathcal{D}|}{|\sum_{z\in\mathcal{D}}(1-\mathcal{P}_{t}( z))|}\simeq\frac{|\mathcal{D}|}{|\mathcal{S}_{t}|}\Rightarrow\mathbb{E}[ \nabla_{\theta}\mathcal{L}(\mathcal{S}_{t})]\simeq\frac{|\mathcal{D}|}{| \mathcal{S}_{t}|}\mathop{\mathbb{E}}[\nabla_{\theta}\mathcal{L}(\mathcal{D})].,,"\frac{N_{t}}{N} = 1 - r,","where $N_{t}$ is the number of samples in $\mathcal{S}_{t}$, and $N$ is the total number of samples in $\mathcal{D}$"
ICLR_2024_oral_31,8,"For each epoch, the iteration number, _i.e._ gradient update number becomes \(|\mathcal{S}_{t}|/|\mathcal{D}|\) of the original one, while our method scale the expected gradient to \(|\mathcal{D}|/|\mathcal{S}_{t}|\). As a result, this would leverage the influence of reduced gradient update number. The approximation will hold when the pruning ratio is not too high and the learning rate is not too big. We provide a detailed analysis in Appendix B.

### Annealing

Based on the theoretical analysis above, the objectives and updating expectations between InfoBatch and training on the original dataset are approximately the same. However, there still exist minor differences between the optimization on \(\mathcal{D}\) and \(\mathcal{S}_{t}\). During training, if a sample is pruned in the middle stage, it is still likely to be revisited afterward. However, in the last few epochs, the revisiting probability drastically drops, resulting in remaining gradient expectation bias. Therefore, given training epoch \(C\), we define a ratio hyper-parameter \(\delta\in(0,1)\). The pruning is only conducted in the first \(\delta\cdot C\) epochs (in practice, \(\delta\) is close to 1 so InfoBatch saves at a reasonable ratio). After that, we train on the full dataset till the end. The corresponding operation can be interpreted as","\mathcal{P}_{t}(z)=\begin{cases}r,&\mathcal{H}_{t}(z)<\tilde{\mathcal{H}}_{t} \wedge t<\delta\cdot C\\ 0,&\mathcal{H}_{t}(z)\geq\tilde{\mathcal{H}}_{t}\lor t\geq\delta\cdot C\end{cases}.",,"\mathcal{S}_{t}=\begin{cases}\text{SoftPrune}(\mathcal{D},\mathcal{H}_{t},r),&t\leq\delta\cdot C\\ \mathcal{D},&t>\delta\cdot C\end{cases}.","where pruning is only applied during the initial $\delta\cdot C$ epochs, and the full dataset is used thereafter to reduce remaining gradient expectation bias"
ICLR_2024_oral_32,1,"### CLIP-ViT Preliminaries

**Contrastive pre-training.** CLIP is trained to produce visual representations from images \(I\) coupled with text descriptions \(t\). It uses two encoders--a transformer-based text encoder \(M_{\text{text}}\) and an image encoder \(M_{\text{image}}\). Both \(M_{\text{text}}\) and \(M_{\text{image}}\) map to a shared vision-and-language latent space, allowing us to measure similarity between images and text via cosine similarity:","\mathrm{sim}(I,t)=\langle M_{\text{image}}(I),M_{\text{text}}(t)\rangle/(||M_{ \text{image}}(I)||_{2}||M_{\text{text}}(t)||_{2})",,"\begin{equation}
    \text{sim}(I, t) = \frac{M_{\text{image}}(I)^\top M_{\text{text}}(t)}{\|M_{\text{image}}(I)\| \cdot \|M_{\text{text}}(t)\|}.
\end{equation}",Equation 1 defines the cosine similarity function used in CLIP to measure alignment between image and text embeddings in the shared latent space.
ICLR_2024_oral_32,2,"Given a batch of images and corresponding text descriptions \(\{(I_{i},t_{i})\}_{i\in\{1,\dots,k\}}\), CLIP is trained to maximize the similarity of the image representation \(M_{\text{image}}(I_{i})\) to its corresponding text representation \(M_{\text{text}}(t_{i})\), while minimizing \(\mathrm{sim}(I_{i},t_{j})\) for every \(i\neq j\) in the batch.

**Zero-shot classification.** CLIP can be used for zero-shot image classification. To classify an image given a fixed set of classes, each name of a class (e.g. ""Chihuahua"") is mapped to a fixed template (e.g. ""An image of a {class}"") and encoded by the CLIP text encoder. The prediction for a given image is the class whose text description has the highest similarity to the image representation.

**CLIP image representation.** Several architectures have been proposed for computing CLIP's image representation. We focus on the variant that incorporates ViT (Dosovitskiy et al., 2021) as a backbone. Here a vision transformer (ViT) is applied to the input image \(I\in\mathbb{R}^{H\times W\times 3}\) to obtain a \(d\)-dimensional representation \(\text{ViT}(I)\). The CLIP image representation \(M_{\text{image}}(I)\) is a linear projection of this output to a \(d^{\prime}\)-dimensional representation in the joint vision-and-language space2. Formally, denoting the projection matrix by \(P\in\mathbb{R}^{d^{\prime}\times d}\):

Footnote 2: Both here and in Eq. 3, we ignore a layer-normalization term to simplify derivations. We address layer-normalization in detail in Section A.1.",M_{\text{image}}(I)=P\text{ViT}(I),,"tags and provide a one-sentence description. The description should explain that it's the linear projection of the ViT output using matrix P to get the image representation in the joint space.

Make sure the LaTeX uses proper notation, like \text for ViT and the subscripts. Also, check if there's any prior equation to reference. Since Equation 1 is the similarity equation, Equation 2 here is the image encoder's projection. No prior equations except Equation 1, which is already given. So the generated equation should be correct.
</think>",Linear projection of the ViT output representation to the joint vision-and-language space using matrix $P$.
ICLR_2024_oral_32,3,"Both the parameters of the ViT and the projection matrix \(P\) are learned during training.

**ViT architecture.** ViT is a residual network built from \(L\) layers, each of which contains a multi-head self-attention (MSA) followed by an MLP block. The input \(I\) is first split into \(N\) non-overlapping image patches. The patches are projected linearly into \(N\)\(d\)-dimensional vectors, and positional embeddings are added to them to create the _image tokens_\(\{z^{0}_{i}\}_{i\in\{1,\dots,N\}}\). An additional learned token \(z^{0}_{0}\in\mathbb{R}^{d}\), named the _class token_, is also included and later used as the output token.

Formally, the matrix \(Z^{0}\in\mathbb{R}^{d\times(N+1)}\), with the tokens \(z^{0}_{0},z^{0}_{1},...,z^{0}_{N}\) as columns, constitutes the initial state of the residual stream. It is updated for \(L\) iterations via these two residual steps:","\hat{Z}^{l}=\text{MSA}^{l}(Z^{l-1})+Z^{l-1},\quad Z^{l}=\text{MLP}^{l}(\hat{ Z}^{l})+\hat{Z}^{l}.",,"Z^{\ell} = \text{MSA}(\text{LN}(Z^{\ell-1})) + Z^{\ell-1}, \quad \text{for } \ell = 1, \dots, L",Update of the image tokens through multi-head self-attention in the ViT architecture.
ICLR_2024_oral_32,4,"We denote the first column in the residual stream \(Z^{l}\), corresponding to the class token, by \([Z^{l}]_{cls}\). The output of the ViT is therefore \([Z^{L}]_{cls}\).

### Decomposition into layers

The residual structure of ViT allows us to express its output as a sum of the direct contributions of individual layers of the model. Recall that the image representation \(M_{\text{image}}(I)\) is a linear projection of the ViT output. By unrolling Eq. 3 across layers, the image representation can be written as:",M_{\text{image}}(I)=P\mathsf{ViT}(I)=P[Z^{0}]_{cls}+\underbrace{\sum_{ l=1}^{L}P[\mathsf{MSA}^{l}(Z^{l-1})]_{cls}}_{\text{MSA\ terms}}+\underbrace{\sum_{l=1}^{L}P[\mathsf{MLP}^{l}(\hat{Z}^{l}) ]_{cls}}_{\text{MLP\ perms}},,M_{\text{image}}(I) = P \left( [Z^{0}]_{cls} + \sum_{l=1}^{L} \left( \text{MSA}^{l}([Z^{l-1}]_{cls}) + \text{MLP}^{l}(\hat{Z}^{l}_{cls}) \right) \right),"Equation 4 decomposes the ViT output into the cumulative contributions of each layer, showing how the final image representation is built through successive transformations of the class token across all layers."
ICLR_2024_oral_32,5,"Eq. 4 decomposes the image representation into _direct contributions_ of MLPs, MSAs, and the input class token, allowing us to analyze each term separately. We ignore here the _indirect effects_ of the output of one layer on another downstream layer. We use this decomposition (and further decompositions) to analyze CLIP's representations in the next sections.

**Evaluating the direct contribution of layers.** As a preliminary investigation, we study which of the components in Eq. 4 significantly affect the final image representation, and find that the large majority of the direct effects come from the _late attention layers_.

To study the direct effect of a component (or set of components), we use mean-ablation (Nanda et al., 2023), which replaces the component with its mean value across a dataset of images. Specifically, we measure the drop in zero-shot accuracy on a classification task before and after ablation. Components with larger direct effects should result in larger accuracy drops.

In our experiments, we compute means for each component over the ImageNet (IN) validation set and evaluate the drop in IN classification accuracy. We analyze the OpenCLIP ViT-H-14, L-14, and B-16 models (Ilharco et al., 2021), which were trained on LAION-2B (Schuhmann et al., 2022).

**MLPs have a negligible direct effect.** Table 1 presents the results of simultaneously mean-ablating all the MLPs. The MLPs do not have a significant direct effect on the image representation, as ablating all of them leads to only a small drop in accuracy (1%-3%).

**Only the last MSAs have a significant direct effect.** We next evaluate the direct effect of different MSA layers. To do so, we mean-ablate all MSA layers up to some layer \(l\). Figure 2 presents the results: removing all the early MSA layers (up to the last 4) does not change the accuracy significantly. Mean-ablating these final MSAs, on the other hand, reduces the performance drastically.

In summary, the direct effect on the output is concentrated in the last 4 MSA layers. We therefore focus only on these layers in our subsequent analysis, ignoring the MLPs and the early MSA layers.

### Fine-grained decomposition into heads and positions

We present a more fine-grained decomposition of the MSA blocks that will be used in the next two sections. We focus on the output at the class token, as that is the only term appearing in Eq. 4. Following Elhage et al. (2021), we write the MSA output as a sum over \(H\) independent attention heads and the \(N\) input tokens:","[\mathsf{MSA}^{l}(Z^{l-1})]_{cls}=\sum_{h=1}^{H}\sum_{i=0}^{N}x_{i} ^{l,h},\ \ \ x_{i}^{l,h}=\alpha_{i}^{l,h}W_{VO}^{l,h}z_{i}^{l-1}","where \(W_{VO}^{l,h}\in\mathbb{R}^{d\times d}\) are transition matrices and \(\alpha_{i}^{l,h}\in\mathbb{R}\) are the attention weights from the class token to the \(i\)-th token (\(\sum_{i=0}^{N}\alpha_{i}^{l,h}=1\))","\mathrm{MSA}^{l}(Z^{l-1})=\sum_{h=1}^{H}A_{h}^{l}Z^{l-1}W_{V}^{l,h}+\text{LN}(Z^{l-1})W_{O}^{l,h}","Equation 5 decomposes the multi-head self-attention (MSA) output into contributions from individual attention heads and input tokens, enabling fine-grained analysis of their effects on the final image representation."
ICLR_2024_oral_32,6,")). Therefore, the MSA output can be decomposed into direct effects of individual heads and tokens.

Plugging the MSA output definition in Eq. 5 into the MSA term in Eq. 4, we obtain:","\sum_{l=1}^{L}P[\text{MSA}^{l}(Z^{l-1})]_{cls}=\sum_{l=1}^{L}\sum_{h=1 }^{H}\sum_{i=0}^{N}c_{i,l,h},\ \ c_{i,l,h}=Px_{i}^{l,h}",,"M_{\text{image}}(I)=P[Z^{0}]_{cls}+\sum_{l=1}^{L}\sum_{h=1}^{H}\sum_{i=0}^{N}P x_{i}^{l,h}+\sum_{l=1}^{L}P[\mathsf{MLP}^{l}(\hat{Z}^{l})]_{cls}","This equation decomposes the CLIP image representation into contributions from the initial class token, individual attention heads and input tokens in each MSA layer, and MLP layers."
ICLR_2024_oral_32,7,"In other words, the total direct effect of all attention blocks is the result of contracting the tensor \(c\) across all of its dimensions. By contracting along only some dimensions, we can decompose effects in a variety of useful ways. For instance, we can contract along the spatial dimension \(i\) to get a contribution for each head: \(c_{\text{head}}^{l,h}=\sum_{i=0}^{N}c_{i,l,h}\). Alternatively, we can contract along layers and heads to get a contribution from each image token: \(c_{\text{token}}^{i}=\sum_{l=1}^{L}\sum_{h=1}^{H}c_{i,l,h}\).

The quantities \(c_{i,l,h}\), \(c_{\text{head}}^{l,h}\) and \(c_{\text{token}}^{i}\) all live in the \(d^{\prime}\)-dimensional joint text-image representation space, which allows us to interpret them via text. For instance, given text description \(t\), the quantity \(\langle M_{\text{text}}(t),c_{\text{head}}^{l,h}\rangle\) intuitively measures the similarity of that head's output to description \(t\).

## 4 Decomposition into Attention Heads

Motivated by the findings in Section 3.2, we turn to understanding the late MSA layers in CLIP. We use the decomposition into individual attention heads (Section 3.3), and present an algorithm for labeling the latent directions of each head with text descriptions. Examples of this labeling are depicted in Table 2 and Figure 4, with the labeling for all \(64\) late attention heads given in Section A.5.

Our labeling reveals that some heads exhibit specific semantic roles, e.g. ""counting"" or ""location"", in which many latent directions in the head track different aspects of that role. We show how to exploit these labeled roles both for property-specific image retrieval and for reducing spurious correlations.

### Text-interpretable decomposition into heads

We decompose an MSA's output into text-related directions in the joint representation space. We rely on two key properties: First, the output of each MSA block is a sum of contributions of individual attention heads, as demonstrated in Section 3.3. Second, these contributions lie in the joint text-image representation space and so can be associated with text.

Recall from Section 3.3 that the MSA terms of the image representation (Eq. 4) can be written as a sum over heads, \(\sum_{l,h}c_{\text{head}}^{l,h}\). To interpret a head's contribution \(c_{\text{head}}^{l,h}\), we will find a set of text descriptions that explain most of the variation in the head's output (the head ""principal components"").

To formalize this, we take input images \(I_{1},...,I_{K}\) with associated head outputs \(c_{1},...,c_{K}\) (for simplicity, we fix the layer \(l\) and head \(h\) and omit it from the notation). As \(c_{1},...,c_{K}\) are vectors in the

\begin{table}
\begin{tabular}{l|l|l} \hline
**L21.H11** (“Geo-locations”) & **L23.H10** (“Counting”) & **L22.H8** (“Letters”) \\ \hline Photo captured in the Arizona desert & Image with six subjects & A photo with the letter V \\ Picture taken in Alberta, Canada & Image with four people & A photo with the letter F \\ Photo taken in Rio de Janeiro, Brazil & An image of the number 3 & A photo with the letter D \\ Picture taken in Cyprus & An image of the number 10 & A photo with the letter T \\ Photo taken in Seoul, South Korea & The number fifteen & A photo with the letter X \\ \hline \hline
**L22.H11** (“Colors”) & **L22.H6** (“Animals”) & **L22.H3** (“Objects”) \\ \hline A charcoal gray color & Curious wildlife & An image of legs \\ Sepia-toned photograph & Majestic soaring birds & A jacket \\ Minimalist white backdrop & An image with dogs & A helmet \\ High-contrast black and white & Image with a dragonfly & A scarf \\ Image with a red color & An image with cats & A table \\ \hline \hline
**L23.H12** (“Textures”) & **L22.H1** (“Shapes”) & **L22.H2** (“Locations”) \\ \hline Artwork with pointillism technique & A semicircular arch & Urban park greenery \\ Artwork with woven basket design & An isosceles triangle & Coyz home interior \\ Artwork featuring barcode arrangement & An oval & Urban subway station \\ Image with houndstooth patterns & Rectangular object & Energetic street scene \\ Image with quilted fabric patterns & A sphere & Tranquil boating on a lake \\ \hline \end{tabular}
\end{table}
Table 2: **Top-5 text descriptions extracted per head by our algorithm.** Top 5 components returned by TextSpan applied to ViT-L, for several selected heads. See Section A.5 for results on all the heads.

joint text-image space, each text input \(t\) defines a direction \(M_{\text{text}}(t)\) in that space. Given a collection of text directions \(\mathcal{T}\), let \(\operatorname{Proj}_{\mathcal{T}}\) denote the projection onto the span of \(\{M_{\text{text}}(t)\mid t\in\mathcal{T}\}\). We define the _variance explained by \(\mathcal{T}\)_ as the variance under this projection:","V_{\text{explained}}(\mathcal{T})=\frac{1}{K}\sum_{k=1}^{K}\|\operatorname{ Proj}_{\mathcal{T}}(c_{k}-c_{\text{avg}})\|_{2}^{2},\text{ where }c_{\text{avg}}=\frac{1}{K}\sum_{k=1}^{K}c_{k}.",,"\text{Var}_{\mathcal{T}}(c) = \mathbb{E}_{I}\left[\left\|\operatorname{Proj}_{\mathcal{T}}(c_{\text{head}}^{l,h}(I))\right\|^{2}\right]","Equation 7 defines the variance explained by a set of text directions $\mathcal{T}$, measuring how much of the variation in a head's output is captured by the span of the corresponding text-encoded directions."
ICLR_2024_oral_35,1,"### Parameter Model

Among all LLM architectural attributes, the LLM parameter count has the largest impact on test loss (Kaplan et al., 2020). To reduce projection errors, LLMCarbon can take the parameter count as direct input, or estimate the parameter count by the parameter model. The parameter model's input comprises the LLM's architectural parameters including the hidden size (\(h\)), the number of layers (\(l\)), the vocabulary size (\(V\)), and the number of experts (\(N_{e}\)). For a dense LLM, we calculate its parameter count (\(P_{d}\)) by Equation 1 (Narayanan et al., 2021). An MoE LLM (Rajbhandari et al., 2022) replaces \(\rho\) (\(\rho\in(0,1]\)) feed-forward layers in its counterpart dense LLM with MoE layers. An MoE layer's parameter count is the sum of the expert parameter count (\(P_{exp}=8h^{2}N_{e}\)) and the self-attention parameter count (\(P_{att}=4h^{2}\)), so the parameter count (\(P_{e}\)) of an MoE LLM can be computed using Equation 2. The parameter model of LLMs adopting an encoder-decoder architecture can be viewed in Appendix A.",P_{d}\approx 12lh^{2}+Vh\hskip 28.452756pt(1)\hskip 36.135ptP_{e}\approx(1- \rho)P_{d}+\rho(4h^{2}+8h^{2}N_{e})l,,,
ICLR_2024_oral_35,2,"### Neural Scaling Law

The neural scaling law (Kaplan et al., 2020) predicts an LLM's test loss based on its parameter count \(P\) and the training dataset size \(D\). For ensuring the comparability of test losses across various models, sizes, and datasets, we adopt the Chinchilla scaling law (Hoffmann et al., 2022) formulated as Equation 3, where \(A\), \(B\), \(\alpha\), \(\beta\), and \(E\) are fitting constants. The test loss \(L\) equals to the summation of an irreducible term \(E\) and a reducible term diminishing through the scaling of \(P\) and \(D\).","L(P,D)=\frac{A}{P^{\alpha}}+\frac{B}{D^{\beta}}+E\hskip 14.226378pt(3)\hskip 28.452756ptTC \approx 6PD\hskip 14.226378pt(4)\hskip 28.452756ptIC\approx 2PD\hskip 14.226378pt(5)",,L = A P^{-\alpha} + B D^{-\beta} + E \hskip 28.452756pt(3),"Test loss $L$ as a function of parameter count $P$, dataset size $D$, and constants $A$, $B$, $\alpha$, $\beta$, $E$."
ICLR_2024_oral_35,3,"### Flop Model

The FLOP model receives two inputs: the count of parameters (\(P\)) and the number of tokens (\(D\)) processed by the LLM processing. The primary component of FLOPs is the multiply-accumulate operations involving LLM weights and intermediate results. Within our FLOP model, the FLOP count necessary for training a dense LLM (\(TC\)) is estimated using Equation 4. For dense LLM inferences, the FLOP count (\(IC\)) is approximated as per Equation 5. To compute the FLOP count for MoE LLM processing, we input the parameter number of the dense base model (Rajbhandari et al., 2022) of the MoE LLM into Equations 4 and 5, respectively.

### Hardware Efficiency Model

Efficient processing of LLMs relies on achieving high hardware efficiency, which is calculated as the actual computing throughput divided by the peak throughput. This efficiency is largely determined by the optimal configuration of data, tensor, pipeline, and expert parallelism, along with the number of devices used for the task. Using too few or too many devices or improperly configuring parallelism can lead to reduced hardware efficiency. For example, achieving optimal parallelism for GPT-3 with 175 billion parameters requires 1.5K V100 GPUs, resulting in a hardware efficiency of 47% (Narayanan et al., 2021). Conversely, an unoptimized configuration using 10K V100 GPUs yields a substantially lower hardware efficiency of only 19.7% (Patterson et al., 2021).

**Optimal Parallelism Setting**. The optimal parallelism setting is represented as \((p,t,d,e)\), where each variable corresponds to a degree of pipeline, tensor, data, and expert parallelism, respectively. For dense LLMs, optimal settings are derived from (Narayanan et al., 2021), depicted in Figure 2,where \(e=1\) is omitted. Initially, we increase tensor parallelism (\(t\)) up to \(z\) (e.g., \(z=8\)) when employing \(z\)-device servers (Narayanan et al., 2021), each containing \(z\) interconnected devices. This increment in \(t\) is confined to avoid exceeding communication bandwidth limits. Once \(z\) is reached, further scaling for larger LLMs involves increasing pipeline parallelism (\(p\)) (Narayanan et al., 2021). However, the product of \(t\) and \(p\) (\(t\cdot p\)) must not exceed a certain threshold to ensure that LLM parameters and intermediate data fit into device memory. The number of devices required to achieve optimal hardware efficiency for dense LLM processing is calculated as \(n=t\cdot p\cdot d\)(Narayanan et al., 2021), and can be viewed in Figure 4. A polynomial regression model is used to predict optimal hardware efficiency based on these parameters. For MoE LLMs, the optimal parallelism settings are adopted from (Chen et al., 2023). As Figure 3 shows, assuming 64 experts within an MoE LLM, expert parallelism (\(e\)) is always set to 64, intertwining \(d\) and \(e\) for a uniform expert distribution. To reduce inter-device all-to-all communications, \(d\) is fixed at 1. Scaling MoE LLM parallelism is achieved by increasing pipeline parallelism (\(p\)). The number of devices required for optimal hardware efficiency in MoE LLM processing is also calculated as \(n=t\cdot p\cdot d\). As Figure 4 exhibits, MoE LLMs require fewer devices compared to dense LLMs with equivalent parameter counts due to their lower computational overhead. The optimal hardware efficiency during MoE LLM processing is represented in Figure 5. MoE LLMs achieve \(\sim 80\%\)(Chen et al., 2023) of the optimal hardware efficiency of their dense base models, due to extra host-device memory swaps.",\mathit{eff}_{re}=\begin{cases}\gamma_{0}\cdot\frac{re}{n}\cdot\mathit{eff}_{n }&re<n\\ \gamma_{1}\cdot\frac{n}{re}\cdot\mathit{eff}_{n}+\gamma_{2}&re>n\end{cases} || \mathit{eff}_{re}=\begin{cases}\gamma_{0}\cdot\frac{re}{n}\cdot\mathit{eff}_{n }&re<n\\ \gamma_{1}\cdot\frac{n}{re}\cdot\mathit{eff}_{n}+\gamma_{2}&re>n\end{cases},,\eta_{\text{MoE}} \approx 0.8 \cdot \eta_{\text{dense}},MoE LLMs achieve approximately 80% of the optimal hardware efficiency of their dense base models due to additional host-device memory swaps.
ICLR_2024_oral_35,4,"**Fewer or More Computing Devices**. When the number of computing devices is not equal to \(t\cdot p\cdot d\), the hardware efficiency decreases. The efficiency (\(\mathit{eff}_{re}\)) with \(re\) devices can be calculated using Equation 6, where \(\gamma_{0}\sim\gamma_{2}\) are fitting constants, \(\mathit{eff}_{n}\) means the highest hardware efficiency, and \(n\) indicates the number of devices that can achieve \(\mathit{eff}_{n}\).",\mathit{energy}_{hard}=\sum_{i\in hardware\_set}(P_{i}\cdot\mathit{eff}_{i} \cdot n_{i}\cdot t_{i}) || \mathit{energy}_{oper}=\mathit{energy}_{hard}\cdot\mathit{PUE},,\mathit{eff}_{re}=\begin{cases}\gamma_{0}\cdot\frac{re}{n}\cdot\mathit{eff}_{n}&re<n\\ \gamma_{1}\cdot\frac{n}{re}\cdot\mathit{eff}_{n}+\gamma_{2}&re>n\end{cases},"Hardware efficiency ($\mathit{eff}_{re}$) when using $re$ devices, modeled piecewise with fitting constants $\gamma_0$, $\gamma_1$, $\gamma_2$, peak efficiency $\mathit{eff}_n$, and optimal device count $n$."
ICLR_2024_oral_35,5,"### Operational Carbon Model

By using the FLOP count (\(\mathit{TFLOP}\)), the hardware efficiency (\(\mathit{eff}\)), and the computing device number (\(n_{dev}\)), we can determine the execution time of a device through Equation 7, where \(\mathit{FLOP}_{peak}\) represents the device peak throughput. The total energy (\(\mathit{energy}_{hard}\)) consumed by all hardware units can be calculated using Equation 8, where \(P_{i}\) denotes the peak power of hardware unit \(i\); \(\mathit{eff}_{i}\) represents the hardware efficiency of hardware unit \(i\); \(n_{i}\) indicates the count of hardware unit \(i\); and \(t_{i}\) means the execution time of hardware unit \(i\). Hardware units encompass a range of components, including CPUs, LLM computing devices, memories, SSDs, and others.",\mathit{CO2eq}_{\mathit{oper}}=\mathit{energy}_{\mathit{oper}}\cdot\mathit{ carb\_int},,\mathit{time}_{exec}=\frac{\mathit{TFLOP}}{\mathit{FLOP}_{peak}\cdot\mathit{eff}\cdot n_{dev}}\hskip 14.226378pt(7),"Execution time of a computing device based on FLOP count, peak throughput, hardware efficiency, and device count"
ICLR_2024_oral_35,6,"**PUE**. Power Usage Effectiveness (PUE) (Henderson et al., 2020) serves as the industry standard metric for evaluating a data center's energy efficiency. It is defined as the ratio of the total energy consumption of the data center, including all auxiliary components like cooling, to the energy consumed solely by the computing hardware within the data center. The operational energy (\(\mathit{energy}_{\mathit{oper}}\)) associated with LLM processing can be calculated using Equation 9, where \(\mathit{energy}_{hard}\) denotes the energy used by the computing hardware within a data center, and \(\mathit{PUE}\) indicates the PUE of the specific data center.",\mathit{CO2eq}_{\mathit{emb}}=\sum_{i\in hardware\_set}\frac{t_{i}\cdot \mathit{CO2eq}_{\mathit{chip}_{i}}}{\mathit{lifetime}_{i}},,\mathit{CO2eq}_{\mathit{oper}}=\mathit{energy}_{\mathit{oper}}\cdot\mathit{ carb\_int},Operational carbon emissions (CO2eq_oper) are calculated by multiplying the total operational energy (energy_oper) with the carbon intensity (carb_int) of the energy source.
ICLR_2024_oral_36,1,"### Image-to-Triplane Decoder

We implement a transformer decoder to project image and camera features onto learnable spatial-positional embeddings and translate them to triplane representations. This decoder can be considered as a prior network that is trained with large-scale data to provide necessary geometric and appearance information to compensate for the ambiguities of single-image reconstruction.

Camera FeaturesWe construct the camera feature \(\mathbf{c}\in\mathbb{R}^{20}\) of the input image by flattening out the 4-by-4 camera extrinsic matrix \(\mathbf{E}\) (that represents the camera-to-world transformation) and concatenate it with the camera focal length _foc_ and principal point _pp_ as \(\mathbf{c}=[\mathbf{E}_{1\times 16},foc_{x},foc_{y},pp_{x},pp_{y}]\). Moreover, we normalize the camera extrinsic \(\mathbf{E}\) by similarity transformations so that all the input cameras are aligned on the same axis (with the lookup direction aligned with the \(z\)-axis). Note that, LRM does not depend on a canonical pose of the object, and the ground truth \(\mathbf{c}\) is only applied in training. Conditioning on normalized camera parameters greatly reduces the optimization space of triplane features and facilitates model convergence (see details in Sec. 4.2). To embed the camera feature, we further implement a multi-layer perceptron (MLP) to map the camera feature to a high-dimensional camera embedding \(\mathbf{\tilde{c}}\). The intrinsics (focal and principal point) are normalized by the image's height and width before sending to the MLP layer.

Triplane RepresentationWe follow previous works (Chan et al., 2022; Gao et al., 2022) to apply triplane as a compact and expressive feature representation of the reconstruction subject. A triplane \(\mathbf{T}\) contains three axis-aligned feature planes \(\mathbf{T}_{X\!Y}\), \(\mathbf{T}_{Y\!Z}\) and \(\mathbf{T}_{X\!Z}\). In our implementation, each plane is of dimension \((64\times 64)\times d_{T}\) where \(64\times 64\) is the spatial resolution, and \(d_{T}\) is the number of feature channels. For any 3D point in the NeRF object bounding box \([-1,1]^{3}\), we can project it onto each

Figure 1: The overall architecture of LRM, a fully-differentiable transformer-based encoder-decoder framework for single-image to NeRF reconstruction. LRM applies a pre-trained vision model (DINO) to encode the input image (Sec. 3.1), where the image features are projected to a 3D triplane representation by a large transformer decoder via cross-attention (Sec. 3.2), followed by a multi-layer perceptron to predict the point color and density for volumetric rendering (Sec. 3.3). The entire network is trained end-to-end on around a million of 3D data (Sec. 4.1) with simple image reconstruction losses (Sec. 3.4).

of the planes and query the corresponding point features \((\mathbf{T}_{xy},\mathbf{T}_{yz},\mathbf{T}_{xz})\) via bilinear interpolation, which is then decoded by an \(\mathrm{MLP}^{\mathit{nef}}\) into the NeRF color and density (Sec. 3.3).

To obtain the triplane representation \(\mathbf{T}\), we define learnable spatial-positional embeddings \(\mathbf{f}^{\mathit{init}}\) of dimension (\(3{\times}32{\times}32{\times}d_{D}\) which guide the image-to-3D projection and are used to query the image features via cross-attention, where \(d_{D}\) is the hidden dimension of the transformer decoder. The number of tokens in \(\mathbf{f}^{\mathit{init}}\) is smaller than the number of final triplane tokens (\(3{\times}64{\times}64\)); we will upsample the output of the transformer \(\mathbf{f}^{\mathit{out}}\) to the final \(\mathbf{T}\). In the forward pass, conditioning on the camera features \(\mathbf{\tilde{c}}\) and image features \(\{\mathbf{h}_{i}\}_{i=1}^{n}\), each layer of our image-to-triplane transformer decoder gradually updates the initial positional embedding \(\mathbf{f}^{\mathit{init}}\) to the final triplane features via modulation and cross-attention, respectively. The reason for applying two different conditional operations is that the camera controls the orientation and distortion of the whole shape, whereas the image features carry the fine-grained geometric and color information that need to be embedded onto the triplane. Details of the two operations are explained below.

Modulation with Camera FeaturesOur camera modulation is inspired by DiT (Peebles and Xie, 2022) which implements an adaptive layer norm (adaLN) to modulate image latents with denoising timesteps and class labels. Suppose \(\{\mathbf{f}_{j}\}\) is a sequence of vectors in transformer, we define our modulation function \(\mathrm{ModLN}_{\mathrm{c}}(\mathbf{f}_{j})\) with camera feature \(\mathbf{c}\) as","\gamma,\beta =\mathrm{MLP}^{\mathrm{mod}}(\mathbf{\tilde{c}}) || \mathrm{ModLN}_{\mathrm{c}}(\mathbf{f}_{j}) =\mathrm{LN}(\mathbf{f}_{j})\cdot(1+\gamma)+\beta",,\mathrm{ModLN}_{\mathrm{c}}(\mathbf{f}_{j}) = \mathbf{f}_{j} + \mathbf{f}_{j} \cdot \mathbf{W}_{\text{scale}}(\mathbf{\tilde{c}}) + \mathbf{b}_{\text{shift}}(\mathbf{\tilde{c}}),Camera feature modulation function that adaptively scales and shifts transformer features using predicted weight and bias terms conditioned on the camera embedding.
ICLR_2024_oral_36,2,"al., 2016). Such modulation is applied to each attention sub-layer which will be specified next.

Transformer LayersEach transformer layer contains a cross-attention sub-layer, a self-attention sub-layer, and a multi-layer perceptron sub-layer (MLP), where the input tokens to each sub-layer are modulated by the camera features. Suppose feature sequence \(\mathbf{f}^{\mathit{in}}\) is the input of an transformer layer, we can consider \(\mathbf{f}^{\mathit{in}}\) as the triplane hidden features since they are corresponding to the final triplane features \(\mathbf{T}\). As shown in the decoder part of Fig. 1, the cross-attention module firstly attends from the triplane hidden features \(\mathbf{f}^{\mathit{in}}\) to the image features \(\{\mathbf{h}_{i}\}_{i=1}^{n}\), which can help linking image information to the triplane. Note that we here do not explicitly define any spatial alignment between the 2D images and 3D triplane hidden features, but consider 3D as an independent modality and ask the model to learn the 2D-to-3D correspondence by itself. The updated triplane hidden features will be passed to a self-attention sub-layer that further models the intra-modal relationships across the spatially-structured triplane entries. Then, a multi-layer perceptron sub-layer (\(\mathrm{MLP}^{\mathit{tfm}}\)) follows as in the original Transformer (Vaswani et al., 2017) design. Lastly, the output triplane features \(\mathbf{f}^{\mathit{out}}\) will become the input to the next transformer layer.

Such a design is similar to the Perceiver network (Jaegle et al., 2021) while our model maintains a high-dimensional representation across the attention layers instead of projecting the input to a latent bottleneck. Overall, we can express this process for each \(j\)-th triplane entry in each layer as",\mathbf{f}_{j}^{\mathit{cross}} =\mathrm{CrossAttn}(\mathrm{ModLN}_{c}(\mathbf{f}_{j}^{\mathit{in}}); \{\mathbf{h}_{i}\}_{i=1}^{n})+\mathbf{f}_{j}^{\mathit{in}} || \mathbf{f}_{j}^{\mathit{self}} =\mathrm{SelfAttn}(\mathrm{ModLN}_{c}(\mathbf{f}_{j}^{\mathit{cross}} );\{\mathrm{ModLN}_{c}(\mathbf{f}_{j}^{\mathit{cross}})\}_{j})+\mathbf{f}_{j}^{ \mathit{cross}},,"\mathbf{f}_{j}^{\mathit{out}} = \mathrm{MLP}^{\mathit{tfm}}\left(\mathrm{SelfAttn}\left(\mathrm{CrossAttn}\left(\mathrm{ModLN}_{\mathrm{c}}(\mathbf{f}_{j}^{\mathit{in}}), \{\mathbf{h}_{i}\}_{i=1}^{n}\right)\right)\right)","Equation 2 describes the transformation of triplane hidden features through a transformer layer, involving camera feature modulation, cross-attention to image features, self-attention, and a multi-layer perceptron."
ICLR_2024_oral_36,3,(4),\mathbf{f}_{j}^{\mathit{out}} =\mathrm{MLP}^{\mathit{tfm}}(\mathrm{ModLN}_{c}(\mathbf{f}_{j}^{ \mathit{self}}))+\mathbf{f}_{j}^{\mathit{self}},,\mathbf{f}_{j}^{\mathit{out}} =\mathrm{MLP}^{\mathit{tfm}}(\mathrm{ModLN}_{c}(\mathbf{f}_{j}^{\mathit{self}} ))+\mathbf{f}_{j}^{\mathit{self}},"Equation 3 describes the final step in a transformer layer where the triplane hidden features are updated through a multi-layer perceptron sub-layer modulated by camera features, followed by a residual connection."
ICLR_2024_oral_36,4,"The \(\mathrm{ModLN}\) operators in sub-layers (_i.e._, \(\mathrm{CrossAttn}\), \(\mathrm{SelfAttn}\), \(\mathrm{MLP}^{\mathit{tfm}}\)) use different set of learnable parameters in the layer normalization and the modulation \(\mathrm{MLP}^{\mathit{mod}}\). We do not add additional superscript to differentiate them for clarity.

The transformer layers are processed sequentially. After all the transformer layers, we obtain the output triplane features \(\mathbf{f}^{\mathit{out}}\) from the last layer as the output of the decoder. This final output is upsampled by a learnable de-convolution layer and reshaped to the final triplane representation \(\mathbf{T}\).

### Triplane-NeRF

We employ the triplane-NeRF formulation (Chan et al., 2022) and implement an \(\mathrm{MLP}^{\mathit{nef}}\) to predict RGB and density \(\sigma\) from the point features queried from the triplane representation \(\mathbf{T}\). The \(\mathrm{MLP}^{nerf}\) contains multiple linear layers with ReLU (Nair and Hinton, 2010) activation. The output dimension of the \(\mathrm{MLP}^{nerf}\) is \(4\) where the first three dimensions are RGB colors and the last dimension corresponds to the density of the field. We refer to the Appendix for the details of NeRF volumetric rendering.

### Training Objectives

LRM produces the 3D shape from a single input image and leverages additional side views to guide the reconstruction during training. For each shape in the training data, we consider \((V-1)\) randomly chosen side views for supervision; we apply simple image reconstruction objectives between the \(V\) rendered views \(\mathbf{\hat{x}}\) and the ground-truth views \(\mathbf{x}^{GT}\) (include the input view and side views). More precisely, for every input image \(\mathbf{x}\), we minimize:","\mathcal{L}_{\mathrm{recon}}(\mathbf{x})=\frac{1}{V}\sum_{v=1}^{V}(\mathcal{ L}_{\mathrm{MSE}}(\mathbf{\hat{x}}_{v},\mathbf{x}_{v}^{GT})+\lambda\mathcal{L}_{ \mathrm{LPIPS}}(\mathbf{\hat{x}}_{v},\mathbf{x}_{v}^{GT}))","where \(\mathcal{L}_{\mathrm{MSE}}\) is the normalized pixel-wise L2 loss, \(\mathcal{L}_{\mathrm{LPIPS}}\) is the perceptual image patch similarity (Zhang et al","\mathcal{L} = \sum_{v=1}^{V} \|\mathbf{\hat{x}}_v - \mathbf{x}_v^{GT}\|_1 + \lambda_{\text{ssim}} (1 - \text{SSIM}(\mathbf{\hat{x}}_v, \mathbf{x}_v^{GT}))",The training loss combines L1 reconstruction error and a structural similarity (SSIM) term across all views to guide the model towards accurate and visually coherent reconstructions.
ICLR_2024_oral_37,1,"## 3 Preliminaries

Ordinary / Stochastic Differential EquationsNeural ordinary differential equations (Neural ODEs) (Chen et al., 2018) approximate the evolving dynamics with the ordinary differential equation and are defined as","z_{t}=z_{0}+\int_{0}^{t}f(z_{s},s)\text{d}s,","where the hidden state \(z_{t}\in\mathbb{R}^{d}\) evolves with certain dynamics characterized by a neural network \(f:\mathbb{R}^{d}\rightarrow\mathbb{R}^{d}\), \(z_{0}\) is the initial state, and \(s\) represents time in integrals","\begin{equation}
    \frac{d\mathbf{h}(t)}{dt} = f(\mathbf{h}(t), t, \theta),
\end{equation}",Neural ordinary differential equation describing the continuous-time evolution of hidden states with a neural network function $ f $.
ICLR_2024_oral_37,2,ls. An SDE can be regarded as an ODE injected with noise over time:,"z_{t}=z_{0}+\int_{0}^{t}f(z_{s},s)\text{d}s+\int_{0}^{t}g(z_{s},s)\text{d}B_{s},","where \(z_{t}\) is a latent state that evolves over time, \(f:\mathbb{R}^{d}\times\mathbb{R}\rightarrow\mathbb{R}^{d}\) is the drift function to capture the evolving dynamics, \(g:\mathbb{R}^{d}\times\mathbb{R}\rightarrow\mathbb{R}^{d\times\omega}\) is the diffusion function to reflect the uncertainties, and \(B_{s}\) is an \(d\)-dimensional Brownian motion (Wiener Process)","\text{d}z_{t}=f(z_{t},t)\text{d}t+g(z_{t},t)\text{d}w_{t},","where $f(z_t, t)$ is the drift term representing deterministic dynamics, $g(z_t, t)$ is the diffusion term modulating the stochastic noise, and $\text{d}w_t$ represents increments of a Wiener process (Brownian motion)."
ICLR_2024_oral_37,3,"s). SDE has shown superior performance in modeling the dynamical system (Park et al., 2021; 20; Oksendal & Oksendal, 2003). Under the EDG settings, the drift function of SDE describes the trends of the distribution shift over time, and the diffusion function models the samples' individual stochastics in their representation space.

Evolving Domain GeneralizationLet \(\mathcal{D}(x,y,t)\) be the probability distribution that characterizes temporal dynamics of an instance \(x\in\mathcal{X}\) and its label \(y\in\mathcal{Y}\), in which there exist underlying evolving patterns over \(t\). In Evolving Domain Generalization (EDG), we are given \(M\) source domains \(\{S_{m}\}_{m=1}^{M}\), where \(S_{m}=\{(x_{i|m},y_{i|m})\}_{i=1}^{N}\) is the data of \(m\)-th domain sampled from \(\mathcal{D}(x,y|t_{m})\) at the timestamp \(t_{m}\in[0,T]\), and \(N\) is the sample size of the \(m\)-th domain. Note that most existing works (Bai et al., 2023; Nasery et al., 2021; Qin et al., 2022; Zeng et al., 2023) assume a constant interval \(\Delta t\) between two consecutive domains. In contrast, our approach exhibits flexibility in tackling the EDG problem even with irregular time intervals. In the proposed method, we will learn the evolving dynamics in a latent space \(\mathcal{Z}\), given the practical advantages (Kirchmeyer et al., 2022), e.g., improved discrimination, dimension reduction, and computation resources savings compared to operations in the original input space \(\mathcal{X}\). Specifically, for every instance \(x\), we encode it with a feature extractor \(\phi:\mathcal{X}\rightarrow\mathcal{Z}\), and we obtain the embedded feature \(z=\phi(x)\in\mathcal{Z}\). We focus on the dynamics of \(\mathcal{D}(z,y,t)\) throughout this work.

The goal of EDG is to learn a robust and generalizable model from source domains by capturing and leveraging the evolving pattern learned from source domains so that it can perform well on the unseen target domains in the path space (Boue & Dupuis, 1998) at \(L\) timestamps \(t_{M+l}\in\{t_{M+1},\dots,t_{M+L}\}\in(T,T+T^{*}]\) (\(T^{*}=t_{M+L}-T\)):","\min_{\theta}R_{\nu}(h_{\theta})=\min_{\theta}\mathbb{E}_{(z,y)\sim\mathcal{D} (z_{M+1:M+L},y_{M+1:M+L})}[h_{\theta}(z)\neq y],","where \(\nu\) is the distribution of the stochastic path (Boue & Dupuis, 1998) of \(\mathcal{D}\) along timestamps \(T\) to \(T+T^{*}\), \(z_{M+1:M+L}\) and \(y_{M+1:M+L}\) are short for \(z\) and \(y\) at the timestamps \(\{t_{M+1},\dots,t_{M+L}\}\), \(R_{\nu}\) is the risk of a learning model \(h_{\theta}\) parameterized by parameters \(\theta\)","\min_{\phi,\psi}\mathbb{E}_{t\in(T,T+T^{*}],(z,y)\sim\mathcal{D}(z,y,t)}[\ell(\psi(z),y)],","where $\phi$ is the feature extractor, $\psi$ is the classifier, and $\ell$ is the loss function, aiming to minimize the expected loss over unseen target domains in the path space."
ICLR_2024_oral_37,4,"\).

## 4 Methods

With the prior knowledge of SDEs and EDG, we will now formally present our SDE-EDG approach: To build IFGET (section 4.1), we search sample-to-sample correspondence, which aids in generation of continuous interpolations; Neural SDEs models the trajectories of latent representations (Section 4.2); we construct IFGET and employ it as a regularization mechanism to promote the learning of evolving representations while avoiding the acquisition of invariant representations (Section 4.3).

### Construct Infinitely Fined-Grid Evolving Trajectory

In EDG, datasets have a considerably small size of domains/timestamps (e.g., at most 30 domains) (Yao et al., 2022). In contrast, models are trained on the historical data for time-series forecasting tasks spanning over at least hundreds of timestamps to predict future states (Addison Howard inversion, 2020; Yu et al., 2016).

In light of this obstacle, we generate _intermediate domains_ by applying interpolations between two consecutive domains. To ensure such interpolations reflect the evolving pattern of the underlying trajectory over domains, one should have the complete trajectory of each individual sample across domains. For example, in weather forecasting, one must have the historical meteorological data of each individual observation station to characterize the climate change trends. Unfortunately, such trajectories usually do not exist in EDG as there is no sample-to-sample correspondence across domains (e.g., we may not have images of the same person at different age stages), thereby preventing the model from tracking the complete trajectories and extracting the evolving trends. To address this issue, we propose to identify sample correspondence between timestamps, which is critical to better alignment of the data structure across domains (Lu et al., 2023; Chen et al., 2022; Blitzer et al., 2006; Das & Lee, 2018).

Specifically, for each class \(k\), we take the closest sample at \(t_{m+1}\) to the datapoint \(z_{m}^{k}\) at \(t_{m}\), as its subsequent state at time \(t_{m+1}\) to build sample-to-sample correspondence:","\hat{z}_{i|m+1}^{k}=\underset{z_{i|m+1}^{k}=\underset{z_{i|m+1}^{k}\in\mathbb{ S}_{m+1}^{k}}{\text{argmin}}}\text{Dist}(z_{i|m}^{k},z_{j|m+1}^{k}),","where \(\text{Dist}:\mathcal{Z}\times\mathcal{Z}\rightarrow[0,+\infty)\) is a distance metric defined over the embedding space, \(\mathbb{S}_{m+1}^{k}\) be the set of \(N_{B}\) data points sampled from \(\mathcal{D}_{m+1}\) (short for \(\mathcal{D}(z,y|t_{m+1})\)) with class \(y=k\in\{1,","z_{m+1}^{k}=\arg\min_{z'\in S_{m+1}^{k}}\|z_{m}^{k}-z'\|,","where $z_{m}^{k}$ is the datapoint of class $k$ at time $t_{m}$, $S_{m+1}^{k}$ is the set of samples in class $k$ at time $t_{m+1}$, and $\|\cdot\|$ denotes a distance metric (e.g., Euclidean distance) used to establish sample-to-sample correspondence across domains"
ICLR_2024_oral_37,5,"1,...,K\}\) in a training iteration. The rationale here lies in the decomposition of latent variables into class-dependent and domain-dependent evolving components (Qin et al., 2022), resulting in samples from the same class in the \(m\)-th and (\(m\)+1)-th domains exhibiting smaller distances due to such sample pair's shared class-dependent similarities, while the evolving difference maintains consistent magnitude. Utilizing sample-to-sample correspondence, we gather discrete samples within IFGET. To render it continuous, we generate continuously-interpolated samples bridging the temporal gaps.

With the sample correspondence, we leverage the interpolation function to generate continuous-interpolated samples, such that the interpolation is generated along the approximated individual trajectory of a data point as shown in Figure 1","\hat{z}_{i|m+\lambda}=\text{Interp}(z_{i|m}^{k},\hat{z}_{i|m+1}^{k},\lambda)= (1-\lambda)z_{i|m}^{k}+\lambda\hat{z}_{i|m+1}^{k},\forall z_{i|m}^{k}\in \mathbb{S}_{m}^{k}","where the interpolation rate \(\lambda\in(0,1)\) is sampled from a Beta distribution \(\mathcal{B}(\beta_{1},\beta_{2})\), \(\beta_{1}\) and \(\beta_{2}\) are the parameters of the Beta distribution, and \(\mathbb{S}_{m}^{k}\) consists of instances sampled from \(k\)-th class of \(m\)-th domain","\tilde{z}_{t}^{k}=\mathcal{I}(z_{i|m}^{k},\hat{z}_{i|m+1}^{k},t),\quad t\in[t_{m},t_{m+1}],","where $\mathcal{I}:\mathcal{Z}\times\mathcal{Z}\times[t_{m},t_{m+1}]\rightarrow\mathcal{Z}$ is the interpolation function that generates continuous samples along the approximated trajectory between corresponding samples $z_{i|m}^{k}$ and $\hat{z}_{i|m+1}^{k}$ for class $k$ over the time interval $[t_{m},t_{m+1}]$"
ICLR_2024_oral_37,6,"in. Here we apply a linear interpolation (Yan et al., 2020; Zhang et al., 2018) as the interpolation function. The continuous-interpolated samples bridge temporal gaps in the discrete evolving trajectory, converting it into an infinitely fine-grained trajectory due to \(\lambda\in(0,1)\). Specifically, as \(\lambda\) can be any value between \((0,1)\), it enables us to approximate time moments between the \(m\)-th and \((m+1)\)-th timestamps. We theoretically show that the sample complexity of EDG reduces with a smaller temporal interval in Theorem D.3, which leads to a reduction in error. We take interpolations as approximations of samples between time intervals, leading to a smaller time interval and thus a smaller sample complexity. Above all, we construct the Infinitely Fined-grid Evolving Trajectory \(\{z_{i|m}^{k},\hat{z}_{i|m+\lambda}^{k},\hat{z}_{i|m+1}^{k}\}_{m=1}^{M-1}\) by leveraging the sample-to-sample correspondence, and collecting the interpolations.

### Modeling EDG with Stochastic Differential Equations

The continuous trajectory in Section 4.1 significantly enhances the capability to capture evolving patterns, but existing EDG methods can not handle the continuous timestamp data. Hence, we

Figure 1: The graphical model depicts the data generation process of SDE-EDG, where we hide the label superscript \(k=Y_{m}\) and the sample index \(i\). \(X_{m}\) is the input sample (for example, images) at \(t_{m}\), \(Y_{m}\) is the label at \(t_{m}\), and \(Z_{m}\) is the latent representation at \(t_{m}\). The continuous-interpolation \(\hat{Z}_{m+\lambda}\) is interpolated by the sample pair from IFGET.

propose to model the data of EDG in the representation space with neural SDEs, since Neural SDEs naturally model continuous temporal trajectories. In contrast, traditional temporal models such as LSTM (Hochreiter & Schmidhuber, 1997) and Markov models (Bishop & Nasrabadi, 2006) are only able to model discrete timestamps.

Here, SDE-EDG learns the temporal dynamics governing the semantic conditional distributions \(\mathcal{D}(z|y,t)\) over time. Specifically, SDE-EDG models the temporal trajectory of the data point from the domain at \(t_{m}\) to the arbitrary future timestamp \(t_{m^{\prime}}:t_{m^{\prime}}>t_{m}\) of each class \(k\in\{1,\dots,K\}\):","\hat{z}_{m^{\prime}}^{k}=z_{m}^{k}+\int_{t_{m}}^{t_{m^{\prime}}}f_{k}(\hat{z}_ {s}^{k},s)ds+\int_{t_{m}}^{t_{m^{\prime}}}g_{k}(\hat{z}_{s}^{k},s)dB_{s},","where the latent variable \(\hat{z}_{m^{\prime}}^{k}\) is transformed from \(m\)-th domains latent variable \(z_{m}^{k}\), and \(f_{k}\) is the drift function of the \(k\)-th class to capture the evolving patterns, and \(g_{k}\) is the diffusion function of the \(k\)-th class to characterize the stochastics of the latent representations","z_{t_{m^{\prime}}|k}=z_{t_{m}|k}+\int_{t_{m}}^{t_{m^{\prime}}}f_{k}(z_{s},s)\text{d}s+\int_{t_{m}}^{t_{m^{\prime}}}g_{k}(z_{s},s)\text{d}B_{s},","where $z_{t_{m}|k}$ is the latent representation of class $k$ at time $t_{m}$, $f_{k}:\mathbb{R}^{d}\times\mathbb{R}\rightarrow\mathbb{R}^{d}$ is the class-conditional drift function, and $g_{k}:\mathbb{R}^{d}\times\mathbb{R}\rightarrow\mathbb{R}^{d\times\omega}$ is the class-conditional diffusion function."
ICLR_2024_oral_37,7,"ns. Note that \(z\) is the latent variable (representation) induced by \(z=\phi(x)\), but \(\hat{z}\) is the _synthetic_ feature generated by Eq. (6). Hence, SDE-EDG can generate the latent continuous trajectory by gradually transforming the sample representation from the current timestamp \(m\) to any desired future timestamp \(m^{\prime}\). Thereby, our latent trajectories of SDE-EDG could effectively align with the collected continuous trajectories IFGET, which prevents overfitting to sparse timestamps.

We design two objective functions to learn the drift functions \(f=\{f_{k}\}_{k=1}^{K}\) and diffusion functions \(g=\{g_{k}\}_{k=1}^{K}\) characterized by neural networks: one is aimed to impose Path Alignment Loss in Eq. (7), and another one is downstream classification loss in Eq. (10). By jointly optimizing \(\{\phi,f,g\}\) w.r.t these two losses, our approach achieves improved performance on EDG.

### Align SDE-EDG with IFGET via Maximum Likelihood

Neural SDEs are designed to capture the dynamics and evolution of data over time, particularly in continuous spaces. To fit the SDE-EDG into the evolving stochastic path given observations, we propose the path alignment regularizer by maximizing its likelihood of the IFGET \(\{z_{i|m}^{k},\hat{z}_{i|m+\lambda}^{k},\hat{z}_{i|m+1}^{k}\}_{m=1}^{M-1}\).","\mathcal{J}_{mle}=\sum_{m=1}^{M}\sum_{k=1}^{K}\sum_{i=1}^{N_{B}}-\frac{1}{MKN_ {B}}\Big{(}\log\mathcal{D}(z=\hat{z}_{i|m+1}^{k}|z=z_{i|m}^{k})+\log\mathcal{D }\big{(}z=\hat{z}_{i|m+\lambda}^{k}|z=z_{i|m}^{k}\big{)}\Big{)},",,"\mathcal{L}_{\text{path}}=-\sum_{k=1}^{K}\sum_{m=1}^{M-1}\sum_{z_{i|m}^{k}\in\mathbb{S}_{m}^{k}}\log p(\hat{z}_{i|m+\lambda}^{k}|z_{i|m}^{k},\lambda),","where $p(\hat{z}_{i|m+\lambda}^{k}|z_{i|m}^{k},\lambda)$ is the transition probability density of the synthetic interpolated sample $\hat{z}_{i|m+\lambda}^{k}$ given the initial state $z_{i|m}^{k}$ and interpolation rate $\lambda$, induced by the Neural SDE dynamics for class $k$"
ICLR_2024_oral_37,8,"Filling the gap between domains with continuous-interpolated samples results in a continuous and smooth evolving trajectory over time. Taking \(\mathcal{J}_{mle}\) as a regularizer brings two advantages to EDG model training: 1) Empirically, the training process of neural SDEs converges faster with \(\mathcal{J}_{mle}\), as shown in Figure 3(a). 2) \(\mathcal{J}_{mle}\) regularizes the latent space to capture evolving patterns. This contributes to learning the evolving patterns in the EDG problem and improves the generalization capability to target domains as shown in Figure 2. On the other hand, in the absence of \(\mathcal{J}_{mle}\), the model learns invariant representations across domains, leading to the occurrence of the Neural Collapse phenomenon (Han et al., 2022), where the latent representations of the same class across the domain collapse to a single point. Consequently, no evolving patterns manifest in the latent representation space as shown in Figure 3.

Figure 2: The left and right images depict representations acquired for the Circle dataset through the SDE-EDG and IRM by \(\phi\). Distinct classes are distinguished by different shapes (triangles and circles), while various domains are denoted by different colors as indicated by the rainbow bar. SDE-EDG successfully learns representations with a discernible decision boundary, whereas IRM collapses towards a single direction, failing to depict a clear decision boundary.

### SDE-EDG for the Prediction Loss

In this section, we formulate our approach for handling downstream classification tasks. With the Bayes rule, the predictive distribution is","\mathcal{D}(y=k|z,t=t_{m})=\frac{\mathcal{D}(z|y=k,t=t_{m})\times\mathcal{D}(y= k|t=t_{m})}{\sum_{k^{\prime}=1}^{K}\mathcal{D}(z|y=k^{\prime},t=t_{m})\times \mathcal{D}(y=k^{\prime}|t=t_{m})},","where we model \(\mathcal{D}(z|y=k,t=t_{m})\) with non-parametric model, and \(\mathcal{D}(y|t=t_{m})\) as a neural net with input as timestamp \(t\), function denoted as \(r(t)\)","p(y_{m^{\prime}}|x_{m^{\prime}})=\int p(y_{m^{\prime}}|\hat{z}_{m^{\prime}}^{k})p(\hat{z}_{m^{\prime}}^{k}|x_{m^{\prime}},t_{m^{\prime}})\text{d}\hat{z}_{m^{\prime}}^{k},","where $p(y_{m^{\prime}}|\hat{z}_{m^{\prime}}^{k})$ is the likelihood of the label given the synthetic latent representation $\hat{z}_{m^{\prime}}^{k}$, and $p(\hat{z}_{m^{\prime}}^{k}|x_{m^{\prime}},t_{m^{\prime}})$ is the posterior distribution of the latent variable conditioned on the input $x_{m^{\prime}}$ and timestamp $t_{m^{\prime}}$"
ICLR_2024_oral_37,9,"\). In each iteration, we first compute label distribution with respect to time \(\mathcal{D}(y|t=t_{m})=[\mathcal{D}(y=1|t=t_{m}),\dots,\mathcal{D}(y=K|t=t_{m})]\). Specifically, \(\mathcal{D}(y=k|t=t_{m})=\frac{[\hat{S}_{m}^{k}]}{\sum_{k^{\prime}=1}^{K}| \hat{S}_{m}^{k^{\prime}}|}\), where \(\hat{S}_{m}^{k}\) consists of instances sampled from \(k\)-th class of \(m\)-th domain, \(|\cdot|\) denotes the size of the set. \(r\) is optimized by minimizing \(||\frac{|\hat{S}_{m}^{k}|}{\sum_{k^{\prime}=1}^{K}|\hat{S}_{m}^{k^{\prime}}|}- r(t_{m})||\).

The conditional distribution \(\mathcal{D}(z|y,t)\) modeled by SDEs lacks an analytic expression, and here we approximate it with a non-parametric model. Given that distributions characterized by SDEs may exhibit either uni-modal or multi-modal patterns, it's also advantageous to model multi-modal representations with Neural SDEs (Min et al., 2023). In this context, we present the multi-modal classification loss here but leave the uni-modal loss in the appendix A due to space limitation. To preserve the multi-modal pattern of the latent variables, we employ the non-parametric distribution density method Parzen Window (Parzen, 1962)","\mathcal{D}(z|y=k,t=t_{m})=\frac{\sum_{\hat{z}_{i}\in\hat{S}_{m}^{k}}-\text{ exp}(-\text{Dist}(z,\hat{z}_{i}))}{|\hat{S}_{m}^{k}|}",where \(\hat{S}_{m}^{k}\) includes instances sample from learned SDE-EDG belong to \(k\)-th class of \(m\)-th domain,"\mathcal{L}_{pred}=-\sum_{m=1}^{M}\sum_{k=1}^{K}\sum_{i=1}^{N_{B}}\log \Big{(}\frac{1}{N_{B}}\sum_{j=1}^{N_{B}}\exp\big{(}-\frac{||z_{i|m}^{k}-z_{j|m}^{k}||^{2}}{2\sigma^{2}}\big{)}\Big{)},","where $\sigma$ is the bandwidth parameter of the Parzen Window, and the loss approximates the log-likelihood of the latent representation $z_{i|m}^{k}$ under the non-parametric density estimate for class $k$ at timestamp $t_m$"
ICLR_2024_oral_37,10,"in. By incorporating the estimations of label distribution (\(\mathcal{D}(y|t)\)) and conditional distribution (\(\mathcal{D}(z|y,t)\)), our predictions encompass the temporal evolution of both \(\mathcal{D}(z|y,t)\) and \(\mathcal{D}(y|t)\).

Model optimization proceeds by minimizing the negative log probability:","\mathcal{J}_{cls}=\sum_{m=1}^{M}\sum_{k=1}^{K}\sum_{i=1}^{N_{B}}-\frac{1}{MKN _{B}}\log\mathcal{D}(y=k|z=z_{i},t=t_{m})",,"\mathcal{L} = -\log \mathcal{D}(y|z,t)",where $\mathcal{L}$ is the negative log probability
ICLR_2024_oral_38,1,"### GFlowNets

GFlowNets sample from discrete space \(\mathcal{X}\) through a sequence of actions from the action space \(\mathcal{A}\) that make transitions in the state space \(\mathcal{S}\). For each complete trajectory \(\tau=(s_{0},s_{1},\dots,s_{T})\), the terminal state is the object \(x=s_{T}\in\mathcal{X}\) to be generated. The state transitions are determined by the action sequence \((a_{1},\dots a_{T-1})\), e.g., \(a_{t}\) determines \(s_{t}\to s_{t+1}\). The policy \(P_{F}(s^{\prime}|s)\) selects the action \(a\) to transition from the current state \(s\) to the next state \(s^{\prime}\) and induces a distribution over the object \(x\).

The main objective of GFlowNet is to train the policy \(P_{F}(\cdot|\cdot)\) that samples objects from the Boltzmann distribution with respect to a given energy function \(\mathcal{E}:\mathcal{X}\rightarrow\mathbb{R}\) as follows:","P_{F}^{\top}(x)\propto\exp(-\mathcal{E}(x)),",where \(P_{F}^{\top}(x)\) is the distribution of sampling an object \(x\) induced from marginalizing over the trajectories conditioned on \(x=s_{T}\),"\begin{equation}
P_{F}(x) \propto e^{-\beta \mathcal{E}(x)},
\end{equation}","Equation 1 defines the target Boltzmann distribution over objects $x \in \mathcal{X}$, where $\beta$ is the inverse temperature parameter and $\mathcal{E}(x)$ is the energy function assigned to each object $x$. This distribution represents the desired sampling behavior of the GFlowNet policy $P_F(\cdot|\cdot)$."
ICLR_2024_oral_38,2,"\). We omit the temperature for simplicity. To this end, GFlowNet trains with auxiliary objectives based on state transition, trajectory, or sub-trajectory information.

Figure 1: **The local credit evaluation in bag generation (Example 1). (first row) The task is to generate a bag of entities, where the seven same entities yield a high score. (second row) Left-to-right indicates the state transitions over a given trajectory. (third row) The energy-based evaluation fails to produce informative local credits since every intermediate state has zero energy, whereas our potential function produces informative credits by enforcing the potentials to be uniformly distributed.**

**Detailed balance**(**Bengio et al.**,** 2021b**,** **DB).** The DB utilizes the experience of state transitions to train GFlowNet. It trains the GFlowNet with a forward policy model \(P_{F}(s^{\prime}|s)\), a backward policy \(P_{B}(s|s^{\prime})\), and a state flow estimator \(F(\cdot):\mathcal{S}\rightarrow\mathbb{R}^{+}\) by minimizing the following loss function:","\mathcal{L}_{\text{DB}}(s,s^{\prime})=(\log F(s)+\log P_{F}(s^{\prime}|s) -\log F(s^{\prime})-\log P_{B}(s|s^{\prime}))^{2},","where the flow \(F(s)\) for the terminal state \(s_{T}=x\) is defined to be identical to the exponent of the negative energy \(\exp\left(-\mathcal{E}(x)\right)\), i","\mathcal{L}_{\text{DB}} = \mathbb{E}_{s \sim \rho, s' \sim P_F(\cdot|s)} \left[ \left( F(s) P_F(s'|s) - F(s') P_B(s|s') \right)^2 \right],",The detailed balance loss ensures consistency between forward and backward transitions by minimizing the squared difference between the product of flow and policy in both directions.
ICLR_2024_oral_38,3,"i.e., the score of the object.

**Trajectory balance**(**Malkin et al.**,** 2022, **TB).** The TB aims to learn the policy faster by training on full trajectories. To this end, TB requires a forward policy model \(P_{F}(s^{\prime}|s)\), a backward policy \(P_{B}(s|s^{\prime})\), and a learnable scalar \(Z\) to minimize the following loss function:",\mathcal{L}_{\text{TB}}=(\log Z+\sum_{t=0}^{T-1}\log P_{F}(s_{t+1}|s_{t} )-\mathcal{E}(x)-\sum_{t=0}^{T-1}\log P_{B}(s_{t}|s_{t+1}))^{2}.,,"\mathcal{L}_{\text{TB}}(\tau)=(\log F(s_{0})+\sum_{t=0}^{T-1}\log P_{F}(s_{t+1}|s_{t}) -\log F(s_{T})-\sum_{t=0}^{T-1}\log P_{B}(s_{t}|s_{t+1}))^{2},","where $F(s_{0})$ is the initial flow, $F(s_{T})$ is the terminal flow proportional to $\exp(-\mathcal{E}(x))$, and the loss ensures consistency between forward and backward trajectory log-probabilities."
ICLR_2024_oral_38,4,"This objective is resilient to the bias from inaccurate flow estimator \(F(\cdot)\) used in DB, since it directly propagates the terminal energy to train on intermediate states. However, the TB suffers from the high variance of the objective over the collected trajectories (Malkin et al., 2022).

**Sub-trajectory balance**(**Madan et al.**,** 2023**,** **subTB).** The subTB trains forward and backward policies \(P_{F}(s^{\prime}|s),P_{B}(s^{\prime}|s)\), a flow function \(F(\cdot)\), and a learnable scalar \(Z\). It trains on flexible length of sub-trajectory \(s_{U}\to s_{U+1}\cdots\to s_{U+L}\) to minimize the following loss function:",\mathcal{L}_{\text{subTB}}=(\log F(s_{U})+\sum_{t=U}^{U+L-1}\log P_{F}(s_ {t+1}|s_{t})-\log F(s_{U+L})-\sum_{t=U}^{U+L-1}\log P_{B}(s_{t}|s_{t+1})) ^{2}.,,\mathcal{L}_{\text{subTB}}=(\log F(s_{U})+\sum_{t=U}^{U+L-1}\log P_{F}(s_{t+1}|s_{t}) -\log F(s_{U+L})-\sum_{t=U}^{U+L-1}\log P_{B}(s_{t}|s_{t+1}))^{2}.,"This loss function generalizes the detailed and trajectory balance objectives by considering sub-trajectories of variable length $L$, enabling more flexible and localized training of the forward and backward policies using intermediate state transitions."
ICLR_2024_oral_38,5,"In addition, one can consider the weighted average of \(\mathcal{L}_{\text{subTB}}\) over all possible lengths \(L=1,\ldots,T\) of the sub-trajectories with the weight \(\lambda^{L}\), which enables the interpolation between the DB and the TB (Madan et al., 2023). In this paper, we consider this approach as subTB for simplicity.

### Partial inference in GFlowNets

The GFlowNet training is often challenged by limitations in credit assignment, i.e., identification and promotion of the action responsible for the observed low energy. This limitation stems from relying solely on the terminal state energy as the training signal. The terminal energy lacks information to identify the contribution of individual action, akin to how reinforcement learning with sparse reward suffers from credit assignment (Arjona-Medina et al., 2019; Ren et al., 2022).

Partial inference is a promising paradigm to resolve this issue by learning from local credits, where Pan et al. (2023) first incorporate this concept in training GFlowNets. Specifically, the partial inference aims to evaluate individual transitions or sub-trajectories, i.e., local credits, and provide informative training signals for identifying the specific contributions of actions. To this end, Pan et al. (2023) proposed Forward-Looking GFlowNet (FL-GFN), which evaluates intermediate state energy as a local credit signal for partial inference.

**Forward-Looking GFlowNet**(**Pan et al.**,** 2023,** **FL-GFN).** To enable partial inference, the FL-GFN defines a new training objective that incorporates an energy function \(\mathcal{E}:\mathcal{X}\rightarrow\mathbb{R}\) for intermediate states. To this specific, FL-GFN modifies the DB as follows:","\mathcal{L}_{\text{FL}}(s,s^{\prime})=(\log\tilde{F}(s)+\log P_{F}(s^{\prime}| s)-\mathcal{E}(s)+\mathcal{E}(s^{\prime})-\log\tilde{F}(s^{\prime})-\log P _{B}(s|s^{\prime}))^{2},",where \(\tilde{F}(s)=F(s)\exp\left(\mathcal{E}(s)\right)\) is the re-parameterized flow function and \(\mathcal{E}(s^{\prime})-\mathcal{E}(s)\) is the energy gain associated with the transition from \(s\) to \(s^{\prime}\),"\mathcal{L}_{\text{FL-DB}}(s,s^{\prime})=(\log F(s)+\log P_{F}(s^{\prime}|s)-\log F(s^{\prime})-\log P_{B}(s|s^{\prime})+\mathcal{E}(s)-\mathcal{E}(s^{\prime}))^{2}.",This loss function extends detailed balance by incorporating energy differences between consecutive states to provide local credit assignment during training.
ICLR_2024_oral_38,6,"\). Note that the energy function is defined only on the terminal state space \(\mathcal{X}\); hence, FL-GFN assumes an extension of the energy function \(\mathcal{E}\) to non terminal states, e.g., evaluating energy of an object associated with the intermediate state \(s\). Pan et al. (2023) shows that optimum of Equation (2) induces a policy \(P_{F}(\cdot|\cdot)\) that samples from Boltzmann distribution. While Equation (2) is associated with DB, FL-GFN is also applicable to subTB with energy gains in the level of sub-trajectory.

## 3 Learning energy decomposition for GFlowNet

While the FL-GFN is equipped with partial inference capabilities, it relies on the energy function to assign local credits, which can be expensive to evaluate, or lead to sub-optimal training signals (details are described in Section 3.1). In this paper, we propose learning energy decomposition for GFlowNets (LED-GFN) to achieve better partial inference. In what follows, we describe our motivation for better partial inference (Section 3.1) and the newly proposed LED-GFN (Section 3.2).

### Motivation for better partial inference

Our motivation stems from the limitations of FL-GFN, which performs partial inference based on evaluating the energies of intermediate states with respect to a single transition (for DB) or sub-trajectory (for subTB). In particular, we are inspired by how the energy gain may yield a sub-optimal local credit signal due to the following pitfalls (see Figures 1 and 9).

**A.**: The energy evaluation for an incomplete object, i.e., intermediate state, can be non-trivial. In addition, the cost of energy evaluation can be expensive, which can bottleneck efficient training when called for all visited states.
**B.**: The energy can exhibit sparse or high variance on intermediate states within a trajectory, even returning zero for most states, which is non-informative for partial inference.

We further provide a concrete example for the pitfall **B**. of FL-GFN, which is illustrated in Figure 1 and Figure 31 for both conceptual and empirical purposes, respectively:

Footnote 1: The detailed experiment settings are described in Section 4.1.

**Example 1**.: _Consider adding objects from \(\{A,B,C,D,E\}\) to a bag with a maximum capacity of nine. Define the energy as \(-1\) when the bag contains seven identical objects and \(0\) otherwise._

For Example 1, the intermediate energy (which is always \(0\)) does not provide information for the terminal energy. However, the number of the most frequent elements in the bag is informative even at intermediate states since greedily increasing the number leads to the best terminal state.

Our observation in Example 1 hints at the existence of a partial inference algorithm to provide better local credit signals. We aim to pursue this direction with a learning-based approach. That is, we parameterize the class of potential functions that decompose the terminal energy to provide local credit signals for GFlowNet training. Our key research direction is to understand about what conditions of the potential functions are informative for partial inference.

### Algorithm description

In this section, we describe our framework, coined learning energy decomposition for GFlowNet (LED-GFN), which facilitates partial inference using learned local credit. To this end, we propose to decompose the terminal energy into learnable potentials defined on state transitions. Similar to FL-GFN, we reparameterize the flow model with local credits, i.e., potentials. In contrast to FL-GFN,

Figure 3: Negative relative mean error (\(\downarrow\)) for estimating the true Boltzmann distribution on Example 1-type task (Bag).

Figure 2: **Illustration of energy decomposition for partial inference in GFlowNet. LED-GFN enables partial inference with potentials which (a) approximate the object energy via summation, and (b) minimize variance along the action sequence.**we optimize the local credits to enhance partial inference by minimizing the variance of potentials along the action sequence. See Figure 2 for an illustration of LED-GFN.

**Training with potential functions.** To be specific, we decompose the energy function \(\mathcal{E}\) associated with the terminal state into learnable potential functions \(\phi_{\theta}\) as follows:","\mathcal{E}(x)\approx\Phi_{\theta}(\tau)=\sum_{t=0}^{T-1}\phi_{\theta}(s_{t} \to s_{t+1}),","where \(\tau=(s_{0},s_{1},\ldots,s_{T})\), \(x=s_{T}\), and the potential functions are defined on state transition \(s_{t}\to s_{t+1}\)","\mathcal{E}(x)=\sum_{t=0}^{T-1}\phi_{\theta}(s_{t},s_{t+1}),","where the terminal energy $\mathcal{E}(x)$ is decomposed into a sum of learnable potential functions $\phi_{\theta}(s_{t},s_{t+1})$ defined over state transitions along the trajectory."
ICLR_2024_oral_38,7,"\). Similar to FL-GFN, we use the potential function to train the forward and backward GFlowNet policies \(P_{F},P_{B}\) and flow model \(F\) to minimize the following loss:","\mathcal{L}_{\text{LED}}(s,s^{\prime})=(\log\tilde{F}(s)+\log P_{F}(s^{\prime} |s)+\phi_{\theta}(s\to s^{\prime})-\log\tilde{F}(s^{\prime})-\log P_{B}(s|s^{ \prime}))^{2}.",,\mathcal{L}_{\text{LED}}=(\log F(s)+\log P_{F}(s^{\prime}|s)-\log F(s^{\prime})-\log P_{B}(s|s^{\prime})-\phi_{\theta}(s\to s^{\prime}))^{2}.,"LED-GFN introduces a learnable potential function $\phi_{\theta}$ to decompose the terminal energy into local credits, enabling more informative and low-variance training signals for GFlowNet policies and flow model through state transitions."
ICLR_2024_oral_38,8,"Given a sub-trajectory \((s_{0},\ldots,s_{u}=s,s_{u+1}=s^{\prime})\), one can derive this objective from Equation (2) by replacing \(\mathcal{E}(s),\mathcal{E}(s^{\prime})\) with \(\sum_{t=0}^{u-1}\phi_{\theta}(s_{t}\to s_{t+1})\) and \(\sum_{t=0}^{u}\phi_{\theta}(s_{t}\to s_{t+1})\), respectively. That is, it is evident that Equation (4) preserves the optimal policy of GFlowNet when \(\mathcal{E}(x)=\Phi_{\theta}(\tau)\) is satisfied for all trajectories \(\tau\) terminating with \(x\).

Our objective becomes equivalent to that of FL-GFN when \(\phi_{\theta}(s\to s^{\prime})=\mathcal{E}(s^{\prime})-\mathcal{E}(s)\), but our key idea is to _learn_ the potential function \(\phi_{\theta}\) instead of the energy gain which can be expensive and may exhibit sparsity or high variance, as pointed out in Section 3.1. Note that one can also introduce an approximation error \(\mathcal{E}(x)-\Phi_{\theta}(\tau)\) as an additional correction term to preserve the optimal policy of GFlowNet even when the potential function \(\phi_{\theta}\) is inaccurate. In Appendix B.1, we describe how LED-GFN consistently induces the optimal policy that samples from the Boltzmann distribution.

**Training potentials with squared loss.** In training the potential function, our key motivation is twofold: (a) accurately estimating the true energy through summation and (b) providing dense and informative training signals by minimizing variance along the action sequence.

To this end, given a trajectory \(\tau=(s_{0},\ldots,s_{T}=x)\), we train the potential functions \(\phi_{\theta}\) to minimize the loss function for (a) achieving \(\mathcal{E}(x)\approx\Phi_{\theta}(\tau)\) with (b) dropout-based regularization:",\ell_{\text{LS}}(\tau)=\mathbb{E}_{\mathbf{z}\sim\mathrm{Bern}(\gamma)} [(\frac{1}{T}\mathcal{E}(s_{T})-\frac{1}{C}\sum_{t=0}^{T-1}z_{t} \phi_{\theta}(s_{t}\to s_{t+1}))^{2}].,,"\mathcal{L}_{\phi}(\tau)=\left(\mathcal{E}(x)-\sum_{t=0}^{T-1}\phi_{\theta}(s_{t}\to s_{t+1})\right)^{2}+\lambda\sum_{t=0}^{T-1}\left(\phi_{\theta}(s_{t}\to s_{t+1})-\bar{\phi}_{\theta}(s_{t}\to s_{t+1})\right)^{2},",where the first term ensures accurate energy estimation through potential summation and the second term regularizes the potentials to minimize variance along the trajectory using a dropout-based approach.
ICLR_2024_oral_39,1,"### Simulating Long-Horizon Interactions through Observation Prediction

With observations from different environments that have been converted to videos, and actions of different formats that have been converted to continuous embeddings, we can formulate interactions with many real-world environments as interacting with a universal simulator. We then formulate the universal simulator as an _observation prediction model_ that predicts observations conditioned on actions and previous observations as shown in Figure 2. We finally show that this observation prediction model can be parametrized using video diffusion.

**Simulating Real-World Interactions.** We define an observation space \(O\) and an action space \(A\) which capture the videos and actions described in Section 2.1. At a specific interactive step \(t\), an agent, having observed a set of history frames \(h_{t-1}\in O\), decides on some temporally extended action \(a_{t-1}\in A\), which can be resolved into a sequence of low-level robot commands to be executed in

Figure 2: **Training and inference of UniSim.** UniSim is a video diffusion model trained to predict the next (variable length) set of observation frames (\(o_{t}\)) given observations from the past (e.g., \(o_{t-1}\)) and action input \(a_{t-1}\). UniSim can handle temporally extended actions in various modalities such as motor controls (\(\Delta x_{1},\Delta\omega_{1},\Delta x_{2},...\)), language descriptions (“wipe table”), and actions extracted from camera motions and other sources. Each dotted arrow indicates concatenating the initial noise sample for the next video segment with the previous frame.

the real world. During the execution, the next set of video frames \(o_{t}\in O\) are captured from the real world. The goal of a simulator is to predict \(o_{t}\) from \(h_{t-1}\) and \(a_{t-1}\). We can formulate this prediction problem as learning an _observation prediction_ model \(p(o_{t}|h_{t-1},a_{t-1})\). While an ideal predictive model should condition on all information of the past, i.e., \((o_{0},a_{0}\dots,a_{t-2},o_{t-1})\), through some recurrent state, we found conditioning on a finite set of frames (e.g., frames from the most recent interaction, \(o_{t-1}\)) greatly simplifies the modeling problem. To simulate long interactions, we can sample from the observation prediction model \(p(o_{t}|h_{t-1},a_{t-1})\) autoregressively conditioned on the previously sampled observations. One advantage of this observation prediction model is that the simulator stays the same across all tasks and can be used in combination with any reward function, which can be separately learned. The learned reward function can then be used to optimize policies \(\pi(a_{t}|h_{t})\) using existing decision making algorithms such as planning and RL, as we will illustrate in Section 4.1 and Section 4.2.

**Parametrizing and Training the Simulator.** We parametrize \(p(o_{t}|h_{t-1},a_{t-1})\) using diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020) as an instantiation of UniSim outlined in Figure 2. Specifically, the reverse process learns a denoising model \(\epsilon_{\theta}(o_{t}^{(k)},k|h_{t-1},a_{t-1})\) that, conditioned on the history, generates the next observationfrom initial noise samples using \(K\) denoising steps. In practice, we only use previous video frames and omit previous actions as history, and concatenate previous video frames with initial noise samples \(o_{t}^{(K)}\sim\mathcal{N}(0,I)\) channelwise to serve as conditional inputs to the denoising model. To condition on an action \(a_{t-1}\), we leverage classifier-free guidance (Ho and Salimans, 2022). The final \(\overline{\mathcal{T}}(o_{t}|h_{t-1},a_{t-1})\) is parametrized by the variance schedule:","\epsilon_{\theta}(o_{t}^{(k)},k|h_{t-1},a_{t-1})=(1+\eta)\epsilon_{\theta}(o_{ t}^{(k)},k|h_{t-1},a_{t-1})-\eta\epsilon_{\theta}(o_{t},k|h_{t-1}),",where \(\eta\) controls action conditioning strength,"\overline{\mathcal{T}}(o_{t}|h_{t-1},a_{t-1}) = \prod_{k=1}^{K} \epsilon_{\theta}(o_{t}^{(k)},k|h_{t-1},a_{t-1})","Equation 1 defines the observation prediction model as a product of denoising steps conditioned on the history and action, forming the basis for the universal simulator."
ICLR_2024_oral_39,2,"th. With this parametrization, we train \(\epsilon_{\theta}\) by minimizing","\mathcal{L}_{\text{MSE}}=\|\epsilon-\epsilon_{\theta}\Big{(}\sqrt{1- \beta^{(k)}}o_{t}+\sqrt{\beta^{(k)}}\epsilon,\,k\Big{|}h_{t-1},a_{t-1}\Big{)} \|^{2},","where \(\epsilon\sim\mathcal{N}(0,I)\), and \(\beta^{(k)}\in\mathbb{R}\) are a set of \(K\) different noise levels for each \(k\in[1,K]\)","\mathcal{L}_{\text{sim}} = \mathbb{E}_{t,k,o_{t}^{(k)},\epsilon} \left[ \left\| \epsilon - \epsilon_{\theta}(o_{t}^{(k)},k|h_{t-1},a_{t-1}) \right\|^2 \right],",the loss function for training the denoising model by minimizing the mean squared error between predicted and target noise.
ICLR_2024_oral_39,3,"\). Given the learned \(\epsilon_{\theta}\), an observation \(o_{t}\) can be generated by sampling from the initial distribution \(o_{t}^{(K)}\sim\mathcal{N}(0,I)\) and iteratively denoising according to the following process for \(k\) from \(K\) to \(0\)","o_{t}^{(k-1)}=\alpha^{(k)}(o_{t}^{(k)}-\gamma^{(k)}\epsilon_{\theta}(o_{t}^{( k)},k|h_{t-1},a_{t-1}))+\xi,\quad\xi\sim\mathcal{N}\big{(}0,\sigma_{k}^{2}I\big{)},","where \(\gamma^{(k)}\) is the denoising step size, \(\alpha^{(k)}\) is a linear decay on the current denoised sample, and \(\sigma_{k}\) is a time varying noise level that depends on \(\alpha^{(k)}\) and \(\beta^{(k)}\)","o_{t}^{(k-1)}=\mu^{(k)}(o_{t}^{(k)},h_{t-1},a_{t-1})+\Sigma^{(k)}(o_{t}^{(k)},h_{t-1},a_{t-1})\cdot \epsilon_{\theta}(o_{t}^{(k)},k|h_{t-1},a_{t-1}),",where $\mu^{(k)}$ and $\Sigma^{(k)}$ are the mean and covariance of the noise distribution at step $k$
ICLR_2024_oral_39,4,"\).

**Architecture and Training.** We use the video U-Net architecture (Ho et al., 2022) to implement UniSim by employing interleaved temporal and spatial attention and convolution layers in both the downsampling and upsampling passes. For history conditioning, we replicate the conditioning frames at all future frame indices, and concatenate the conditioning frames with the noise sample for each of the future frame to serve as input to the U-Net. UniSim model has 5.6B parameters and requires 512 TPU-v3 and 20 days to train on all data. See more details in Appendix C.

## 3 Simulating Real-World Interactions

We now demonstrate emulating real-world manipulation and navigation environments by simulating both action-rich and long-horizon interactions for both humans and robots.

### Action-Rich, Long-Horizon, and Diverse Interactions

**Action-Rich Simulation.** We first demonstrate action-rich interactions through natural language actions. Figure 3 shows simulation of human manipulation and navigation starting from the same initial observation (left-most column). We can instruct a person in the initial frame to perform various kitchen tasks (top left), press different switches (top right), or navigate scenes (bottom). The model only trained on generic internet data, without action-rich manipulation data such as EPIC-KITCHENS (Damen et al., 2018), fails to simulate action-rich manipulations (Appendix F).

**Long-Horizon Simulation.** Next, we illustrate 8 _sequential_ interactions in Figure 4. We condition the simulation of each interaction on previous observations and new language action as described in Section 2.2. UniSim successfully preserves objects manipulated by previous instructions (e.g., the orange and can are preserved in the drawers in Columns 4, 5, 7, 8 after being put in the drawers). See additional long-horizon interactions in Appendix A.1.

**Diversity and Stochasticity in the Simulator.** UniSim can also support highly diverse and stochastic environment transitions, e.g., diverse objects being revealed after removing the towel on top (Figure 5 left), diverse object colors and locations (cups and pens in Figure 5 right), and real-world variabilities such as change in camera angles. Flexibility in diffusion models promotes simulation of highly stochastic environments that cannot be controlled by actions, so that a policy can learn to only control the controllable part (Yang et al., 2022).

### Ablation and Analysis

**Frame Conditioning Ablations.** We ablate over choices of past frames to condition on using a validation split of the Ego4D dataset (Grauman et al., 2022), which contains egocentric movement requiring proper handling of observation history. We compare UniSim conditioned on different

\begin{table}
\begin{tabular}{l|c|c|c|c} Condition & FID \(\downarrow\) & FVD \(\downarrow\) & IS \(\uparrow\) & CLIP \(\uparrow\) \\ \hline
1 frame & 59.47 & 315.69 & 3.03 & 22.55 \\
4 distant & 34.89 & 237 & 3.43 & 22.62 \\
4 recent & **34.63** & **211.3** & **3.52** & **22.63** \\ \end{tabular}
\end{table}
Table 1: **Ablations of history conditioning using FVD, FID, and Inception score, and CLIP score on Ego4D. Conditioning on multiple frames is better than on a single frame, and recent history has an edge over distant history.**

Figure 4: **Long-horizon simulations.** UniSim sequentially simulates 8 interactions autoregressively. The simulated interactions maintain temporal consistency across long-horizon interactions, correctly preserving objects and locations (can on counter in column 2-7, orange in drawer in column 4-5).

Figure 3: **Action-rich simulations.** UniSim can support manipulation actions such as “cut carrots”, “wash hands”, and “pickup bowl” from the same initial frame (top left) and other navigation actions.

numbers of past frames in Table 1. Conditioning on 4 frames is better than conditioning on a single frame, but conditioning on history that is too far in the past (4 frames with exponentially increasing distances) can hurt performance. Increasing the number of conditioning frames beyond 4 did not further improve performance on Ego4D, but it could be helpful for applications that require memory from distant past (e.g., navigation for retrieval).

**Simulating Low-Data Domains.** During joint training of UniSim on diverse data, we found that naively combining datasets of highly varying size can result in low generation quality in low-data domains. While we can increase the weight of these domains in the data mixture during training, we found that attaching a domain identifier such as the name of the dataset to the actions being conditioned on improves generation quality in low-data domains, as shown in Figure 6. While such domain identifier improves in-distribution generation quality, we found domain-specific identifiers to hurt generalization to other domains, and should only be applied with the test domain is in distribution of the training domain.

## 4 Applications of UniSim

We now demonstrate how UniSim can be used to train other types of machine intelligence such as vision-language policies, RL agents, and vision-language models through simulating highly realistic experiences.

### Training Long-Horizon Vision-Language Policies through Hindsight Labeling.

Language models and vision language models (VLM) have recently been used as policies that can operate in image or text based observation and action spaces (Du et al., 2023; Driess et al., 2023; Brohan et al., 2023). One major challenge in learning such agents lies in the need for large amounts of language action labels. The labor intensity in data collection only increases as tasks increase in horizon and complexity. UniSim can generate large amounts of training data for VLM policies through hindsight relabeling.

**Setup and Baseline.** We use data from the Language Table environment (Lynch and Sermanet, 2020) for learning geometric rearrangements of blocks on a table. We train an image-goal conditioned VLM policy to predict language instructions and the motor controls from the start and goal images using the PALM-E architecture (Driess et al., 2023) (See data and model details in Appendix D.1). For the baseline, the goal is set to the last frame of the original short-horizon trajectories. During each evaluation run, we set the long-horizon goal by modifying the location of 3-4 blocks, and measure the blocks' distance to their goal states after executing 5 instructions using the VLM policy. We define the reduction in distance to goal (RDG) metric as","\text{RDG}=\frac{\|s_{0}-s_{\text{goal}}\|_{2}-\|s_{T}-s_{\text{goal}}\|_{2}}{ \|s_{0}-s_{\text{goal}}\|_{2}},","where \(s_{T}\) represents the underlying block locations after executing the policy, \(s_{0}\) and \(s_{\text{goal}}\) represents the initial and goal block locations","\text{RDG} = \frac{1}{N}\sum_{i=1}^{N} \left(1 - \frac{\|p_i - g_i\|}{\|p_i^{(0)} - g_i\|}\right),","where $p_i$ is the current position of block $i$, $g_i$ is the goal position, and $p_i^{(0)}$ is the initial position, measuring the normalized reduction in distance to the goal across $N$ blocks."
ICLR_2024_oral_4,1,"## 1 Introduction

Autoregressive large language models (LLMs) trained on general-domain data are vast stores of world knowledge (Petroni et al., 2019). They are typically optimized by predicting a token given its preceding context; therefore, tractable inference over this knowledge is limited to sampling conditioned on a prefix. Many useful tasks, such as infilling (Zhu et al., 2019; Liu et al., 2019), generating text conditioned on length or lexical constraints (Hokamp and Liu, 2017; Hu et al., 2019), and finding the most likely sequence continuation, involve intractable inference in LLMs.

Such tasks are related to the problem of reasoning, which has been framed as one of probabilistic inference (Gershman and Goodman, 2014). Correspondingly, the linguistic expression of reasoning can be seen as inference over language. For example, we can interpret chain-of-thought reasoning (Wei et al., 2022; Kojima et al., 2022), a paradigm of reasoning in language models, as a problem of intractable posterior inference. Given a question-answer pair \((X,Y)\), we are interested in finding latent chains of thought - token sequences \(Z\) that contribute the most to the conditional likelihood","p(Y\mid X)=\sum_{Z}p_{\text{LM}}(ZY\mid X)=\sum_{Z}p_{\text{LM}}(Y\mid XZ)p_{ \text{LM}}(Z\mid X),",where \(p_{\text{LM}}\) denotes the likelihood assigned to a sequence by a language model and apposition of variables (_e,"p(Y \mid X) = \sum_{Z} p(Y, Z \mid X)","The marginal likelihood of the answer $Y$ given the question $X$, obtained by summing over all possible latent chains of thought $Z$."
ICLR_2024_oral_4,2,"_e.g._, \(XZY\)) denotes the concatenation of the token sequences.

While past work has relied on prompting and in-context learning to produce \(Z\)'s that lead to the correct \(Y\), treating \(Z\) as a hidden variable in a latent variable model (LVM) renders chain-of-thought reasoning a Bayesian inference problem (Fig. 1). For this LVM, the distribution we must sample from is the posterior \(p_{\text{LM}}(Z\mid X,Y)=\frac{p_{\text{LM}}(XZY)}{\sum_{Z^{\prime}}p_{\text{LM }}(XZY)}\). Such sampling is intractable: while it is easy to evaluate \(p_{\text{LM}}(XZY)\), the conditional distributions needed to sample \(Z\) from \(p_{\text{LM}}(Z\mid X,Y)\) one token at a time are not easy to compute.

A standard method to sample approximately from intractable posterior distributions is Markov chain Monte Carlo (MCMC), but it is difficult to craft good proposal distributions for multi-modal distributions over language data (Miao et al., 2019; Zhang et al., 2020; Lew et al., 2023), and inference on a new input may be prohibitively slow. Alternatively, one can turn to reinforcement learning (RL) approaches such as proximal policy optimization (PPO; Schulman et al., 2017), where the language model is treated as a policy to be fine-tuned. However, these do not aim to model the full diversity of the distribution; instead, learned policies settle around a small number of modes. In both cases, issues with this mode collapse are exacerbated when the target distribution is misspecified, leading to the undesirable behavior of overoptimized samplers (Gao et al., 2023).

Amortized probabilistic inference - that is, training a model to approximate a distribution of interest - provides a principled, efficient, and potentially scalable way to draw samples from the distribution (Beal, 2003). One way to implement amortized inference for high-dimensional discrete data such as text is using generative flow networks (GFlowNets; Bengio et al., 2021), which are diversity-seeking reinforcement learning algorithms that train policies to sample objects (such as a token sequence \(Z\)) with probability proportional to a given reward function, such as the joint \(p_{\text{LM}}(XZY)\).

In this work, we present a method that initializes the GFlowNet policy with a pretrained LLM and continues to train it with a reward objective that can be evaluated with the same LLM. The result is a different type of fine-tuning (FT) procedure for text generation that has a number of advantages, including improved sample diversity, data efficiency, and out-of-distribution generalization. GFlowNet fine-tuning makes the language model sample from the target distribution, enabling amortized inference in a number of applications (Fig. 1).

Leveraging this approach, we empirically demonstrate the possibilities and benefits of learning to sample from intractable distributions over text continuations, latent reasoning chains, and tool use sequences using GFlowNet fine-tuning. Notably, the diversity of samples from the models trained with GFlowNet fine-tuning is beneficial in Bayesian model averaging settings, such as when aggregating answers to questions obtained via multiple reasoning chains. For example, using a pretrained language model with 6B parameters, our method shows an absolute improvement of 10.9% over supervised fine-tuning on subjectivity classification with only 10 labeled examples (SS4.3) and outperforms supervised fine-tuning and PPO by 63% on integer arithmetic with 50 demonstrations, with notable improvements in out-of-distribution generalization (SS4.4). Moreover, the benefits of amortized inference allow us to efficiently sample from the fine-tuned model at scale. Our contributions include:

1. A general algorithm for amortized sampling from intractable LLM posteriors.
2. A probabilistic approach to fine-tuning LLMs to perform chain-of-thought reasoning.
3. Empirical results on sequence continuation, natural language reasoning, integer arithmetic with tool use, and story infilling.

## 2 Motivating example: Generating random numbers with LLMs

We consider a simple task that highlights the limitations of reward-maximizing reinforcement learning (RL) methods in fine-tuning LLMs. For readers unfamiliar with RL, we refer to Sutton & Barto (2018) and include a glossary in SSA.1 to define key terms used throughout this paper. The task involves generating random numbers from a given distribution when prompted _'The following is a random integer drawn uniformly between 0 and 100: '_. This task is a minimal instantiation of the problem we study in the paper: sample from a target distribution given an unnormalized density.

Figure 1: **Left:** Three problems of reasoning in language – sentence infilling, chain-of-thought reasoning, and problem-solving with external tool use – can all be seen as instances of the latent variable model at the top left, where an input (\(X\)) generates the output (\(Y\)) via a latent variable (\(Z\)). **Right:** We fine-tune an LLM to sample from the Bayesian posterior over \(Z\), conditioning on \(X\) and optionally on \(Y\). If conditioned on \(Y\), the trained policy can be used to sample diverse latent sequences (_e.g._, for infilling, §4.2). If not conditioned on \(Y\), the policy can sample \(Z\), and thus predict \(Y\), for inputs \(X\) not seen during training (_e.g._, for classification and multi-step reasoning, §4.3, 4.4). As shown in §4.4, modeling the full diversity of the posterior aids generalization.

Although the target distribution is tractable, making the task seemingly straightforward, it serves as a useful illustration of the behaviors of different fine-tuning methods.

Renda et al. (2023) found that pretrained LLMs perform quite poorly on this task: the distribution of numbers generated with the above prompt will be far from uniform (Fig. 1(a) shows an example using an instruction fine-tuned GPT-J 6B (Wang and Komatsuzaki, 2021)1). There may be many reasons for this, among them the effects of instruction fine-tuning and the LLM's possible bias towards numbers that are more frequent in the training data (_e.g._, numbers starting with '1' are more frequent due to the properties of many natural data-generating processes (Benford, 1938)).

Footnote 1: We use the Instruct-GPT-J model available at \(\boxplus\)co/nlpcloud/instruct-gpt-j-fp16.

While reward-maximizing RL can teach the model to generate valid numbers (by penalizing outputs that are not numbers from 1 to 100), it would not resolve the distribution skew introduced during pretraining. Indeed, rewarding all valid integers equally leads to an expected gradient of zero for policy gradient methods. Fig. 1(b) shows that while most samples are valid numbers after PPO training, the distribution remains highly skewed.

Instead, we can take a principled approach by training the LLM to match the target distribution with a GFlowNet learning objective. Such an objective directly optimizes the likelihood of the model generating a number to be proportional to the reward for that number, which is the number's (potentially unnormalized) probability under the target distribution. When the policy is initialized as the pretrained LLM, the resulting distribution after GFlowNet fine-tuning is shown in Fig. 1(c). Quantitatively, the KL divergence from the sampling distribution to the target (uniform) distribution decreases from 3.37 for the original LLM (on the support \([0,100]\)) to \(9.75\cdot 10^{-5}\) for the GFlowNet fine-tuned model.

This example illustrates a general point: GFlowNet objectives provide a principled and flexible approach to fine-tuning LLMs to _match_ a target distribution where reward-maximizing RL fails to. On this simple task, this distribution matching could also be achieved through supervised fine-tuning; however, this would require access to samples from the target distribution, which are unavailable in general (though not in this simple example). In the following sections, we further illustrate this point in non-trivial problems involving intractable inference, reasoning with latent variables, and tool use.

## 3 Fine-tuning LLMs to sample from intractable distributions

We first describe how intractable inference emerges from interesting applications of LLMs, one of which is chain-of-thought reasoning seen through the lens of latent variable models, where the posterior distribution over the latent variable is intractable. We then discuss how GFlowNet objectives can be used to train amortized samplers to perform such intractable inference.

### Problem: intractable inference in large language models

Autoregressive language models decompose the distribution over sequences of tokens as a product of ordered conditionals: \(p(w_{1:N})=p(w_{1})p(w_{2}\mid w_{1})\cdots p(w_{N}\mid w_{i:N-1})\). While this decomposition makes left-to-right sampling from the distribution tractable, sampling from other conditional distribu

Figure 2: Empirical distributions of 512,000 integers from 1 to 100 generated by GPT-J fine-tuned with PPO (reward-maximizing; b) and GFlowNet fine-tuning (distribution-matching; c). Note the logarithmic \(y\)-scale.

tions is intractable. Various problems of language modeling can be viewed as sampling from such intractable conditionals in the distribution over sequences of an LLM; we give two such examples and related terminologies in Table 1. Some tasks we study in SS4 are instances of these examples.

Tempered and contrastive sampling.In many applications (_e.g._, translation, summarization, dialogue systems), one wishes to sample from a low-temperature distribution over sequences \(Z\) conditioned on a prefix \(X\), _i.e._, \(q(Z\mid X)\propto p_{\text{LM}}(XZ)^{1/T}\) for some temperature \(T<1\), as high-likelihood samples are more likely to be fluent or accurate continuations of \(X\)(Tillmann and Ney, 2003). The limit of \(T\to 0\) gives a distribution that is peaky on the most likely continuation. However, sampling from \(q\), or finding its mode, is intractable, and it is common to resort to approximations, such as tempering the _tokenwise_ conditional distributions or using beam search to search for a mode. A related problem is sampling a continuation with a correction for its unconditional likelihood, _e.g._, \(q(Z\mid X)\propto p_{\text{LM}}(XZ)^{\alpha}p_{\text{LM}}(Z)^{\beta}\) with \(\beta<0\) and \(\alpha>0\), where applications again resort to approximating the next-token conditionals of \(q\) by tempering (Malkin et al., 2022; Li et al., 2023).

Infilling and reverse generation.Infilling is the task of sampling a sequence of tokens conditioned on both its prior and subsequent context, which can be understood as sampling from the distribution \(q(Z\mid X,Y)\propto p_{\text{LM}}(XZY)\), where \(X\) and \(Y\) are fixed. Reverse generation is a special case, where \(X\) is an empty sequence. Besides being a meaningful task in its own right (Liu et al., 2019; Zhu et al., 2019; Donahue et al., 2020; Susanto et al., 2020; Lu et al., 2022), infilling and reverse generation are key components of newly emerging methods of LLM prompting, such as when LLMs are tasked with optimizing their own instruction sequences or reasoning steps (Zhou et al., 2023; Sordoni et al., 2023; Xu et al., 2023). Current applications achieve this by resorting to hand-engineered instructions and inverted prompts.

Constrained generation.Sampling of text with constraints and penalties - for example, those on the presence or the absence of certain words or on the score of an auxiliary classifier evaluated on the text - can be understood as sampling from a distribution \(q(Z)\propto p_{\text{LM}}(Z)c(Z)\), where \(c\) is an externally specified constraint. Current approaches to the problem use tokenwise approximations (Liu et al., 2021), various problem-specific beam search and local search techniques (_e.g._, Schmaltz et al., 2016; Hokamp and Liu, 2017; Hu et al., 2019; Sha, 2020; Lu et al., 2022) or classifier-guided conditional generation approaches (_e.g._, Yang and Klein, 2021; Meng et al., 2022).

### Reasoning through latent variables

Chain-of-thought reasoning (Wei et al., 2022; Kojima et al., 2022) helps LLMs solve complex problems by producing a reasoning chain before giving the final answer. LLMs pretrained on general domain data can learn to produce useful chains of thoughts given demonstrations, which are usually handcrafted or generated by prompting the LM. Interestingly, although the capacity for chain-of-thought reasoning only emerges in large language models, knowledge can also be extracted from smaller language models when they are carefully fine-tuned (Schick and Schutze, 2021).

Motivated by this, we connect chain-of-thought reasoning to the general problem of inference in latent variable models illustrated in Fig. 1. Here, reasoning can be seen as posterior inference: sampling from the posterior distribution over a string of tokens \(Z\) conditioned on a prefix \(X\) and a suffix \(Y\), given an autoregressive language model \(p_{\text{LM}}\). The posterior is defined as","p_{\text{LM}}(Z\mid X,Y)=\frac{p_{\text{LM}}(XZY)}{\sum_{Z^{\prime}}p_{\text{ LM}}(XZY^{\prime}Y)}\propto p_{\text{LM}}(XZY).",,"p_{\text{LM}}(Z\mid X,Y) = \frac{p_{\text{LM}}(XZY)}{\sum_{Z'} p_{\text{LM}}(XZ'Y)}","The posterior distribution over latent token sequences $Z$ conditioned on prefix $X$ and suffix $Y$, derived from the autoregressive language model $p_{\text{LM}}$."
ICLR_2024_oral_4,3,"Our goal is to train models to sample \(Z\) from this posterior distribution. Intuitively, this allows us to sample likely reasoning chains that lead to the desired outcome \(Y\). Although we take \(Z\) to be a string

\begin{table}
\begin{tabular}{l l l} \hline \hline Object & Meaning & Example 1 (infilling) & Example 2 (subjectivity classification) \\ \hline \(X\) & case / condition / question & _The cat was hungry._ & _A deeply moving storyline._ \\ \(Z\) & mechanism / reasoning chain & _She at a mouse._ & _This revise expresses personal feelings._ \\ \(Y\) & effect / answer & _Now the cat is sleory, not hungry._ & _Answer: Subjective_ \\ \(p(Z\mid X)\) & conditional prior & \(p_{\text{LM}}(Z\mid X)\) \\ \(p(Y\mid X,Z)\) & likelihood of effect given & \(p_{\text{LM}}(Y\mid XZ)\) \\ cause and mechanism & & \(p_{\text{LM}}(ZY\mid X)\) \\ \(p(Z,Y\mid X)\) & conditional joint, reward for \(Z\) & \(p_{\text{LM}}(ZY\mid X)\) \\ \hline \(p(Z\mid X,Y)\) & posterior (**intractable?**) & approximated and amortized by GIFlowNet \(q_{\text{GEN}}(Z\mid X|,Y)\) \\ \(q(Y\mid X)\) & posterior predictive / & approximated as \(\sum_{Z}q_{\text{GEN}}(Z\mid X)p_{\text{LM}}(Y\mid XZ)\), \\ Bayesian model average & sampled as \(Z\to q_{\text{GEN}}(Z\mid X),Y\to p_{\text{LM}}(Y\mid XZ)\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Objects in language posterior inference. Given a pretrained ‘teacher’ LM \(p_{\text{LM}}\), we train a GIFlowNet \(q_{\text{GFN}}\) to sample the posterior \(p(Z\mid X,Y)\). Amortization and generalization are achieved by making \(X\), and optionally \(Y\), an input to \(q_{\text{GFN}}\).

of tokens, the same formalism and the GFlowNet objectives apply to other structured latent objects, such as trees or sets of natural language statements, as long as one has access to a likelihood model \(p(Y\mid XZ)\). While not investigated in this work, these generalizations could be important for formal reasoning and multi-step chains of inference. See, _e.g._, Yao et al. (2023); Hao et al. (2023); Besta et al. (2024) for approaches to reasoning in language using tree- or list-structured state spaces.

A latent variable model of this form is useful when the marginal distribution \(p_{\text{LM}}(Y\mid X)\) is harder to model than \(p_{\text{LM}}(Z\mid X)\) and \(p_{\text{LM}}(Y\mid XZ)\), _i.e._, a difficult inference is broken down into a chain of easier ones. By training a model to match the Bayesian posterior \(p_{\text{LM}}(Z\mid X,Y)\), we can learn to sample latent reasoning chains that increase the likelihood of producing \(Y\) from \(X\) via the sampled \(Z\).

However, we can also fine-tune the language model \(p_{\text{LM}}(Z\mid XY)\) itself to maximize the likelihood of data pairs \((X,Y)\) under the LVM. While it is generally intractable to directly maximize the data likelihood \(p_{\text{LM}}(X,Y)=\sum_{Z}p_{\text{LM}}(XZY)\) because of the summation over \(Z\), the (variational) expectation-maximization (EM) algorithm (Dempster et al., 1977; Beal, 2003; Koller and Friedman, 2009) can be used for this purpose. In the expectation step (E-step), we draw samples from the posterior over the latent variable \(p_{\text{LM}}(Z\mid X,Y)\), which could come from an amortized sampler of \(Z\). In the maximization step (M-step), we maximize the log-likelihood of the joint probability of the sampled latent variables \(\mathbb{E}_{Z-p_{\text{LM}}(Z\mid X,Y)}\) log \(p_{\text{LM}}(XZY)\) with respect to the parameters of the language model \(p_{\text{LM}}\). This combination of amortized inference (learning to sample the chain of thought) and supervised fine-tuning (optimizing the language model with the'supervision' involving \(Z\) sampled from the amortized posterior) will be illustrated in one of our experiments (SS4.3, Table 3).

### Amortized inference with GFlowNet objectives

For inference in the latent variable model, we leverage the probabilistic framework of generative flow networks (GFlowNets; Bengio et al., 2021, 2023). Using notation for Malkin et al. (2022), we briefly introduce relevant GFlowNet concepts pertaining to autoregressive sequence generation. Here, GFlowNets learn policies to sample sequences \(Z=z_{1}z_{2}\dots z_{n}\top\in\mathcal{Z}\) (where \(\top\) denotes a stop symbol) from a distribution over the space of sequences \(\mathcal{Z}\), given an unnormalized density (reward) \(R:\mathcal{Z}\rightarrow\mathbb{R}_{\geqslant 0}\). The generative process is the same as in autoregressive language models: generation begins with an empty string, and at the \(i\)-th step a token \(z_{i}\) is sampled from a policy \(q_{\text{GN}}(z_{i}\mid z_{1:i-1})\), which is then appended to the sequence. This process continues until a stop symbol \(\top\) is generated.

The marginal likelihood \(q_{\text{FNN}}^{\top}(Z)\) of sampling a terminal state \(Z=z_{1:n}\top\) is given by \(\prod_{i=1}^{n}q_{\text{GN}}(z_{i}\mid z_{1:i-1})q_{\text{GFN}}(\top\mid z)\), where \(z_{1:0}\) is understood to be the empty string. The goal of GFlowNet training is to fit a parametric policy \(q_{\text{GFN}}(\cdot\mid\cdot;\theta)\) such that \(q_{\text{GFN}}^{\top}(Z)\propto R(Z)\), _i.e._, the likelihood of generating a complete sequence is proportional to its reward.

Learning objective.We use a modified version of the subtrajectory balance (SubTB; Madan et al., 2023) objective to account for trajectories being terminable at all states (Deleu et al., 2022). The objective for a sequence \(Z=z_{1:n}\top\) is","\mathcal{L}(Z;\theta)=\sum_{0\leq i<j\leq n}(\log\frac{R(z_{1:i}\top) \prod_{k=i+1}^{j}q_{\text{GFN}}(z_{k}\mid z_{1:k-1})q_{\text{GFN}}(\top\mid z_ {1:j})}{R(z_{1:j}\top)q_{\text{GFN}}(\top\mid z_{1:i})})^{2},",,"\mathcal{L}_{\text{SubTB}}(Z) = \left( \sum_{i=0}^{n} \log q_{\text{GFN}}(z_{i+1} \mid z_{1:i}) - \log \tilde{R}(z_{1:i}) \right)^2,","where $\tilde{R}(z_{1:i})$ is the reward function defined on partial sequences $z_{1:i}$, and the objective minimizes the squared difference between the log-policy and the log-reward along the generation trajectory."
ICLR_2024_oral_41,1,"## 2 Related work

The significant advancements of diffusion models have been witnessed in recent years in the domain of image generation. (Karras et al., 2022; Lu et al., 2022b; Dockhorn et al., 2021; Bao et al., 2022; Lu et al., 2022; Bao et al., 2022; Zhang et al., 2023). It (Sohl-Dickstein et al., 2015; Ho et al.,

Figure 1: (a) **Conceptual comparison** between DDPM (Ho et al., 2020) (I) and our proposed early timestep-shared diffusion model (E-TSDM) (II). DDPM trains the network \(\epsilon_{\theta}(\cdot,t)\) with varying timestep conditions \(t\) at each denoising step, whereas E-TSDM uniformly divides the near-zero timestep interval \(t\in[0,\tilde{t})\) with high Lipschitz constants into \(n\) sub-intervals and **shares the condition \(t\) within each sub-interval**. Here, \(\tilde{t}\) denotes the length of the interval for sharing conditions. When \(t\geq\tilde{t}\), E-TSDM follows the same procedure as DDPM. However, when \(t<\tilde{t}\), E-TSDM shares timestep conditions. (b) **Quantitative comparison** of the Lipschitz constants between DDPM and our proposed early timestep-shared diffusion model (E-TSDM). **The Lipschitz constants tend to be extremely large near zero point for DDPM**. However, our sharing approach allows E-TSDM to force the Lipschitz constants in each sub-interval to be zero, thereby **reducing the overall Lipschitz constants** in the timestep interval \(t\in[0,\tilde{t})\), where \(\tilde{t}\) is set as a default value 100.

2020; Song et al., 2021) defines a Markovian forward process \(\{\mathbf{x}_{t}\}_{t\in[0,T]}\) that gradually destroys the data \(\mathbf{x}_{0}\) with Gaussian noise. For any \(t\in[0,T]\), the conditional distribution \(q_{0t}(\mathbf{x}_{t}|\mathbf{x}_{0})\) satisfies","q_{0t}(\mathbf{x}_{t}|\mathbf{x}_{0})=\mathcal{N}(\mathbf{x}_{ t}|\alpha_{t}\mathbf{x}_{0},\sigma_{t}^{2}\mathbf{I}),","where \(\alpha_{t}\) and \(\sigma_{t}\) are referred to as the noise schedule, satisfying \(\alpha_{t}^{2}+\sigma_{t}^{2}=1\)","q_{0t}(\mathbf{x}_{t}|\mathbf{x}_{0}) = \mathcal{N}(\mathbf{x}_{t}; \sqrt{\alpha_t}\mathbf{x}_{0}, (1 - \alpha_t)\mathbf{I}),","Equation 1 defines the conditional distribution of the forward process in diffusion models, where the data is gradually corrupted by Gaussian noise over time $t$."
ICLR_2024_oral_41,2,"\). Generally, \(\alpha_{t}\) decreases from \(1\) to \(0\) as \(t\) increases, to ensure that the marginal distribution of \(\mathbf{x}_{t}\) gradually changes from the data distribution \(q_{0}(x_{0})\) to Gaussian. Kingma et al. (2021) further prove that the following stochastic differential equation (SDE) has the same transition distribution \(q_{0t}(\mathbf{x}_{t}|\mathbf{x}_{0})\) as in Equation (1) for any \(t\in[0,T]\):","\mathrm{d}\mathbf{x}_{t}=f(t)\mathbf{x}_{t}\mathrm{d}t+g(t )\mathrm{d}\mathbf{w}_{t},\quad\mathbf{x}_{0}\sim q_{0}(x_{0}),","where \(\mathbf{w}_{t}\) is the standard Wiener process, \(f(t)=\frac{\mathrm{d}\log\alpha_{t}}{\mathrm{d}t}\) and \(g(t)=2\sigma_{t}^{2}\frac{\mathrm{d}\log(\sigma_{t}/\alpha_{t})}{\mathrm{d}t}\)","\mathrm{d}\mathbf{x}_{t}=\frac{\mathrm{d}}{\mathrm{d}t}\left(\frac{\alpha_{t}}{\sigma_{t}}\right)\mathbf{x}_{t}\mathrm{d}t+\sqrt{\frac{\mathrm{d}}{\mathrm{d}t}\left(\frac{\alpha_{t}^{2}}{\sigma_{t}^{2}}\right)}\mathrm{d}\mathbf{w}_{t},",Stochastic differential equation (SDE) formulation that matches the transition distribution of the forward process in Equation 1.
ICLR_2024_oral_41,3,"\).

Song et al. (2021) point out that the following reverse-time SDE has the same marginal distribution \(q_{t}(\mathbf{x}_{t})\) for any \(t\in[0,T]\):","\mathrm{d}\mathbf{x}_{t}=[f(t)\mathbf{x}_{t}-g(t)^{2} \nabla_{\mathbf{x}_{t}}\log q_{t}(\mathbf{x}_{t})]\mathrm{d}t+g (t)\mathrm{d}\bar{\mathbf{w}}_{t},\quad\mathbf{x}_{T}\sim q_{T} (\mathbf{x}_{T}),",where \(\bar{\mathbf{w}}_{t}\) is a standard Wiener process in the reverse time,"\mathrm{d}\mathbf{x}_{t}=\left[f(t)\mathbf{x}_{t}-g(t)^{2}\nabla_{\mathbf{x}_{t}}\log q_{t}(\mathbf{x}_{t})\right]\mathrm{d}t+g(t)\mathrm{d}\mathbf{w}_{t},",where the reverse-time stochastic differential equation (SDE) governs the denoising process by using the score function $\nabla_{\mathbf{x}_{t}}\log q_{t}(\mathbf{x}_{t})$ to guide the generation towards the data distribution.
ICLR_2024_oral_41,4,"me. Once the score function \(\nabla_{\mathbf{x}_{t}}\log q_{t}(\mathbf{x}_{t})\) is known, we can simulate Equation (3) for sampling. However, directly learning the score function is problematic, as it involves an explosion of training loss when having a small \(\sigma_{t}(\text{Song et al., 2021)}\). In practice, the noise prediction model \(\mathbf{\epsilon}_{\theta}(\mathbf{x}_{t},t)\) is often adopted to estimate \(-\sigma_{t}\nabla_{\mathbf{x}_{t}}\log q_{t}(\mathbf{x}_{t})\). The network \(\mathbf{\epsilon}_{\theta}(\mathbf{x}_{t},t)\) can be trained by minimizing the objective:","\mathcal{L}(\theta):=\mathbb{E}_{t\sim\mathcal{U}(0,T), \mathbf{x}_{0}\sim q_{0}(\mathbf{x}_{0}),\epsilon\sim\mathcal{N} (0,\mathbf{I})}[\|\mathbf{\epsilon}_{\theta}(\alpha_{t} \mathbf{x}_{0}+\sigma_{t}\epsilon,t)-\epsilon\|_{2}^{2}].",,"\mathbb{E}_{t,\mathbf{x}_{0},\mathbf{\epsilon}}\left[\left\Vert \mathbf{\epsilon}-\mathbf{\epsilon}_{\theta}(\alpha_{t}\mathbf{x}_{0}+\sigma_{t}\mathbf{\epsilon},t)\right\Vert ^{2}\right],","where the expectation is over the data distribution and the noise schedule, and $\mathbf{\epsilon}_{\theta}(\mathbf{x}_{t},t)$ represents the neural network used to estimate the noise component"
ICLR_2024_oral_41,5,"In this work, our observation of Lipschitz singularities on noise-prediction and v-prediction diffusion models reveals the inherent price of such an approach.

**Numerical stability near zero point.** Achieving numerical stability is essential for high-quality samples in diffusion models, where the sampling process involves solving a reverse-time SDE. Nevertheless, numerical instability is frequently observed near \(t=0\) in practice (Song et al., 2021; Vahdat et al., 2021). To address this singularity, one possible approach is to set a small non-zero starting time \(\tau>0\) in both training and inference (Song et al., 2021; Vahdat et al., 2021). Kim et al. (2022) resolve the trade-off between density estimation and sample generation performance by introducing randomization to the fixed \(\tau\). In contrast, we enhance numerical stability by reducing the Lipschitz constants to zero near \(t=0\), which leads to improved sample quality in diffusion models. It is worth noting that the numerical issues observed by aforementioned works are mainly caused by the singularity of transition kernel \(q_{0t}(\mathbf{x}_{t}|\mathbf{x}_{0})\). This transition kernel will degrade to a Dirac kernel \(\delta(\mathbf{x}_{t}-\alpha_{t}\mathbf{x}_{0})\) as \(\sigma_{t}\to 0\). However, our observation is _the infinite Lipschitz constants of the noise prediction model \(\mathbf{\epsilon}_{\theta}\left(\mathbf{x},t\right)\) w.r.t time variable \(t\)_, and this is caused by the explosion of \(\frac{\mathrm{d}\sigma_{t}}{\mathrm{d}t}\) as \(t\to 0\). To the best of our knowledge, this has not been observed before.

## 3 Lipschitz singularities in diffusion models

**Lipschitz singularities issue.** In this section, we elucidate the vexing propensity of diffusion models to exhibit infinite Lipschitz near the zero point. We achieve this by analyzing the partial derivative \(\partial\mathbf{\epsilon}_{\theta}(\mathbf{x},t)/\partial t\) of the network \(\mathbf{\epsilon}_{\theta}(\mathbf{x},t)\). In essence, the emergence of Lipschitz singularities, characterized by \(\limsup_{t\to 0+}\left\|\frac{\partial\mathbf{\epsilon}_{\theta}(\mathbf{x},t)}{ \partial t}\right\|\to\infty\), can be attributed to the fact that the prevailing noise schedules conform to the behavior of \(d\sigma_{t}/dt\to\infty\) as the parameter \(t\) tends towards zero.

Figure 2: **Quantitative comparison of the errors caused by a perturbation on the input between E-TSDM and DDPM (Ho et al., 2020). Results show that \(\mathbf{\mathrm{E}}\)-TSDM is more stable, as its prediction is less affected, _e.g._, the perturbation error of DDPM is 42.0% larger than E-TSDM when the perturbation scale is 0.2.**

**Theoretical analysis.** Now we theoretically prove that the infinite Lipschitz happens near the zero point in diffusion models, where the distribution of data is an arbitrary complex distribution. We focus particularly on the scenario where the network \(\mathbf{\epsilon}_{\theta}(\mathbf{x},t)\) is trained to predict the noises added to images (v-prediction model (Salimans and Ho, 2022) has a similar singularity problem, and is analyzed in Appendix C.2). The network \(\mathbf{\epsilon}_{\theta}(\mathbf{x},t)\) exhibits a relationship with the score function \(\nabla_{\mathbf{x}}\log q_{t}(\mathbf{x})\) that \(\mathbf{\epsilon}_{\theta}(\mathbf{x},t)=-\sigma_{t}\nabla_{\mathbf{x}}\log q_{t} (\mathbf{x})\)(Song et al., 2021), where \(\sigma_{t}\) is the standard deviation of the forward transition distribution \(q_{0t}(\mathbf{x}|\mathbf{x}_{0})=\mathcal{N}(\mathbf{x};\alpha_{t}\mathbf{x} _{0},\sigma_{t}^{2}\mathbf{I})\). Specifically, \(\alpha_{t}\) and \(\sigma_{t}\) satisfy \(\alpha_{t}^{2}+\sigma_{t}^{2}=1\).

**Theorem 3.1**: _Given a noise schedule, since \(\sigma_{t}=\sqrt{1-\alpha_{t}^{2}}\), we have \(\frac{d\sigma_{t}}{dt}=-\frac{\alpha_{t}}{\sqrt{1-\alpha_{t}^{2}}}\frac{d \alpha_{t}}{dt}\). As \(t\) gets close to 0, the noise schedule requires \(\alpha_{t}\to 1\), leading to \(d\sigma_{t}/dt\to\infty\) as long as \(\frac{d\alpha_{t}}{dt}|_{t=0}\neq 0\). The partial derivative of the network can be written as_","\frac{\partial\mathbf{\epsilon}_{\theta}(\mathbf{x},t)}{\partial t}= \frac{\alpha_{t}}{\sqrt{1-\alpha_{t}^{2}}}\frac{d\alpha_{t}}{dt}\nabla_{ \mathbf{x}}\log q_{t}(\mathbf{x})-\frac{\partial\nabla_{\mathbf{x} }\log q_{t}(\mathbf{x})}{\partial t}\sigma_{t}.",,"\frac{\partial\mathbf{\epsilon}_{\theta}(\mathbf{x},t)}{\partial t}=-\frac{d\sigma_{t}}{dt}\nabla_{\mathbf{x}}\log q_{t}(\mathbf{x})-\sigma_{t}\nabla_{\mathbf{x}}\left(\frac{\partial}{\partial t}\log q_{t}(\mathbf{x})\right).","Equation describing the partial derivative of the noise prediction network with respect to time, revealing the source of Lipschitz singularities near t=0 due to the explosion of the derivative of σₜ."
ICLR_2024_oral_41,6,"_Note that \(\alpha_{t}\to 1\) as \(t\to 0\), thus if \(\frac{d\alpha_{t}}{dt}|_{t=0}\neq 0\), and \(\nabla_{\mathbf{x}}\log q_{t}(\mathbf{x})|_{t=0}\neq\mathbf{0}\), then one of the following two must stand_","\lim\sup_{t\to 0+}\|\frac{\partial\mathbf{\epsilon}_{\theta}(\mathbf{x},t )}{\partial t}\|\to\infty;\quad\lim\sup_{t\to 0+}\|\frac{ \partial\nabla_{\mathbf{x}}\log q_{t}(\mathbf{x})}{\partial t} \sigma_{t}\|\to\infty.",,"\limsup_{t\to 0+}\left\|\frac{\partial\mathbf{\epsilon}_{\theta}(\mathbf{x},t)}{\partial t}\right\|=\infty.","Equation 6 demonstrates that under common noise schedules, the partial derivative of the noise prediction model with respect to time tends to infinity near $t=0$, indicating the presence of Lipschitz singularities in diffusion models."
ICLR_2024_oral_41,7,"Note that \(\frac{d\alpha_{t}}{dt}|_{t=0}\neq 0\) stands for a wide range of noise schedules, including linear, cosine, and quadratic schedules (see details in Appendix C.1). Besides, we can safely assume that \(q_{t}(\mathbf{x})\) is a smooth process. Therefore, we may often have \(\lim\sup_{t\to 0+}\left\|\frac{\partial\mathbf{\epsilon}_{\theta}\left(\mathbf{x},t \right)}{\partial t}\right\|\to\infty\), indicating the infinite Lipschitz constants around \(t=0\).

**Simple case illustration.** Take a simple case that the distribution of data \(p(\mathbf{x}_{0})\sim\mathcal{N}(\mathbf{0},\mathbf{I})\) for instance, the score function for any \(t\in[0,T]\) can be written as",\nabla_{\mathbf{x}}\log q_{t}(\mathbf{x})=\nabla_{\mathbf{x}}\log (\frac{1}{\sqrt{2\pi}}\exp(-\frac{\|\mathbf{x}\|_{2}^{2}}{2}) )=-\mathbf{x}.,,\nabla_{\mathbf{x}}\log q_{t}(\mathbf{x})=-\frac{\mathbf{x}}{\sigma_{t}^{2}+\alpha_{t}^{2}}.,"Score function for the case where the data distribution follows a standard Gaussian, $p(\mathbf{x}_{0})\sim\mathcal{N}(\mathbf{0},\mathbf{I})$"
ICLR_2024_oral_41,8,"Due to the relationship \(\mathbf{\epsilon}_{\theta}(\mathbf{x},t)=-\sigma_{t}\nabla_{\mathbf{x}}\log q_{t} (\mathbf{x})\) and the fact that the deviation \(\frac{\mathrm{d}\sigma_{t}}{\mathrm{d}t}\) tends toward \(\infty\) as \(t\to 0\), we have \(\left\|\frac{\partial\mathbf{\epsilon}_{\theta}\left(\mathbf{x},t\right)}{ \partial t}\right\|\to\infty\).

**Case in reality.** After theoretically proving that diffusion models suffer infinite Lipschitz near the zero point, we show it empirically. We estimate the Lipschitz constants of a network by","K(t,t^{\prime})=\frac{\mathbb{E}_{\mathbf{x}_{t}}[\|\mathbf{\epsilon}_{\theta} (\mathbf{x}_{t},t)-\mathbf{\epsilon}_{\theta}(\mathbf{x}_{t},t^{ \prime})\|_{2}]}{\Delta t},",where \(\Delta t=|t-t^{\prime}|\),"\mathcal{L}_{t}(x):=\left\|\frac{\partial\mathbf{\epsilon}_{\theta}(\mathbf{x},t)}{\partial t}\right\|.","where $\mathcal{L}_{t}(x)$ denotes the Lipschitz constant of the noise prediction network $\mathbf{\epsilon}_{\theta}(\mathbf{x},t)$ with respect to the time variable $t$"
ICLR_2024_oral_41,9,"\). For a network \(\mathbf{\epsilon}_{\theta}(\mathbf{x}_{t},t^{\prime})\) of DDPM baseline (Ho et al., 2020) trained on FFHQ \(256\times 256\)(Karras et al., 2019) (see training details in Section 5.1 and more results of the Lipschitz constants \(K(t,t^{\prime})\) on other datasets in Appendix D.1), the variation of the Lipschitz constants \(K(t,t^{\prime})\) as the noise level \(t\) varies is seen in Figure 1b, showing that the Lipschitz constants \(K(t,t^{\prime})\) get extremely large in the interval with low noise levels. Such large Lipschitz constants support the above theoretical analysis and pose a threat to the stability and accuracy of the diffusion process, which relies on integral operations.

## 4 Mitigating Lipschitz singularities by sharing conditions

**Proposed method.** In this section, we propose the Early Timestep-shared Diffusion Model (E-TSDM), which aims to alleviate the Lipschitz singularities by sharing the timestep conditions in the interval with large Lipschitz constants. To avoid impairing the network's ability, E-TSDM performs a stepwise operation of sharing timestep condition values. Specifically, we consider the interval near the zero point suffering from large Lipschitz constants, denoted as \([0,t)\), where \(\tilde{t}\) indicates the length of the target interval. E-TSDM uniformly divides this interval into \(n\) sub-intervals represented as a sequence \(\mathbb{T}=\{t_{0},t_{1},\cdots,t_{n}\}\), where\(t_{i-1},\forall i=1,2,\cdots,n\). For each sub-interval, E-TSDM employs a single timestep value (the left endpoint of the sub-interval) as the condition, both during training and inference. Utilizing this strategy, E-TSDM effectively enforces zero Lipschitz constants within each sub-interval, with only the timesteps located near the boundaries of the sub-intervals having a Lipschitz constant greater than zero. As a result, the overall Lipschitz constants of the target interval \(t\in[0,\tilde{t})\) are significantly reduced. The corresponding training loss can be written as","\mathcal{L}(\epsilon_{\theta}):=\mathbb{E}_{t\sim\mathcal{U}(0,T), \mathbf{x}_{0}\sim q(\mathbf{x}_{0}),\epsilon\sim\mathcal{N}(0,\mathbf{I})} [\|\epsilon_{\theta}(\alpha_{t}\mathbf{x}_{0}+\sigma_{t}\epsilon,f_{ \mathbb{T}}(t))-\epsilon\|_{2}^{2}],","where \(f_{\mathbb{T}}(t)=\max_{1\leq i\leq n}\{t_{i-1}\in\mathbb{T}:t_{i-1}\leq t\}\) for \(t<\tilde{t}\), while \(f_{\mathbb{T}}(t)=t\) for \(t\geq\tilde{t}\)","\mathcal{L}_{\text{E-TSDM}}(\theta):=\mathbb{E}_{t\sim\mathcal{U}(0,T), \mathbf{x}_{0}\sim q_{0}(\mathbf{x}_{0}),\epsilon\sim\mathcal{N} (0,\mathbf{I})}[\|\mathbf{\epsilon}_{\theta}(\alpha_{t} \mathbf{x}_{0}+\sigma_{t}\epsilon,\tau(t))-\epsilon\|_{2}^{2}],",where $\tau(t)$ is the shared timestep condition within the sub-interval containing $t$.
ICLR_2024_oral_41,10,\). The corresponding reverse process can be represented as,"p_{\theta}(\mathbf{x}_{t-1}|\mathbf{x}_{t})=\mathcal{N}( \mathbf{x}_{t-1};\frac{\alpha_{t-1}}{\alpha_{t}}(\mathbf{x}_{t}-\frac{ \beta_{t}}{\sigma_{t}}\epsilon_{\theta}(\mathbf{x}_{t},f_{\mathbb{T}} (t))),\eta_{t}^{2}\mathbf{I}),","where \(\beta_{t}=1-\frac{\alpha_{t}}{\alpha_{t-1}}\), and \(\eta_{t}^{2}=\beta_{t}\)","\mathrm{d}\mathbf{x}_{t}=[f(t)\mathbf{x}_{t}-g(t)^{2} \nabla_{\mathbf{x}_{t}}\log q_{t}(\mathbf{x}_{t})]\mathrm{d}t+g (t)\mathrm{d}\bar{\mathbf{w}}_{t},\quad\mathbf{x}_{T}\sim q_{T} (\mathbf{x}_{T}),","where $\bar{\mathbf{w}}_{t}$ is a standard Wiener process in the reverse time, and $f_{\mathbb{T}}(t)$ denotes the shared timestep condition"
ICLR_2024_oral_41,11,"\). E-TSDM is easy to implement, and the algorithm details are provided in Appendix B.2.

**Analysis of estimation error.** Then we show that the estimation error of E-TSDM can be bounded by an infinitesimal, and thus the impact of E-TSDM on the estimation accuracy is insignificant. The detailed proof is shown in Appendix C.3.

**Theorem 4.1**: _Given the chosen \(f_{\mathbb{T}}(t)\), when \(t\in[0,\tilde{t})\), the difference between the optimal \(\boldsymbol{\epsilon}_{\theta}(\mathbf{x},f_{\mathbb{T}}(t))\) denoted as \(\boldsymbol{\epsilon}^{*}(\mathbf{x},f_{\mathbb{T}}(t))\), and \(\boldsymbol{\epsilon}(\mathbf{x},t)=-\sigma_{t}\nabla_{\mathbf{x}}\log q_{t}( \mathbf{x})\), can be bounded by_","\|\boldsymbol{\epsilon}^{*}(\mathbf{x},f_{\mathbb{T}}(t) -\boldsymbol{\epsilon}(\mathbf{x},t)\|\leq\sigma_{\tilde{t}}K (\mathbf{x})\Delta t+B(\mathbf{x})\Delta\sigma_{\max},",,"\|\boldsymbol{\epsilon}^{*}(\mathbf{x},f_{\mathbb{T}}(t))-\boldsymbol{\epsilon}(\mathbf{x},t)\|_{2}\leq C\Delta t,",where $C$ is a constant and $\Delta t$ is the maximum sub-interval length in the partition $\mathbb{T}$
ICLR_2024_oral_41,12,_where_,"K(\mathbf{x})=\sup_{t\neq\tau}\frac{\|\nabla_{\mathbf{x}}\log q_{t }(\mathbf{x})-\nabla_{\mathbf{x}}\log q_{\tau}(\mathbf{x} )\|}{|t-\tau|},\quad B(\mathbf{x})=\sup_{t}\|\nabla_{ \mathbf{x}}\log q_{t}(\mathbf{x})\|,",,"\Delta t = \max_{1 \leq i \leq n} (t_i - t_{i-1}), \quad \Delta\sigma_{\max} = \max_{t,t' \in [0,\tilde{t})} |\sigma_t - \sigma_{t'}|.","where $\Delta t$ is the maximum length of the sub-intervals, $\Delta\sigma_{\max}$ is the maximum difference in $\sigma_t$ within the interval $[0,\tilde{t})$, and $K(\mathbf{x})$, $B(\mathbf{x})$ are bounded functions of $\mathbf{x}$"
ICLR_2024_oral_44,1,"## 2 Why Might Previous Unsupervised RL Methods Fail To Scale?

The goal of unsupervised RL is to acquire useful knowledge, such as policies, world models, or exploratory data, by interacting with the environment in an unsupervised manner (_i.e._, without tasks or reward functions). Typically, this knowledge is then leveraged to solve downstream tasks more efficiently. Prior work in unsupervised RL can be categorized into two main groups: pure exploration methods and unsupervised skill discovery methods. Pure exploration methods aim to cover the entire state space or fully capture the environment dynamics. They encourage exploration by maximizing uncertainty (Pathak et al., 2017; Shyam et al., 2019; Burda et al., 2019; Pathak et al., 2019; Sekar et al., 2020; Mazzaglia et al., 2022) or state entropy (Lee et al., 2019; Pong et al., 2020; Liu and Abbeel, 2021; Yarats et al., 2021). Based on the data collected by the exploration

Figure 1: **Illustration of METRA. Our main idea for scalable unsupervised RL is to cover only the most “important” low-dimensional subset of the state space, analogously to PCA. Specifically, METRA covers the most “temporally spread-out” (non-linear) manifold, which would lead to _approximate_ coverage of the state space \(\mathcal{S}\). In the example above, the two-dimensional \(\mathcal{Z}\) space captures behaviors running in all directions, not necessarily covering every possible leg pose.**

policy, these methods learn a world model (Rajeswar et al., 2023), train a goal-conditioned policy (Pong et al., 2020; Pitis et al., 2020; Mendonca et al., 2021; Hu et al., 2023), learn skills via trajectory autoencoders (Campos Camunez et al., 2020; Mazzaglia et al., 2023), or directly fine-tune the learned exploration policy (Laskin et al., 2021) to accelerate downstream task learning. While these pure exploration-based approaches are currently the leading methods in unsupervised RL benchmarks (Mendonca et al., 2021; Laskin et al., 2021; Mazzaglia et al., 2023; Rajeswar et al., 2023), their scalability may be limited in complex environments with large state spaces because it is often computationally infeasible to completely cover every possible state or fully capture the dynamics. In Section 5, we empirically demonstrate that these approaches even fail to cover the state space of the state-based \(29\)-dimensional Ant environment.

Another line of research in unsupervised RL aims to learn diverse behaviors (or _skills_) that are distinguishable from one another, and our method also falls into this category. The most common approach to unsupervised skill discovery is to maximize the mutual information (MI) between states and skills (Gregor et al., 2016; Eysenbach et al., 2019; Sharma et al., 2020; Hansen et al., 2020):","I(S;Z)=D_{\text{KL}}(p(s,z)\|p(s)p(z)).",,"\begin{equation}
\max_{\pi} \, I(S; Z) = H(S) - H(S \mid Z),
\end{equation}",Equation 1 formulates the objective of maximizing mutual information between states $S$ and skills $Z$ in unsupervised skill discovery methods.
ICLR_2024_oral_44,2,"By associating different skill latent vectors \(z\) with different states \(s\), these methods learn diverse skills that are mutually distinct. However, they share the limitation that they often end up discovering simple, static behaviors with limited state coverage (Campos Camunez et al., 2020; Park et al., 2022). This is because MI is defined by a KL divergence (Equation (1)), which is a _metric-agnostic_ quantity (_e.g._, MI is invariant to scaling; see Figure 2). As a result, the MI objective only focuses on the distinguishability of behaviors, regardless of ""how different"" they are, resulting in limited state coverage (Campos Camunez et al., 2020; Park et al., 2022). To address this limitation, prior works combine the MI objective with exploration bonuses (Campos Camunez et al., 2020; Strouse et al., 2022; Park and Levine, 2023) or propose different objectives that encourage maximizing distances in the state space (He et al., 2022; Park et al., 2022; Park et al., 2022; 2023b). Yet, it remains unclear whether these methods can scale to complex, high-dimensional environments, because they either attempt to completely capture the entire MDP (Campos Camunez et al., 2020; Strouse et al., 2022; Park and Levine, 2023) or assume a compact, structured state space (He et al., 2022; Park et al., 2022; 2023b). Indeed, to the best of our knowledge, _no_ previous unsupervised skill discovery methods have succeeded in discovering locomotion behaviors on pixel-based locomotion environments. Unlike these approaches, our method learns a compact set of diverse behaviors that are maximally different in terms of the _temporal distance_. As a result, they can approximately cover the state space, even in a complex, high-dimensional environment. We discuss further related work in Appendix B.

## 3 Preliminaries and Problem Setting

We consider a controlled Markov process, an MDP without a reward function, defined as \(\mathcal{M}=(\mathcal{S},\mathcal{A},\mu,p)\). \(\mathcal{S}\) denotes the state space, \(\mathcal{A}\) denotes the action space, \(\mu:\Delta(\mathcal{S})\) denotes the initial state distribution, and \(p:\mathcal{S}\times\mathcal{A}\rightarrow\Delta(\mathcal{A})\) denotes the transition dynamics kernel. We consider a set of latent vectors \(z\in\mathcal{Z}\), which can be either discrete or continuous, and a latent-conditioned policy \(\pi(a|s,z)\). Following the terminology in unsupervised skill discovery, we refer to latent vectors \(z\) (and their corresponding policies \(\pi(a|s,z)\)) as _skills_. When sampling a trajectory, we first sample a skill from the prior distribution, \(z\sim p(z)\), and then roll out a trajectory with \(\pi(a|s,z)\), where \(z\) is fixed for the entire episode. Hence, the joint skill-trajectory distribution is given as

Figure 2: **Sketch comparing different unsupervised RL objectives.** Pure exploration approaches try to cover every possible state, which is infeasible in complex environments (_e.g._, such methods might be “stuck” at forever finding novel joint angle configurations of a robot, without fully exploring the environment; see Figure 3). The mutual information \(I(S;Z)\) has no underlying distance metrics, and thus does not prioritize coverage enough, only focusing on skills that are discriminable. In contrast, our proposed Wasserstein dependency measure \(I_{\text{W}}(S;Z)\) maximizes the distance metric \(d\), which we choose to be the temporal distance, forcing the learned skills to span the “longest” subspaces of the state space, analogously to (temporal, non-linear) PCA.

\(p(z)p(s_{0})\prod_{t=0}^{T-1}\pi(a_{t}|s_{t},z)p(s_{t+1}|s_{t},a_{t})\), where \(\tau\) denotes \((s_{0},a_{0},s_{1},a_{1},\ldots,s_{T})\). Our goal in this work is to learn a set of diverse, useful behaviors \(\pi(a|s,z)\), without using any supervision, data, or prior knowledge.

## 4 A Scalable Objective for Unsupervised RL

**Desiderata.** We first state our two desiderata for a scalable unsupervised RL objective. First, instead of covering every possible state in a given MDP, which is infeasible in complex environments, we want to have a compact latent space \(\mathcal{Z}\) of a _tractable_ size and a latent-conditioned policy \(\pi(a|s,z)\) that translates latent vectors into actual behaviors. Second, we want the behaviors from different latent vectors to be different, collectively covering as much of the state space as possible. In other words, we want to **maximize state coverage under the given capacity** of \(\mathcal{Z}\). An algorithm that satisfies these two desiderata would be scalable to complex environments, because we only need to learn a compact set of behaviors that approximately cover the MDP.

**Objective.** Based on the above, we propose the following novel objective for unsupervised RL:","I_{\mathcal{W}}(S;Z)=\mathcal{W}(p(s,z),p(s)p(z)),",where \(I_{\mathcal{W}}(S;Z)\) is the Wasserstein dependency measure (WDM) (Ozair et al,"\max_{\pi} \mathbb{E}_{z \sim p(z)} \left[ \mathbb{E}_{\tau \sim \pi(\cdot|z)} \left[ \sum_{t=0}^{T} \gamma^t d(s_t, s_{t+1}) \right] \right],","This objective maximizes the expected temporal distance between consecutive states across skills, encouraging the policy to learn diverse behaviors that span the most temporally extended directions of the state space."
ICLR_2024_oral_44,3,"al., 2019) between states and skills, and \(\mathcal{W}\) is the \(1\)-Wasserstein distance on the metric space \((\mathcal{S}\times\mathcal{Z},d)\) with a _distance metric_\(d\). Intuitively, the WDM objective in Equation (2) can be viewed as a ""Wasserstein variant"" of the previous MI objective (Equation (1)), where the KL divergence in MI is replaced with the Wasserstein distance. However, despite the apparent similarity, there exists a significant difference between the two objectives: MI is completely agnostic to the underlying distance metric, while WDM is a _metric-aware_ quantity. As a result, the WDM objective (Equation (2)) not only discovers diverse skills that are different from one another, as in the MI objective, but also actively maximizes distances \(d\) between different skill trajectories (Figure 2). This makes them collectively cover the state space as much as possible (in terms of the given metric \(d\)). The choice of metric for \(d\) is critical for effective skill discovery, and simple choices like Euclidean metrics on the state space would generally _not_ be effective for non-metric state representations, such as images. Therefore, instantiating this approach with the right metric is an important part of our contribution, as we will discuss in Section 4.2. Until then, we assume that we have a given metric \(d\).

### Tractable Optimization

While our objective \(I_{\mathcal{W}}(S;Z)\) has several desirable properties, it is not immediately straightforward to maximize this quantity in practice. In this section, we describe a simple, tractable objective that can be used to maximize \(I_{\mathcal{W}}(S;Z)\) in practice. We begin with the Kantorovich-Rubenstein duality (Villani et al., 2009; Ozair et al., 2019), which provides a tractable way to maximize the Wasserstein dependency measure:","I_{\mathcal{W}}(S;Z)=\sup_{\|f\|_{L}\leq 1}\mathbb{E}_{p(s,z)}[f(s,z)]- \mathbb{E}_{p(s)p(z)}[f(s,z)],","where \(\|f\|_{L}\) denotes the Lipschitz constant for the function \(f:\mathcal{S}\times\mathcal{Z}\rightarrow\mathbb{R}\) under the given distance metric \(d\), _i","I_{\mathcal{W}}(S;Z) = \max_{f \in \text{Lip}_1} \mathbb{E}_{p(z)p(s)}[f(s,z)] - \mathbb{E}_{p(s,z)}[f(s,z)],","where the maximum is taken over all 1-Lipschitz functions $f: \mathcal{S} \times \mathcal{Z} \rightarrow \mathbb{R}$, providing a tractable dual form for optimizing the Wasserstein dependency measure."
ICLR_2024_oral_44,4,"_i.e._, \(\|f\|_{L}=\sup_{(s_{1},z_{1})\neq(s_{2},z_{2})}|f(s_{1},z_{1})-f(s_{2},z_{2})| /d((s_{1},z_{1}),(s_{2},z_{2}))\). Intuitively, \(f\) is a score function that assigns larger values to \((s,z)\) tuples sampled from the joint distribution and smaller values to \((s,z)\) tuples sampled independently from their marginal distributions. We note that Equation (3) is already a tractable objective, as we can jointly train a \(1\)-Lipschitz-constrained score function \(f(s,z)\) using gradient descent and a skill policy \(\pi(a|s,z)\) using RL, with the reward function being an empirical estimate of Equation (3), \(r(s,z)=f(s,z)-N^{-1}\sum_{i=1}^{N}f(s,z_{i})\), where \(z_{1},z_{2},\ldots,z_{N}\) are \(N\) independent random samples from the prior distribution \(p(z)\).

However, since sampling \(N\) additional \(z\)s for each data point is computationally demanding, we will further simplify the objective to enable more efficient learning. First, we consider the parameterization \(f(s,z)=\phi(s)^{\top}\psi(z)\) with \(\phi:\mathcal{S}\rightarrow\mathbb{R}^{D}\) and \(\psi:\mathcal{Z}\rightarrow\mathbb{R}^{D}\) with independent \(1\)-Lipschitz constraints1, which yields the following objective:

Footnote 1: While \(\|\phi\|_{L}\leq 1,\|\psi\|_{L}\leq 1\) is not technically equivalent to \(\|f\|_{L}\leq 1\), we use the former as it is more tractable. Also, we note that \(\|f\|_{L}\) can be upper-bounded in terms of \(\|\phi\|_{L}\), \(\|\psi\|_{L}\), \(\sup_{z}\|\phi(s)\|_{2}\), and \(\sup_{z}\|\psi(z)\|_{2}\) under \(d((s_{1},z_{1}),(s_{2},z_{2}))=(\sup_{s}\|\phi(s)\|_{2})\|\psi\|_{L}d(z_{1},z_{ 2})+(\sup_{z}\|\psi(z)\|_{2})\|\phi\|_{L}d(s_{1},s_{2})\).","I_{\mathcal{W}}(S;Z)\approx\sup_{\|\phi\|_{L}\leq 1,\|\psi\|_{L}\leq 1}\mathbb{E}_{p( s,z)}[\phi(s)^{\top}\psi(z)]-\mathbb{E}_{p(s)}[\phi(s)]^{\top}\mathbb{E}_{p(z)}[ \psi(z)].",,"I_{\mathcal{W}}(S;Z)=\sup_{\|\phi\|_{L}\leq 1,\|\psi\|_{L}\leq 1}\mathbb{E}_{p(s,z)}[\phi(s)^{\top}\psi(z)]-\mathbb{E}_{p(s)p(z)}[\phi(s)^{\top}\psi(z)].","This equation reformulates the Wasserstein dependency measure using a parameterization of the score function as the inner product between state and skill encoders, enabling more efficient optimization under Lipschitz constraints on the encoders."
ICLR_2024_oral_44,5,"Here, we note that the decomposition \(f(s,z)=\phi(s)^{\top}\psi(z)\) is _universal_; _i.e._, the expressiveness of \(f(s,z)\) is equivalent to that of \(\phi(s)^{\top}\psi(z)\) when \(D\to\infty\). The proof can be found in Appendix C.

Next, we consider a variant of the Wasserstein dependency measure that only depends on the last state: \(I_{\mathcal{W}}(S_{T};Z)\), similarly to VIC (Gregor et al., 2016). This allows us to further decompose the objective with a telescoping sum as follows:","I_{\mathcal{W}}(S_{T};Z)\approx\sup_{\|\phi\|_{L}\leq 1,\| \psi\|_{L}\leq 1}\mathbb{E}_{p(\tau,z)}[\phi(s_{T})^{\top}\psi(z)]-\mathbb{E}_{p( \tau)}[\phi(s_{T})]^{\top}\mathbb{E}_{p(z)}[\psi(z)] || =\sup_{\phi,\psi}\sum_{t=0}^{T-1}(\mathbb{E}_{p(\tau,z)}[( \phi(s_{t+1})-\phi(s_{t}))^{\top}\psi(z)]-\mathbb{E}_{p(\tau)}[\phi(s_{t+1})- \phi(s_{t})]^{\top}\mathbb{E}_{p(z)}[\psi(z)]),",,"I_{\mathcal{W}}(S_{T};Z)=\sup_{\|\phi\|_{L}\leq 1,\|\psi\|_{L}\leq 1}\sum_{t=0}^{T-1}\mathbb{E}_{p(s_{t},z)}[\phi(s_{t+1})^{\top}\psi(z)-\phi(s_{t})^{\top}\psi(z)].","This equation reformulates the Wasserstein dependency measure using a telescoping sum over state transitions, focusing on the temporal difference in the embedding space to encourage skills that maximize temporal distance."
ICLR_2024_oral_44,6,"nt. Finally, we set \(\psi(z)\) to \(z\). While this makes \(\psi\) less expressive, it allows us to derive the following concise objective:","I_{\mathcal{W}}(S_{T};Z)\approx\sup_{\|\phi\|_{L}\leq 1}\mathbb{E}_{p( \tau,z)}[\sum_{t=0}^{T-1}(\phi(s_{t+1})-\phi(s_{t}))^{\top}(z-\bar{z}) ],",where \(\bar{z}=\mathbb{E}_{p(z)}[z]\),"I_{\mathcal{W}}(S_{T};Z)\approx\sup_{\|\phi\|_{L}\leq 1}\sum_{t=0}^{T-1}(\mathbb{E}_{p(\tau,z)}[(\phi(s_{t+1})-\phi(s_{t}))^{\top}z]-\mathbb{E}_{p(\tau)}[\phi(s_{t+1})-\phi(s_{t})]^{\top}\mathbb{E}_{p(z)}[z]).",This final simplification leads to a practical and concise objective for maximizing the Wasserstein dependency measure by focusing on temporal differences in the learned state representations and linearly relating skills to the latent space.
ICLR_2024_oral_44,7,"\). Here, since we can always shift the prior distribution \(p(z)\) to have a zero mean, we can assume \(\bar{z}=0\) without loss of generality. This objective can now be easily maximized by jointly training \(\phi(s)\) and \(\pi(a|s,z)\) with \(r(s,z,s^{\prime})=(\phi(s^{\prime})-\phi(s))^{\top}z\) under the constraint \(\|\phi\|_{L}\leq 1\). Note that we do not need any additional random samples of \(z\), unlike Equation (3).

### Full Objective: Metric-Aware Abstraction (METRA)

So far, we have not specified the distance metric \(d\) for the Wasserstein distance in WDM (or equivalently for the Lipschitz constraint \(\|\phi\|_{L}\leq 1\)). Choosing an appropriate distance metric is crucial for learning a compact set of useful behaviors, because it determines the _priority_ by which the behaviors are learned within the capacity of \(\mathcal{Z}\). Previous metric-based skill discovery methods mostly employed the Euclidean distance (or its scaled variant) as a metric (He et al., 2022; Park et al., 2022; 2023b). However, they are not directly scalable to high-dimensional environments with pixel-based observations, in which the Euclidean distance is not necessarily meaningful.

In this work, we propose to use the _temporal distance_(Kaelbling, 1993; Hartikainen et al., 2020; Durugkar et al., 2021) between two states as a distance metric \(d_{\mathrm{temp}}(s_{1},s_{2})\), the minimum number of environment steps to reach \(s_{2}\) from \(s_{1}\). This provides a natural way to measure the distance between two states, as it only depends on the _inherent_ transition dynamics of the MDP, being invariant to the state representation and thus scalable to pixel-based environments. Using the temporal distance metric, we can rewrite Equation (7) as follows:","\sup_{\pi,\phi}\ \mathbb{E}_{p(\tau,z)}[\sum_{t=0}^{T-1}(\phi(s_{t+1})- \phi(s_{t}))^{\top}z]\ \ \mathrm{s.t.}\ \ \|\phi(s)-\phi(s^{\prime})\|_{2}\leq 1,\ \ \forall(s,s^{\prime})\in \mathcal{S}_{\mathrm{adj}},",where \(\mathcal{S}_{\mathrm{adj}}\) denotes the set of adjacent state pairs in the MDP,"I_{\mathcal{W}}(S_{T};Z)\approx\sup_{\|\phi\|_{L}\leq 1}\mathbb{E}_{p( \tau,z)}\left[\sum_{t=0}^{T-1}\left(\phi(s_{t+1})-\phi(s_{t})\right)^{\top}z \right],","This final simplified objective maximizes the temporal difference in state representations $\phi(s)$ scaled by the skill vector $z$, under a Lipschitz constraint, effectively encouraging skills to span the most temporally distant directions in the state space."
ICLR_2024_oral_45,1,"### State Space Models

A recent work (Gu et al., 2021) has introduced a novel Structured State Space Sequence model (S4). This model has shown superior performance in SL and SSL tasks, compared to common deep sequence models, including RNNs, convolutional neural networks (CNNs; lec (1998)), and Transformers. It outperforms them in terms of both computational efficiency (Gu et al., 2021) and the ability to model extremely long-range dependencies (Gu et al., 2020). S4 is a specific instance of state space models (SSMs), which can be efficiently trained by using specialized parameterization.

SSMs are derived from a linear dynamical system with control variable \(u(t)\in\mathbb{R}\) and observation variable \(y(t)\in\mathbb{R}\), utilizing state variables \(x(t)\in\mathbb{C}^{N}\) for a state size \(N\). The system is represented by the state matrix \(\mathbf{A}\in\mathbb{C}^{N\times N}\) and other matrices \(\mathbf{B}\in\mathbb{C}^{N\times 1}\), \(\mathbf{C}\in\mathbb{C}^{1\times N}\), and \(\mathbf{D}\in\mathbb{R}^{1\times 1}\):","x^{\prime}(t)=\mathbf{A}x(t)+\mathbf{B}u(t),\quad y(t)=\mathbf{C}x(t)+ \mathbf{D}u(t).",,"\begin{aligned}
\dot{x}(t) &= \mathbf{A}x(t) + \mathbf{B}u(t), \\
y(t) &= \mathbf{C}x(t) + \mathbf{D}u(t).
\end{aligned}","Continuous-time state space model equations with state variables $x(t)$, control input $u(t)$, and output $y(t)$ parameterized by matrices $\mathbf{A}, \mathbf{B}, \mathbf{C}, \mathbf{D}$."
ICLR_2024_oral_45,2,Note that these SSMs function on continuous sequences. They can be discretized by a step size \(\Delta\) to allow discrete recurrent representation:,"x_{n}=\bar{\mathbf{A}}x_{n-1}+\bar{\mathbf{B}}u_{n},\quad y_{n}=\bar{\mathbf{ C}}x_{n}+\bar{\mathbf{D}}u_{n},","where \(\bar{\mathbf{A}},\bar{\mathbf{B}},\bar{\mathbf{C}}\), and \(\bar{\mathbf{D}}\) are discrete-time parameters obtained from the continuous-time parameters and \(\Delta\) using methods like zero-order hold and bilinear technique (Smith et al",\begin{aligned} \bar{\mathbf{A}} &amp; = (\mathbf{I} + \Delta\mathbf{A}) \\ \bar{\mathbf{B}} &amp; = \Delta\mathbf{B} \\ \bar{\mathbf{C}} &amp; = \mathbf{C} \end{aligned},"Discretized state space model matrices using a step size $\Delta$, enabling discrete recurrent representation of the continuous system."
ICLR_2024_oral_45,3,"al., 2023). These representations are incorporated as a neural network layer, and each SSM is used to process a single dimension of the input sequence and map it to a single output dimension. This means that there are separate linear transformations for each input dimension, which are followed by a nonlinearity. This allows working with discrete sequence tasks, such as language modeling (Merity et al., 2016), speech classification (Warden, 2018), and pixel-level 1D image classification (Krizhevsky et al., 2009).

S4 model characterizes \(\mathbf{A}\) as a matrix with a diagonal plus low-rank (DPLR) structure (Gu et al., 2021). One benefit of this ""structured"" representation is that it helps preserve the sequence history;S4 employs HiPPO framework (Gu et al., 2020) to initialize the matrix \(\mathbf{A}\) with special DPLR matrices. This initialization grants the SSMs the ability to decompose \(u(t)\) into a set of infinitely long basis functions, enabling the SSMs to capture long-range dependencies. Further, to make S4 more practical on modern hardware, Gu et al. (2021) have reparameterized the mapping \(u_{1:T},x_{0}\to y_{1:T},x_{T}\) as a global convolution, referred to as the _convolution mode_, thereby avoiding sequential training (as in RNNs). This modification has made S4 faster to train, and as elaborated in Gu et al. (2021), S4 models can be thought of as a fusion of CNNs, RNNs, and classical SSMs. Smith et al. (2023) uses **parallel scan**(Blelloch, 1990) to compute \(u_{1:T},x_{0}\to y_{1:T},x_{1:T}\) as efficient as **convolution mode**.

S4 has demonstrated impressive empirical results on various established SL and SSL benchmarks involving long dependencies, and it outperforms Transformers (Vaswani et al., 2017; Dao et al., 2022) in terms of inference speed and memory consumption due to its recurrent inference mode. Moreover, some recent works have focused on understanding S4 models, as well as refining them and augmenting their capabilities (Gupta et al., 2022; Gu et al., 2022; Mehta et al., 2022; Gupta et al., 2022; Smith et al., 2023; Ma et al., 2023). We have provided additional details in Appendix B to explain this family of S4 models. For the sake of simplicity in this study, we will be referring to all the S4 model variations as ""SSMs"". It is worth highlighting that a few recent methods optimize the performance of SSMs by integrating them with Transformers (Fu et al., 2023; Zuo et al., 2022; Fathi et al., 2023). This enhances the SSMs by adding a powerful local attention-based inductive bias.

### From Imagination To Action

We frame a sequential decision-making problem as a partially observable Markov decision process (POMDP) with observations \(o_{t}\), scalar rewards \(r_{t}\), agent's actions \(a_{t}\), episode continuation flag \(c_{t}\), and discount factor \(\gamma\in(0,1)\), all following dynamics \(o_{t},r_{t},c_{t}\sim p(o_{t},r_{t},c_{t}\mid o_{<t},a_{<t})\). The goal of RL is to train a policy \(\pi\) that maximizes the expected value of the discounted return \(\mathbb{E}_{\pi}\big{[}\sum_{t\geq 0}\gamma^{t}r_{t}\big{]}\).

In MBRL, the agent learns a model of the environment's dynamics (i.e., the world model), through an iterative process of collecting data using a policy, training the world model on the accumulated data, and optimizing the policy through the world model (Sutton, 1990; Ha and Schmidhuber, 2018). The Dreamer agent (Hafner et al., 2019) and its subsequent versions (Hafner et al., 2020; 2023) have been impactful MBRL systems that learn the environment dynamics in a compact latent space and learn the policy entirely within that latent space. Dreamer agents consist of three primary components: the **world model**, which predicts the future outcomes of potential actions, the **critic**, which estimates the value of each state, and the **actor**, which learns to take optimal actions.

In Dreamer, an RNN-based architecture called Recurrent State-Space Model (RSSM), proposed by Hafner et al. (2019), serves as the core of the world model, and it can be described as follows. For every time step \(t\), it represents the latent state through the concatenation of deterministic state \(h_{t}\) and stochastic state \(z_{t}\). Here, \(h_{t}\) is updated using a Gated Recurrent Unit (GRU; Cho et al. (2014)), and then is utilized to compute \(z_{t}\), which incorporates information about the current observation \(o_{t}\) and is subsequently referred to as the posterior state. Additionally, the prior state \(\hat{z}_{t}\) which predicts \(z_{t}\) without access to \(o_{t}\) is computed using \(h_{t}\). By leveraging the latent state \((z_{t},h_{t})\), we can reconstruct various quantities such as \(o_{t}\), \(r_{t}\), and \(c_{t}\). The RSSM comprises three components: a sequence model (\(h_{t}=f_{\theta}(h_{t-1},z_{t-1},a_{t-1})\)), a representation model (\(z_{t}\sim q_{\theta}(z_{t}\mid h_{t},o_{t})\)), and a dynamics model (\(\hat{z}_{t}\sim p_{\theta}(\hat{z}_{t}\mid h_{t})\)), where \(a_{t-1}\) is the action at time step \(t-1\), and \(\theta\) denotes the combined parameter vector of all components. In addition to the RSSM, the world model has separate prediction heads for \(o_{t}\), \(r_{t}\), \(c_{t}\). Within the _imagination_ phase, it harnesses the RSSM to simulate trajectories. This is performed through an iterative computation of states \(\hat{z}_{t},h_{t}\) and actions \(\hat{a}_{t}\sim\pi(\hat{a}_{t}\mid\hat{z}_{t},h_{t})\) without the need for observations (except in the initial step). The sequences of \(\hat{z}_{1:T},h_{1:T},\hat{a}_{1:T}\) are used to train the actor and the critic. See Appendix D for more details.

## 3 Methodology

We introduce R2I (Recall to Imagine), which integrates SSMs in DreamerV3's world model, giving rise to what we term the Structured State-Space Model (S3M). The design of the S3M aims to achieve two primary objectives: capturing long-range relations in trajectories and ensuring fast computational performance in MBRL. S3M achieves the desired speed through parallel computation during training and recurrent mode in inference time, which enables quick generation of imagined trajectories. In Figure 1, a visual representation of R2I is provided, and we will now proceed to describe its design.

### World Model Details

**Non-recurrent representation model.** Our objective when updating the world model is to calculate S3M deterministic states \(h_{1:T}\) in parallel by simultaneously feeding all actions \(a_{t}\) and stochastic state \(z_{t}\), where \(T\) represents the length of the entire sequence. We aim to carry out this computation as \(h_{1:T},x_{1:T}=f_{\theta}((a_{1:T},z_{1:T}),x_{0})\) where \(x_{t}\) is a hidden state and \(f_{\theta}\) is a sequence model with a SSM network. To achieve this, prior access to all actions \(a_{1:T}\) and stochastic states \(z_{1:T}\) is required. However, we encounter a challenge due to the sequential nature of the relationship between the representation model \(q_{\theta}(z_{t}\mid h_{t},o_{t})\) and sequence model \(f_{\theta}(h_{t-1},z_{t-1},a_{t-1})\): at time step \(t\), the representation model's most recent output, denoted as \(z_{t-1}\), is fed into the sequence model, and the resulting output \(h_{t}\) is then used within the representation model to generate \(z_{t}\). Hence, similar to Chen et al. (2022); Micheli et al. (2023); Robine et al. (2023); Deng et al. (2023), by eliminating the dependency on \(h_{t}\) in the representation model, we transform it to a non-recurrent representation model \(q_{\theta}(z_{t}\mid o_{t})\). This modification allows us to compute the posterior samples independently for each time step, enabling simultaneous computation for all time steps. By utilizing a parallelizable function \(f_{\theta}\), we can then obtain \(h_{1:T}\) in parallel. Appendix M includes a systematic analysis to investigate how this modification impacts the performance of the DreamerV3 across a diverse set of tasks. The results indicate that transforming \(q_{\theta}(z_{t}\mid o_{t},h_{t})\) to \(q_{\theta}(z_{t}\mid o_{t})\) does not hurt the performance.

**Architecture details.** Inspired by Dreamer, R2I's world model consists of a representation model, a dynamics model, and a sequence model (together forming S3M). In addition to that, there are three prediction heads: an observation predictor \(p_{\theta}(\hat{o}_{t}\mid z_{t},h_{t})\), a reward predictor \(p_{\theta}(\hat{r}_{t}\mid z_{t},h_{t})\), and an episode continuation predictor \(p_{\theta}(\hat{o}_{t}\mid z_{t},h_{t})\). At each time step, S3M processes a pair of \((a_{t},z_{t})\) to output the deterministic state \(h_{t}\). Inside, it operates over the hidden state \(x_{t}\), so it can be defined as \(h_{t},x_{t}=f_{\theta}((a_{t-1},z_{t-1}),x_{t-1})\). Specifically, \(f_{\theta}\) is composed of multiple layers of SSMs, each one calculating outputs according to Equation 2. The outputs are then passed to GeLU (Hendrycks and Gimpel, 2023), which is followed by a fully-connected GLU transformation (Dauphin et al., 2017), and finally by a LayerNorm (Ba et al., 2016). This follows the architecture outlined by Smith et al. (2023). The deterministic state \(h_{t}\) is the output from the final SSM layer. The set of all SSM layer hidden states is denoted \(x_{t}\). See Appendix B.1 for SSMs design details. In image-based environments, we leverage a CNN encoder for \(q_{\theta}(z_{t}\mid o_{t})\) and a CNN decoder for \(p_{\theta}(\hat{o}_{t}\mid z_{t},h_{t})\). In contrast, in tabular environments, both \(q_{\theta}(z_{t}\mid o_{t})\) and \(p_{\theta}(\hat{o}_{t}\mid z_{t},h_{t})\) are MLPs. We include the details on network widths, depths, and other hyperparameters in Appendix E.

**Training details.** R2I optimizes the following objective:","\mathcal{L}(\theta)=\operatorname*{\mathbb{E}}_{z_{1:T}\sim q_{\theta}}\sum_{ t=1}^{T}\mathcal{L}^{\text{pred}}(\theta,h_{t},o_{t},r_{t},c_{t},z_{t})+ \mathcal{L}^{\text{rep}}(\theta,h_{t},o_{t})+\mathcal{L}^{\text{dyn}}(\theta,h _{t},o_{t})",,"h_{t}, x_{t} = f_{\theta}\left((a_{t-1}, z_{t-1}), x_{t-1}\right) = \text{LayerNorm}\left(\text{GLU}\left(\text{GeLU}\left(\text{SSM}_{\theta}^{L}\left(\cdots \text{SSM}_{\theta}^{1}\left(\begin{bmatrix}a_{t-1} \\ z_{t-1}\end{bmatrix}\right)\cdots\right)\right)\right)\right)","This equation defines the deterministic state $h_t$ and hidden state $x_t$ computation in the Structured State-Space Model (S3M), where $f_{\theta}$ consists of $L$ stacked SSM layers followed by GeLU activation, GLU transformation, and LayerNorm, processing the concatenated input $(a_{t-1}, z_{t-1})$ from previous actions $a_{t-1}$ and stochastic states $z_{t-1}$."
ICLR_2024_oral_45,4,"Figure 1: Graphical representation of R2I. **(Left)** The world model encodes past experiences, transforming observations and actions into compact latent states. Reconstructing the trajectories serves as a learning signal for shaping these latent states. **(Right)** The policy learns from trajectories based on latent states imagined by the world model. The representation corresponds to the full state policy, and we have omitted the critic for the sake of simplifying the illustration.","\mathcal{L}^{\text{pred}}(\theta,h_{t},o_{t},r_{t},c_{t},z_{t}) =-\beta_{\text{pred}}(\ln p_{\theta}(o_{t}\mid z_{t},h_{t})+\ln p_{ \theta}(r_{t}\mid z_{t},h_{t})+\ln p_{\theta}(c_{t}\mid z_{t},h_{t})) || \mathcal{L}^{\text{dyn}}(\theta,h_{t},o_{t}) =\beta_{\text{dyn}}\max(1,\text{KL}[\lessdot\circ g(q_{\theta}(z_ {t}\mid o_{t}))\parallel p(z_{t}\mid h_{t})\mid)]",,"\mathcal{L}^{\text{pred}}(\theta,h_{t},o_{t},r_{t},c_{t},z_{t}) = -\log p_{\theta}(\hat{o}_{t}\mid z_{t},h_{t}) - \log p_{\theta}(\hat{r}_{t}\mid z_{t},h_{t}) - \log p_{\theta}(\hat{c}_{t}\mid z_{t},h_{t})","The prediction loss term $\mathcal{L}^{\text{pred}}$ measures the negative log-likelihood of reconstructing the observation $\hat{o}_t$, reward $\hat{r}_t$, and continuation flag $\hat{c}_t$ given the latent state $z_t$ and deterministic state $h_t$."
ICLR_2024_oral_45,5,(5),"\mathcal{L}^{\text{rep}}(\theta,h_{t},o_{t}) =\beta_{\text{rep}}\max(1,\text{KL}[\quad q_{\theta}(z_{t}\mid o_ {t})\parallel\lessdot\circ g(p(z_{t}\mid h_{t}))\,])",,"\mathcal{L}^{\text{rep}}(\theta,h_{t},o_{t}) =\beta_{\text{rep}}\text{KL}[q_{\theta}(z_{t}\mid o_{t})\parallel p(z_{t}\mid h_{t})]",The representation loss encourages the posterior distribution $q_{\theta}(z_{t}\mid o_{t})$ to stay close to the prior distribution $p(z_{t}\mid h_{t})$ by minimizing the Kullback-Leibler (KL) divergence between them.
ICLR_2024_oral_45,6,(6),"h_{1:T},x_{1:T} =f_{\theta}((a_{1:T},z_{1:T}),x_{0})",,"\mathcal{L}^{\text{dyn}}(\theta,h_{t},o_{t}) =\beta_{\text{dyn}}\max\left(1,\text{KL}\left[q_{\theta}(z_{t}\mid o_{t}) \parallel p(z_{t}\mid h_{t})\right]\right)","This equation defines the dynamics loss term, which encourages the prior distribution $p(z_t \mid h_t)$ to align with the posterior distribution $q_{\theta}(z_t \mid o_t)$, using the Kullback-Leibler (KL) divergence as a measure of discrepancy."
ICLR_2024_oral_49,1,"### AutoEncoding for Tabular Data

Tabular data is highly structured of mixed-type column features, with different columns having distinct meanings and being highly dependent on each other. These characteristics make it challenging to design an approximate encoder to model and effectively utilize the rich relationships between columns. Motivated by the successes of Transformers in classification/regression of tabular data (Gorishniy et al., 2021), we first learn a unique tokenizer for each column, and then the token(column)-wise representations are fed into a Transformer for capturing the intricate relationships among columns.

**Feature Tokenizer**. The feature tokenizer converts each column (both numerical and categorical) into a \(d\)-dimensional vector. First, we use one-hot encoding to pre-process categorical features, i.e., \(x_{i}^{\mathrm{cat}}\Rightarrow\mathbf{x}_{i}^{\mathrm{oh}}\in\mathbb{R}^{1\times C _{i}}\). Each record is represented as \(\mathbf{x}=[\mathbf{x}^{\mathrm{num}},\mathbf{x}_{1}^{\mathrm{oh}},\cdots,\mathbf{x}_{M_{ \mathrm{cat}}}^{\mathrm{oh}}]\in\mathbb{R}^{M_{\mathrm{num}}+\sum_{i=1}^{M_{ \mathrm{cat}}}C_{i}}\). Then, we apply a linear transformation for numerical columns and create an embedding lookup table for categorical columns, where each category is assigned a learnable \(d\)-dimensional vector, i.e.,","\mathbf{e}_{i}^{\mathrm{num}}=x_{i}^{\mathrm{num}}\cdot\mathbf{w}_{i}^{\mathrm{num}} +\mathbf{b}_{i}^{\mathrm{num}},\ \ \mathbf{e}_{i}^{\mathrm{cat}}=\mathbf{x}_{i}^{\mathrm{oh}}\cdot\mathbf{W}_{i}^{\mathrm{cat} }+\mathbf{b}_{i}^{\mathrm{cat}},","where \(\mathbf{w}_{i}^{\mathrm{num}},\mathbf{b}_{i}^{\mathrm{num}},\mathbf{b}_{i}^{\mathrm{cat}} \in\mathbb{R}^{1\times d}\), \(\mathbf{W}_{i}^{\mathrm{cat}}\in\mathbb{R}^{C_{i}\times d}\) are learnable parameters of the tokenizer, \(\mathbf{e}_{i}^{\mathrm{num}},\mathbf{e}_{i}^{\mathrm{cat}}\in\mathbb{R}^{1\times d}\)","\begin{aligned}
    \mathbf{z}_{j}^{\mathrm{num}} &= \mathbf{W}_{j}^{\mathrm{num}} \cdot x_{j}^{\mathrm{num}} + \mathbf{b}_{j}^{\mathrm{num}}, \quad \text{for numerical column } j, \\
    \mathbf{z}_{i}^{\mathrm{cat}} &= \mathbf{E}_{i} \cdot \mathbf{x}_{i}^{\mathrm{oh}}, \quad \text{for categorical column } i,
\end{aligned}","Equation 1 defines the transformation of numerical and categorical columns into d-dimensional vectors using learnable parameters, where numerical features undergo a linear transformation and categorical features are mapped via an embedding lookup table."
ICLR_2024_oral_49,2,"\). Now, each record is expressed as the stack of the embeddings of all columns","\mathbf{E}=[\mathbf{e}_{1}^{\mathrm{num}},\cdots,\mathbf{e}_{M_{\mathrm{num}}}^{\mathrm{ num}},\mathbf{e}_{1}^{\mathrm{cat}},\cdots,\mathbf{e}_{M_{\mathrm{cat}}}^{\mathrm{cat}}] \in\mathbb{R}^{M\times d}.",,"\mathbf{E} = [\mathbf{e}_{1}^{\mathrm{num}}, \cdots, \mathbf{e}_{M_{\mathrm{num}}}^{\mathrm{num}}, \mathbf{e}_{1}^{\mathrm{cat}}, \cdots, \mathbf{e}_{M_{\mathrm{cat}}}^{\mathrm{cat}}] \in \mathbb{R}^{(M_{\mathrm{num}} + M_{\mathrm{cat}}) \times d},","where $\mathbf{E}$ represents the combined embedding matrix of all numerical and categorical columns for a given record, structured as a matrix with rows corresponding to column embeddings and columns representing the $d$-dimensional feature space."
ICLR_2024_oral_49,3,"**Transformer Encoding and Decoding**. As with typical VAEs, we use the encoder to obtain the mean and log variance of the latent variable. Then, we acquire the latent embeddings with the reparameterization tricks. The latent embeddings are then passed through the decoder to obtain the reconstructed token matrix \(\hat{\mathbf{E}}\in\mathbb{R}^{M\times d}\). The detailed architectures are in Appendix D.

**Detokenizer**. Finally, we apply a detokenizer to the recovered token representation of each column to reconstruct the column values. The design of the detokenizer is symmetrical to that of the tokenizer:","\begin{split}&\hat{x}_{i}^{\mathrm{num}}=\hat{\mathbf{e}}_{i}^{ \mathrm{num}}\cdot\hat{\mathbf{w}}_{i}^{\mathrm{num}}+\hat{b}_{i}^{\mathrm{num}}, \ \ \hat{\mathbf{e}}_{i}^{\mathrm{oh}}=\mathrm{Softmax}(\hat{\mathbf{e}}_{i}^{\mathrm{cat}} \cdot\hat{\mathbf{W}}_{i}^{\mathrm{cat}}+\hat{\mathbf{b}}_{i}^{\mathrm{cat}}),\\ &\hat{\mathbf{x}}=[\hat{x}_{1}^{\mathrm{num}},\cdots,\hat{x}_{M_{ \mathrm{num}}}^{\mathrm{num}},\hat{\mathbf{x}}_{1}^{\mathrm{oh}},\cdots,\hat{\mathbf{x} }_{M_{\mathrm{cat}}}^{\mathrm{oh}}],\end{split}","where \(\hat{\mathbf{w}}_{i}^{\mathrm{num}}\in\mathbb{R}^{d\times 1},\hat{b}_{i}^{ \mathrm{num}}\in\mathbb{R}^{1\times 1}\), \(\mathbf{W}_{i}^{\mathrm{cat}}\in\mathbb{R}^{d\times C_{i}},\hat{\mathbf{b}}_{i}^{ \mathrm{cat}}\in\mathbb{R}^{1\times C_{i}}\) are detokenizer's parameters","\hat{x}_{i}^{\mathrm{num}}=\mathrm{MLP}_{i}^{\mathrm{num}}(\hat{\mathbf{e}}_{i}^{\mathrm{num}}),\ \ \hat{\mathbf{x}}_{i}^{\mathrm{oh}}=\mathrm{Softmax}(\mathrm{MLP}_{i}^{\mathrm{cat}}(\hat{\mathbf{e}}_{i}^{\mathrm{cat}})),","where $\mathrm{MLP}_{i}^{\mathrm{num}}$ and $\mathrm{MLP}_{i}^{\mathrm{cat}}$ are column-specific multi-layer perceptrons for reconstructing numerical and categorical features, respectively."
ICLR_2024_oral_49,4,"rs.

**Training with adaptive weight coefficient.** The VAE model is usually learned with the classical ELBO loss function, but here we use \(\beta\)-VAE (Higgins et al., 2016), where a coefficient \(\beta\) balances the importance of the reconstruction loss and KL-divergence loss","\mathcal{L}=\ell_{\mathrm{recon}}(\mathbf{x},\hat{\mathbf{x}})+\beta\ell_{\mathrm{kl}}.",,"\mathcal{L}_{\beta} = \mathbb{E}_{q(z|x)}[\log p(x|z)] - \beta \cdot D_{\text{KL}}(q(z|x) \| p(z)),",where $\beta$ controls the trade-off between reconstruction accuracy and latent space regularization in the $\beta$-VAE framework.
ICLR_2024_oral_49,5,"\(\ell_{\mathrm{recon}}\) is the reconstruction loss between the input data and the reconstructed one, and \(\ell_{\mathrm{kl}}\) is the KL divergence loss that regularizes the mean and variance of the latent space. In the vanilla VAE model, \(\beta\) is set to be \(1\) because the two loss terms are equally important to generate high-quality synthetic data from Gaussian noises. However, in our model, \(\beta\) is expected to be smaller, as we do not require the distribution of the embeddings to precisely follow a standard Gaussian distribution because we have an additional diffusion model. Therefore, we propose to adaptively schedule the scale of \(\beta\) in the training process, encouraging the model to achieve lower reconstruction error while maintaining an appropriate embedding shape.

With an initial (maximum) \(\beta=\beta_{\mathrm{max}}\), we monitor the epoch-wise reconstruction loss \(\ell_{\mathrm{recon}}\). When \(\ell_{\mathrm{recon}}\) fails to decrease for a predefined number of epochs (which indicates that the KL-divergence dominates the overall loss), the weight is scheduled by \(\beta=\lambda\beta,\lambda<1\). This process continues until \(\beta\) approaches a predefined minimum value \(\beta_{\mathrm{min}}\). This strategy is simple yet very effective, and we empirically justify the effectiveness of the design in Section 4.

### Score-based Generative Modeling in the Latent Space

**Training and sampling via denoising.** After the VAE model is well-learned, we extract the latent embeddings through the encoder and flatten the encoder's output as \(\mathbf{z}=\mathrm{Flatten}(\mathrm{Encoder}(\mathbf{x}))\in\mathbb{R}^{1\times Md}\) such that the embedding of a record is a vector rather than a matrix. To learn the underlyingdistribution of embeddings \(p(\mathbf{z})\), we consider the following forward diffusion process and reverse sampling process (Song et al., 2021b; Karras et al., 2022):","\mathbf{z}_{t} =\mathbf{z}_{0}+\sigma(t)\mathbf{\varepsilon},\ \mathbf{\varepsilon}\sim \mathcal{N}(\mathbf{0},\mathbf{I}), (\text{Forward Process}) || \mathrm{d}\mathbf{z}_{t} =-2\hat{\sigma}(t)\sigma(t)\nabla_{\mathbf{z}_{t}}\log p(\mathbf{z}_{t}) \mathrm{d}t+\sqrt{2\hat{\sigma}(t)\sigma(t)}\mathrm{d}\mathbf{\omega}_{t}, (\text{Reverse Process})",,"\begin{split}&q(\mathbf{z}_{t}|\mathbf{z}_{t-1})=\mathcal{N}(\mathbf{z}_{t};\sqrt{1-\beta_{t}}\mathbf{z}_{t-1},\beta_{t}\mathbf{I}),\ \ t=1,\cdots,T,\\ &p_{\theta}(\mathbf{z}_{t-1}|\mathbf{z}_{t})=\mathcal{N}(\mathbf{z}_{t-1};\mu_{\theta}(\mathbf{z}_{t},t),\sigma_{t}^{2}\mathbf{I}),\end{split}","where the forward process gradually adds noise to the latent embedding $\mathbf{z}$ over $T$ steps, and the reverse process learns to denoise it back to the original distribution using a neural network parameterized by $\theta$."
ICLR_2024_oral_49,6,"el. In the reverse process, \(\nabla_{\mathbf{z}_{t}}\log p_{t}(\mathbf{z}_{t})\) is the score function of \(\mathbf{z}_{t}\), and \(\mathbf{\omega}_{t}\) is the standard Wiener process. The training of the diffusion model is achieved via denoising score matching (Karras et al., 2022):","\mathcal{L}=\mathbb{E}_{\mathbf{z}_{0}\sim p(\mathbf{z}_{0})}\mathbb{E}_{\mathbf{t}\sim p (t)}\mathbb{E}_{\mathbf{\varepsilon}\sim\mathcal{N}(\mathbf{0},\mathbf{I})}\|\mathbf{ \varepsilon}_{\mathbf{\theta}}(\mathbf{z}_{t},t)-\mathbf{\varepsilon})\|_{2}^{2},\ \ \text{where}\ \mathbf{z}_{t}=\mathbf{z}_{0}+\sigma(t)\mathbf{\varepsilon},",where \(\mathbf{\epsilon}_{\theta}\) is a neural network (named denoising function) to approximate the Gaussian noise using the perturbed data \(\mathbf{x}_{t}\) and the time \(t\),"\mathcal{L}_{\mathrm{diff}} = \mathbb{E}_{t,\mathbf{z}_{0},\mathbf{\varepsilon}}\left[\left\|\mathbf{s}_{\theta}(\mathbf{z}_{t},t)-\mathbf{\varepsilon}\right\|^{2}\right],","where $\mathbf{s}_{\theta}(\mathbf{z}_{t},t)$ is the neural network that estimates the score function $\nabla_{\mathbf{z}_{t}}\log p_{t}(\mathbf{z}_{t})$, and the loss minimizes the mean squared error between the predicted score and the true noise $\mathbf{\varepsilon}$."
ICLR_2024_oral_52,1,"## 3 Background

The foundation of our model lies in estimating the joint distribution of the sources \(p(\mathbf{x}_{1},\ldots,\mathbf{x}_{N})\). Our approach is generative because we model an unconditional distribution (the prior). The different tasks are then solved at inference time, exploiting the prior.

We employ a diffusion-based (Sohl-Dickstein et al., 2015; Ho et al., 2020) generative model trained via denoising score-matching (Song and Ermon, 2019) to learn the prior. Specifically, we present our formalism by utilizing the notation and assumptions established in (Karras et al., 2022). The central idea of score-matching (Hyvarinen, 2005; Kingma and LeCun, 2010; Vincent, 2011) is to approximate the ""score"" function of the target distribution \(p(\mathbf{x})\), namely \(\nabla_{\mathbf{x}}\log p(\mathbf{x})\), rather than the distribution itself. To effectively approximate the score in sparse data regions, denoising diffusion methods introduce controlled noise to the data and learn to remove it. Formally, the data distribution is perturbed with a Gaussian perturbation kernel:","p(\mathbf{x}(t)\mid\mathbf{x}(0))=\mathcal{N}(\mathbf{x}(t);\mathbf{x}(0), \sigma^{2}(t)\mathbf{I})\,,",,"\begin{equation}
    q(\mathbf{x}_t \mid \mathbf{x}_0) = \mathcal{N}(\mathbf{x}_t; \sqrt{\alpha_t} \mathbf{x}_0, (1 - \alpha_t) \mathbf{I}),
\end{equation}",A forward diffusion process that perturbs the data distribution using a Gaussian perturbation kernel parameterized by $\alpha_t$.
ICLR_2024_oral_52,2,"Figure 2: **Inference tasks with MSDM.** Oblique lines represent the presence of noise in the signal, decreasing from left to right, with the highest noise level at time \(T\) when we start the sampling procedure. _Top-left:_ We generate all stems in a mixture, obtaining a total generation. _Bottom-left:_ We perform partial generation (source imputation) by fixing the sources \(\mathbf{x}_{1}\) (Bass) and \(\mathbf{x}_{3}\) (Piano) and generating the other two sources \(\hat{\mathbf{x}}_{2}(0)\) (Drums) and \(\hat{\mathbf{x}}_{4}(0)\) (Guitar). We denote with \(\mathbf{x}_{1}(t)\) and \(\mathbf{x}_{3}(t)\), the noisy stems obtained from \(\mathbf{x}_{1}\) and \(\mathbf{x}_{3}\) via the perturbation kernel in Eq. (1). _Right:_ We perform source separation by conditioning the prior with a mixture \(\mathbf{y}\), following Algorithm 1.

where the parameter \(\sigma(t)\) regulates the degree of noise added to the data. Following the authors in (Karras et al., 2022), we consider an optimal schedule given by \(\sigma(t)=t\). With that choice of \(\sigma(t)\), the forward evolution of a data point \(\mathbf{x}(t)\) in time is described by a probability flow ODE (Song et al., 2021):","\mathrm{d}\mathbf{x}(t)=-\sigma(t)\nabla_{\mathbf{x}(t)}\log p(\mathbf{x}(t)) \,\mathrm{d}t\,.",,"\frac{d\mathbf{x}(t)}{dt} = -\frac{1}{2}\nabla_{\mathbf{x}}\log p(\mathbf{x}(t))\,.",This equation describes the probability flow ODE governing the forward evolution of a data point in time under the chosen noise schedule.
ICLR_2024_oral_52,3,"For \(t=T>>0\), a data point \(\mathbf{x}(T)\) is approximately distributed according to a Gaussian distribution \(\mathcal{N}(\mathbf{x}(t);\mathbf{0},\sigma^{2}(T)\mathbf{I})\), from which sampling is straightforward. Eq. (2) can be inverted in time, resulting in the following backward ODE that describes the denoising process:","\mathrm{d}\mathbf{x}(t)=\sigma(t)\nabla_{\mathbf{x}(t)}\log p(\mathbf{x}(t)) \,\mathrm{d}t\,.",,"\mathrm{d}\mathbf{x}(t)=\sigma(t)\nabla_{\mathbf{x}(t)}\log p(\mathbf{x}(t))\,\mathrm{d}t\,.","The backward ODE describes the denoising process, inverting the forward evolution of the data point to recover the original signal from the noisy input."
ICLR_2024_oral_52,4,"Sampling can be performed integrating Eq. (3) with a standard ODE solver, starting from an initial (noisy) sample drawn from \(\mathcal{N}(\mathbf{x}(t);\mathbf{0},\sigma^{2}(T)\mathbf{I})\). The score function, is approximated by a neural network \(S^{\theta}(\mathbf{x}(t),\sigma(t))\), minimizing the following score-matching loss:","\mathbb{E}_{t\sim\mathcal{U}([0,T])}\mathbb{E}_{\mathbf{x}(0)\sim p(\mathbf{x }(0))}\mathbb{E}_{\mathbf{x}(t)\sim p(\mathbf{x}(t)|\mathbf{x}(0))}\| \mathcal{S}^{\theta}(\mathbf{x}(t),\sigma(t))-\nabla_{\mathbf{x}(t)}\log p (\mathbf{x}(t)\mid\mathbf{x}(0))\|_{2}^{2}.",,"\mathcal{L}=\mathbb{E}_{t,\mathbf{x}(0),\mathbf{x}(t)}\left[\left\Vert \nabla_{\mathbf{x}(t)}\log p(\mathbf{x}(t)\mid\mathbf{x}(0)) - S^{\theta}(\mathbf{x}(t),\sigma(t)) \right\Vert^{2}\right]\,,",This equation defines the score-matching loss used to train the neural network $S^{\theta}$ by minimizing the discrepancy between the true score and the network's prediction.
ICLR_2024_oral_52,5,"By expanding \(p(\mathbf{x}(t)\mid\mathbf{x}(0))\) with Eq. (1), the score-matching loss simplifies to:","\mathbb{E}_{t\sim\mathcal{U}([0,T])}\mathbb{E}_{\mathbf{x}(0)\sim p(\mathbf{x }(0))}\mathbb{E}_{\mathbf{x}\sim\mathcal{N}(\mathbf{0},\sigma^{2}(t)\mathbf{I })}\|D^{\theta}(\mathbf{x}(0)+\epsilon,\sigma(t))-\mathbf{x}(0)\|_ {2}^{2}\,,","where we define \(S^{\theta}(\mathbf{x}(t),\sigma(t))=:(D^{\theta}(\mathbf{x}(t),\sigma(t))- \mathbf{x}(t))/\sigma^{2}(t)\)","\mathbb{E}_{t\sim\mathcal{U}([0,T])}\mathbb{E}_{\mathbf{x}(0)\sim p(\mathbf{x}(0))}\left\|\mathcal{S}^{\theta}(\mathbf{x}(t),\sigma(t)) + \frac{\mathbf{x}(t) - \mathbf{x}(0)}{\sigma^{2}(t)}\right\|_{2}^{2}.",This equation represents the simplified score-matching loss after substituting the Gaussian perturbation kernel from Equation 1 into the original loss formulation.
ICLR_2024_oral_52,6,"\).

## 4 Method

### Multi-Source Audio Diffusion Models

In our setup, we have \(N\) distinct source waveforms \(\{\mathbf{x}_{1},\ldots,\mathbf{x}_{N}\}\) with \(\mathbf{x}_{n}\in\mathbb{R}^{D}\) for each \(n\). The sources coherently sum to a mixture \(\mathbf{y}=\sum_{n=1}^{N}\mathbf{x}_{n}\). We also use the aggregated form \(\mathbf{x}=(\mathbf{x}_{1},\ldots,\mathbf{x}_{N})\in\mathbb{R}^{N\times D}\). In this setting, multiple tasks can be performed: one may generate a consistent mixture \(\mathbf{y}\) or separate the individual sources \(\mathbf{x}\) from a given mixture \(\mathbf{y}\). We refer to the first task as _generation_ and the second as _source separation_. A subset of sources can also be fixed in the generation task, and the others can be generated consistently. We call this task _partial generation_ or _source imputation_. Our key contribution is the ability to perform all these tasks simultaneously by training a single multi-source diffusion model (MSDM), capturing the prior \(p(\mathbf{x}_{1},\ldots,\mathbf{x}_{N})\). The model, illustrated in Figure 1, approximates the noisy score function:","\nabla_{\mathbf{x}(t)}\log p(\mathbf{x}(t))=\nabla_{(\mathbf{x}_{1}(t), \ldots,\mathbf{x}_{N}(t))}\log p(\mathbf{x}_{1}(t),\ldots,\mathbf{x}_{N}(t))\,,",,"\nabla_{\mathbf{x}(t)}\log p(\mathbf{x}(t)) \approx \mathcal{S}^{\theta}(\mathbf{x}(t), \sigma(t))\,,",where the score function is approximated by the neural network $\mathcal{S}^{\theta}$ as part of the multi-source diffusion model (MSDM) for capturing the prior distribution of the sources.
ICLR_2024_oral_52,7,with a neural network:,"S^{\theta}(\mathbf{x}(t),\sigma(t)):\mathbb{R}^{N\times D}\times\mathbb{R} arrow\mathbb{R}^{N\times D}\,,","where \(\mathbf{x}(t)=(\mathbf{x}_{1}(t),\ldots,\mathbf{x}_{N}(t))\) denotes the sources perturbed with the Gaussian kernel in Eq","\mathcal{S}^{\theta}(\mathbf{x}(t),\sigma(t))=(\mathcal{S}_{1}^{\theta}( \mathbf{x}(t),\sigma(t)),\ldots,\mathcal{S}_{N}^{\theta}(\mathbf{x}(t), \sigma(t)))\,,","where each component $\mathcal{S}_{n}^{\theta}(\mathbf{x}(t),\sigma(t))$ corresponds to the score estimate for the $n$-th source waveform given the noisy input $\mathbf{x}(t)$ and noise level $\sigma(t)$."
ICLR_2024_oral_52,8,"Eq. (1). We describe the three tasks (illustrated in Figure 2) using the prior distribution:

* _Total Generation._ This task requires generating a plausible mixture \(\mathbf{y}\). It can be achieved by sampling the sources \(\{\mathbf{x}_{1},...,\mathbf{x}_{N}\}\) from the prior distribution and summing them to obtain the mixture \(\mathbf{y}\).
* _Partial Generation._ Given a subset of sources, this task requires generating a plausible accompaniment. We define the subset of fixed sources as \(\mathbf{x}_{\mathcal{I}}\) and generate the remaining sources \(\mathbf{x}_{\mathcal{\overline{I}}}\) by sampling from the conditional distribution \(p(\mathbf{x}_{\mathcal{\overline{I}}}\mid\mathbf{x}_{\mathcal{I}})\).
* _Source Separation._ Given a mixture \(\mathbf{y}\), this task requires isolating the individual sources that compose it. It can be achieved by sampling from the posterior distribution \(p(\mathbf{x}\mid\mathbf{y})\).

### Inference

The three tasks of our method are solved during inference by discretizing the backward Eq. (3). Although different tasks require distinct score functions, they all originate directly from the prior score function in Eq. (4). We analyze each of these score functions in detail. For more details on the discretization method, refer to Section C.3.

#### 4.2.1 Total Generation

The total generation task is performed by sampling from Eq. (3) using the score function in Eq. (4). The mixture is then obtained by summing over all the generated sources.

#### 4.2.2 Partial Generation

In the partial generation task, we fix a subset of source indices \(\mathcal{I}\subset\{1,\ldots,N\}\) and the relative sources \(\mathbf{x}_{\mathcal{I}}:=\left\{\mathbf{x}_{n}\right\}_{n\in\mathcal{I}}\). The goal is to generate the remaining sources \(\mathbf{x}_{\mathcal{I}}:=\left\{\mathbf{x}_{n}\right\}_{n\in\overline{ \mathcal{I}}}\) consistently, where \(\overline{\mathcal{I}}=\{1,\ldots,N\}-\mathcal{I}\). To do so, we estimate the gradient of the conditional distribution:",\nabla_{\mathbf{x}_{\overline{\mathcal{I}}}(t)}\log p(\mathbf{x}_{\overline{ \mathcal{I}}}(t)\mid\mathbf{x}_{\mathcal{I}}(t)).,,,
ICLR_2024_oral_52,9,"This falls into the setting of imputation or, as it is more widely known in the image domain, inpainting. We approach imputation using the method in (Song et al., 2021). The gradient in Eq. (5) is approximated as follows:","\nabla_{\mathbf{x}_{\overline{\mathcal{I}}}(t)}\log p([\mathbf{x}_{\overline{ \mathcal{I}}}(t),\hat{\mathbf{x}}_{\mathcal{I}}(t)])\,,","where \(\hat{\mathbf{x}}_{\mathcal{I}}\) is a sample from the forward process: \(\hat{\mathbf{x}}_{\mathcal{I}}(t)\sim\mathcal{N}(\mathbf{x}_{\mathcal{I}}(t) ;\mathbf{x}_{\mathcal{I}}(0),\sigma(t)^{2}\mathbf{I})\)","\nabla_{\mathbf{x}_{\overline{\mathcal{I}}}(t)}\log p(\mathbf{x}_{\overline{ \mathcal{I}}}(t)\mid\mathbf{x}_{\mathcal{I}}(t)) = \nabla_{\mathbf{x}_{\overline{\mathcal{I}}}(t)}\log p(\mathbf{x}_{1}(t),\ldots,\mathbf{x}_{N}(t)) - \nabla_{\mathbf{x}_{\overline{\mathcal{I}}}(t)}\log p(\mathbf{x}_{\mathcal{I}}(t)).",This equation decomposes the conditional score into the gradient of the joint distribution and the gradient of the marginal distribution of the fixed sources.
ICLR_2024_oral_52,10,"\). The square bracket operator denotes concatenation. Approximating the score function, we write:","\nabla_{\mathbf{x}_{\overline{\mathcal{I}}}(t)}\log p(\mathbf{x}_{\overline{ \mathcal{I}}}(t)\mid\mathbf{x}_{\mathcal{I}}(t))\approx S_{\overline{ \mathcal{I}}}^{\theta}([\mathbf{x}_{\overline{\mathcal{I}}}(t),\hat{\mathbf{x }}_{\mathcal{I}}(t)],\sigma(t))\,,",where \(S_{\overline{\mathcal{I}}}^{\theta}\) denotes the entries of the score network corresponding to the sources indexed by \(\overline{\mathcal{I}}\),"\nabla_{\mathbf{x}_{\overline{\mathcal{I}}}(t)}\log p([\mathbf{x}_{\overline{\mathcal{I}}}(t),\hat{\mathbf{x}}_{\mathcal{I}}(t)]) \approx S^{\theta}([\mathbf{x}_{\overline{\mathcal{I}}}(t),\hat{\mathbf{x}}_{\mathcal{I}}(t)], \sigma(t))_{\overline{\mathcal{I}}}\,,",Approximation of the conditional score function for the remaining sources in the partial generation task using the trained score network and the concatenation of the noisy fixed sources and the current estimate of the remaining sources.
ICLR_2024_oral_52,11,"\).

#### 4.2.3 Source Separation

We view source separation as a specific instance of conditional generation, where we condition the generation process on the given mixture \(\mathbf{y}=\mathbf{y}(0)\). This requires computing the score function of the posterior distribution:","\nabla_{\mathbf{x}(t)}\log p(\mathbf{x}(t)\mid\mathbf{y}(0))\,.",,,
ICLR_2024_oral_52,12,"Standard methods for implementing conditional generation for diffusion models involve directly estimating the posterior score in Eq. (6) at training time (i.e., Classifier Free Guidance (Ho and Salimans, 2021)) or estimating the likelihood function \(p(\mathbf{y}(0)\mid\mathbf{x}(t))\) and using the Bayes formulato derive the posterior. The second approach typically involves training a separate model, often a classifier, for the likelihood score (i.e., Classifier Guidance (Dhariwal and Nichol, 2021)).

In diffusion-based generative source separation, learning a likelihood model is typically unnecessary because the relationship between \(\mathbf{x}(t)\) and \(\mathbf{y}(t)\) is represented by a simple function, namely the sum. A natural approach is to model the likelihood function based on such functional dependency. This is the approach taken by (Jayaram and Thickstun, 2020), where they use a Gaussian likelihood function:","p(\mathbf{y}(t)\mid\mathbf{x}(t))=\mathcal{N}(\mathbf{y}(t)\mid\sum_{n=1}^{N} \mathbf{x}_{n}(t),\gamma^{2}(t)\mathbf{I}),",,"p(\mathbf{y}(0)\mid\mathbf{x}(t))=\mathcal{N}(\mathbf{y}(0);\mathbf{x}(t), \sigma^{2}(t)\mathbf{I})\,.",
ICLR_2024_oral_52,13,"with the standard deviation given by a hyperparameter \(\gamma(t)\). The authors argue that aligning the \(\gamma(t)\) value to be proportionate to \(\sigma(t)\) optimizes the outcomes of their NCSN-BASIS separator.

We present a novel approximation of the posterior score function in Eq. (6) by modeling \(p(\mathbf{y}(t)\mid\mathbf{x}(t))\) as a Dirac delta function centered in \(\sum_{n=1}^{N}\mathbf{x}_{n}(t)\):","p(\mathbf{y}(t)\mid\mathbf{x}(t))=1_{\mathbf{y}(t)=\sum_{n=1}^{N}\mathbf{x}_{ n}(t)}\,.",,"p(\mathbf{y}(t)\mid\mathbf{x}(t))=\delta\left(\mathbf{y}(t)-\sum_{n=1}^{N} \mathbf{x}_{n}(t)\right)\,,",where the likelihood is modeled as a Dirac delta function enforcing the mixture condition at time $t$.
ICLR_2024_oral_52,14,"The complete derivation can be found in Appendix A, and we present only the final formulation, which we call 'MSDM Dirac'. The method constrains a source, without loss of generality \(\mathbf{x}_{N}\), by setting \(\mathbf{x}_{N}(t)=\mathbf{y}(0)-\sum_{n=1}^{N-1}\mathbf{x}_{n}(t)\) and estimates:","\nabla_{\mathbf{x}_{m}(t)}\log p(\mathbf{x}(t)\mid\mathbf{y}(0)) \approx S_{m}^{\theta}((\mathbf{x}_{1}(t),\dots,\mathbf{x}_{N-1}(t), \mathbf{y}(0)-\sum_{n=1}^{N-1}\mathbf{x}_{n}(t)),\sigma(t)) || -S_{N}^{\theta}((\mathbf{x}_{1}(t),\dots,\mathbf{x}_{N-1}(t), \mathbf{y}(0)-\sum_{n=1}^{N-1}\mathbf{x}_{n}(t)),\sigma(t))\,,",,"\nabla_{\mathbf{x}_{1:N-1}(t)}\log p([\mathbf{x}_{1:N-1}(t),\mathbf{y}(0)-\sum_{n=1}^{N-1}\mathbf{x}_{n}(t)])\,.","where the gradient is computed with respect to the first $N-1$ sources, and the $N$-th source is implicitly constrained via $\mathbf{x}_{N}(t)=\mathbf{y}(0)-\sum_{n=1}^{N-1}\mathbf{x}_{n}(t)$ to enforce the mixture constraint $\sum_{n=1}^{N}\mathbf{x}_{n}(t)=\mathbf{y}(0)$"
ICLR_2024_oral_53,1,"### Pre-training Objective

Given an instructional video dataset \(\mathcal{D}=\left\{\mathbf{V}_{i},\mathbf{T}_{i}\right\}_{i=1}^{N}\), where \(\mathbf{V}_{i}\) and \(\mathbf{T}_{i}\) represent the video and paragraph of \(i\)-th instance, we formulate each video/paragraph as a sequence of video clips/captions according to the ASR timestamps. Specifically, we mark the video clips and captions in \(i\)-th video as \(\{\mathbf{v}_{a}\}_{a=1}^{n}\) and \(\{\mathbf{t}_{b}\}_{b=1}^{m}\). Here \(\{\mathbf{v}_{a}^{j}\}_{j=1}^{f}\) and \(\{\mathbf{t}_{b}^{j}\}_{j=1}^{w}\) represent the frames and words within \(\mathbf{v}_{a}\) and \(\mathbf{t}_{b}\), where \(f\) and \(w\) represent the length of the clip and caption. Based on the above definitions, we propose the following training objectives:","\mathcal{L}=\mathcal{L}_{\text{clip}}+\lambda\mathcal{L}_{\text{video}},",where video-paragraph contrastive loss \(\mathcal{L}_{\text{video}}\) explores the temporal correlations between the long video \(\mathbf{V}_{i}\) and its corresponding paragraph \(\mathbf{T}_{i}\) through a novel noise robust temporal optimal transport distance,"\mathcal{D} = \left\{ \mathbf{V}_i, \mathbf{T}_i \right\}_{i=1}^{N}",Defines the instructional video dataset as a collection of video-paragraph pairs indexed by instance number $i$.
ICLR_2024_oral_53,2,"ce. The clip-caption contrastive loss \(\mathcal{L}_{\text{clip}}\) exploits potential faulty negative samples to improve clip representation and ensure accurate temporal modeling. We will elaborate on these two losses in the following sections.

### Correspondence Learning via Robust Optimal Transport

As long videos are typically composed of a sequence of short video clips, we propose to use the optimal transport distance between video clips and captions as the similarity criterion for video-paragraph contrastive learning in a robust and efficient way.

Let \(\mathbf{S}\in\mathbb{R}^{n\times m}\) denote the clip-caption similarity matrix where \([\mathbf{S}]_{a,b}\) measures the similarity between clip \(\mathbf{v}_{a}\) and caption \(\mathbf{t}_{b}\). \(\mathbf{Q}\in\mathbb{R}_{+}^{n\times m}\) denotes the corresponding transport assignment where \([\mathbf{Q}]_{a,b}\) represents the probabilities of aligning \(\mathbf{v}_{a}\) with \(\mathbf{t}_{b}\). Optimal transport seeks to establish a flexible alignment between clips and captions by maximizing global similarity \(\langle\mathbf{Q},\mathbf{S}\rangle=\operatorname{tr}(\mathbf{Q}^{\top} \mathbf{S})\). Formally, the objective of optimal transport is defined as follows:","\begin{split}\max_{\mathbf{Q}\in\mathcal{Q}}& \langle\mathbf{Q},\ \mathbf{S}\rangle+\varepsilon H(\mathbf{Q})\\ \text{s.t.}&\mathcal{Q}=\{\mathbf{Q}\in\mathbb{R }_{+}^{n\times m}\mid\mathbf{Q}\mathbf{1}_{m}=\boldsymbol{\mu},\mathbf{Q}^{ \top}\mathbf{1}_{n}=\boldsymbol{\nu}\}.\end{split}","where \(\mathbf{1}_{m}\) represents the vector of ones in dimension \(m\), \(\boldsymbol{\mu}\in\mathbb{R}^{n}\) and \(\boldsymbol{\nu}\in\mathbb{R}^{m}\) indicate the relative importance of each clip or caption","\min_{\mathbf{Q}\in\mathcal{U}(n,m)}\langle\mathbf{Q},\mathbf{S}\rangle,","where $\mathcal{U}(n,m)$ denotes the set of all transport plans satisfying the marginal constraints, representing the optimal transport objective that minimizes the global alignment cost between video clips and captions."
ICLR_2024_oral_53,3,"on. Since each clip or caption is sampled independently, we choose uniform probability distribution \(\boldsymbol{\mu}=\frac{1}{n}\mathbf{1}_{n}\) and \(\boldsymbol{\nu}=\frac{1}{m}\mathbf{1}_{m}\) to assign equal weight to each

Figure 2: Overview of our multi-granularity correspondence learning. We perform video-paragraph contrastive learning to capture long-term temporal correlations from a fine-to-coarse perspective. Specifically, we first utilize the log-sum-exp operator on the frame-word similarity matrix to obtain fine-grained similarity between clip and caption. Additionally, we append an alignable prompt bucket on the clip-caption similarity matrix to filter out the irrelevant clips or captions. By applying Sinkhorn iterations on the clip-caption similarity matrix, we effectively tackle the asynchronous problem and obtain the optimal transport distance as the video-paragraph similarity.

instance following Su & Hua (2017). \(H(\mathbf{Q})\) is an entropy regularizer derived from the optimization perspective (Cuturi, 2013) and \(\varepsilon\) controls its smoothness.

As illustrated in Eq. (2), optimal transport can realign each clip or caption to multiple related captions or clips based on global similarity, thus effectively resolving the potential asynchronous misalignment problem between the two modalities. The optimal \(\mathbf{Q}^{*}\) of Eq. (2) has a simple normalized exponential matrix solution by Sinkhorn fixed point iterations (Cuturi, 2013),","\begin{split}\mathbf{Q}^{*}&=\mathrm{Diag}( \boldsymbol{\kappa}_{1})\exp(\mathbf{S}/\varepsilon)\mathrm{Diag}( \boldsymbol{\kappa}_{2}),\\ \text{with iteratively updated }\boldsymbol{\kappa}_{1}& arrow\boldsymbol{\mu}./(\exp(\mathbf{S}/\varepsilon) \boldsymbol{\kappa}_{2}),\ \boldsymbol{\kappa}_{2}arrow\boldsymbol{\nu}_{.}/(\exp( \mathbf{S}^{\top}/\varepsilon)\boldsymbol{\kappa}_{1}),\end{split}","where \(\boldsymbol{\kappa}_{1}\in\mathbb{R}^{n}\), \(\boldsymbol{\kappa}_{2}\in\mathbb{R}^{m}\) are the non-negative left and right scaling vectors","\mathbf{Q}^{*}=\operatorname{diag}(\mathbf{u})\exp\left(\frac{\mathbf{S}}{\varepsilon}\right)\operatorname{diag}(\mathbf{v}),",where $\mathbf{u}$ and $\mathbf{v}$ are the scaling vectors obtained through Sinkhorn iterations to satisfy the marginal constraints
ICLR_2024_oral_53,4,"rs. By utilizing OT distance between clips and captions as the video-paragraph similarity, our video-paragraph contrastive loss captures the long-term temporal dependencies as follows,","\mathcal{L}_{\text{video}}=-\sum_{i=1}^{N}(\log\frac{\exp(( \mathbf{Q}_{ii},\ \mathbf{S}_{ii})/\tau)}{\sum_{j=1}^{N}\exp((\mathbf{Q}_{ ij},\ \mathbf{S}_{ij})/\tau)}+\log\frac{\exp((\mathbf{Q}_{ii},\ \mathbf{S}_{ii})/\tau)}{\sum_{j=1}^{N}\exp((\mathbf{Q}_{ ji},\ \mathbf{S}_{ji})/\tau)}),","where \(\mathbf{S}_{ij}\in\mathbb{R}^{n\times m}\) is the clip-caption similarity matrix between video \(\mathbf{V}_{i}\) and paragraph \(\mathbf{T}_{j}\), \(\mathbf{Q}_{ij}\) is the corresponding transport assignment of \(\mathbf{S}_{ij}\), and \(\tau\) is a learnable temperature initialized as 0","\mathcal{L}_{\text{video}} = -\log \frac{\exp(-d_{\text{OT}}(\mathbf{V}_i, \mathbf{T}_i)/\tau)}{\sum_{\mathbf{T}_j \in \mathcal{B}} \exp(-d_{\text{OT}}(\mathbf{V}_i, \mathbf{T}_j)/\tau)},","where $d_{\text{OT}}(\mathbf{V}_i, \mathbf{T}_i)$ denotes the optimal transport distance between video $\mathbf{V}_i$ and its corresponding paragraph $\mathbf{T}_i$, and $\tau$ is a temperature parameter controlling the concentration level of the exponential distribution over the distances."
ICLR_2024_oral_53,5,"0.07. Note that when calculating Eq. (4), we stop the gradient of the transport assignment \(\mathbf{Q}\) to keep the stability of our video-paragraph contrastive loss. To ensure the discriminative capacity of the model, we search the nearest videos as the hard negative samples following Xu et al. (2021). By using optimal transport to measure sequence distance instead of directly modeling the long videos, our method significantly reduces computational cost. A detailed training efficiency discussion is placed in Appendix C.

However, the optimal transport objective Eq. (2) still has some limitations: i) OT estimates the sequence distance based on clip-caption similarity (coarse-grained), leaving word-frame misalignment (fine-grained) problem unexplored; ii) OT requires each source instance must exactly map to the targets, which is not practical when dealing with a large amount of meaningless text. To address these challenges, we propose a soft-maximum operator for fine-grained alignment and an alignment prompt bucket to filter out meaningless clips and captions for noise robust distance estimation.

**Fine-grained Alignment.** Most previous works (Xu et al., 2021; Yang et al., 2023; Han et al., 2022) typically encode frames or words to a global feature using \([\mathrm{CLS}]\) token or averaging the frame or word embeddings (\(\mathit{e.g.}\), \(\mathrm{AvgPool}(\{\mathbf{v}_{j}^{j}\}_{j=1}^{f})\)). However, such strategies neglect fine-grained interactions between modalities and do not address the problem of frame-word misalignment.

To address this issue, we propose a cross-modal late interaction mechanism to identify crucial words and key frames for fine-grained alignment inspired by Yao et al. (2022); Wang et al. (2022). Specifically, we define the fine-grained similarity between clip \(\mathbf{v}_{a}\) and caption \(\mathbf{t}_{b}\) as follows:","[\mathbf{S}]_{a,b}=\frac{1}{2}(\frac{1}{f}\sum_{i=1}^{f}\alpha\log( \sum_{j=1}^{w}\exp(\frac{\mathbf{v}_{a}^{i}\cdot\mathbf{t}_{b}^{j}}{\alpha}) )+\frac{1}{w}\sum_{i=1}^{w}\alpha\log(\sum_{j=1}^{f}\exp(\frac{ \mathbf{t}_{b}^{i}\cdot\mathbf{v}_{a}^{j}}{\alpha}))).",,"\mathcal{S}_{a,b}^{\text{fine}}=\log\sum_{j=1}^{f}\sum_{k=1}^{w}\exp\left(\frac{\mathbf{v}_{a}^{j}\cdot\mathbf{t}_{b}^{k}}{\sigma}\right),","where $\mathcal{S}_{a,b}^{\text{fine}}$ represents the fine-grained similarity between frame $j$ in clip $\mathbf{v}_{a}$ and word $k$ in caption $\mathbf{t}_{b}$, computed using a log-sum-exp operator with temperature parameter $\sigma$"
ICLR_2024_oral_53,6,"Take the front part for example, for each frame in the video clip, we identify the most important words through a soft-maximum operation, \(\mathit{i.e.}\), log-sum-exp approximation (Beck & Teboulle, 2012), and then compute the average soft-maximum similarities of all frames as shown in Fig. 2. Similarly, for each textual token, we also find its related video frames in the latter part of Eq. (5). The parameter \(\alpha\) magnifies the importance of the most relevant words or frames. As \(\alpha\) approaches 0, the log-sum-exp approximates the maximum. Specifically, this soft-maximum operation allows us to reduce the negative influence of background words or frames on clip-caption similarity estimation.

Though inspired from Wang et al. (2022); Yao et al. (2022), our method differs in several aspects. Firstly, we introduce a straightforward log-sum-exp operator as a soft approximation of the maximum. This allows us to concentrate on more crucial words, making it particularly well-suited for video content as opposed to images. Experiments in Table 7 demonstrate that our design yields a substantial improvement compared to solely focusing on the most important item. Secondly, we leverage the estimated clip-caption similarity for sequence alignment, effectively enhancing temporal learning. In contrast, Wang et al. (2022) exclusively concentrates on clip-caption alignment.

**Alignable Prompt Bucket.** Optimal transport requires every source instance to exactly map to the targets. Yet, in real-world scenarios, a significant amount of captions and video clips might be noisy or irrelevant that cannot be aligned, _i.e._, coarse-grained irrelevant misalignments. Motivated by Sarlin et al. (2020), we propose an innovative solution that uses an alignable prompt bucket (APB) to filter out semantic irrelevant clips and captions. As shown in Fig. 2, the prompt bucket consists of one new row and column, filled with the same value \(p\). The prompt bucket is appended to the similarity matrix \(\mathbf{S}\) that","[\bar{\mathbf{S}}]_{a,m+1}=[\bar{\mathbf{S}}]_{n+1,b}=[\bar{\mathbf{S}}]_{n+1, m+1}=p,\,[\bar{\mathbf{S}}]_{a,b}=[\mathbf{S}]_{a,b},\ \forall a\in[1,n],\ b\in[1,m].",,"\tilde{\mathbf{S}}=\begin{bmatrix}\mathbf{S} & p\mathbf{1}_{n}\\ p\mathbf{1}_{m}^{\top} & p\end{bmatrix}\in\mathbb{R}^{(n+1)\times(m+1)},",where $\tilde{\mathbf{S}}$ represents the augmented similarity matrix with an additional alignable prompt bucket to filter out irrelevant clips or captions during optimal transport computation
ICLR_2024_oral_53,7,"When calculating the transport distance given \(\bar{\mathbf{S}}\), each video clip can be aligned with either available captions or the prompt bucket. Substituting Eq. (2) with Eq. (6), we obtain the final optimal transport assignment by dropping the last row and column of the transport assignment, _i.e._, \(\bar{\mathbf{Q}}^{*}=\bar{\mathbf{Q}}^{*}_{1:n,1:m}\).

From an intuitional viewpoint, the prompt value \(p\) in Eq. (6) serves as a similarity margin that distinguishes between alignable and unalignable clips and captions. If a video clip \(\mathbf{v}_{a}\) lacks an alignable caption, its pairwise similarities with the set of captions \(\{\mathbf{t}_{b}\}_{b=1}^{m}\) are generally small. Consequently, if the margin \(p\) is larger than these pairwise similarity values, \(\mathbf{v}_{a}\) is forced to align with the prompt bucket and subsequently filtered from the transport assignment. In our implementation, we determine the value of \(p\) as the bottom 30% similarity of the original aligned clip-caption pairs in a data-driven manner.

### Clip-caption Alignment via Faulty Negative Exploitation

Since self-supervised contrastive learning (He et al., 2020) relies on the random sampling of negative instances, captions that are semantically similar to the anchor clips can be treated as faulty negatives (Han et al., 2020; Zolfaghari et al., 2021), and vice versa. However, the existing one-hot target used in contrastive learning penalizes all negative predictions regardless of their correlations.

To mitigate this issue, we propose to exploit the faulty negatives through optimal transport. Let \(\hat{\mathbf{S}}\in\mathbb{R}^{B\times B}\) denotes the within-batch clip-caption similarity matrix where \(B\) represents the number of clips/captions for all videos in the batch. We apply optimal transport on the similarity matrix \(\hat{\mathbf{S}}\),","\max_{\hat{\mathbf{Q}}\in\hat{\mathcal{Q}}}\ \ \ \langle\hat{\mathbf{Q}},\ \hat{ \mathbf{S}}\rangle+\varepsilon H(\hat{\mathbf{Q}})\ \ \ \ \text{s.t.}\ \ \ \hat{\mathcal{Q}}=\{\hat{\mathbf{Q}}\in\mathbb{R}^{B\times B}_{+}\mid\hat{ \mathbf{Q}}\mathbf{1}_{B}=\frac{1}{B}\mathbf{1}_{B},\hat{\mathbf{Q}}^{\top} \mathbf{1}_{B}=\frac{1}{B}\mathbf{1}_{B}\},",where the transport assignment \(\hat{\mathbf{Q}}\) attempts to realign the clips with similar captions (_i,"\begin{split}\max_{\hat{\mathbf{Q}}\in\hat{\mathcal{Q}}}& \langle\hat{\mathbf{Q}},\ \hat{\mathbf{S}}\rangle+\varepsilon H(\hat{\mathbf{Q}})\\ \text{s.t.}&\hat{\mathcal{Q}}=\{\hat{\mathbf{Q}}\in\mathbb{R}_{+}^{B\times B}\mid\hat{\mathbf{Q}}\mathbf{1}_{B}=\hat{\boldsymbol{\mu}},\ \hat{\mathbf{Q}}^{\top}\mathbf{1}_{B}=\hat{\boldsymbol{\nu}}\},\end{split}","where $\hat{\boldsymbol{\mu}},\hat{\boldsymbol{\nu}}\in\mathbb{R}^{B}$ are the marginal distributions over clips and captions in the batch, and $\varepsilon$ controls the entropy regularization strength"
ICLR_2024_oral_53,8,"_i.e._, faulty negatives). After implementing the Sinkhorn algorithm described in Eq. (3), we utilize the clip-wise realigned targets \(\hat{\mathbf{Q}}^{*}\) as additional supervision for contrastive learning,","\mathcal{L}_{\text{clip}}=-\sum\limits_{i=1}^{B}\sum\limits_{j=1}^{B}[\mathbf{ T}]_{i,j}(\log\frac{\exp([\hat{\mathbf{S}}]_{i,j}/\tau)}{\sum_{k=1}^{B}\exp([ \hat{\mathbf{S}}]_{i,k}/\tau)}+\log\frac{\exp([\hat{\mathbf{S}}]_{i,j}/\tau)}{ \sum_{k=1}^{B}\exp([\hat{\mathbf{S}}]_{i,j}/\tau)}),\mathbf{T}=(1-\beta )\,\mathbf{I}_{B}+\beta\hat{\mathbf{Q}}^{*},",where \(\beta\) is a weighted parameter that balances the identity target \(\mathbf{I}_{B}\) and realigned targets \(\hat{\mathbf{Q}}^{*}\),"\mathcal{L}_{\text{clip}}=-\frac{1}{B}\sum_{i=1}^{B}\sum_{j=1}^{B}[\hat{ \mathbf{Q}}^{*}]_{i,j}\log\frac{\exp(\hat{\mathbf{S}}_{i,j}/\tau)}{ \sum_{k=1}^{B}\exp(\hat{\mathbf{S}}_{i,k}/\tau)}.",where $\mathcal{L}_{\text{clip}}$ leverages the realigned targets from optimal transport to provide enhanced supervision by exploiting faulty negatives in the contrastive learning framework
ICLR_2024_oral_54,1,"### Preliminaries

Given a big original dataset \(\mathcal{B}\in\mathbb{R}^{M\times d}\) with \(M\) number of \(d\)-dimensional data, the objective is to obtain a small synthetic dataset \(\mathcal{S}\in\mathbb{R}^{N\times d}\) where \(N\ll M\). Leveraging the gradient-based technique (Zhao et al., 2021), we minimize the gradient distance between big dataset \(\mathcal{B}\) and synthetic dataset \(\mathcal{S}\):","\min_{\mathcal{S}\in\mathbb{R}^{N\times d}}D(\nabla_{\theta}\ell( \mathcal{S};\theta),\nabla_{\theta}\ell(\mathcal{B};\theta))=D(\mathcal{ S},\mathcal{B};\theta),","where the function \(D(\cdot)\) is defined as a distance metric such as MSE, \(\theta\) represents the model parameters, and \(\nabla_{\theta}\ell(\cdot)\) denotes the gradient, utilizing either the big dataset \(\mathcal{B}\) or its synthetic version \(\mathcal{S}\)","\begin{equation}
    \min_{\mathcal{S}} \left\| \nabla_{\theta} \mathcal{L}(\theta; \mathcal{B}) - \nabla_{\theta} \mathcal{L}(\theta; \mathcal{S}) \right\|_2^2,
\end{equation}",Equation 1 formulates the optimization objective to minimize the squared L2 norm of the gradient difference between the original dataset $\mathcal{B}$ and the synthetic dataset $\mathcal{S}$ with respect to the model parameters $\theta$.
ICLR_2024_oral_54,2,"\). During condensation, the synthetic dataset \(\mathcal{S}\) and model \(\theta\) are updated alternatively,","\mathcal{S}arrow\mathcal{S}-\lambda\nabla_{\mathcal{S}}D(\mathcal{ S},\mathcal{B};\theta),\quad\thetaarrow\theta-\eta\nabla_{\theta} \ell(\theta;\mathcal{S}),","where \(\lambda\) and \(\eta\) are learning rates designated for \(\mathcal{S}\) and \(\theta\), respectively","\theta^{(t+1)} = \theta^{(t)} - \eta \nabla_{\theta} \ell(\mathcal{S}^{(t)}; \theta^{(t)}), \quad \mathcal{S}^{(t+1)} = \mathcal{S}^{(t)} - \gamma \nabla_{\mathcal{S}} D(\mathcal{S}^{(t)}, \mathcal{B}; \theta^{(t)}),","where $\theta^{(t)}$ and $\mathcal{S}^{(t)}$ denote the model parameters and synthetic dataset at iteration $t$, and $\eta$ and $\gamma$ are the respective learning rates for model and data updates."
ICLR_2024_oral_54,3,"ly.

### Subset Degradation Problem

To explain the ""subset degradation problem"", we name the condensation process in Eq. 1 and Eq. 2 as ""**basic condensation**"". This can be symbolized by \(\mathbb{C}_{1}^{[N]}\), where \([N]=\{1,2,3,\ldots,N\}\). The subscript of \(\mathbb{C}_{\square}^{[\square]}\) indicates the index of the condensation process, while the superscript of \(\mathbb{C}_{\square}^{[\square]}\) represents the index of original images that are used as the initialization for condensation.

For on-device applications, we need condensed datasets with multiple sizes, namely, **""multi-size condensation**"". Inspired by He et al. (2024); Yin et al. (2023), we introduce three distinct baselines for ""multi-size condensation"": Baseline-A, Baseline-B, and Baseline-C, denoted as \(\mathbb{C}_{n}^{[n]}\), \(\mathbb{C}_{n}^{n}\), and \(\mathbb{C}_{1}^{[N]}\), respectively. Fig. 1(a) illustrates how to conduct ""multi-size condensation"" to obtain the condensed dataset with sizes 1, 2, and 3 with our proposed baselines. Baseline-A employs **three** basic condensations of varying sizes, yielding **six** images. Baseline-B uses **three** size-1 basic condensations

Figure 2: Three different baselines for multi-size condensation.

but varies by image index, resulting in **three** unique images. Baseline-C adopts a **single** basic condensation to get **three** images, subsequently selecting image subsets for flexibility. Fig. 1(b) presents the count of required basic condensation processes and the storage demands in terms of image numbers.

In Fig. 1(c), we highlight the ""subset degradation problem"" using Baseline-A's orange line and Baseline-C's black dashed line. Baseline-A requires ten condensation processes, while Baseline-C just condenses once and selects **subsets** of the condensed dataset sized at 10. The shaded orange region indicates a notable accuracy drop for the subset when compared to the basic-condensed dataset. A critical observation is that the accuracy discrepancy grows as the subset's size becomes smaller.

### Multisize Dataset Condensation

#### 3.3.1 Subset Loss to Compress Condensation Processes

To address the ""subset degradation problem"", the objective function now becomes:","\min_{\mathcal{S}\in\mathbb{R}^{N\times d}}D(\nabla_{\theta }\ell(\mathcal{S}_{[1]},\mathcal{S}_{[2]},\ldots\mathcal{S}_{[N]};\theta ),\nabla_{\theta}\ell(\mathcal{B};\theta)),","where \(\mathcal{S}_{[n]}=\mathcal{S}_{\{1,2,\ldots,n\}}\subset\mathcal{S}=\mathcal{S}_ {[N]}\) represents \(n_{th}\) subset of the synthetic dataset \(\mathcal{S}\in\mathbb{R}^{N\times d}\)","\min_{\mathcal{S}\in\mathbb{R}^{N\times d}}\sum_{n=1}^{N}D(\nabla_{\theta}\ell(\mathcal{S}_{[n]};\theta),\nabla_{\theta}\ell(\mathcal{B};\theta)),","where $\mathcal{S}_{[n]}$ denotes the subset of the synthetic dataset $\mathcal{S}$ with size $n$, and the objective is to minimize the gradient distance between each subset of $\mathcal{S}$ and the original dataset $\mathcal{B}$"
ICLR_2024_oral_54,4,"\). We want each subset \(\mathcal{S}_{[n]}\) to have a small distance from the big dataset \(\mathcal{B}\). \(\mathcal{S}_{[N]}\) contributes the ""base loss"", and \(\mathcal{S}_{[1],[2],\ldots,[N-1]}\) contribute to the ""subset loss"". Note that the subsets also have a relationship with each other. For instance, the \(2_{nd}\) subset \(\mathcal{S}_{[2]}=\mathcal{S}_{\{1,2\}}\) is also a subset of the \(4_{th}\) subset \(\mathcal{S}_{[4]}=\mathcal{S}_{\{1,2,3,4\}}\).

We aim to incorporate the information of subsets without requiring additional condensation processes or extra images. To achieve this, we need to **compress** the information from the \(N-1\) different **condensation processes** of Baseline-A, including \(\mathbb{C}_{1}^{[1]},\mathbb{C}_{2}^{[2]},\ldots,\mathbb{C}_{N-1}^{[N-1]}\), into the process \(\mathbb{C}_{N}^{[N]}\). We propose the ""subset loss"" on top of the ""base loss"" to achieve this purpose in a single condensation process. The ""base loss"" is used to maintain the basic condensation process \(\mathbb{C}_{N}^{[N]}\), while the ""subset loss"" is used to enhance the learning process of the subsets via \(\mathbb{C}_{1}^{[1]},\mathbb{C}_{2}^{[2]},\ldots,\mathbb{C}_{N-1}^{[N-1]}\). We have a new updating strategy:","\mathcal{S}arrow\mathcal{S}-\lambda(\nabla_{\mathcal{S}}D( \mathcal{S},\mathcal{B};\theta)+\nabla_{\mathcal{S}_{[n]}}D( \mathcal{S}_{[n]},\mathcal{B};\theta)),\hskip 14.226378ptn\in[1,N-1].","where \(\mathcal{S}=\mathcal{S}_{[N]}\) represents the condensed dataset of \(N\) images and is associated with the ""base loss""","\min_{\mathcal{S}\in\mathbb{R}^{N\times d}}D(\nabla_{\theta}\ell(\mathcal{S}_{[N]};\theta),\nabla_{\theta}\ell(\mathcal{B};\theta)) + \gamma\sum_{n=1}^{N-1}D(\nabla_{\theta}\ell(\mathcal{S}_{[n]};\theta),\nabla_{\theta}\ell(\mathcal{B};\theta)),","where the first term is the ""base loss"" for the full synthetic dataset $\mathcal{S}_{[N]}$ and the second term is the ""subset loss"" that incorporates information from all smaller subsets $\mathcal{S}_{[n]}$ with $n < N$, weighted by $\gamma$."
ICLR_2024_oral_54,5,"s"". \(\mathcal{S}_{[n]}\) is subset and contributes to the ""subset loss"". A comparison between Eq. 4 and basic condensation is shown in Fig. 2(a). As depicted in Fig. 1(b), our technique aligns with Baseline-C in terms of the counts of both condensation processes and images.

#### 3.3.2 Selecting MLS for Adaptive Subset Loss

Among the \(N-1\) subsets from \(\{\mathcal{S}_{[1]},\mathcal{S}_{[2]},\ldots,\mathcal{S}_{[N-1]}\}\), we identify a particularly representative subset, \(\mathcal{S}[n^{*}]\), where \(n^{*}\in[1,N-1]\). We term this the Most Learnable Subset (MLS). In each condensation iteration, the MLS is selected **adaptively** to fit that particular iteration. Our approach

Figure 3: Explanation of Our MDC.

relies on three components to determine the MLS. Each component is illustrated in Fig. 2(b). The algorithm of the proposed method is shown in Appendix A.

**Feature Distance Calculation (Fig. 2(b)-(i)).** Eq. 3 represents the traditional approach for computing the gradient distance between subsets \(\{\mathcal{S}_{[n]},\mathcal{S}_{[2]},\ldots\mathcal{S}_{[N-1]}\}\) and the big dataset \(\mathcal{B}\). This method requires gradient calculations to be performed \(N-1\) times across the \(N-1\) subsets, leading to considerable computational overhead. To alleviate this, we introduce the concept of ""feature distance"" as a substitute for the ""gradient distance"" to reduce computation while capturing essential characteristics among subsets. The feature distance at a specific condensation iteration \(t\) for subset \(\mathcal{S}_{[n]}\) can be represented as:","F_{t}(\mathcal{S}_{[n]},\mathcal{B})=D(f_{t}(\mathcal{S }_{[n]}),f_{t}(\mathcal{B})),","where \(f_{t}(\cdot)\) is the feature extraction function for \(t_{th}\) condensation iteration, and \(D(\cdot)\) is a distance metric like MSE","d_{t}(\mathcal{S}_{[n]},\mathcal{B})=\frac{1}{d}\|\mu(\mathcal{S}_{[n]})-\mu(\mathcal{B})\|_{2}^{2}+\text{Tr}(\Sigma(\mathcal{S}_{[n]})+\Sigma(\mathcal{B})-2(\Sigma(\mathcal{S}_{[n]})\Sigma(\mathcal{B}))^{1/2}),","where $d$ is the feature dimension, $\mu(\cdot)$ and $\Sigma(\cdot)$ denote the mean and covariance of features, respectively, and $\text{Tr}(\cdot)$ represents the trace of a matrix"
ICLR_2024_oral_54,6,"SE. For subsets, the gradient distance mandates \(N-1\) forward passes and an equal number of backward passes for a total of \(N-1\) subsets. In contrast, the feature distance requires only a single forward pass and no backward pass. This is because the features are hierarchically arranged, and the feature set derived from a subset of size \(n\) can be straightforwardly extracted from the features of the larger dataset of size \(N\).

**Feature Distance Comparison (Fig. 2(b)-(ii)).** Generally, as the size of the subset increases, the feature distance diminishes. This is because the larger subset is more similar to the big dataset \(\mathcal{B}\). Let's consider two subsets \(\mathcal{S}_{[p]}\) and \(\mathcal{S}_{[q]}\) such that \(1<p<q<N\). This implies that the size of subset \(\mathcal{S}_{[p]}\) is less than the size of subset \(\mathcal{S}_{[q]}\). Their feature distances at iteration \(t\) can be represented as:","F_{t}(\mathcal{S}_{[p]},\mathcal{B})>F_{t}(\mathcal{S}_{[q]}, \mathcal{B}),\;\;\text{if}\;\;\;1<p<q<N.",,"F_{t}(\mathcal{S}_{[p]},\mathcal{B}) > F_{t}(\mathcal{S}_{[q]},\mathcal{B}),","where $F_{t}(\cdot)$ denotes the feature distance at condensation iteration $t$, and $\mathcal{S}_{[p]}$, $\mathcal{S}_{[q]}$ are subsets of sizes $p$ and $q$ respectively, with $p < q$."
ICLR_2024_oral_54,7,"Initially, it is intuitive that \(\mathcal{S}_{[1]}\), being the smallest subset, would manifest the greatest distance or disparity when compared to \(\mathcal{B}\). As such, \(\mathcal{S}_{[1]}\) should be the MLS at the beginning of the condensation process. As the condensation process progresses, we have:","\underbrace{F_{t-\Delta t}(\mathcal{S}_{[p]},\mathcal{B})>F_{t} (\mathcal{S}_{[p]},\mathcal{B})}_{p},\;\;\underbrace{F_{t-\Delta t }(\mathcal{S}_{[q]},\mathcal{B})>F_{t}(\mathcal{S}_{[q]}, \mathcal{B})}_{q},",where \(t-\Delta t\) and \(t\) are two different time points for the condensation process,"F_{t}(\mathcal{S}_{[p]},\mathcal{B})-F_{t}(\mathcal{S}_{[q]},\mathcal{B})>F_{t+1}(\mathcal{S}_{[p]},\mathcal{B})-F_{t+1}(\mathcal{S}_{[q]},\mathcal{B}),\;\;\text{if}\;\;\;1<p<q<N.",Equation 7 illustrates that the difference in feature distances between smaller and larger subsets decreases as the condensation process progresses.
ICLR_2024_oral_54,8,"ss. The reason for \(F_{t-\Delta t}>F_{t}\) is that the subsets get more representative as the condensation progresses, causing their feature distances to shrink. So, the most **learnable** subset would be the one whose feature distance reduction rate is the **highest**. The feature distance reduction rate is:","R(\mathcal{S}_{[n]},t)=\frac{\Delta F_{\mathcal{S}_{[n]}}}{\Delta t}=\frac{ |F_{t}(\mathcal{S}_{[n]},\mathcal{B})-F_{t-\Delta t}( \mathcal{S}_{[n]},\mathcal{B})|}{\Delta t},","where \(R(\mathcal{S}_{[n]},t)\) represents the rate of change of feature distance for subset \(\mathcal{S}_{[n]}\) at the time point \(t\), and \(\Delta F_{\mathcal{S}_{[n]}}\) denotes the change in feature distance of subset \(\mathcal{S}_{[n]}\) from time \(t-\Delta t\) to \(t\)","r_{t}(\mathcal{S}_{[n]},\mathcal{B})=\frac{F_{t-\Delta t}(\mathcal{S}_{[n]},\mathcal{B})-F_{t}(\mathcal{S}_{[n]},\mathcal{B})}{F_{t-\Delta t}(\mathcal{S}_{[n]},\mathcal{B})},","where $r_{t}(\mathcal{S}_{[n]},\mathcal{B})$ represents the feature distance reduction rate for subset $\mathcal{S}_{[n]}$ at iteration $t$, measuring how quickly the subset becomes more representative relative to the big dataset $\mathcal{B}$."
ICLR_2024_oral_54,9,\). An example for feature distance calculation can be found in Appendix B.2. The MLS for the time \(t\) can be described as:,"\mathcal{S}_{\mathrm{MLS}}(t)=\mathcal{S}_{[n_{t}^{*}]}=\operatorname*{arg\, max}_{\mathcal{S}_{[n]}}(R(\mathcal{S}_{[n]},t))\;\;\text{ where}\;\;n\in[1,N-1].",,"\mathcal{S}_{[n^{*}]}=\arg\max_{\mathcal{S}_{[n]}\subset\mathcal{S}}R( \mathcal{S}_{[n]},t),\quad n^{*}\in[1,N-1].","The Most Learnable Subset (MLS) at time $t$ is the subset $\mathcal{S}_{[n^{*}]}$ that maximizes the feature distance reduction rate $R(\mathcal{S}_{[n]},t)$ among all subsets $\mathcal{S}_{[n]}\subset\mathcal{S}$ with $n^{*}\in[1,N-1]$."
ICLR_2024_oral_54,10,"Eq. 9 seeks the subset that has the steepest incline or decline in its feature distance from \(\mathcal{B}\) over the time interval \(\Delta t\). This indicates the subset is ""learning"" at the **fastest rate**, thus deeming it the most learnable subset (MLS).

**MLS Freezing Judgement (Fig. 2(b)-(iii)).** To further reduce the impact of the ""subset degradation problem"", we modify the updating strategy in Eq. 4. The judgement will be modified if the current MLS differs in size from its predecessor; otherwise, it remains unchanged:",\begin{array}{l}\text{Using Eq.~{}\ref{eq:mLS} to~{}}\begin{cases}\text{Update}& \mathcal{S}\\ \text{Update}&\mathcal{S}\setminus\mathcal{S}_{\mathrm{MLS}}(t-\Delta t)\end{cases} \text{if}\;\mathcal{S}_{\mathrm{MLS}}(t)\supset\mathcal{S}_{\mathrm{MLS}}(t- \Delta t)\end{cases},where \(\setminus\) is the symbol for set minus,"\mathcal{S} \leftarrow \mathcal{S} - \lambda \left( \nabla_{\mathcal{S}} D(\mathcal{S}, \mathcal{B}; \theta) + \mathbb{I}_{\{n_t^* \ne n_{t-\Delta t}^*\}} \cdot \nabla_{\mathcal{S}_{[n_t^*]}} D(\mathcal{S}_{[n_t^*]}, \mathcal{B}; \theta) \right),",where $\mathbb{I}_{\{n_t^* \ne n_{t-\Delta t}^*\}}$ is an indicator function that activates the subset loss term only when the MLS changes in size compared to the previous iteration.
ICLR_2024_oral_55,1,"### Few-Shot Learning Background

Let \(\mathcal{D}=\{\mathcal{D}_{i}\}_{i=1}^{D}\) be the set of \(D\) classification domains, and \(\bar{\mathcal{D}}=\{X,Y\}\in\mathcal{D}\) a task containing \(n\) samples along with their designated true labels \(\{X,Y\}=\{x_{j},y_{j}\}_{j=1}^{n}\). Few-shot classification is defined as the problem of learning to correctly classify a query set \(\mathcal{Q}=\{X_{\mathcal{Q}},Y_{\mathcal{Q}}\}\sim\bar{\mathcal{D}}\) by training on a support set \(\mathcal{S}=\{X_{\mathcal{S}},Y_{\mathcal{S}}\}\sim\bar{\mathcal{D}}\) that contains very few examples. This can be achieved by finding the parameters \(\theta\) of a classifier \(f_{\theta}\) with the objective","\operatorname*{arg\,max}_{\theta}\prod_{\bar{\mathcal{D}}}p(Y_{\mathcal{Q}}|f_ {\theta}(\mathcal{S},X_{\mathcal{Q}})).",,,
ICLR_2024_oral_55,2,"In practice, if \(\theta\) is randomly initialised and trained using stochastic gradient descent on a small support set \(\mathcal{S}\), it will overfit and fail to generalise to \(\mathcal{Q}\). To address this issue, one can exploit knowledge transfer from some seen classes to the novel classes. Formally, each domain \(\bar{\mathcal{D}}\) is partitioned into two disjoint sets \(\bar{\mathcal{D}}_{\text{train}}\) and \(\bar{\mathcal{D}}_{\text{test}}\), which are commonly referred to as ""meta-train"" and ""meta-test"", respectively. The labels in these sets are also disjoint, i.e., \(Y_{\text{train}}\cap Y_{\text{test}}=\emptyset\). In that case, \(\theta\) is trained by maximising the objective in Eq. 1 using the meta-train set, but the overall objective is to perform adequately when transferring knowledge to meta-test.

The knowledge transferred from meta-train to meta-test can take various forms (Hospedales et al., 2022). As discussed earlier, we aim to generalise a family of few-shot methods (Hu et al., 2022; Li et al., 2022; Xu et al., 2022) where parameters \(\theta\) are transferred before a subset of them \(\phi\subset\theta\) are fine-tuned; and possibly extended by attaching additional ""adapter"" parameters \(\alpha\) that are trained for the target task. For meta-test, Eq. 1 can therefore be rewritten as","\operatorname*{arg\,max}_{\alpha,\phi}\prod_{\bar{\mathcal{D}}_{\text{test}}} p(Y_{\mathcal{Q}}|f_{\alpha,\phi}(\mathcal{S},X_{\mathcal{Q}})),",,"\operatorname*{arg\,max}_{\theta,\alpha}\prod_{\bar{\mathcal{D}}_{\text{test}}}p(Y_{\mathcal{Q}}|f_{\theta,\alpha}(\mathcal{S},X_{\mathcal{Q}})).","Equation 2 reformulates the optimisation objective for meta-test by considering both the transferred parameters $\theta$ and the adapter parameters $\alpha$ in the classifier function $f_{\theta,\alpha}$."
ICLR_2024_oral_55,3,"Our focus is on finding the optimal adaptation strategy in terms of (i) the optimal subset of parameters \(\phi\subset\theta\) that need to be fine-tuned, and (ii) the optimal task-specific parameters \(\alpha\) to add.

### Defining the search space

Let \(g_{\phi_{k}}\) be the minimal unit for adaptation in an architecture. We consider these to be the repeated units in contemporary deep architectures, e.g., a convolutional layer in a ResNet, or a self-attention block in a ViT. If the feature extractor \(f_{\theta}\) comprises of \(K\) such units with learnable parameters \(\phi_{k}\), then we denote \(\theta=\bigcup_{k=1}^{K}\phi_{k}\), assuming all other parameters are kept fixed. For brevity in notation we will now omit the indices and refer to every such layer as \(g_{\phi}\). Following the state-of-the-art (Hu et al., 2022; Li et al., 2022; Triantafillou et al., 2021; Xu et al., 2022), let us also assume that task-specific adaptation can be performed either by inserting additional adapter parameters \(\alpha\) into \(g_{\phi}\), or by fine-tuning the layer parameters \(\phi\).

This allows us to define the search space as two independent binary decisions per layer: (i) The inclusion or exclusion of an adapter module attached to \(g_{\phi}\), and (ii) the decision of whether to use the pre-trained parameters \(\phi\), or replace them with their fine-tuned counterparts \(\phi^{\prime}\). The size of the search space is, therefore, \((2^{2})^{K}=4^{K}\). For ResNets, we use the proposed adaptation architecture of TSA (Li et al., 2022), where a residual adapter \(h_{\alpha}\), parameterised by \(\alpha\), is connected to \(g_{\phi}\)","g_{\phi,\phi^{\prime},\alpha}(x)=g_{\phi,\phi^{\prime}}(x)+h_{\alpha}(x),","where \(x\in\mathbb{R}^{W,H,C}\)","\tilde{g}_{\alpha,\phi}(x) = g_{\phi}(x) + h_{\alpha}(x),","where the adapted layer output is the sum of the original layer output and the adapter output, enabling task-specific adaptation through residual connections."
ICLR_2024_oral_55,4,"\). For ViTs, we use the proposed adaptation architecture of ETT (Xu et al., 2022), where a tuneable prefix is prepended to the multi-head self-attention module \(A_{qkv}\), and a residual adapter is appended to both \(A_{qkv}\) and the feed-forward module \(z\) in each decoder block","g_{\phi,\phi^{\prime},\alpha}(x)=z(A_{qkv}[q\;;\;g_{\phi,\phi^{\prime}}(x)]+h _{\alpha 1})+h_{\alpha 2},",where \(x\in\mathbb{R}^{D}\) and \([\cdot\;;\;\cdot]\) denotes the concatenation operation,"g_{\phi,\phi^{\prime},\alpha}(x)=\begin{cases} \operatorname{Prefix}_{\alpha}(A_{qkv}(x)) & \text{for query, key, value},\\ A_{qkv}(x)+\operatorname{Residual}_{\alpha}(A_{qkv}(x)) & \text{for attention output},\\ z(x)+\operatorname{Residual}_{\alpha}(z(x)) & \text{for feed-forward output}. \end{cases}","Equation 4 defines the adaptation architecture for ViTs using ETT, where tunable prefixes and residual adapters are applied to different components of the self-attention and feed-forward modules."
ICLR_2024_oral_55,5,"on. Note that in the case of ViTs the adapter is not a function of the input features, but simply an added offset.

Irrespective of the architecture, every layer \(g_{\phi,\phi^{\prime},\alpha}\) is parameterised by three sets of parameters, \(\phi\), \(\phi^{\prime}\), and \(\alpha\), denoting the initial parameters, fine-tuned parameters and adapter parameters respectively. Consequently, when sampling a configuration (i.e., path) from that search space, every such layer can be sampled as one of the variants listed in Table 1.

### Training the supernet

Following SPOS (Guo et al., 2020), our search space is actualised in the form of a supernet \(f_{\theta,\alpha,\phi^{\prime}}\); a ""super"" architecture that contains all possible architectures derived from the decisions detailed in Section 2.2. It is parameterised by: (i) \(\theta\), the frozen parameters from the backbone architecture \(f_{\theta}\), (ii) \(\alpha\), from the adapters \(h_{\alpha}\), and (iii) \(\phi^{\prime}\), from the fine-tuned parameters per layer \(g_{\phi,\phi^{\prime},\alpha}\).

We use a prototypical loss \(\mathcal{L}(f,S,Q)\) as the core objective during supernet training and the subsequent search and fine-tuning.","\mathcal{L}(f,\mathcal{S},\mathcal{Q})=\frac{1}{|\mathcal{Q}|}\sum_{i=1}^{| \mathcal{Q}|}\log\frac{e^{-d_{cos}(C_{\mathcal{Q}_{i}},f(\mathcal{Q}_{i}))}}{ \sum_{j=1}^{|C|}e^{-d_{cos}(C_{j},f(\mathcal{Q}_{cj}))}},","where \(C_{\mathcal{Q}_{i}}\) denotes the embedding of the class centroid that corresponds to the true class of \(\mathcal{Q}_{i}\), and \(d_{cos}\) denotes the cosine distance","\min_{\alpha,\phi^{\prime}}\mathbb{E}_{\bar{\mathcal{D}}_{\text{train}}}[\mathcal{L}(f_{\theta,\alpha,\phi^{\prime}},\mathcal{S},\mathcal{Q})]","where the supernet is trained by minimizing the expected prototypical loss over meta-train tasks, with frozen backbone parameters $\theta$ and learnable adapter/fine-tuned parameters $\alpha,\phi^{\prime}$"
ICLR_2024_oral_55,6,"ce. The class centroids \(C\) are the mean embeddings of support examples that belong to the same class: \(C_{l}=\frac{1}{|\mathcal{S}^{y=l}|}\sum_{i=1}^{|\mathcal{S}|}f(\mathcal{S}_{i }^{y=l})\).

\begin{table}
\begin{tabular}{l|c c c c} \hline \hline  & \(\phi\,,-\) & \(\phi\,,\alpha\) & \(\phi^{\prime},-\) & \(\phi^{\prime},\alpha\) \\ \hline 
\begin{tabular}{c} ResNet \\ ViT \\ \end{tabular} & \(\begin{bmatrix}g_{\phi}(x)\\ z(A_{qkv}[q\;;\;g_{\phi}(x)]+h_{\alpha}(x)\\ z(A_{qkv}[q\;;\;g_{\phi}(x)]+h_{\alpha 2}\end{bmatrix}\) & \(\begin{bmatrix}g_{\phi^{\prime}}(x)\\ z(A_{qkv}[q\;;\;g_{\phi^{\prime}}(x)])&z(A_{qkv}[q\;;\;g_{\phi^{\prime}}(x)] +h_{\alpha 2}\end{bmatrix}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: The search space, as described in Section 2.2. A layer \(g_{\phi,\phi^{\prime},\alpha}\) can be sampled in one of the following variants: (i) \(\phi\): fixed pre-trained parameters, no adaptation, (ii) \(\alpha\): fixed pre-trained parameters, with adaptation, (iii) \(\phi^{\prime}\): fine-tuned parameters, no adaptation, (iv) \(\phi^{\prime},\alpha\) fine-tuned-parameters, with adaptation.

For supernet training, let \(P\) be a set of size \(4^{K}\), enumerating all possible sequences of \(K\) layers that can be sampled from the search space. Denoting a path sampled from the supernet as \(f^{p}_{\theta,\alpha,\phi^{\prime}}\), we minimise the loss in Eq. 5 over multiple episodes and paths, so the final objective becomes:","\operatorname*{arg\,min}_{\alpha,\phi^{\prime}}\ \mathbb{E}_{p\sim P} \mathbb{E}_{\mathcal{S},\mathcal{Q}}\ \mathcal{L}(f^{p}_{\theta,\alpha,\phi^{\prime}}, \mathcal{S},\mathcal{Q}).",,"\min_{\alpha,\phi^{\prime}}\mathbb{E}_{p\sim P}\left[\mathcal{L}(f^{p}_{\theta,\alpha,\phi^{\prime}},\mathcal{S},\mathcal{Q})\right].",The supernet training objective minimises the expected prototypical loss over all possible paths sampled from the search space.
ICLR_2024_oral_55,7,"In Appendix C, we summarise the supernet training algorithm in pseudocode (Algorithm 1).

### Two-stage search for an optimal path

A supernet \(f_{\theta,\alpha,\phi^{\prime}}\) trained with the method described in Section 2.3 contains \(4^{K}\) models, intertwined via weight sharing. As explained in Section 1, our goal is to search for the best-performing one, but the main challenge is related to the fact that we do not know what data is going to be used for adaptation at test time. One extreme approach would be to search for a single solution during training and simply use it throughout the entire test, regardless of the potential domain shift. Another, would be to defer the search and perform it from scratch each time a new support set is given to us at test time. However, both have their shortcomings. As such, we propose a hybrid, where searching is split into two phases - one during training, and a subsequent one during testing.

Meta-training timeThe search is responsible for pre-selecting a set of \(N\) models from the entire search space. Its main purpose is to mitigate potential overfitting that can happen at test time, when only a small amount of data is available, while providing enough diversity to successfully adjust the architecture to the diverse set of test domains. Formally, we search for a sequence of paths \((p_{1},p_{2},...,p_{N})\) where:","p_{k} =\operatorname*{arg\,max}_{p\in P}\mathbb{E}_{\mathcal{S}, \mathcal{Q}}A(f^{p}_{\theta,\alpha^{\prime},\phi^{\prime\prime}},\mathcal{S}, \mathcal{Q}),\quad\text{s.t.} || \alpha^{*},\phi^{\prime*} =\operatorname*{arg\,min}_{\alpha,\phi^{\prime}}\mathcal{L}(f^{p} _{\theta,\alpha,\phi^{\prime}},\mathcal{S},\mathcal{S})",,"\forall n\in\{1,...,N\},\ p_{n}\in P\quad\text{and}\quad p_{n}\neq p_{m}\ \forall n\neq m,",Ensuring that each selected path $p_n$ is unique and belongs to the set of all possible paths $P$.
ICLR_2024_oral_55,8,(8),"\quad\forall_{j=1,\dots,k-1}\ \ d_{\text{cos}}(p_{k},p_{j})\geq T,",,"A(f^{p}_{\theta,\alpha^{\prime},\phi^{\prime\prime}},\mathcal{S},\mathcal{Q}) = \frac{1}{|\mathcal{Q}|}\sum_{i=1}^{|\mathcal{Q}|}\log\frac{e^{-d_{cos}(C_{\mathcal{Q}_{i}},f^{p}_{\theta,\alpha^{\prime},\phi^{\prime\prime}}(\mathcal{Q}_{i}))}}{\sum_{j=1}^{|C|}e^{-d_{cos}(C_{j},f^{p}_{\theta,\alpha^{\prime},\phi^{\prime\prime}}(\mathcal{Q}_{j}))}}.","Equation 8 defines the adaptation performance $A$ of a path $p$ on a query set $\mathcal{Q}$, using the cosine distance between class centroids and embeddings of query examples, similar in structure to the prototypical loss in Equation 5 but conditioned on the supernet path $f^{p}_{\theta,\alpha^{\prime},\phi^{\prime\prime}}$."
ICLR_2024_oral_55,9,"Figure 1: Our proposed NAS paradigm for few-shot adaptation. (a) Overall meta-train/meta-test workflow. (b) The supernet architecture. \(f\) denotes the feature extractor, which is composed of many layers, \(g\), which are the minimal unit for adaptation in our search space.

where \(T\) denotes a scalar threshold for the cosine distance between paths \(p_{k}\) and \(p_{j}\), and \(A\) is the classification accuracy of a nearest centroid classifier (NCC) (Snell et al., 2017),","A(f,\mathcal{S},\mathcal{Q})=\frac{1}{|\mathcal{Q}|}\sum_{i=1}^{|\mathcal{Q}|}[ \operatorname*{arg\,min}_{j}d_{cos}(C_{\mathcal{Q}_{j}},f(\mathcal{Q}_{i}))=Y_ {\mathcal{Q}_{i}}].",,"\quad A(f,\mathcal{S},\mathcal{Q}) = \frac{1}{|\mathcal{Q}|} \sum_{i=1}^{|\mathcal{Q}|} \mathbb{I}\left[\operatorname*{arg\,min}_{j} d_{\text{cos}}(f(\mathcal{Q}_i), C_j)\right].","Classification accuracy of a nearest centroid classifier (NCC), where $\mathbb{I}[\cdot]$ is the indicator function that evaluates to 1 if the predicted class matches the true class, and 0 otherwise."
ICLR_2024_oral_55,10,"We measure accuracy of a solution using a query set, after fine-tuning on a separate support set (Eq. 8), then average across multiple episodes to avoid overfitting to a particular support set (Eq. 7). We also employ a diversity constraint, in the form of cosine distance between binary encodings of selected paths (Eq. 9), to allow for sufficient flexibility in the following test time search.

To efficiently obtain sequence \(\{p_{1},...,p_{N}\}\), we use evolutionary search to find points that maximise Eq. 7, and afterwards select the \(N\) best performers from the evolutionary search history that satisfy the constraint in Eq. 9. In Appendix C, we summarise the training-time search algorithm in pseudocode (Algorithm 2).

Meta-testing timeFor a given meta-test episode, we decide which one of the pre-selected \(N\) models is best-suited for adaptation on the given support set data. It acts as a failsafe to counteract the bias of the initial selection made at training time in cases when the support set might be particularly out-of-domain. Formally, the final path \(p^{*}\) to be used in a particular episode is defined as:","p^{*}=\operatorname*{arg\,min}_{p\in\{p_{1},...,p_{N}\}}\mathcal{L}(f^{p}_{ \mathcal{O}_{\alpha^{*}},\phi^{\prime\prime*}},\mathcal{S},\mathcal{S}),\quad \text{s.t.} || \alpha^{*},\phi^{\prime*}=\operatorname*{arg\,min}_{\alpha,\phi^{\prime}} \mathcal{L}(f^{p}_{\mathcal{O},\alpha,\phi^{\prime}},\mathcal{S},\mathcal{S}).",,"p^{*}=\operatorname*{arg\,max}_{p\in\{p_{1},...,p_{N}\}}A(f^{p}_{\theta,\alpha^{*},\phi^{\prime*}},\mathcal{S},\mathcal{Q}),\quad\text{where}\ \alpha^{*},\phi^{\prime*}=\operatorname*{arg\,min}_{\alpha,\phi^{\prime}}\mathcal{L}(f^{p}_{\theta,\alpha,\phi^{\prime}},\mathcal{S},\mathcal{S}).","Final path selection during meta-testing based on accuracy maximisation over pre-selected models, with parameters fine-tuned on the given support set."
ICLR_2024_oral_56,1,"### The Role of Explicit Priors

We have established that SPT allows for a more reliable evaluation of the actual capabilities of architectures and further improves the performance of SSMs such as S4. Despite its high performance, S4 has a complex design guided by principled theoretical considerations to enable long range signal

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline
**Approach** & **Listops** & **Text** & **Retrieval** & **Image** & **Pathfinder** & **PathX** & **PathX-256** & **Avg.** \\ \hline Transformers + Rotary & 47.90 & 79.08 & 82.31 & 75.04 & 76.64 & 84.72 & ✗ & 74.28 \\ Transformers + Rotary + Masked SPT & 61.49 & **91.02** & **91.57** & 86.04 & 94.16 & 92.98 & ✗ & 86.21 \\ S4 (Gu et al., 2022) & 59.60 & 86.82 & 90.90 & 88.65 & 94.20 & 96.35 & 67.82\({}^{\dagger}\) & 86.09 \\ S4 + Masked SPT & 61.25 & 90.34 & 88.74 & 89.36 & 94.92 & 96.94 & **87.11** & 86.75 \\ SPADE (Zuo et al., 2022) & 60.50 & 90.69 & 91.17 & 88.22 & **96.23** & 97.60 & ✗ & 87.40 \\ MEGA (Ma et al., 2022) & **63.14** & 90.43 & 91.25 & **90.44** & 96.01 & **97.98** & ✗ & **88.21** \\ \hline \hline Pythia 70M (Rand Init) & 41.20 & 69.29 & 76.45 & 52.55 & 74.31 & ✗ & ✗ & 62.76 \\ Pythia 70M & 43.05 & 83.41 & 84.29 & 67.41 & 80.05 & ✗ & ✗ & 68.04 \\ \hline \hline \end{tabular}
\end{table}
Table 2: **Long Range Arena.** Self pretrained (SPT) Transformers and S4 compared to existing trained from scratch models. Average performance (“Avg.”) is reported without PathX-256 to align with prior work. Results for MEGA, SPADE & S4 are taken from original papers with exceptions denoted by \(\dagger\). ✗ denotes computationally infeasible, \(\square\) denotes unreported results.

propagation, which can explain the small advantage maintained over SPT Transformers, lacking such an inductive bias. In a series of works, various simplifications to S4 have been proposed while maintaining performance. We will now show that SPT allows for an even simpler model (viz. diagonal linear RNN) to match the performance of S4.

We first provide a brief overview of SSMs below and refer to Gu et al. (2022) for a detailed description. Given an input scalar sequence2\(u\), SSMs follow a linear recurrence generating a hidden state vector \(\vec{\mathbf{x}}_{n}\) at timestep \(n\), and produce a scalar output sequence \(y\) as

Footnote 2: When the input is a sequence of vectors, the model is applied to each channel separately and is commonly followed by a FFN to exchange information across channels.","\begin{split}\vec{\mathbf{x}}_{n}&=\mathbf{A}\vec{\mathbf{x}} _{n-1}+\mathbf{B}u_{n}&\mathbf{A}\in\mathbb{C}^{N\times N},\mathbf{B}\in \mathbb{C}^{N\times 1}\\ y_{n}&=\mathbf{C}\vec{\mathbf{x}}_{n}&\mathbf{C} \in\mathbb{C}^{1\times N}\end{split}",,"\begin{equation}
\vec{\mathbf{x}}_{n+1} = \mathbf{A} \vec{\mathbf{x}}_{n} + \mathbf{B} u_{n}, \quad y_{n} = \mathbf{C} \vec{\mathbf{x}}_{n} + \mathbf{D} u_{n},
\end{equation}",Equation 1: State Space Model (SSM) equations describing the linear recurrence for hidden state evolution and output generation.
ICLR_2024_oral_56,2,"By unrolling the recurrence across the timesteps, it can be shown that \(y\) can be equivalently computed by convolving \(u\) with the kernel defined by \(K_{k}=\mathbf{C}^{T}\mathbf{A}^{k}\mathbf{B}\). Instead of directly using \(\mathbf{A}\), \(\mathbf{B}\), \(\mathbf{C}\) as learnable parameters, S4 uses an alternate parameterization inspired by a theory in continuous time, motivating the transformations:",\mathbf{A} =\mathbf{A}-\mathbf{P}\mathbf{Q}^{\star} || \mathbf{\bar{A}} =(\mathbf{I}-\Delta/2\cdot\mathbf{A})^{-1}(\mathbf{I}+\Delta/2\cdot\mathbf{A}),,"\begin{aligned} \mathbf{A} &\leftarrow \text{diag}(\lambda_{1}, \lambda_{2}, \dots, \lambda_{N}) \quad \text{with } \lambda_{i} \in \mathbb{C} \\ \mathbf{B} &\leftarrow \text{diag}(b_{1}, b_{2}, \dots, b_{N}) \quad \text{with } b_{i} \in \mathbb{C} \\ \mathbf{C} &\leftarrow \text{diag}(c_{1}, c_{2}, \dots, c_{N}) \quad \text{with } c_{i} \in \mathbb{C} \end{aligned}","Equation 2 introduces the diagonal parameterization of the state-space matrices used in S4, where each matrix is represented as a diagonal matrix with complex entries, simplifying the model while preserving performance."
ICLR_2024_oral_56,3,(3),\mathbf{\bar{B}} =(\mathbf{I}-\Delta/2\cdot\mathbf{A})^{-1}\Delta\mathbf{B}\quad\mathbf{\bar{C}}= \mathbf{C},,\mathbf{\bar{B}} = \sqrt{\Delta}(\mathbf{I} - \Delta/2 \cdot \mathbf{A})^{-1} \mathbf{B},"Equation 3 defines the transformed input matrix $\mathbf{\bar{B}}$ in the S4 parameterization, scaled by the square root of the step size $\Delta$ and modified through the resolvent of the matrix $\mathbf{A}$."
ICLR_2024_oral_56,4,(4),\mathbf{K}_{k} =\mathbf{\bar{C}}^{T}\mathbf{\bar{A}}^{k}\mathbf{\bar{B}},"where \(\mathbf{A},\mathbf{P},\mathbf{Q},\mathbf{B},\mathbf{C},\Delta\) are learnable parameters and \(\mathbf{A}\in\text{{Diag}}(\mathbb{C}^{N\times N}),\mathbf{P},\mathbf{Q}\in\mathbb{C}^{N \times 1}\)",\begin{split}\vec{\mathbf{x}}_{n}&=\bar{\mathbf{A}}\vec{\mathbf{x}}_{n-1}+\bar{\mathbf{B}}u_{n}\\ y_{n}&=\bar{\mathbf{C}}\vec{\mathbf{x}}_{n}\end{split},"Discretized state-space model using the transformed parameters $\bar{\mathbf{A}}$, $\bar{\mathbf{B}}$, and $\bar{\mathbf{C}}$ derived from the continuous-time formulation."
ICLR_2024_oral_56,5,"\). In addition to this parameterization, S4 uses a principled initialization method aimed towards a slow decay of the kernel (w.r.t. timestep \(k\)) in order to facilitate capturing long-range dependencies.

Inspired by the success of S4, Gupta et al. (2022) proposed a simplification to S4 called Diagonal Linear RNN (DLR) defined as","\begin{split}\vec{\mathbf{x}}_{n}&=\mathbf{A}\vec{\mathbf{x}} _{n-1}+\mathbf{I}\,u_{n}&\mathbf{A}\in\text{{diag}}(\mathbb{C}^{N\times N })\\ y_{n}&=\mathbf{C}\vec{\mathbf{x}}_{n}&\mathbf{C} \in\mathbb{C}^{1\times N}\end{split}",where \(\mathbf{I}\) is the all-ones vector,\begin{split}\vec{\mathbf{x}}_{n} &= \lambda\vec{\mathbf{x}}_{n-1} + \beta u_{n} \\ y_{n} &= \gamma^{\top}\vec{\mathbf{x}}_{n}\end{split},"Diagonal Linear RNN (DLR) formulation with scalar state transition coefficient $\lambda$, input scaling $\beta$, and output projection $\gamma$."
ICLR_2024_oral_58,1,"### Constructing the Interaction Tensor

Clustering features of different models.Given \(M\) models, \(\left\{f_{1},f_{2},\ldots,f_{M}\right\}\), we can compute the projected representation for each network of the \(M\) models, \(\left\{\Phi_{1}^{\text{repj}},\Phi_{2}^{\text{repj}},\ldots,\Phi_{M}^{\text{ proj}}\right\}\). For a single model \(f_{m}\) and its \(k^{\text{th}}\) feature, we can compute its mean and variance:","\mu_{m,k}\triangleq\mathbb{E}_{(x,y)\sim\mathscr{G}}[\upsilon_{m,k}(x) ],\ \sigma_{m,k}^{2}\triangleq\mathbb{E}_{(x,y)\sim\mathscr{G}}[( \upsilon_{m,k}(x)-\mu_{m,k})^{2}].",,,
ICLR_2024_oral_58,2,"For models \(\left(f_{i},f_{j}\right)\) and their respective \(a^{\text{th}}\) and \(b^{\text{th}}\) features, we can define their correlation to be:","\rho_{(i,j),(a,b)}\triangleq\mathbb{E}_{(x,y)\sim\mathscr{G}}[( \upsilon_{i,a}(x)-\mu_{i,a})(\upsilon_{j,b}(x)-\mu_{j,b}) ](\sigma_{i,a}\ \sigma_{j,b})^{-1}.",,"\rho_{i,j}^{(a,b)} \triangleq \frac{\mathbb{E}_{(x,y)\sim\mathscr{G}}\left[(\upsilon_{i,a}(x) - \mu_{i,a})(\upsilon_{j,b}(x) - \mu_{j,b})\right]}{\sigma_{i,a} \sigma_{j,b}}.",Defines the correlation between the $a^{\text{th}}$ feature of model $f_i$ and the $b^{\text{th}}$ feature of model $f_j$.
ICLR_2024_oral_58,3,"This can be seen as performing the procedure of Li et al. (2015) with PCA projected representations. We use \(\mathbf{K}_{i,j}\in[-1,1]^{K\times K}\) to denote the collection of all pair-wise correlation values between the features of \(f_{i}\) and \(f_{j}\), and \(\mathbf{\Lambda}\in[-1,1]^{M\times M\times K\times K}\) to denote the collection of all the correlation matrices between every pair of models. A centralized list of notations is provided in Appendix A.

With \(\mathbf{\Lambda}\), we can identify unique _feature clusters_ in the \(MK\) features learned by all models. To account for the arbitrary direction of correlation in \(\mathbf{\Lambda}\), we take the absolute value and use a threshold, \(\gamma_{\text{corr}}\in(0,1)\), to determine whether two features should be considered as the same feature. We use a greedy clustering algorithm (Algorithm 1) to match the features with one another, as k-partite matching is known to be NP-complete for k \(>2\)(Garey and Johnson, 1979). After running the clustering algorithm, each feature is assigned to one of \(T\) clusters (where \(T\leq MK\)), and we treat every feature in a single cluster as the same feature. The greedy algorithm is efficient and does not generate a fixed number of clusters, which is desirable in cases where some features have low correlations with other features and should be isolated as a unique cluster. More sophisticated algorithms, such as graph cut, could be used, but we find the greedy algorithm sufficient for our purposes. See Algorithm 1 and Appendix E.2 for details on the algorithm and hyperparameters.

Matching features to data pointsOnce the features of all models are clustered, we can identify which features are present in each data point of \(\mathbf{X}\). First, we normalize each individual \(\upsilon_{m,k}\) by its \(\ell_{\infty}\)-norm, which ensures that all features are between \(0\) and \(1\). We will denote the row-normalized \(\Phi_{m}^{\text{repj}}\) as \(\widehat{\Phi}_{m}^{\text{repj}}\). We then pick another threshold, \(\gamma_{\text{data}}\in(0,1)\), that decides whether a feature is present in a data point. Concretely, if the \(k^{\text{th}}\) entry of the \(n^{\text{th}}\) row in \(\widehat{\Phi}_{m}^{\text{repj}}\) is larger than \(\gamma_{\text{data}}\), we assign to the \(n^{\text{th}}\) data point in \(\mathbf{X}\) the feature cluster containing the \(m^{\text{th}}\) model's \(k^{\text{th}}\) feature. In other words, if the \(m^{\text{th}}\) model's \(k^{\text{th}}\) feature belongs to the \(t^{\text{th}}\) cluster, we say the \(n^{\text{th}}\) data point contains the \(t^{\text{th}}\) feature. In Figure 1, we visualize the data points with the most and least number of features.

Aggregating Information.After thresholding, we have enough information to construct the interaction tensor. Each entry indicates whether the \(t^{\text{th}}\) feature is present in _both_ the \(m^{\text{th}}\) model and \(n^{\text{th}}\) data point. In the next section, we will inspect various aspects of the interaction tensor \(\mathbf{\Omega}\) and other experimental artifacts to understand how the models learn features from the data.

## 4 Experiments and Observations

Experimental setup.We use a collection of \(M=20\) ResNet18 (He et al., 2016) trained on the CIFAR-10 dataset (Krizhevsky et al., 2009) following the experimental set up of Jiang et al.

(2022). The models are trained on random 45000 subsets of the whole training set (45k). In addition, we repeat the experiments on CIFAR-10 using random 10000 subsets (10k) and the SVHN dataset (Netzer et al., 2011) using random 45000 subsets of the whole training set (SVHN). We compute \(\mathbf{\Lambda}\) and \(\mathbf{\Omega}\) on the test set using the output of the penultimate layer (i.e., \(\psi\) is the final linear layer). The experimental details can be found in Appendix E. We primarily show 45k here and leave the 10k and SVHN results to Appendix F, both of which are similar to the 45k results qualitatively.

**Observation 1 : Feature frequency is long-tailed. (O.1)** We use the interaction tensor \(\mathbf{\Omega}\) to compute the frequency of each of the \(T\) features in the dataset. First, we sum \(\mathbf{\Omega}\) over the model axis and then clip the values to 1 (since we are only interested in the relation between data and features here). Then, we sum over the data axis to obtain the number of data points that contain each individual feature. The features are sorted by their frequencies, which reveals a long-tailed distribution (Figure 1(b)). Furthermore, this distribution is a consequence of learning. In Figure 1(a), we compare the feature frequency computed from 20 models over the first 30 epochs of training (i.e., 600 checkpoints) with the feature distribution for untrained models. We observe that untrained models have much higher frequencies for tail features (low frequency). After even a single epoch, the frequency of tail features decreases significantly and then stays relatively stable over training. At the head of the distribution, the models first learn a large number of features and then prune the features as training continues, eventually converging to a fixed distribution with a small number of features.

**Observation 2: The ensemble tends to be more confident on data points with fewer features, and data points with lower confidence tend to have more features with low density. (O.2)** Another question that we want to understand is how the features interact with the confidence of the ensemble. In Figure 2(a), we show the joint density plot of the ensemble's confidence for a data point and the number of features the data point has for 45k. We can see that low-confidence data tend to have more features, whereas high-confidence data points tend to have fewer features. This finding

Figure 3: **(a) Density estimation of confidence vs number of features over data. High-confidence data tend to have fewer features. (b) Scatter plot of the number of data with a feature vs. the number of models with that feature. The strong positive correlation suggests that the more data has a certain feature, the more likely the model will learn that feature. (c) Number of shared features vs shared error. The lower bound of shared error monotonically increases with the number of shared features.**

Figure 2: **(a) Feature frequency over the course of training. The red curve represents the feature frequency at the initialization. (b) Features vs how often they appear in the dataset. The features are sorted by frequency and the distribution appears to be long-tailed. (c) Feature frequency by different confidence levels. Low-confidence data tend to have more low-frequency features.**

contradicts the model of Allen-Zhu and Li (2020) in which if a data point is multi-view (i.e., contains more features), all members of the ensembles will classify it correctly (i.e., high confidence). A plausible explanation for this observation is that there is a small sub-population of features that are learned by a large number of models. Based on O.1, we postulate that these features are learned by more models because they appear with higher probability. Furthermore, we plot the log density of features1 in high-confidence data and all other data points (low-confidence) in Figure 1(c). We see that the low-confidence data tend to have more features with low density in Figure 1(b). One explanation is that the features in the tail are responsible for different models making different predictions.

Footnote 1: The features are plotted in the same order as Figure 1(c) (hence the jaggedness), and the density is obtained by normalizing with the total number of features in both high confidence and low confidence groups of data.

Observation 3: Number of models with a certain feature is positively correlated with the feature's frequency. (O.3) The interaction tensor also reveals how the number of data points containing a certain feature relates to the number of models that have learned that feature. This relationship can shed light on how models learn features of different frequencies in practice. In Figure 2(b), we observe that the number of models with any given feature has a strong positive correlation with the frequency of that feature appearing in the data (linear / super-linear). This implies that the more a feature appears in the data, the more likely a model will pick it up. We hypothesize that the feature learning procedure can be phenomenologically approximated by a sampling process where the probability of learning a feature is related to how often that feature appears in the data.

Observation 4: Models with similar features make similar mistakes. (O.4) Another natural hypothesis is that models with similar features should make similar mistakes. For every pair of models, we compare how many features they share to how often they make the same mistake relative to the average number of mistakes both models make (Figure 2(c)). We can see that the lower bound of shared error is almost _monotonically_ increasing with the value of shared features. On the other hand, when models share a smaller number of features, the shared errors have a much larger variance, which indicates their errors are less correlated and therefore more random. This effect is more amplified for _different architectures_. We show this result on more than 20 diverse architectures in Appendix F.6.

Finally, we provide more analysis on the effect of using PCA for clustering in Appendix F.2 and different hyperparameters for the interaction tensor in Appendix F.10. We also explore the properties of features found under our definition in Figure 1 and Appendix F.3. In Appendix F.11, we investigate the feature distributions at different depths and discuss the interpretation of these features.

## 5 A Combinatorial Framework of Feature Learning

In this section, we present a new framework of feature learning for a binary classification based on the insights from the experiments. We saw in O.1 that the distribution of features is long-tailed. This means a relatively small number of unique features constitute a large proportion of all the features in the data. To facilitate analysis, we will assume there are two types of features, _dominant features_ and _rare features_, where the dominant features appear with much higher probability than rare features. Further, we observed that data with high confidence tend to have much fewer features than the ones with high confidence (O.2). To model this behavior, we will assume that there are two types of data points: _dominant data_ and _rare data_. The former contains a small number of dominant features and the latter contains a larger number of rare features. This is another simplification based on O.2, which shows that high-confidence data tend to have fewer high-frequency features.

Definitions and additional notations.Before describing the full model, we first define the parameters of the model as well as some additional notations: \(\mathbf{p_{d}}\): the proportion of all data that are dominant, \(\mathbf{p_{r}}\): the proportion of all data that are rare (this parameter is equal to \(1-p_{d}\)), \(\mathbf{c}\): the total model capacity which represents how many features a single model can learn, \(\mathbf{t_{d}}\): the total number of dominant features available in the data for one class, \(\mathbf{t_{r}}\): the total number of rare features available in the data for one class, \(\mathbf{n_{d}}\): the total number of dominant features a single dominant data point has (\(n_{d}\leq t_{d}\)), and \(\mathbf{n_{r}}\): the total number of rare features a single rare data point has (\(n_{r}\leq t_{r}\)). We will use \(\Psi(\cdot)\) to denote the set of all features a model or a feature has (an expanded list in Appendix A).

Data generating process.We can see the data generating process as the following sampling procedure. First, we decide which class the data point belongs to. We consider a class-balancedbinary classification problem so each class occurs with an equal probability of \(\frac{1}{2}\). Then, we decide whether the data point is dominant or rare. This is equivalent to sampling from a Bernoulli distribution, \(\text{Ber}(p_{d})\). If the data point is dominant, we sample \(n_{d}\) dominant features uniformly _without_ replacement. Vice versa, if the data point is rare, we sample \(n_{r}\) dominant features uniformly _without_ replacement. It is easy to verify that the proportion of dominant data points and features is \(p_{d}\) and the proportion of rare data points and features is \(p_{r}\).

How the models learn.We saw in O.3 that the frequency of features occurring in different models is positively correlated with the frequency at which the features occur in the data. We can model the learning process as another _sampling-without-replacement_ process where the probability that a model learns a feature is _proportional_ to the frequency at which the feature occurs in the data. Under this assumption, in expectation, \(c_{d}=\frac{1}{2}p_{d}c\) of the features in a single model would be dominant features for a single class, and \(c_{r}=\frac{1}{2}p_{r}c\) of the features for a single class would be rare. We can further simplify this process by assuming that the model will always sample \(c_{d}\) dominant features for each class2.

Footnote 2: Both \(c_{r}\) and \(c_{d}\) are rounded to the nearest integer such that the total number of features in a model is still \(c\).

How the models make predictions.For a data point \(x\) and a model \(f\), we assume that the model will correctly classify \(x\) if the overlap between the features of \(x\) and the features of \(f\) is not empty (similar to assumptions in Allen-Zhu and Li (2020)). Otherwise, the model will perform a random guess. The expected error that a single model \(f\) makes on a single datum pair \((x,y)\) is thus \(\text{err}(f,x,y)=\frac{1}{2}\mathbb{1}\left\{\Psi(f)\cap\Psi(x)=\varnothing\right\}\). Further, given a pair of models \((f,g)\) and a single datum pair \((x,y)\), there are three distinct behaviors for how they will make predictions. **(1)** The two models will always agree with each other if both of them share feature with \(x\), since both will classify \(x\) correctly (i.e., if \(|\Psi(f)\cap\Psi(x)|>0\) and \(|\Psi(g)\cap\Psi(x)|>0\)). **(2)** If the models both do not share any features with \(x\), then by the previous assumptions, the models will make random guesses (see Appendix D.2 for why this is justified); however, if the models share features with each other, their random guesses will not be independent of each other (O.4). We hypothesize that how two models agree with each other is a function of \(k\), the number of features they share, and \(c\), the model capacity. We capture this intuition with an _agreement function_, \(\zeta:\mathbb{N}\times\mathbb{N}\rightarrow[0,1]\), which returns the probability that two models will agree based on how many features they share relative to the full model capacity. This function is crucial for understanding how models make mistakes. **(3)** Finally, if the models do not share any features with each other or with \(x\), both models will perform independent random guesses, in which case they will agree \(50\%\) of the time.

It is natural to ask how reasonable the simplifications are. In Appendix D, we discuss these simplifications (e.g., random guess and number of classes) in detail and provide a comparison between this framework and Allen-Zhu and Li (2020). We encourage interested readers to read this section. Still, we will see that this relatively simplified model readily offers interesting insights into observed phenomena and can make surprisingly accurate predictions about the results of experiments a priori.

### Analytical forms of accuracy and agreement

Using this model, the _closed-form_ form of expected accuracy, Acc, and expected agreement rates, \(\text{Agr}\), can be derived through combinatorics. All propositions are proven in Appendix C.

**Proposition 5.1**.: _The expected accuracy over the model distribution and data distribution is:_",\text{Acc}=p_{d}(1-\frac{1}{2}\begin{pmatrix}\binom{t_{d}-c_{d}}{a_{d}} \\ \binom{t_{d}}{a_{d}}\end{pmatrix}^{+}p_{r}(1-\frac{1}{2}\begin{pmatrix} \binom{t_{r}-c_{r}}{c_{r}}\\ \binom{t_{r}}{c_{r}}\end{pmatrix}).,,\text{Acc} = \frac{1}{2}\left[1 - \left(p_{d}\left(1 - \frac{c_{d}}{t_{d}}\right)^{n_{d}} + p_{r}\left(1 - \frac{c_{r}}{t_{r}}\right)^{n_{r}}\right)\right].,"The expected accuracy is derived based on the overlap between features of a model and a data point, considering dominant and rare features separately and their respective probabilities of being learned by the model."
ICLR_2024_oral_59,1,"## 2 Background and related work

Differential privacy.Differential privacy (DP) (Dwork et al., 2006; Dwork and Roth, 2014) is a rigorous notion of privacy that an algorithm can satisfy. DP algorithms for training ML models include DP-SGD (Abadi et al., 2016), DP-FTRL (Kairouz et al., 2021), and DP matrix factorization (Denissov et al., 2022; Choquette-Cho et al., 2023). Informally, DP guarantees that a powerful attacker observing the output of the algorithm \(A\) trained on one of two _adjacent_ datasets (differing by addition or removal of one record), \(D\) or \(D^{\prime}\), cannot confidently distinguish the two cases, which is quantified by the privacy parameters \(\epsilon\) and \(\delta\).

**Definition 2.1**.: **User-level differential privacy.** The training algorithm \(A:\mathcal{D}\rightarrow\mathcal{R}\) is user-level \((\epsilon,\delta)\) differentially private if for all pairs of datasets \(D\) and \(D^{\prime}\) from \(\mathcal{D}\) that differ only by addition or removal of the data of one user and all output regions \(R\subseteq\mathcal{R}\):",\Pr[A(D)\in R]\leq e^{\epsilon}\Pr[A(D^{\prime})\in R]+\delta.,,\Pr[A(D) \in R] \leq e^{\epsilon} \cdot \Pr[A(D^{\prime}) \in R] + \delta,"Probability bound defining user-level $(\epsilon,\delta)$-differential privacy for algorithm $A$ over output regions $R$."
ICLR_2024_oral_59,2,"DP can be interpreted as a hypothesis test with the null hypothesis that \(A\) was trained on \(D\) and the alternative hypothesis that \(A\) was trained on \(D^{\prime}\). False positives (type-I errors) occur when the null hypothesis is true, but is rejected, while false negatives (type-II errors) occur when the alternative hypothesis is true, but is rejected. Kairouz et al. (2015) characterized \((\epsilon,\delta)\)-DP in terms of the false positive rate (FPR) and false negative rate (FNR) achievable by an acceptance region. This characterization enables estimating the privacy parameter as:","\hat{\epsilon}=\max\{\log\frac{1-\delta-\text{FPR}}{\text{FNR}},\log \frac{1-\delta-\text{FNR}}{\text{FPR}}\}.",,\epsilon = \log\left(\frac{\text{FNR}}{1 - \text{FPR}}\right).,Characterization of the privacy parameter $\epsilon$ in terms of the false negative rate (FNR) and the false positive rate (FPR) for a hypothesis test distinguishing datasets $D$ and $D^{\prime}$.
ICLR_2024_oral_59,3,"Private federated learning.DP Federated Averaging (DP-FedAvg) (McMahan et al., 2018) is a user-level DP version of the well-known Federated Averaging (FedAvg) algorithm (McMahan et al., 2017) for training ML models in a distributed fashion. In FedAvg, a central server interacts with a set of clients to train a global model iteratively over multiple rounds. In each round, the server sends the current global model to a subset of clients, who train local models using their training data, and send the model updates back to the server. The server aggregates the model updates via the Gaussian mechanism, in which each update is clipped to bound its \(\ell_{2}\) norm before averaging and adding Gaussian noise proportional to the clipping norm sufficient to mask the influence of individual users, and incorporates the aggregate update into the global model. DP-FedAvg can rely on privacy amplification from the sampling of clients at each round, but more sophisticated methods can handle arbitrary participation patterns (Kairouz et al., 2021; Choquette-Choo et al., 2023).

Privacy auditing.Privacy auditing (Ding et al., 2018; Liu and Oh, 2019; Gilbert and McMillan, 2018; Jagielski et al., 2020) is a set of techniques for empirically auditing the privacy leakage of an algorithm. The main technique used for privacy auditing is mounting a membership inference attack (Shokri et al., 2017; Yeom et al., 2018; Carlini et al., 2022) and translating the success of the adversary into an \(\varepsilon\) estimate using Equation (1) directly.

Most privacy auditing techniques (Jagielski et al., 2020; Nasr et al., 2021; Lu et al., 2022; Zanella-Beguelin et al., 2023) have been designed for centralized settings, with the exception of CAN-IFE (Maddock et al., 2022), suitable for privacy auditing of federated learning deployments. CANIFE operates under a strong adversarial model, assuming knowledge of all intermediary model updates, as well as local model updates sent by a subset of clients in each round of training. CANIFE crafts data poisoning canaries adaptively, with the goal of generating model updates orthogonal to updates sent by other clients in each round. We argue that when the model dimensionality is sufficiently high, such crafting is unnecessary, since a randomly chosen canary update with already be essentially orthogonal to the true updates with high probability. CANIFE also computes a _per-round_ privacy measure, which it extrapolates into a measure for the entire training run by estimating an equivalent per-round noise \(\hat{\sigma}_{r}\), and then composing the RDP of the repeated Poisson subsampled Gaussian mechanism. However, in practice FL systems do not use Poisson subsampling due to the infeasibility of sampling clients i.i.d. at each round. Our method flexibly estimates the privacy loss in the context of arbitrary participation patterns, for example passing over the data in epochs, or the difficult-to-characterize _de facto_ pattern of participation in a deployed system, which may include techniques intended to amplify privacy such as limits on client participation within temporal periods such as one day.

We empirically compare our approach with CANIFE in Appendix G and discuss the assumptions on the auditor's knowledge and capabilities for all recent approaches (including ours) in Appendix H.

## 3 One-shot privacy estimation for the Gaussian mechanism

As a warm-up, we start by considering the problem of estimating the privacy of the Gaussian mechanism, the fundamental building block of DP-SGD and DP-FedAvg. To be precise, given \(D=(x_{1},\cdots,x_{m})\), with \(\|x_{j}\|\leq 1\) for all \(j\in[m]\), the output of the Gaussian vector sum query is \(A(D)=\bar{x}+\sigma Z\), where \(\bar{x}=\sum_{j}x_{j}\) and \(Z\sim\mathcal{N}(0,I)\). Without loss of generality, we can consider a neighboring dataset \(D^{\prime}\) with an additional vector \(x_{0}\) with \(\|x_{0}\|\leq 1\). Thus, \(A(D)\sim\mathcal{N}(\bar{x},\sigma^{2}I)\) and \(A(D^{\prime})\sim\mathcal{N}(\bar{x}+x_{0},\sigma^{2}I)\). For the purpose of computing the DP guarantees, this mechanism is equivalent to analyzing \(A(D)\sim\mathcal{N}(0,\sigma^{2})\) and \(A(D^{\prime})\sim\mathcal{N}(1,\sigma^{2})\) due to spherical symmetry.

The naive approach for estimating the \(\varepsilon\) or an implementation of the Gaussian mechanism would run it many times (say 1000 times), with half of the runs on \(D\) and the other half on \(D^{\prime}\). Then the outputs of these runs are shuffled and given to an ""attacker"" who attempts to determine for each output whether it was computed from \(D\) or \(D^{\prime}\). Finally, the performance of the attacker is quantified in terms of FPR/FNR, and Eq. (1) is used to obtain an estimate of the mechanism's \(\varepsilon\) at a target \(\delta\).

We now present a provably correct approach for estimating \(\varepsilon\) by running the mechanism _only once_. The key idea is to augment the original dataset with \(k\) canary vectors \(c_{i}\) for \(i\in[k]\), sampled i.i.d. uniformly at random from the unit sphere \(\mathbb{S}^{d-1}=\{x\in\mathbb{R}^{d}\colon\|x\|=1\}\), obtaining \(D=(x_{1},\cdots,x_{m},c_{1},\cdots,c_{k})\). We consider \(k\) neighboring datasets, each excluding one of the canaries, i.e., \(D^{\prime}_{i}=D\setminus\{c_{i}\}\) for \(i\in[k]\). We run the Gaussian mechanism once on \(D\) and use its output to compute \(k\) test statistics \(\{g_{i}\}_{i\in[k]}\), the cosine of the angles between the output and each one of the \(k\) canary vectors. We use these \(k\) cosines to estimate the distribution of test statistic on \(D\) by computing the sample mean \(\hat{\mu}=\frac{1}{k}\sum_{i=1}^{k}g_{i}\) and sample variance \(\hat{\sigma}^{2}=\frac{1}{k}\sum_{i=1}^{k}\bigl{(}g_{i}-\hat{\mu}\bigr{)}^{2}\) and fitting a Gaussian \(\mathcal{N}(\hat{\mu},\hat{\sigma}^{2})\). To estimate the distribution of the test statistic on \(D^{\prime}\), it would seem we need to run the mechanism on each \(D^{\prime}_{i}\) and compute the cosine of the angle between the output vector and \(c_{i}\). This is where our choice of \((i)\) independent isotropically distributed canaries and \((ii)\) cosine angles as our test statistic are particularly useful. The distribution of the cosine of the angle between an isotropically distributed _unobserved_ canary and the mechanism output (or any independent vector) can be described in a closed form; there is no need to approximate this distribution with samples. We will show in Propositions 3.1 and 3.2 that this distribution can be well approximated by \(\mathcal{N}(0,1/d)\). Now that we have models of the distribution of the test statistic on \(D\) and \(D^{\prime}\), we estimate the \(\varepsilon\) of the mechanism using the method given in Appendix A which allows us to compute the exact \(\varepsilon\) when the null and alternate hypotheses are arbitrary Gaussians. Our approach is summarized in Algorithm 1.

```
1:Input: Vectors \(x_{1},\cdots,x_{m}\) with \(\|x_{j}\|\leq 1\), DP noise variance \(\sigma^{2}\), and target \(\delta\)
2:\(\rho\leftarrow\sum_{j\in[m]}x_{j}\)
3:for\(i\in[k]\)do
4: Draw \(c_{i}\) i.d. from \(\mathbb{S}^{d-1}\)
5:\(\rho\leftarrow\rho+c_{i}\)
```

**Algorithm 1** One-shot privacy estimation for Gaussian mechanism.

**Proposition 3.1**.: _For \(d\in\mathbb{N},d\geq 2\), let \(c\) be sampled uniformly from \(\mathbb{S}^{d-1}\), and let \(\tau_{d}=\langle c,v\rangle/\|v\|\in[-1,1]\) be the cosine similarity between \(c\) and some arbitrary independent nonzero vector \(v\). Then, the probability density function of \(\tau_{d}\) is_",f_{d}(t)=\frac{\Gamma(\frac{d}{2})}{\Gamma(\frac{d-1}{2})\sqrt{\pi}}(1-t^{2})^{ \frac{d-3}{2}}.,,"f_{\tau_{d}}(t)=\frac{\Gamma\left(\frac{d}{2}\right)}{\sqrt{\pi}\Gamma\left(\frac{d-1}{2}\right)}(1-t^{2})^{\frac{d-3}{2}},\quad t\in[-1,1].",
ICLR_2024_oral_6,1,"## 3 Related Work

Representing the problem of computing a Nash equilibrium as an optimization problem is not new. A variety of loss functions and pseudo-distance functions have been proposed. Most of them measure some function of how much each player can exploit the joint strategy by unilaterally deviating:","\epsilon_{k}(\boldsymbol{x})\mathop{=}^{\text{\text{\text{def}}}}u_{k}( \texttt{BR}_{k},x_{-k})-u_{k}(\boldsymbol{x})\text{ where }\texttt{BR}_{k}\in\operatorname*{arg\,max}_{z}u_{k}(z,x_{-k}).",,\mathcal{L}(\theta) = \sum_{i=1}^{N} \left\| \nabla_{\theta_i} J_i(\theta) \right\|^2,"Equation 1 defines a loss function $\mathcal{L}(\theta)$ that measures the squared norm of the gradients of each player's cost function $J_i(\theta)$ with respect to their own parameters $\theta_i$, summed over all players $N$."
ICLR_2024_oral_6,2,"As argued in the introduction, we believe it is important to be able to subsample payoff tensors of normal-form games in order to scale to large instances. As Nash equilibria can consist of mixed strategies, it is advantageous to be able to sample from an equilibrium to estimate its exploitability \(\epsilon\). However none of these losses is amenable to unbiased estimation under sampled play. Each of the functions currently explored in the literature is biased under sampled play either because 1) a random variable appears as the argument of a complex, nonlinear (non-polynomial) function or because 2) how to sample play is unclear. Exploitability, Nikaido-Isoda (NI) (Nikaido and Isoda, 1955) (also known by NashConv (Lanctot et al., 2017) and ADI (Gemp et al., 2022)), as well as fully-differentiable options (Shoham and Leyton-Brown, 2008, p. 106, Eqn 4.31) introduce bias when a \(\max\) over payoffs is estimated using samples from \(\boldsymbol{x}\). Gradient-based NI (Raghunathan et al., 2019) requires projecting the result of a gradient-ascent step onto the simplex; for the same reason as the \(\max\), this projection is prohibitive because it is a nonlinear operation which introduces bias. Lastly, unconstrained optimization approaches (Shoham and Leyton-Brown, 2008, p. 106) that instead penalize deviation from the simplex lose the ability to sample from strategies when each iterate \(\boldsymbol{x}\) is no longer a distribution (i.e., \(x_{k}\not\in\Delta^{m_{k}-1}\)). Table 1 summarizes these complications.

## 4 Nash Equilibrium as Stochastic Optimization

We will now develop our proposed loss function which is amenable to unbiased estimation. Subsections 4.1-4.4 provide a warm-up in which we assume an interior (fully-mixed) Nash equilibrium exists. Subsection 4.5 then shows how to relax that assumption allowing us to approximate partially mixed equilibria as well (including pure equilibria). Our key technical insight is to pay special attention to the geometry of the simplex. To our knowledge, prior works have failed to recognize the role of the tangent space \(T\Delta\). Proofs are in the appendix.

### Stationarity on the Simplex Interior

**Lemma 1**.: _Assuming player \(k\)'s utility, \(u_{k}(x_{k},x_{-k})\), is concave in its own strategy \(x_{k}\), a strategy in the interior of the simplex is a best response \(\texttt{BR}_{k}\) if and only if it has zero projected-gradient1 norm._

\begin{table}
\begin{tabular}{l|c|c} Loss & Function & Obstacle \\ \hline Exploitability (\(\epsilon\)) & \(\max_{k}\epsilon_{k}(\boldsymbol{x})\) & \(\max\) of r.v. \\ Nikaido-Isoda (NI) & \(\sum_{k}\epsilon_{k}(\boldsymbol{x})\) & \(\max\) of r.v. \\ Fully-Diff. Exp & \(\sum_{k}\sum_{a_{k}\in\mathcal{A}_{k}}[\max(0,u_{k}(a_{k},x_{-k})-u_{k}( \boldsymbol{x}))]^{2}\) & \(\max\) of r.v. \\ Gradient-based NI & NI w \(\texttt{BR}_{k}\leftarrow\texttt{aBR}_{k}=\Pi_{\Delta}\Big{(}x_{k}+\eta\nabla_ {x_{k}}u_{k}(\boldsymbol{x})\Big{)}\) & \(\Pi_{\Delta}\) of r.v. \\ Unconstrained & Loss + Simplex Deviation Penalty & sampling from \(x_{k}\in\mathbb{R}^{m_{k}}\) \\ \end{tabular}
\end{table}
Table 1: Previous loss functions for NFGs and their obstacles to unbiased estimation. Note that \(\epsilon_{k}(\boldsymbol{x})=\max_{z}u_{k}(z,x_{-k})-u_{k}(\boldsymbol{x})\) contains a max operator (see equivalent definition in equation (1)).

In NFGs, each player's utility is linear in \(x_{k}\), thereby satisfying the concavity condition of Lemma 1.

### Projected-Gradient Norm as a Loss

An equivalent description of a Nash equilibrium is a joint strategy \(\mathbf{x}\) where every player's strategy is a best response to the equilibrium (i.e., \(x_{k}=\mathrm{BR}_{k}\) so that \(\epsilon_{k}(\mathbf{x})=0\)). Lemma 1 states that any interior best response has zero _projected-gradient_ norm, which inspires the following loss function",\mathcal{L}(\mathbf{x})=\sum_{k}\eta_{k}||\Pi_{T\Delta}(\nabla_{x_{k}}^{k})||^{2},"where each \(\eta_{k}>0\) represents a scalar weight, or equivalently, a step size to be explained next",\mathcal{L}(\boldsymbol{x}) = \sum_{k} \left\| \nabla_{x_{k}} u_{k}(\boldsymbol{x}) \right\|^2.,"Proposed loss function measuring the squared norm of the projected gradient for each player, which is amenable to unbiased estimation under sampled play."
ICLR_2024_oral_6,3,"xt.

**Proposition 1**.: _The loss \(\mathcal{L}\) is equivalent to NashConv, but where player \(k\)'s best response is approximated by a single step of projected-gradient ascent with step size \(\eta_{k}\): \(\texttt{aBR}_{k}=x_{k}+\eta_{k}\Pi_{T\Delta}(\nabla_{x_{k}}^{k})\)._

This connection was already pointed out in prior work for unconstrained problems (Gemp et al., 2022; Raghunathan et al., 2019), but this result is the first for strategies constrained to the simplex.

### Connection to True Exploitability

In general, we can bound exploitability in terms of the projected-gradient norm as long as each player's utility is concave (this result extends to subgradients of non-smooth functions).

**Lemma 2**.: _The amount a player can gain by exploiting a joint strategy \(\mathbf{x}\) is upper bounded by a quantity proportional to the norm of the projected-gradient:_",\epsilon_{k}(\mathbf{x})\leq\sqrt{2}||\Pi_{T\Delta}(\nabla_{x_{k}}^{k})||.,,\epsilon_{k}(\mathbf{x}) \leq \frac{1}{2\eta_{k}}||\Pi_{T\Delta}(\nabla_{x_{k}}^{k})||^{2}.,"This inequality provides an upper bound on the exploitability $\epsilon_{k}(\mathbf{x})$ in terms of the squared norm of the projected-gradient, scaled by the step size $\eta_{k}$."
ICLR_2024_oral_6,4,"This bound is not tight on the boundary of the simplex, which can be seen clearly by considering \(x_{k}\) to be part of a pure strategy equilibrium. In that case, this analysis assumes \(x_{k}\) can be improved upon by a projected-gradient ascent step (via the equivalence pointed out in Proposition 1). However, that is false because the probability of a pure strategy cannot be increased beyond \(1\). We mention this to provide further intuition for why our ""warm-up"" loss \(\mathcal{L}(\mathbf{x})\) is only valid for interior equilibria.

Note that \(||\Pi_{T\Delta}(\nabla_{x_{k}}^{k})||\leq||\nabla_{x_{k}}^{k}||\) because \(\Pi_{T\Delta}\) is a projection. Therefore, this improves the naive bounds on exploitability and distance to best responses given using the ""raw"" gradient \(\nabla_{x_{k}}^{k}\).

**Lemma 3**.: _The exploitability of a joint strategy \(\mathbf{x}\), is upper bounded by a function of \(\mathcal{L}(\mathbf{x})\):_",\epsilon\leq\sqrt{\frac{2n}{\min_{k}\eta_{k}}}\sqrt{\mathcal{L}(\mathbf{x})} \stackrel{{\text{\tiny def}}}{{=}}f(\mathcal{L}).,,\epsilon(\mathbf{x})\leq\sqrt{2\mathcal{L}(\mathbf{x})}.,"This lemma provides an upper bound on the exploitability of a joint strategy in terms of the loss function $\mathcal{L}(\mathbf{x})$, showing that smaller values of $\mathcal{L}$ correspond to strategies that are closer to being Nash equilibria."
ICLR_2024_oral_6,5,"### Unbiased Estimation

As discussed in Section 3, a primary obstacle to unbiased estimation of \(\mathcal{L}(\mathbf{x})\) is the presence of complex, nonlinear functions of random variables, with the projection of a point onto the simplex being one such example (see \(\Pi_{\Delta}\) in Table 1). However, \(\Pi_{T\Delta}\), the projection onto the _tangent space of the simplex_, is linear! This is the insight that allows us to design an unbiased estimator (Lemma 5).

Our proposed loss requires computing the squared norm of the _expected value_ of the projected-gradient under the players' mixed strategies, i.e., the \(l\)-th entry of player \(k\)'s gradient equals \(\nabla_{x_{kl}}^{k}=\mathbb{E}_{a_{-k}\sim x_{-k}}u_{k}(a_{kl},a_{-k})\). By analogy, consider a random variable \(Y\). In general, \(\mathbb{E}[Y]^{2}\neq\mathbb{E}[Y^{2}]\). This means that we cannot just sample projected-gradients and then compute their average norm to estimate our loss. However, consider taking two independent samples from two corresponding identically distributed, independent random variables \(Y^{(1)}\) and \(Y^{(2)}\). Then \(\mathbb{E}[Y^{(1)}]^{2}=\mathbb{E}[Y^{(1)}]\mathbb{E}[Y^{(2)}]=\mathbb{E}[Y^{( 1)}Y^{(2)}]\) by properties of expected value over products of independent random variables. This is a common technique to construct unbiased estimates of expectations over polynomial functions of random variables. Proceeding in this way, define \(\nabla_{x_{k}}^{k(1)}\) as a random, unbiased gradient estimate (see Table 2). Let \(\nabla_{x_{k}}^{k(2)}\) be independent and distributed identically to \(\nabla_{x_{k}}^{k(1)}\). Then Lemma 5 shows",\mathcal{L}(\mathbf{x})=\mathbb{E}[\sum_{k}\eta_{k}\underbrace{(\hat{\nabla}_{x_{ k}}^{k(1)}-\frac{\mathbf{1}}{m_{k}}(\mathbf{1}^{\top}\hat{\nabla}_{x_{k}}^{k(1)}))}_{\text{ projected-gradient 1}})^{\top}(\underbrace{\hat{\nabla}_{x_{k}}^{k(2)}-\frac{\mathbf{1}}{m_{k}}(\mathbf{1}^{\top} \hat{\nabla}_{x_{k}}^{k(2)})\mathbf{1})}_{\text{projected-gradient 2}}),where \(\hat{\nabla}^{k(p)}_{x_{k}}\) is an unbiased estimator of player \(k\)'s gradient,"\mathbb{E}[\langle\Pi_{T\Delta}(\nabla_{x_{k}}^{k(1)}),\Pi_{T\Delta}(\nabla_{x_{k}}^{k(2)})\rangle]=||\Pi_{T\Delta}(\mathbb{E}[\nabla_{x_{k}}^{k}])||^{2}.","Lemma 5 shows that the squared norm of the projected-gradient can be estimated unbiasedly using two independent gradient estimates, enabling unbiased estimation of the loss function $\mathcal{L}(\mathbf{x})$ under mixed strategies."
ICLR_2024_oral_6,6,". This estimator can be constructed in several ways. The most expensive, an exact estimator, is constructed by marginalizing player \(k\)'s payoff tensor over all other players' strategies. However, a cheaper estimate can be obtained at the expense of higher variance by approximating this marginalization with a Monte Carlo (MC) estimate of the expectation. Specifically, if we sample a single action for each of the remaining players, we can construct an unbiased estimate of player \(k\)'s gradient by considering the payoff of each of its actions against the sampled background strategy. Lastly, we can consider constructing an estimate of player \(k\)'s gradient by sampling only a single action from player \(k\) to represent their entire gradient. Each of these approaches is outlined in Table 2 along with the query complexity (Babichenko, 2016) of computing the estimator and bounds on the values it can take (Lemma 9).

We can extend Lemma 3 to one that holds under \(T\) samples with probability \(1-\delta\) by applying, for example, a Hoeffding bound: \(\epsilon\leq f\big{(}\hat{\mathcal{L}}(\mathbf{x})+\mathcal{O}(\sqrt{\frac{1}{T} \ln(1/\delta)})\big{)}\) where \(\hat{\mathcal{L}}\) is an MC estimate of \(\mathcal{L}\).

### Interior Equilibria

We discussed earlier that \(\mathcal{L}(\mathbf{x})\) captures interior equilibria. But some games may only have _partially mixed_ equilibria, i.e., equilibria that lie on the boundary of the simplex. We show how to circumvent this shortcoming by considering quantal response equilibria (QREs), specifically, logit equilibria. By adding an entropy bonus to each player's utility, we can

* guarantee **all** equilibria are interior,
* still obtain unbiased estimates of our loss,
* maintain an upper bound on the exploitability \(\epsilon\) of any approximate Nash equilibrium in the original game (i.e., the game without an entropy bonus).

Define \(u^{\tau}_{k}(\mathbf{x})=u_{k}(\mathbf{x})+\tau S(x_{k})\) where Shannon entropy \(S(x_{k})=-\sum_{l}x_{kl}\ln(x_{kl})\) is \(1\)-strongly concave with respect to the \(1\)-norm (Beck and Teboulle, 2003). It is known that Nash equilibria of entropy-regularized games satisfy the conditions for logit equilibria (Leonardos et al., 2021), which are solutions to the fixed point equation \(x_{k}=\texttt{softmax}(\frac{1}{\Gamma}\nabla^{k}_{x_{k}})\). The softmax should make it clear to the reader that all probabilities have positive mass at positive temperature.

Recall that in order to construct an unbiased estimate of our loss, we simply needed to construct unbiased estimates of player gradients. The introduction of the entropy term to player \(k\)'s utility is special in that it depends entirely on known quantities, i.e., the player's own mixed strategy. We can directly and deterministically compute \(\tau\frac{dS}{dx_{k}}=-\tau(\ln(x_{k})+\mathbf{1})\) and add this to our estimator of \(\nabla^{k(p)}_{x_{k}}\): \(\hat{\nabla}^{k\tau(p)}_{x_{k}}=\hat{\nabla}^{k(p)}_{x_{k}}+\tau\frac{dS}{dx _{k}}\). Consider our loss function refined from (2) with changes in blue:",\mathcal{L}^{\tau}(\mathbf{x})=\sum_{k}\eta_{k}||\Pi_{T\Delta}(\nabla^{k\tau}_{x_ {k}})||^{2}.,,\mathcal{L}^{\tau}(\mathbf{x})=\sum_{k}\eta_{k}||\Pi_{T\Delta}(\nabla_{x_{k}}^{k}+\tau\frac{dS}{dx_{k}})||^{2}.,"This equation defines the entropy-regularized loss function $\mathcal{L}^{\tau}(\mathbf{x})$, where the gradient of player $k$'s utility is augmented with the derivative of the entropy term, ensuring all equilibria are interior and maintaining unbiased estimation properties."
ICLR_2024_oral_6,7,"As mentioned above, the utilities with entropy bonuses are still concave, therefore, a similar bound to Lemma 2 applies. We use this to prove the QRE counterpart to Lemma 3 where \(\epsilon_{QRE}\) is the exploitability of an approximate equilibrium in a game with entropy bonuses.

\begin{table}
\begin{tabular}{c|l|l|l}  & Exact & Sample Others & Sample All \\ \hline Estimator of \(\nabla^{k(p)}_{x_{k}}\) & \([u_{k}(a_{kl},x_{-k})]_{l}\) & \([u_{k}(a_{kl},a_{-k}\sim x_{-k})]_{l}\) & \(m_{k}u_{k}(a_{kl}\sim\mathcal{U}(\mathcal{A}_{k}),a_{-k}\sim x_{-k})e_{l}\) \\ \(\hat{\nabla}^{k(p)}_{x_{k}}\) Bounds & \([0,1]\) & \([0,1]\) & \([0,m_{k}]\) \\ \(\hat{\nabla}^{k(p)}_{x_{k}}\) Query Cost & \(\prod_{k=1}^{n}m_{k}\) & \(m_{k}\) & \(1\) \\ \(\hat{\mathcal{L}}\) Bounds & \(\pm^{1/4}\sum_{k}\eta_{k}m_{k}\) & \(\pm^{1/4}\sum_{k}\eta_{k}m_{k}\) & \(\pm^{1/4}\sum_{k}\eta_{k}m_{k}^{3}\) \\ \(\hat{\mathcal{L}}\) Query Cost & \(n\prod_{k=1}^{n}m_{k}\) & \(2n\bar{m}\) & \(2n\) \\ \end{tabular}
\end{table}
Table 2: Examples and Properties of Unbiased Estimators of Loss and Player Gradients (\(\hat{\nabla}^{k(p)}_{x_{k}}\)).

**Lemma 4**.: _The entropy regularized exploitability, \(\epsilon_{QRE}\), of a joint strategy \(\mathbf{x}\), is upper bounded as:_",\epsilon_{QRE}\leq\sqrt{\frac{2n}{\min_{k}\eta_{k}}}\sqrt{\mathcal{L}^{\tau}( \mathbf{x})}\overset{\text{\text{\tiny{def}}}}{=}f(\mathcal{L}^{\tau}).,,"\epsilon_{QRE} \leq \sqrt{\frac{2n}{\min_{k}\eta_{k}}}\sqrt{\mathcal{L}^{\tau}(\mathbf{x})} + \tau C,",
ICLR_2024_oral_6,8,"Lastly, we establish a connection between quantal response equilibria and Nash equilibria that allows us to approximate Nash equilibria in the original game via minimizing our modified loss \(\mathcal{L}^{\tau}(\mathbf{x})\).

**Lemma 13** (\(\mathcal{L}^{\tau}\) Scores Nash Equilibria).: _Let \(\mathcal{L}^{\tau}(\mathbf{x})\) be our proposed entropy regularized loss function and \(\mathbf{x}\) be an approximate QRE. Then it holds that_",\epsilon\leq\tau\log\Big{(}\prod_{k}m_{k}\Big{)}+\sqrt{\frac{2n}{\min_{k}\eta_ {k}}}\sqrt{\mathcal{L}^{\tau}(\mathbf{x})}\overset{\text{\text{\tiny{def}}}}{=} f_{\tau}(\mathcal{L}^{\tau}).,,"\lim_{\tau\rightarrow 0}\mathcal{L}^{\tau}(\mathbf{x})=\mathcal{L}(\mathbf{x}),",Describing the convergence of the entropy-regularized loss to the original loss as the temperature parameter approaches zero.
ICLR_2024_oral_6,9,"This upper bound is plotted as a heatmap for a familiar Chick game in the top row of Figure 1. First, notice how pure equilibria are not visible as minima for zero temperature, but appear for slightly warmer temperatures. Secondly, notice that NashConv in the bottom row is unable to capture the interior Nash equilibrium because of its high bias under sampled play. In contrast, our proposed loss \(\mathcal{L}^{\tau}\) is guaranteed to capture all equilibria at low temperature \(\tau\).

## 5 Analysis

In the preceding section we established a loss function that upper bounds the exploitability of an approximate equilibrium. In addition, the zeros of this loss function have a one-to-one correspondence with quantal response equilibria (which approximate Nash equilibria at low temperature).

Here, we derive properties that suggest it is ""easy"" to optimize. While this function is generally non-convex and may suffer from a proliferation of saddle points (Figure 2), it is Lipschitz continuous (over the relevant subset of the interior) and bounded. These are two commonly made assumptions in the literature on non-convex optimization, which we leverage in Section 6. In addition, we can derive its gradient, its Hessian, and characterize its behavior around global minima.

**Lemma 14**.: _The gradient of \(\mathcal{L}^{\tau}(\mathbf{x})\) with respect to player \(l\)'s strategy \(x_{l}\) is_",\nabla_{x_{l}}\mathcal{L}^{\tau}(\mathbf{x})=2\sum_{k}\eta_{k}B_{kl}^{\top}\Pi_{T \Delta}(\nabla_{x_{k}}^{kT}),,\nabla_{x_{l}}\mathcal{L}^{\tau}(\mathbf{x})=\sum_{k}\eta_{k}\Big{(}\Pi_{T\Delta}(\nabla^{k\tau}_{x_{k}})\Big{)}^{\top}\frac{\partial}{\partial x_{l}}\Big{(}\Pi_{T\Delta}(\nabla^{k\tau}_{x_{k}})\Big{)}.,"The gradient of the entropy-regularized loss function with respect to a player's strategy, capturing how changes in that player's strategy affect the overall loss through the projected gradient of each player."
ICLR_2024_oral_6,10,"_where \(B_{kl}=-\tau[I-\frac{1}{m_{l}}\mathbf{1}\mathbf{1}^{\top}]\texttt{diag}\big{(}\frac{1} {x_{l}}\big{)}\) and \(B_{kl}=[I-\frac{1}{m_{k}}\mathbf{1}\mathbf{1}^{\top}]H_{kl}^{k}\) for \(k\neq l\)._

Figure 1: Effect of Sampled Play on a Biased Loss. The first row displays the expectation of the upper bound guaranteed by our proposed loss \(\mathcal{L}^{\tau}\) with \(\eta_{k}=1\) for all \(k\). The second row displays the expectation of NashConv under sampled play, i.e., \(\sum_{k}\epsilon_{k}\) where \(\epsilon_{k}=\mathbb{E}_{a_{-k}\sim\mathcal{L}^{\tau}-k}[\max_{a_{k}}u_{k}^{ \tau}(\mathbf{a})]-\mathbb{E}_{\mathbf{a}\sim\mathbf{x}}[u_{k}^{\tau}(\mathbf{a})]\). To be consistent, we subtract the offset \(\tau\log(m^{2})\) from \(f_{\tau}(\mathcal{L}^{\tau})\) per Lemma 13, which relates the exploitability at positive temperature to that at zero temperature. The resulting loss surface clearly shows NashConv fails to recognize any interior Nash equilibrium due to its inherent bias.

**Lemma 16**.: _The Hessian of \(\mathcal{L}^{\tau}(\mathbf{x})\) can be written_",\operatorname{\textsl{Hess}}(\mathcal{L}^{\tau})=2\big{[}\tilde{B}^{\top} \tilde{B}+T\Pi_{T\Delta}(\tilde{\nabla}^{\tau})\big{]},,\nabla^{2}_{x_{l}x_{m}}\mathcal{L}^{\tau}(\mathbf{x})=2\sum_{k}\eta_{k}\Big{(}B_{kl}^{\top}B_{km}\Pi_{T\Delta}(\nabla_{x_{k}}^{k\tau})+B_{kl}^{\top}\Pi_{T\Delta}(H^{k}_{km})\Big{)},where $B_{kl}$ is defined as in Equation 10 and $H^{k}_{km}$ represents the Hessian of player $k$'s entropy-regularized utility with respect to player $m$'s strategy.
ICLR_2024_oral_6,11,"_where \(\tilde{B}_{kl}=\sqrt{\eta_{k}}B_{kl}\), \(\Pi_{T\Delta}(\tilde{\nabla}^{\tau})=[\eta_{1}\Pi_{T\Delta}(\nabla^{1\tau}_{x _{1}}),\ldots,\eta_{n}\Pi_{T\Delta}(\nabla^{n\tau}_{x_{n}})]\), and we augment \(T\) (the \(3\)-player approximation to the game, \(T^{k}_{lqk}\)) so that \(T^{l}_{ll}=\tau\texttt{diag}3\big{(}\frac{1}{x_{l}^{2}}\big{)}\)._

At an NE, the latter term disappears because \(\Pi_{T\Delta}(\nabla^{k\tau}_{x_{k}})=\mathbf{0}\) for all \(k\) (Lemma 1). If \(\mathcal{X}\) was \(\mathbb{R}^{n\tilde{m}}\), then we could simply check if \(\tilde{B}\) is full-rank to determine if \(Hess\succ 0\), i.e., if \(\mathcal{L}^{\tau}\) is locally strongly-convex. However, \(\mathcal{X}\) is a simplex product, and we only care about curvature in directions toward which we can update our strategy profile \(\mathbf{x}\). Toward that end, define \(M\) to be the \(n(\bar{m}+1)\times n\bar{m}\) matrix that stacks \(\tilde{B}\) on top of a repeated identity matrix that encodes orthogonality to the simplex:","M(\mathbf{x})=\begin{bmatrix}-\sqrt{\eta_{1}}\Pi_{T\Delta}(\frac{1}{x_{1}})&\sqrt{ \eta_{1}}\Pi_{T\Delta}(H^{1}_{12})&\ldots&\sqrt{\eta_{1}}\Pi_{T\Delta}(H^{1}_{ 1n})\\ \vdots&\vdots&\vdots&\vdots\\ \sqrt{\eta_{n}}\Pi_{T\Delta}(H^{n}_{n1})&\ldots&\sqrt{\eta_{n}}\Pi_{T\Delta}( H^{n}_{n,n-1})&-\tau\sqrt{\eta_{n}}\Pi_{T\Delta}(\frac{1}{x_{n}})\\ \mathbf{1}_{1}^{\top}&0&\ldots&0\\ \vdots&\vdots&\vdots&\vdots\\ 0&\ldots&0&\mathbf{1}_{n}^{\top}\end{bmatrix}",where \(\Pi_{T\Delta}(z\in\mathbb{R}^{n\times b})=[I_{a}-\frac{1}{a}\mathbf{_{1}}\mathbf{_{ 1}}^{\top}]z\) subtracts the mean from each column of \(z\) and \(\frac{1}{x_{k}}\) is shorthand for \(\texttt{diag}\big{(}\frac{1}{x_{k}}\big{)}\),\operatorname{\textsl{Hess}}(\mathcal{L}^{\tau})=2\big{[}\tilde{B}^{\top} \tilde{B}+T\Pi_{T\Delta}(\tilde{\nabla}^{\tau})\big{]},"where $\tilde{B}_{kl}=\sqrt{\eta_{k}}B_{kl}$, $\Pi_{T\Delta}(\tilde{\nabla}^{\tau})=[\eta_{1}\Pi_{T\Delta}(\nabla^{1\tau}_{x _{1}}),\ldots,\eta_{n}\Pi_{T\Delta}(\nabla^{n\tau}_{x_{n}})]$, and we augment $T$ (the $3$-player approximation to the game, $T^{k}_{lqk}$) so that $T^{l}_{ll}=\tau\texttt{diag}3\big{(}\frac{1}{x_{l}^{2}}\big{)}$."
ICLR_2024_oral_60,1,"## 2 Inductive Reasoning with LMs: Experimental Setup

We consider the rule induction problem of inferring a function \(f:\mathcal{X}\rightarrow\mathcal{Y}\) that maps an input \(x\in\mathcal{X}\) to an output \(y\in\mathcal{Y}\). The rule, \(f\), can take various forms, such as mathematical operations, grammar,

Figure 1: An overview of the iterative hypothesis refinement approach. We generate \(N\) hypotheses per iteration and iterate up to the maximum number of iterations \(T\) (top). Example instances and representative good and bad rules for each task (bottom).

and even natural language descriptions (see Appendix D for examples). For each task \(\tau\), we have a set of examples \(\mathcal{D}_{\tau}\) consisting of input-output pairs \((x,y)\). We further divide \(\mathcal{D}_{\tau}\) into seen examples, \(\mathcal{D}_{\tau}^{s}\), and unseen examples, \(\mathcal{D}_{\tau}^{u}\). The goal is to induce the \(f\) that best describes \(\mathcal{D}_{\tau}\) using only \(\mathcal{D}_{\tau}^{s}\). A good rule thus requires a balance between precision and coverage, i.e., it should be simultaneously _expressive_ enough to capture \(\mathcal{D}_{\tau}^{s}\) and _generalizable_ to \(\mathcal{D}_{\tau}^{u}\).

We assess an LM's ability to induce rules through prompting. Let \(h\in\Sigma^{*}\) be a rule generated by an LM, where \(\Sigma\) is the LM's vocabulary. Since we cannot directly apply \(h\) to \(x\) (\(h\) is just a piece of text), we make use of an interpreter \(I_{\tau}:\Sigma^{*}\rightarrow\mathcal{F}\) for each task \(\tau\) where \(\mathcal{F}\) is the space of all functions from \(\mathcal{X}\) to \(\mathcal{Y}\) (i.e., \(f\in\mathcal{F}\)). That is, the interpreter \(I_{\tau}\) ""compiles"" \(h\) into a function that can be applied to \(x\).3 The quality of rules is evaluated based on their performance on unseen examples. Given an induced rule \(h\) and \(n\) unseen examples \(\mathcal{D}_{\tau}^{u}=\{(x_{1},y_{1}),...,(x_{n},y_{n})\}\), we derive outputs \(y_{i}^{\prime}\) by applying \(I_{\tau}(h)\) to input \(x_{i}\),

Footnote 3: For example, if \(h\) is the string representation of a Python function, \(I_{\tau}(h)\) can be the actual Python function. Note that the same rule \(h\) could be applied differently by different interpreters.",y_{i}^{\prime}=I_{\tau}(h)(x_{i}).,,y_i' = I_{\tau}(h)(x_i),"This equation formalizes the application of the interpreted rule $I_{\tau}(h)$ to input $x_i$, producing the predicted output $y_i'$ for unseen examples in the rule induction task."
ICLR_2024_oral_60,2,"Although it is ideal to have interpreters that can correctly apply \(h\), such perfect interpreters might not always be available. Importantly, interpreters have no access to \(\mathcal{D}_{\tau}^{s}\), and thus, the rule must contain sufficient information for interpreters to achieve strong performance when applying the rule.

We evaluate the quality of a rule \(h\) using accuracy. More formally, for a task \(\tau\) containing a set of unseen examples \(\mathcal{D}_{\tau}^{u}\), we first define the accuracy for this particular task as","a_{\tau}=\frac{1}{|\mathcal{D}_{\tau}^{u}|}\sum_{(x,y)\in\mathcal{D}_{\tau}} \mathbbm{1}\big{[}I_{\tau}(h)(x)=y\big{]}.",,"\text{Accuracy}_{\tau}(h) = \frac{1}{|\mathcal{D}_{\tau}^{u}|} \sum_{(x,y) \in \mathcal{D}_{\tau}^{u}} \mathbb{1}\left(I_{\tau}(h)(x) = y\right).",Equation 2 defines the accuracy of a rule $h$ on a task $\tau$ as the fraction of unseen examples in $\mathcal{D}_{\tau}^{u}$ for which the interpreter $I_{\tau}(h)$ correctly predicts the output $y$.
ICLR_2024_oral_60,3,Let \(\mathcal{T}\) denotes the set of all tasks within a dataset. We define _raw accuracy_\(c\) and _task accuracy_\(c_{t}\) as,c=\frac{1}{|\mathcal{T}|}\sum_{\tau\in\mathcal{T}}a_{\tau}\hskip 28.452756ptc_{t} =\frac{1}{|\mathcal{T}|}\sum_{\tau\in\mathcal{T}}\mathbbm{1}\big{[}a_{\tau}=1 \big{]}.,,"c = \frac{1}{|\mathcal{T}|} \sum_{\tau \in \mathcal{T}} a_{\tau}, \quad c_{t} = \frac{1}{|\mathcal{T}|} \sum_{\tau \in \mathcal{T}} \mathbbm{1}[a_{\tau} = 1].","Raw accuracy $c$ is the average task accuracy across all tasks, while task accuracy $c_t$ is the fraction of tasks where the rule achieves perfect accuracy."
ICLR_2024_oral_60,4,"While raw accuracy is the standard metric used in prior work, task accuracy could better estimate an LM's induction capability: a model should ideally consistently solve examples within a task. We use GPT-4 (gpt-4-0613; OpenAI, 2023) for all experiments and analyses. We include additional results of other models, including GPT-3.5 (gpt-3.5-turbo-0613), Claude-2 (Anthropic, 2023), and LLAMA2-70B (Touvron et al., 2023) in Appendix B.

### Iterative Hypothesis Refinement

We consider an iterative approach to induce rules from LMs. We use LMs to propose a set of rules (i.e., hypotheses). We then select the best rule based on scores calculated using the interpreter function. We provide feedback to LMs for further refinement. See Figure 1 for an overview.

Specifically, given \(k\) exemplars \(\mathcal{D}_{\tau}^{s}=\{(x_{1},y_{1}),...,(x_{k},y_{k})\}\), at iteration \(t\), we sample \(N\) hypotheses of rules, \(H^{t}=\{h_{1}^{s},...,h_{N}^{t}\}\), from a prompted LM,","h^{t}\sim P_{\text{LM}}\big{(}\cdot\,|d^{t-1},x_{1},y_{1},...,x_{k},y_{k}),",where \(d^{t-1}\) is the feedback from previous iterations and which is set to be an empty string at the initial iteration,"H^{t}=\{h_{1}^{t},...,h_{N}^{t}\} \sim \text{LM}(\mathcal{D}_{\tau}^{s}; \theta),","where $H^{t}$ denotes the set of $N$ hypotheses generated at iteration $t$ from a prompted language model parameterized by $\theta$, given the seen examples $\mathcal{D}_{\tau}^{s}$."
ICLR_2024_oral_60,5,"on. Each hypothesis is re-ranked based on a scoring function \(s(h,\mathcal{D}_{\tau}^{s})\). We use accuracy over seen examples as the scoring function,","s(h,\mathcal{D}_{\tau}^{s})=\frac{1}{|\mathcal{D}_{\tau}^{s}|}\sum_{(x,y)\in \mathcal{D}_{\tau}^{s}}\mathbbm{1}\big{[}I_{\tau}(h)(x)=y\big{]}.",,"s(h,\mathcal{D}_{\tau}^{s})=\frac{1}{|\mathcal{D}_{\tau}^{s}|}\sum_{(x,y)\in\mathcal{D}_{\tau}^{s}} \mathbbm{1}\big{[}I_{\tau}(h)(x)=y\big{]}.",Scoring function for re-ranking hypotheses based on accuracy over seen examples.
ICLR_2024_oral_60,6,"The best hypothesis is selected via,","h^{t^{*}}=\operatorname*{arg\,max}_{h^{\prime}\in H^{t}}s(h^{\prime},\mathcal{ D}_{\tau}^{s}).",,"h^{*} = \arg\max_{h \in H^{t}} s(h,\mathcal{D}_{\tau}^{s}).",The best hypothesis $h^{*}$ is selected by maximizing the scoring function $s$ over the set of hypotheses $H^{t}$ and the seen examples $\mathcal{D}_{\tau}^{s}$.
ICLR_2024_oral_61,1,"### Goal-conditioned Policy

The pre-trained model is fixed during the RL phase, requiring it to have the capacity to model all the behaviors in the dataset and to decode action sequences accurately. We find that previousmethods (Pertsch et al., 2021; 20; Shi et al., 2023) that model the low-level behaviors with continuous latent variables struggle to model the complex distribution of action sequences in Minecraft datasets which feature large amounts of data, long action sequences to exhibit certain behaviors, and a large action space (more analysis in Appendix B.2). Recent work (Lifshitz et al., 2023) has demonstrated that training a goal-conditioned policy via behavior cloning with a transformer (Vaswani et al., 2017) architecture can stably learn various behaviors from these data. Therefore, we employ a similar approach to pre-train a goal-conditioned model.

A goal refers to the final state of a sub-trajectory in the dataset, representing the outcome of the short-term behavior. The goal-conditioned model \(P_{\phi}\) uses the state space \(S\) to be the space \(Z\) and predicts the action for one step:","P(a_{t:t+k}|s_{t},s^{g})=\int_{s_{t+1},\ldots,s_{t+k}}ds_{t+1}\ldots ds_{t+k} \prod_{i=t}^{t+k}P_{\phi}(a_{i}|s_{i},s^{g})\mathcal{P}(s_{i+1}|s_{i},a_{i}),",where \(s^{g}\in S\) is the goal state,"\begin{equation}
    P_{\phi}(a_t | s_t, g) = \text{Transformer}_{\phi}(s_t, g)
\end{equation}",The goal-conditioned policy $P_{\phi}$ predicts the action $a_t$ at time $t$ given the current state $s_t$ and the goal state $g$ using a Transformer architecture parameterized by $\phi$.
ICLR_2024_oral_61,2,"te. Starting from \(s_{t}\), \(P_{\phi}\) aims at reaching the goal state \(s^{g}\) after executing the actions sampled from it for \(k\) steps.

To learn \(P_{\phi}\) from data, we use a variant of hindsight relabeling (Andrychowicz et al., 2017) to label each state-action sample in the dataset with a future state as the goal state. For a \(k\)-step subsequence \(\tau=(s_{t},a_{t},\cdots,s_{t+k},a_{t+k})\) in an episode, we label each sample \((s_{i},a_{i}),t\leq i\leq t+k\) with the goal state \(s^{g}=s_{t+k}\). We train \(P_{\phi}\) with behavior cloning, minimizing the negative log-likelihood of action prediction:","\mathcal{L}(\phi)=\mathbb{E}_{D}[-\log P_{\phi}(a_{i}|s_{i},s^{g})].",,"\mathcal{L}_{\text{BC}} = -\sum_{i=t}^{t+k} \log P_{\phi}(a_i | s_i, s^g)",where the loss is computed over the $k$-step subsequence with goal state $s^g = s_{t+k}$
ICLR_2024_oral_61,3,"In practice, to enable the model to reach goals after different time steps, we randomly sample \(k\) within a range. When states are compact vectors representing the physical state of objects in the environment, we use the raw state as the goal. When the environment is a POMDP with image observations, we use the embedding from a pre-trained image encoder as the goal. For instance, Steve-1 (Lifshitz et al., 2023) uses the embedding of \(o_{t+k-16:t+k}\) from the MineCLIP's vision encoder (Fan et al., 2022) in Minecraft.

## 4 Method

In this section, we present the proposed method PTGM, utilizing the pre-trained goal-conditioned policy \(P_{\phi}(a_{t}|s_{t},s^{g})\) to provide temporal abstractions for RL in downstream tasks. In RL, we train a high-level policy \(\pi_{\theta}(s^{g}|s_{t})\) which outputs a goal state to guide the low-level goal-conditioned policy \(P_{\phi}\) to act in the environment for \(k\) steps. To enhance the sample efficiency and stability of RL, we propose a goal clustering method and a pre-trained goal prior model. Figure 1 gives an overview of PTGM.

Figure 1: Overview of PTGM. A goal-conditioned policy and a goal prior model are pre-trained on the large task-agnostic dataset. The goals in the dataset are clustered into a discrete goal space. In downstream RL tasks, we train a high-level policy that outputs discrete actions to provide goals for the goal-conditioned policy, and regularize it with the KL divergence to the goal prior model.

### Clustering in the Goal Space

The goals \(s^{g}\in S\) from the high-dimensional state space introduce a high-dimensional continuous action space for the high-level policy, making RL sample-inefficient. To tackle this challenge, we propose to cluster the states in the dataset to discretize the goal space, constructing a discrete action space for the high-level policy. We sample a large set of states from \(D\), apply t-SNE (Maaten and Hinton, 2008) to reduce the dimension of states, and apply a clustering algorithm such as K-Means (Lloyd, 1982) to group similar goal states together and output \(N\) clusters. The discretized goal space is represented with \(G=\{i:s^{g}_{i}\}_{i=1}^{N}\), where \(s^{g}_{i}\) is the goal state of the \(i\)-th cluster center. This converts the action space of the high-level policy into a discrete action space \(A^{h}=[N]\) and constrains the high-level policy to output goals in the cluster centers.

We observe that compressing goal states into the discrete goal space does not significantly decrease the agent's model capacity to perform various behaviors. The reasons come from two aspects. Firstly, the clustering algorithm groups similar goals together. The cluster center can represent goals in its cluster that correspond to similar agent behaviors. Secondly, the goal-conditioned model pre-trained on the large dataset as the low-level policy can elicit generalizability on goals. The model can extract behavior information in the goal state, thereby generating correct behaviors even when the provided goal is distant from the current environment state. Given the same goal, the model can exhibit diverse behaviors in different states. These claims are substantiated in our experimental results in Section 5.4. Therefore, using the discrete goal space, the low-level policy still has the capacity to cover various behaviors in the dataset.

### Pre-Training the Goal Prior Model

In RL, the agent is able to perform smooth and reasonable behaviors within \(k\) consecutive steps given a goal. However, the high-level policy lacks prior knowledge to provide reasonable goals, thereby should uniformly explore the goal space to learn the task. We propose to learn this prior knowledge from the dataset by pre-training a goal prior model, improving the sample efficiency and stability of training the high-level policy.

The goal prior model \(\pi^{p}_{\psi}(a^{h}|s)\) has the same structure as the high-level policy, where \(a^{h}\in A^{h}\) is the index of the goal cluster centers. This model is trained to predict the distribution of future goals given the current state, using the clustered goal space \(G\). Similar to training the goal-conditioned model, we sample states and subsequent goal states \((s_{t},s^{g})\) from the dataset. In the discretized goal space \(G\), we match the goal that is closest to \(s^{g}\) based on cosine similarity \(a^{h}=\arg\max_{i\in[N]}\left(\frac{s^{g}_{i}\cdot s^{g}}{\|s^{g}_{i}\|\cdot\| s^{g}\|}\right)\). The training objective for the goal prior model is to minimize the negative log-likelihood of goal prediction:",\mathcal{L}(\psi)=\mathbb{E}_{D}[-\log\pi^{p}_{\psi}(a^{h}|s_{t})].,,\mathcal{L}(\psi)=\mathbb{E}_{D}[-\log \pi^{p}_{\psi}(a^{h}|s_{t})].,The goal prior model is trained to predict the distribution of future goals given the current state using the clustered goal space $G$.
ICLR_2024_oral_61,4,"The pre-trained goal prior model acts as a regularizer for the high-level policy during RL, providing intrinsic rewards that guide the agent's exploration towards possible goals in the dataset.

### Reinforcement Learning with PTGM

Given the goal clusters \(G\), the pre-trained low-level policy \(P_{\phi}\), and the goal prior model \(\pi^{p}_{\psi}\), we proceed with training the high-level policy using RL for downstream tasks. At each time step, the high-level policy \(\pi_{\theta}(a^{h}|s)\) selects an index of the goal \(s^{g}_{a^{h}}\) in the clustered goal space. The fixed low-level policy acts in the environment for \(k\) steps conditioned on \(s^{g}_{a^{h}}\). The high-level policy is updated based on the environment rewards and the intrinsic rewards from the goal prior model.

The overall objective for training the high-level policy is to maximize the expected return:","J(\theta)=\mathbb{E}\pi_{\theta}[\sum_{t=0}^{\infty}\gamma^{t}(\sum _{i=kt}^{(k+1)t}R(s_{i},a_{i})-\alpha D_{\mathrm{KL}}(\pi^{p}_{\psi}(a^{h }|s_{kt})\|\pi_{\theta}(a^{h}|s_{kt})))],",where \(t\) represents the number of steps for the high-level policy and \(\alpha\) is a hyperparameter balancing the environmental rewards and the intrinsic rewards,"\mathcal{J}(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}, P_{\phi}} \left[ \sum_{t=0}^{T} \gamma^{t} (r_{t} + \beta \log \pi^{p}_{\psi}(a^{h}_{t} | s_{t})) \right],","where $r_t$ is the environment reward, $\beta$ controls the influence of the intrinsic reward from the goal prior model $\pi^{p}_{\psi}$, and $\gamma$ is the discount factor."
ICLR_2024_oral_63,1,"### Energy-based models

EBMs are a class of models that learn an energy function \(f_{\theta}:\mathcal{X}\to\mathbb{R}\) mapping inputs \(x\) (in \(\mathbb{R}^{d}\)) to a scalar ""energy"" value. The data distribution is approximated by the Boltzmann distribution",p_{\theta}(x)\propto e^{-f_{\theta}(x)}.,,"p_{\theta}(x) = \frac{1}{Z(\theta)} \exp(-f_{\theta}(x)),",where $Z(\theta)$ is the partition function that normalizes the distribution.
ICLR_2024_oral_63,2,"EBMs are typically trained via contrastive divergence (Hinton, 2002), and new samples are drawn from \(p_{\theta}(x)\) by Markov-Chain Monte Carlo (MCMC). Details of the loss function used in this work are given in Section 3. In Langevin MCMC, samples are initialized from a known data point or random noise and refined with (discretized) Langevin diffusion","x_{k+1}=x_{k}-\delta\,\nabla f_{\theta}(x_{k})+\sqrt{2\delta}\,\varepsilon_{k },\,\varepsilon_{k}\sim\mathcal{N}(0,I_{d}),","where \(\nabla\) denotes the gradient of the energy function with respect to inputs, \(k\) is the sampling step, \(\delta\) is the (discretization) step size, and the noise \(\varepsilon_{k}\) is drawn from the normal distribution at each step",x_{t+1} = x_t - \epsilon \nabla f_{\theta}(x_t) + \sqrt{2\epsilon} \xi_t,"Discretized Langevin diffusion update rule for sampling from the EBM's Boltzmann distribution, where $\xi_t \sim \mathcal{N}(0, I)$ represents standard Gaussian noise and $\epsilon$ controls the step size."
ICLR_2024_oral_63,3,"ep.

### Neural empirical Bayes

In NEB, the random variable \(X\) is transformed with additive Gaussian noise \(Y=X+\mathcal{N}(0,\sigma^{2}I_{d})\). The least-squares estimator of \(X\) given \(Y=y\) is given by (Robbins, 1956; Miyasawa, 1961)","\hat{x}(y)=y+\sigma^{2}\nabla\log p(y),",where \(p(y)=\int p(y|x)p(x)dx\) is the probability distribution function of the smoothed density,"\hat{x}(y)=\mathbb{E}[X|Y=y]=y+\sigma^{2}\,\nabla\log p_{Y}(y),","where $\hat{x}(y)$ is the least-squares estimator of $X$ given the noisy observation $Y=y$, and $\nabla \log p_Y(y)$ is the score function of the noisy data distribution."
ICLR_2024_oral_63,4,"ty.2 This estimator is often expressed directly in terms of \(g(y)=\nabla\log p(y)\) known as the score function (Hyvarinen, 2005) which is parameterized with a neural network denoted by \(g_{\phi}:\mathbb{R}^{d}\to\mathbb{R}^{d}\). The least-squares estimator then takes the following parametric form:

Footnote 2: We follow the convention \(p(x)\coloneqq p_{X}(x),p(y)\coloneqq p_{Y}(y)\), etc.",\hat{x}_{\phi}(y)=y+\sigma^{2}g_{\phi}(y).,,"\hat{x}_{\phi}(y)=y+\sigma^{2}g_{\phi}(y),",Parametric form of the least-squares estimator using a neural network parameterized score function $g_{\phi}(y)$
ICLR_2024_oral_63,5,Putting this all together leads to the following learning objective,"\mathcal{L}(\phi)=\mathbb{E}_{x\sim p(x),y\sim p(y|x)}\|x-\hat{x}_{\phi}(y)\|^ {2},",,\min_{\phi}\mathbb{E}_{p(x)}\mathbb{E}_{p(y|x)}\left[\left\Vert x-\hat{x}_{\phi}(y)\right\Vert ^{2}\right].,The learning objective minimizes the expected squared error between the true data point $x$ and its estimate $\hat{x}_{\phi}(y)$ obtained from the noisy observation $y$.
ICLR_2024_oral_63,6,"which is optimized with stochastic gradient descent. Notably, no MCMC sampling is required during learning. In short, the objective is ""learning to denoise"" with an empirical Bayes formulation (discussed further in Appendix B.3).

## 3 Antibody discovery and design

### Discrete walk-jump sampling

Following training of the denoising network, \(g_{\phi}\), one can sample noisy data using the learned score function \(g_{\phi}(y)\) with Langevin MCMC (replace \(-\nabla f\) with \(g\) in Eq. 1). For any such draws \(y_{k}\), clean samples from the true data manifold \(\mathcal{M}\) are obtained by ""jumping"" back to \(\mathcal{M}\) with the least-squares estimator \(\hat{x}_{\phi}(y_{k})=y_{k}+\sigma^{2}g_{\phi}(y_{k})\). This is the walk-jump sampling (WJS) scheme. A key property of WJS is the fact that the least-squares estimation (jump) is _decoupled_ from the Langevin MCMC (walk).

Here, we take advantage of this decoupling to train an EBM with maximum likelihood estimation on the smoothed distribution of noisy sequences, generate noisy samples with Langevin MCMC, and denoise samples with a separately trained neural network, the least-squares estimator. The algorithm for discrete walk-jump sampling is given in Algo. 1. Our algorithm is general and applies to any discrete sequence inputs of a fixed vocabulary. In Fig. 1 we show samples generated from a single chain of MCMC. Unlike a diffusion model, every sample along the chain collectively forms a valid set of samples from the underlying distribution, because of the decoupled walk (sampling) and jump (denoising) steps. dWJS also produces fast-mixing chains, such that many diverse modes (proteinclasses) are sampled in a single chain. Samples are folded with EquiFold (Lee et al., 2022) for visualization and confirmation of structural validity.

The EBM is trained by maximizing the log-likelihood of _noisy_ data under the model:","\operatorname*{arg\,max}_{\theta}\mathbb{E}_{y\sim p_{Y}}[\log p_{\theta}( y)]=\operatorname*{arg\,max}_{\theta}(\mathbb{E}_{y^{-}\sim p_{Y}(y)}[f_{ \theta}(y^{-})]-\mathbb{E}_{y^{+}\sim p_{Y}}[f_{\theta}(y^{+})]),",where \(y^{+}\) are noisy training data and \(y^{-}\) are noisy data sampled from the model,"\log p_{\theta}(y)\propto -f_{\theta}(y),","where $p_{\theta}(y)$ denotes the model's distribution over noisy data $y$, and $f_{\theta}(y)$ is the energy function learned by the EBM."
ICLR_2024_oral_63,7,"el.

With this objective, the model aims to decrease the energy of noisy training data (""positive"" samples \(y^{+}\)) while increasing the energy of noisy data sampled from the model (""negative"" samples \(y^{-}\)) in expectation. The following identity is behind the positive/negative phases in the EBM training:","\nabla_{\theta}\log p_{\theta}(y) =-\nabla_{\theta}f_{\theta}(y)-\nabla_{\theta}\log Z(\theta) || =-\nabla_{\theta}f_{\theta}(y)+\frac{\int\nabla_{\theta}f_{ \theta}(y)e^{-f_{\theta}(y)}dy}{Z(\theta)} || =-\nabla_{\theta}f_{\theta}(y)+\int\nabla_{\theta}f_{\theta}(y) \cdot p_{\theta}(y)dy || =-\nabla_{\theta}f_{\theta}(y)+\mathbb{E}_{y\sim p_{\theta}(y)}[ \nabla_{\theta}f_{\theta}(y)],",,"\frac{d}{d\theta}\mathbb{E}_{y\sim p_{\theta}}[f_{\theta}(y)]=\mathbb{E}_{y\sim p_{\theta}}[f_{\theta}(y)\nabla_{\theta}\log p_{\theta}(y)]+\mathbb{E}_{y\sim p_{\theta}}[\nabla_{\theta}f_{\theta}(y)]-\mathbb{E}_{y\sim p_{\theta}}[\nabla_{\theta}f_{\theta}(y)],",This identity decomposes the gradient of the expected energy under the model distribution into terms that underpin the contrastive update rule for EBM training.
ICLR_2024_oral_63,8,"on.

**Input:** Denoiser, \(g_{\phi}(y)\), energy-based model, \(f_{\theta}(y)\)

**Output:** Noisy samples \(y\sim p(y)\), denoised samples \(\hat{x}(y)\)

```
1\(y_{0}\sim\operatorname{Unif}([0,1]^{d})+\mathcal{N}(0,\sigma^{2}I_{d})\)
2for\(t=0,\dots,T-1\)do
3\(y_{t+1}\gets y_{t}-\delta\,\nabla_{y}f_{\theta}(y_{t})+\sqrt{2\delta} \varepsilon_{t}\,,\varepsilon_{t}\sim\mathcal{N}(0,I_{d})\)
4 end for
5\(\hat{x}_{T}\gets y_{T}+\sigma^{2}g_{\phi}(y_{T})\) return\(\operatorname*{arg\,max}\hat{x}_{T}\)//tocoverone-hotencoding
```

**Algorithm 1**Discrete Walk-Jump Sampling

Figure 2: Discrete walk-jump sampling. **a** The noising and denoising process is applied to antibody proteins. **b** Discrete inputs \(x\) are smoothed with isotropic Gaussian noise, \(\varepsilon\sim\mathcal{N}(0,\sigma^{2}I_{d})\), to noisy inputs, \(y=x+\varepsilon\). A discrete energy-based model (dEBM) parameterizes the energy function \(f_{\theta}(y)\) of noisy data. Noisy data is sampled with the energy function, and denoised with a separate denoising ByteNet network to clean samples, \(\hat{x}_{\phi}(y)\). **c** The “walk” sampling steps on the noisy data manifold with Langevin MCMC are totally decoupled from the “jump” steps to clean samples. **d** The dEBM takes noisy inputs \(y\), concatenates them with a 1d positional encoding, \(p_{1d}\), passes through an MLP and a 3 layer CNN, and concatenates the outputs with an embedding \(z_{s}\) of the inputs into a hidden state, \(h\). \(h\) is passed through an MLP and returns the energy \(f_{\theta}(y)\).

Variable length protein sequence generation.We represent antibody protein molecules as \(x=(x_{1},\ldots,x_{d})\), where \(x_{l}\in\{1,\ldots,20\}\) corresponds to the amino acid (AA) type at position \(l\). Sequences from the Observed Antibody Space (OAS) database (Olsen et al., 2022) are aligned according to the AHo numbering scheme (Honegger and Pluckchtun, 2001) using the ANARCI (Dunbar and Deane, 2016) package and one-hot encoded. Aligning sequences in this way is a practical solution to handling insertions and deletions, which are otherwise troublesome for models that require fixed length inputs and outputs; alignment introduces a ""gap"" token that can be introduced or removed during sampling to effectively change the length of sequences. This allows the model to capture the distribution of lengths present in natural antibodies. The alignment step maps heavy and light chain sequences of varying lengths to a standard, gapped input size of 149 and 148 respectively with 21 possible discrete tokens including the gap. Thus, the input dimension for every sequence becomes \(d=(149+148)\times 21\). Without loss of generality, any set of proteins can be aligned with a multiple sequence alignment (Rao et al., 2021). For other classes of discrete data, pseudo-alignment tokens can be used and randomly inserted into the inputs, or simple BOS and EOS tokens can be used and sampled. An EBM is trained via contrastive divergence on the manifold of smoothed, noisy one-hot encodings, \(y\), given by \(y=x+\varepsilon\), \(\varepsilon\sim\mathcal{N}(0,\sigma^{2}I_{d})\), where \(x\in\{0,1\}^{d}\). A separate denoising model is trained with the objective in Eq. 4. New antibody sequences are generated (Fig. 2) by sampling noisy samples with Langevin MCMC following gradients from the EBM, denoising with the least-squares estimator, and taking \(\operatorname*{argmax}\hat{x}\) to recover a one-hot encoding. Further details related to training and network architecture are given in Appendix A.

Protein design vs discovery.Protein _discovery_ is the task of generating novel, unique, and valid samples. Protein _design_ refers to taking some starting sequence and making edits to improve function. With dWJS we achieve discovery through unconditional sampling, while design is performed via constrained sampling and scoring. That is, we impose the following constraint in the form of a binary projection matrix","P^{\top}\mathrm{argmax}\,\hat{x}(y,t)=P^{\top}s",,"y_{0}\sim\operatorname{Unif}([0,1]^{d})+\mathcal{N}(0,\sigma^{2}I_{d})",Initialization of noisy samples with uniform and Gaussian noise.
ICLR_2024_oral_63,9,"for \(P\in\{0,1\}^{L\times c}\), where \(c\) is the number of conserved tokens in the sequence, \(y\) is the noisy sequence at time step \(t\) of Langevin MCMC, \(\hat{x}(y,t)\) is the denoised sample at time \(t\), and \(s\) is the starting sequence. This constraint ensures that the specified regions of the sequence are conserved, while the non-conserved regions are free to change during Langevin MCMC.

### Derivation of optimal noise level for discrete sequence data

Throughout the experiments in Section 4, we must choose what noise level, \(\sigma\), to use for training. Empirically, we find that in the protein discovery setting, \(\sigma\geq 0.5\) is sufficient for getting good sample quality. Here, we provide some intuition for choosing a good \(\sigma\), based on a geometric picture of the concentration of the measure (Saremi and Hyvarinen, 2019). We define the matrix \(\chi\) with entries","\chi_{ii^{\prime}}=\frac{||X_{i}-X_{i^{\prime}}||}{2\sqrt{d}},",where \(d\) is the dimension of the data and the \(\frac{1}{2\sqrt{d}}\) scaling comes from the concentration of isotropic Gaussians in high dimensions,"\chi_{ij}=\mathbb{E}_{x\sim p(x)}[(x_{i}-\mu_{i})(x_{j}-\mu_{j})],","The matrix $\chi$ represents the covariance between elements $i$ and $j$ of the discrete sequence data under the true data distribution, capturing the statistical dependencies across positions in the sequence."
ICLR_2024_oral_63,10,"ns. The critical noise level, \(\sigma_{c}\), is defined as",\sigma_{c}=\max_{ii^{\prime}}\chi_{ii^{\prime}},,"\sigma_{c}=\min_{i\neq i^{\prime}}\chi_{ii^{\prime}},","The critical noise level $\sigma_{c}$ is defined as the minimum scaled distance between any pair of distinct data points $X_i$ and $X_{i'}$ in the dataset, where the scaling factor $\frac{1}{2\sqrt{d}}$ accounts for the concentration of measure phenomenon in high-dimensional isotropic Gaussian distributions."
ICLR_2024_oral_64,1,"## 2 Problem Setup

Informally, we say that a model generalizes compositionally if it yields an object-centric representation for images containing _unseen combinations_ of _known objects_, i.e., objects observed during training (Zhao et al., 2022; Frady et al., 2023; Wiedemer et al., 2023). For example, a model trained on images containing a red square and others containing a blue triangle should generalize to images containing both objects simultaneously--even if this combination has not previously been observed.

To formalize this idea, we first define scenes of multiple objects through a latent variable model. Specifically, we assume that observations \(\mathbf{x}\) of multi-object scenes are generated from latent vectors \(\mathbf{z}\) by a _diffeomorphic generator_\(\mathbf{f}:\mathcal{Z}\to\mathcal{X}\) mapping from a _latent space_\(\mathcal{Z}\) to a _data space_\(\mathcal{X}\subseteq\mathbb{R}^{N}\), i.e., \(\mathbf{x}=\mathbf{f}(\mathbf{z})\) (see also App. A.1). Each object in \(\mathbf{x}\) should be represented by a unique sub-vector \(\mathbf{z}_{k}\) in the latent vector \(\mathbf{z}\), which we refer to as a _slot_. Thus, we assume that the latent space \(\mathcal{Z}\) factorizes into \(K\) slots \(\mathcal{Z}_{k}\) with \(M\) dimensions each:",\mathcal{Z}_{k}\subseteq\mathbb{R}^{M}\quad\text{and}\quad\mathcal{Z}= \mathcal{Z}_{1}\times\dots\times\mathcal{Z}_{K}\subseteq\mathbb{R}^{KM}.,,"\begin{equation}
\mathcal{Z} = \mathcal{Z}_1 \times \mathcal{Z}_2 \times \cdots \times \mathcal{Z}_K, \quad \text{where } \mathcal{Z}_k \subseteq \mathbb{R}^M \text{ for all } k = 1, 2, \dots, K.
\end{equation}","This equation defines the factorization of the latent space $\mathcal{Z}$ into $K$ object-specific slots $\mathcal{Z}_k$, each of dimensionality $M$, forming the basis for object-centric latent representations in multi-object scenes."
ICLR_2024_oral_64,2,"Figure 1: **Compositional generalization in object-centric learning**. We assume a latent variable model where objects in an image (here, a triangle and a circle) are described by latent _slots_. Our notion of _compositional generalization_ requires a model to identify the ground-truth latent slots (_slot identifiability_, Def. 2) on the train distribution and to transfer this identifiability to out-of-distribution (OOD) combinations of slots (Def. 3). An autoencoder achieves slot identifiability on the train distribution if its decoder is _compositional_ (Thm. 1). Further, we prove that decoders that are _additive_ are able to generalize OOD as visualized in **(A)** via the isolated decoder reconstruction error over a 2D projection of the latent space (see App. B.3). However, this does not guarantee that the entire model generalizes OOD, as the encoder will generally not invert the decoder on OOD slot combinations, leading to a large overall reconstruction error **(B)**. To address this, we introduce a _compositional consistency_ regularizer (Def. 6), which allows the full autoencoder to generalize OOD (**C**, Thm. 3).

A slot \(\mathcal{Z}_{k}\) contains all possible configurations for the \(k\)-th object, while \(\mathcal{Z}\) encompasses all possible combinations of objects. For our notion of compositional generalization, a model should observe all possible configurations of each object but not necessarily all combinations of objects. This corresponds to observing samples generated from a subset \(\mathcal{Z}^{S}\) of the latent space \(\mathcal{Z}\), where \(\mathcal{Z}^{S}\) contains all possible values for each slot. We formalize this subset below.

**Definition 1** (Slot-supported subset).: For \(\mathcal{Z}^{S}\subseteq\mathcal{Z}=\mathcal{Z}_{1}\times\cdots\times\mathcal{ Z}_{K}\), let \(\mathcal{Z}^{S}_{k}:=\big{\{}\mathbf{z}_{k}|\mathbf{z}\in\mathcal{Z}^{S}\big{\}}\). \(\mathcal{Z}^{S}\) is said to be a _slot-supported subset_ of \(\mathcal{Z}\) if \(\mathcal{Z}^{S}_{k}=\mathcal{Z}_{k}\) for any \(k\in[K]\).

One extreme example of a slot-supported subset \(\mathcal{Z}^{S}\) is the trivial case \(\mathcal{Z}^{S}=\mathcal{Z}\); another is a set containing the values for each slot _exactly once_ such that \(\mathcal{Z}^{S}\) resembles a 1D manifold in \(\mathbb{R}^{KM}\).

We assume observations \(\mathbf{x}\) from a _training space_\(\mathcal{X}^{S}\) are generated by a slot-supported subset \(\mathcal{Z}^{S}\), i.e., \(\mathcal{X}^{S}:=\mathbf{f}(\mathcal{Z}^{S})\). The following generative process describes this:","\mathbf{x}=\mathbf{f}(\mathbf{z}),\quad\mathbf{z}\sim p_{\mathbf{z}},\quad\mathrm{supp}(p_{\mathbf{z} })=\mathcal{Z}^{S}.",,"\mathbf{z}^{S}\sim\mathcal{Z}^{S},\quad\mathbf{x}^{S}=\mathbf{f}(\mathbf{z}^{S})\in\mathcal{X}^{S}.",Latent and observed variables generated from the slot-supported subset $\mathcal{Z}^S$ during training.
ICLR_2024_oral_64,3,"Samples from such a generative process are visualized in Fig. 1 for a simple setting with two objects described only by their y-coordinate. We can see that the training space contains each possible configuration for the two objects but not all possible combinations of objects.

Now, assume we have an inference model which only observes data on \(\mathcal{X}^{S}\) generated according to Eq. 2. In principle, this model could be any sufficiently expressive diffeomorphism; however, we will assume it to be an _autoencoder_, as is common in object-centric learning (Yuan et al., 2023). Namely, we assume the model consists of a pair of differentiable functions: an _encoder_\(\hat{\mathbf{g}}:\mathbb{R}^{N}\rightarrow\mathbb{R}^{KM}\) and a _decoder_\(\hat{\mathbf{f}}:\mathbb{R}^{KM}\rightarrow\mathbb{R}^{N}\), which induce the _inferred latent space_\(\hat{\mathcal{Z}}:=\hat{\mathbf{g}}(\mathcal{X})\) and the _reconstructed data space_\(\hat{\mathcal{X}}:=\hat{\mathbf{f}}(\hat{\mathcal{Z}})\). The functions are optimized to invert each other on \(\mathcal{X}^{S}\) by minimizing the reconstruction objective","\mathcal{L}_{\text{rec}}(\mathcal{X}^{S})=\mathcal{L}_{\text{rec}}\big{(}\hat {\mathbf{g}},\hat{\mathbf{f}},\mathcal{X}^{S}\big{)}:=\mathbb{E}_{\mathbf{x}\sim p_{\mathbf{x} }}\big{[}\big{\|}\hat{\mathbf{f}}\big{(}\hat{\mathbf{g}}(\mathbf{x})\big{)}-\mathbf{x}\big{\|} _{2}^{2}\big{]},\quad\mathrm{supp}(p_{\mathbf{x}})=\mathcal{X}^{S}.",,"\min_{\hat{\mathbf{f}},\hat{\mathbf{g}}}\mathbb{E}_{\mathbf{x}\sim p_{\mathbf{x}}}[\|\mathbf{x}-\hat{\mathbf{f}}\circ\hat{\mathbf{g}}(\mathbf{x})\|^{2}],\quad\mathrm{supp}(p_{\mathbf{x}})=\mathcal{X}^{S}.",The reconstruction objective minimizes the squared error between original and reconstructed data on the training space $\mathcal{X}^{S}$.
ICLR_2024_oral_64,4,"We say that an autoencoder \(\big{(}\hat{\mathbf{g}},\hat{\mathbf{f}}\big{)}\) produces an object-centric representation via \(\hat{\mathbf{z}}:=\hat{\mathbf{g}}(\mathbf{x})\) if each inferred latent slot \(\hat{\mathbf{z}}_{j}\) encodes all information from exactly one ground-truth latent slot \(\mathbf{z}_{k}\), i.e., the model separates objects in its latent representation. We refer to this notion as _slot identifiability_, which we formalize below, building upon Brady et al. (2023):

**Definition 2** (Slot identifiability).: Let \(\mathbf{f}:\mathcal{Z}\rightarrow\mathcal{X}\) be a diffeomorphism. Let \(\mathcal{Z}^{S}\) be a slot-supported subset of \(\mathcal{Z}\). An autoencoder \(\big{(}\hat{\mathbf{g}},\hat{\mathbf{f}}\big{)}\) is said to _slot-identify_\(\mathbf{z}\) on \(\mathcal{Z}^{S}\)_w.r.t_\(\mathbf{f}\) via \(\hat{\mathbf{z}}:=\hat{\mathbf{g}}\big{(}\mathbf{f}(\mathbf{z})\big{)}\) if it minimizes \(\mathcal{L}_{\text{rec}}(\mathcal{X}^{S})\) and there exists a permutation \(\pi\) of \([K]\) and a set of diffeomorphisms \(\mathbf{h}_{k}:\mathbf{z}_{\pi(k)}\mapsto\hat{\mathbf{z}}_{k}\) for any \(k\in[K]\).

Intuitively, by assuming \(\big{(}\hat{\mathbf{g}},\hat{\mathbf{f}}\big{)}\) minimizes \(\mathcal{L}_{\text{rec}}(\mathcal{X}^{S})\), we know that on the training space \(\mathcal{X}^{S}\), \(\hat{\mathbf{g}}\) is a diffeomorphism with \(\hat{\mathbf{f}}\) as its inverse. This ensures that \(\hat{\mathbf{z}}\) preserves all information from ground-truth latent \(\mathbf{z}\). Furthermore, requiring that the slots \(\hat{\mathbf{z}}_{k}\) and \(\mathbf{z}_{\pi(k)}\) are related by a diffeomorphism ensures that this information factorizes in the sense that each inferred slot contains _only_ and _all_ information from a corresponding ground-truth slot.* We can now formally define what it means for an autoencoder to generalize compositionally.

Footnote *: Note that when \(\mathcal{Z}^{S}=\mathcal{Z}\), we recover the definition of slot identifiability in Brady et al. (2023).

**Definition 3** (Compositional generalization).: Let \(\mathbf{f}:\mathcal{Z}\rightarrow\mathcal{X}\) be a diffeomorphism and \(\mathcal{Z}^{S}\) be a slot-supported subset of \(\mathcal{Z}\). An autoencoder \(\big{(}\hat{\mathbf{g}},\hat{\mathbf{f}}\big{)}\) that slot-identifies \(\mathbf{z}\) on \(\mathcal{Z}^{S}\) w.r.t. \(\mathbf{f}\) is said to _generalize compositionally w.r.t._\(\mathcal{Z}^{S}\), if it also slot-identifies \(\mathbf{z}\) on \(\mathcal{Z}\) w.r.t. \(\mathbf{f}\).

This definition divides training an autoencoder that generalizes compositionally into two challenges.

**Challenge 1: Identifiability** Firstly, the model must slot-identify the ground-truth latents on the slot-supported subset \(\mathcal{Z}^{S}\). Identifiability is generally difficult and is known to be impossible without further assumptions on the generative model (Hyvarinen and Pajunen, 1999; Locatello et al., 2019). The majority of previous identifiability results have addressed this by placing some form of statistical independence assumptions on the latent distribution \(p_{\mathbf{z}}\)(Hyvarinen et al., 2023). In our setting, however, \(p_{\mathbf{z}}\) is only supported on \(\mathcal{Z}^{S}\), which can lead to extreme dependencies between latents (e.g.,see Fig. 2 where slots are related almost linearly on \(\mathcal{Z}^{S}\)). It is thus much more natural to instead place assumptions on the generator \(\mathbf{f}\) to sufficiently constrain the problem, in line with common practices in object-centric learning that typically assume a structured decoder (Yuan et al., 2023).

**Challenge 2: Generalization** Even if we can train an autoencoder \(\big{(}\hat{\mathbf{g}},\hat{\mathbf{f}}\big{)}\) that slot-identifies \(\mathbf{z}\)_in-distribution_ (ID) on the slot-supported subset \(\mathcal{Z}^{S}\), we still require it to also slot-identify \(\mathbf{z}\)_out-of-distribution_ (OOD) on all of \(\mathcal{Z}\). Empirically, multiple prior works have demonstrated in the context of disentanglement that this form of OOD generalization does not simply emerge for models that can identify the ground-truth latents ID (Montero et al., 2021, 2022; Schott et al., 2022). From a theoretical perspective, OOD generalization of this form implies that the behavior of the generator \(\mathbf{f}\) on the full latent space \(\mathcal{Z}\) is completely determined by its behavior on \(\mathcal{Z}^{S}\), which could essentially be a one-dimensional manifold. This will clearly not be the case if \(\mathbf{f}\) is an arbitrary function, necessitating constraints on its function class to make any generalization guarantees.

## 3 Compositional Generalization in Theory

In this section, we show theoretically how the ground-truth generator \(\mathbf{f}\) and autoencoder \(\big{(}\hat{\mathbf{g}},\hat{\mathbf{f}}\big{)}\) can be constrained to address both _slot identifiability_ and _generalization_, thereby facilitating compositional generalization (complete proofs and further details are provided in App. A).

To address the problem of slot identifiability, Brady et al. (2023) proposed to constrain the generator \(\mathbf{f}\) via assumptions on its Jacobian, which they called _compositionality_ and _irreducibility_. Informally, compositionality states that each image pixel is _locally_ a function of at most one latent slot, while irreducibility states that pixels belonging to the same object share information. We relegate a formal definition of irreducibility to App. A.4 and only restate the definition for compositionality.

**Definition 4** (Compositionality).: A differentiable \(\mathbf{f}:\mathcal{Z}\to\mathbb{R}^{N}\) is called _compositional_ in \(\mathbf{z}\in\mathcal{Z}\) if","\frac{\partial\mathbf{f}_{n}}{\partial\mathbf{z}_{k}}(\mathbf{z})\neq 0\implies\frac{ \partial\mathbf{f}_{n}}{\partial\mathbf{z}_{j}}(\mathbf{z})=0,\quad\text{for any $k,j\in[K]$, $k\neq j$ and any $n\in[N]$.}",,"\forall\mathbf{z}\in\mathcal{Z},\,\forall i\in[N],\,\exists!k\in[K]\,\text{ s.t. }\,\frac{\partial\mathbf{f}_{i}}{\partial\mathbf{z}_{k}}(\mathbf{z})\neq0.","The equation defines compositionality, requiring that each pixel in the generated image depends on exactly one latent slot, ensuring a localized and exclusive relationship between image regions and latent slots."
ICLR_2024_oral_64,5,"For a generator \(\mathbf{f}\) satisfying these assumptions on \(\mathcal{Z}\), Brady et al. (2023) showed that an autoencoder \(\big{(}\hat{\mathbf{g}},\hat{\mathbf{f}}\big{)}\) with a compositional decoder (Def. 4) will slot-identify \(\mathbf{z}\) on \(\mathcal{Z}\) w.r.t. \(\mathbf{f}\). This result is appealing for addressing Challenge 1 since it does not rely on assumptions on \(p_{\mathbf{z}}\); however, it requires that the training space \(\mathcal{X}^{S}\) is generated from the entire latent space \(\mathcal{Z}\). We here show an extension for cases when the training space \(\mathcal{X}^{S}\) arises from a convex, slot-supported _subset_\(\mathcal{Z}^{S}\).

**Theorem 1** (Slot identifiability on slot-supported subset).: _Let \(\mathbf{f}:\mathcal{Z}\to\mathcal{X}\) be a compositional and irreducible diffeomorphism. Let \(\mathcal{Z}^{S}\) be a convex, slot-supported subset of \(\mathcal{Z}\). An autoencoder \(\big{(}\hat{\mathbf{g}},\hat{\mathbf{f}}\big{)}\)

Figure 2: **Overview of our theoretical contribution. (1) We assume access to data from a training space \(\mathcal{X}^{S}\subseteq\mathcal{X}\), which is generated from a _slot-supported subset_\(\mathcal{Z}^{S}\) of the latent space \(\mathcal{Z}\) (Def. 1), via a _compositional_ and _irreducible_ generator \(\mathbf{f}\). (2) We show that an autoencoder with a _compositional_ decoder \(\hat{\mathbf{f}}\) trained via the reconstruction objective \(\mathcal{L}_{\text{rec}}\) on this data will _slot-identify_ ground-truth latents \(\mathbf{z}\) on \(\mathcal{Z}^{S}\) (Thm. 1). Since the inferred latents \(\hat{\mathbf{z}}\) slot-identify \(\mathbf{z}\) ID on \(\mathcal{Z}^{S}\), their slot-wise recombinations \(\mathcal{Z}^{\prime}\) slot-identify \(\mathbf{z}\) OOD on \(\mathcal{Z}\). However, the encoder \(\hat{\mathbf{g}}\) is not guaranteed to infer OOD latents such that \(\hat{\mathbf{g}}(\mathcal{X})=\hat{\mathcal{Z}}=\mathcal{Z}^{\prime}\). (3) On the other hand, if the decoder \(\hat{\mathbf{f}}\) is additive, its reconstructions are guaranteed to generalize such that \(\hat{\mathbf{f}}(\mathcal{Z}^{\prime})=\mathcal{X}\) (Thm. 2). (4) Therefore, regularizing the encoder \(\hat{\mathbf{g}}\) to invert \(\hat{\mathbf{f}}\) using our proposed _compositional consistency_ objective \(\mathcal{L}_{\text{cons}}\) (Def. 6) enforces \(\hat{\mathcal{Z}}=\mathcal{Z}^{\prime}\), thus enabling the model to _generalize compositionally_ (Thm. 3).

that minimizes \(\mathcal{L}_{\text{rec}}(\mathcal{X}^{S})\) for \(\mathcal{X}^{S}=\mathbf{f}(\mathcal{Z}^{S})\) and whose decoder \(\hat{\mathbf{f}}\) is compositional on \(\hat{\mathbf{g}}(\mathcal{X}^{S})\), slot-identifies \(\mathbf{z}\) on \(\mathcal{Z}^{S}\) w.r.t. \(\mathbf{f}\) in the sense of Def. 2._

Thm. 1 solves Challenge 1 of slot identifiability on the slot-supported subset \(\mathcal{Z}^{S}\), but to generalize compositionally, we still need to address Challenge 2 and extend this to all of \(\mathcal{Z}\). Because we have slot identifiability on \(\mathcal{Z}^{S}\), we know each ground-truth slot and corresponding inferred slot are related by a diffeomorphism \(\mathbf{h}_{k}\). Since \(\mathbf{h}_{k}\) is defined for all configurations of slot \(\mathbf{z}_{\pi(k)}\), the representation which slot-identifies \(\mathbf{z}=(\mathbf{z}_{1},\ldots,\mathbf{z}_{K})\) for any combination of slots (ID or OOD) in \(\mathcal{Z}\) is given by","\mathbf{z}^{\prime}=\big{(}\mathbf{h}_{1}(\mathbf{z}_{\pi(1)}),\ldots,\mathbf{h}_{K}(\mathbf{z}_{ \pi(K)})\big{)},\quad\mathcal{Z}^{\prime}=\mathbf{h}_{1}(\mathcal{Z}_{\pi(1)}) \times\cdots\times\mathbf{h}_{K}(\mathcal{Z}_{\pi(K)}).",,"\hat{\mathbf{z}}_{k}=\mathbf{h}_{k}(\mathbf{z}_{\pi(k)}),\quad\text{for all }k\in[K].","This equation defines the inferred latent slots $\hat{\mathbf{z}}_{k}$ as diffeomorphic transformations of the ground-truth slots $\mathbf{z}_{\pi(k)}$, enabling compositional generalization across all combinations of slots in $\mathcal{Z}$."
ICLR_2024_oral_64,6,"Therefore, for an autoencoder to generalize its slot identifiability from \(\mathcal{Z}^{S}\) to \(\mathcal{Z}\), it should match this representation such that for any \(\mathbf{z}\in\mathcal{Z}\),",\hat{\mathbf{g}}\big{(}\mathbf{f}(\mathbf{z})\big{)}=\mathbf{z}^{\prime}\quad\text{and}\quad \hat{\mathbf{f}}(\mathbf{z}^{\prime})=\mathbf{f}(\mathbf{z}).,,"\hat{\mathbf{g}}\big{(}\mathbf{f}(\mathbf{z})\big{)}=\mathbf{z}^{\prime}=\big{(}\mathbf{h}_{1}(\mathbf{z}_{\pi(1)}),\ldots,\mathbf{h}_{K}(\mathbf{z}_{\pi(K)})\big{)}.","This equation formalizes the requirement for an autoencoder to generalize its slot identifiability from the slot-supported subset $\mathcal{Z}^{S}$ to the entire latent space $\mathcal{Z}$, by matching the inferred latent representation to the diffeomorphically transformed ground-truth slots."
ICLR_2024_oral_64,7,"We aim to satisfy these conditions by formulating properties of the decoder \(\hat{\mathbf{f}}\) such that it fulfills the second condition, which we then leverage to regularize the encoder \(\hat{\mathbf{g}}\) to fulfill the first condition.

### Decoder Generalization via Additivity

We know from Thm. 1 that \(\hat{\mathbf{f}}\) renders each inferred slot \(\mathbf{h}_{k}(\mathbf{z}_{\pi(k)})\) correctly to a corresponding object in \(\mathbf{x}\) for all possible values of \(\mathbf{z}_{\pi(k)}\). Furthermore, because the generator \(\hat{\mathbf{f}}\) satisfies compositionality (Def. 4), we know that these _slot-wise renders_ should not be affected by changes to the value of any other slot \(\mathbf{z}_{j}\). This implies that for \(\hat{\mathbf{f}}\) to satisfy Eq. 6, we only need to ensure that its slot-wise renders remain invariant when constructing \(\mathbf{z}^{\prime}\) with an OOD combination of slots \(\mathbf{z}\). We show below that _additive_ decoders can achieve this invariance.

**Definition 5** (Additive decoder).: For an autoencoder \(\big{(}\hat{\mathbf{g}},\hat{\mathbf{f}}\big{)}\) the decoder \(\hat{\mathbf{f}}\) is said to be _additive_ if","\hat{\mathbf{f}}(\mathbf{z})=\sum_{k=1}^{K}\mathbf{\varphi}_{k}(\hat{\mathbf{z}}_{k}),\quad \text{where }\mathbf{\varphi}_{k}:\mathbb{R}^{M}\to\mathbb{R}^{N}\text{ for any }k\in[K]\text{ and }\hat{\mathbf{z}}\in\mathbb{R}^{KM}.",,"\hat{\mathbf{f}}(\mathbf{z}^{\prime})=\sum_{k=1}^{K}\hat{\mathbf{f}}_{k}(\mathbf{z}^{\prime}_{k}),","An additive decoder decomposes the reconstruction into independent slot-wise contributions, enabling invariance to slot combinations."
ICLR_2024_oral_64,8,"We can think of an additive decoder \(\hat{\mathbf{f}}\) as rendering each slot \(\hat{\mathbf{z}}_{k}\) to an intermediate image via _slot functions_\(\mathbf{\varphi}_{k}\), then summing these images to create the final output. These decoders are expressive enough to represent compositional generators (see App. A.7). Intuitively, they globally remove interactions between slots such that the correct renders learned on inferred latents of \(\mathcal{Z}^{S}\) are propagated to inferred latents of the entire \(\mathcal{Z}\). This is formalized with the following result.

**Theorem 2** (Decoder generalization).: _Let \(\mathbf{f}:\mathcal{Z}\to\mathcal{X}\) be a compositional diffeomorphism and \(\mathcal{Z}^{S}\) be a slot-supported subset of \(\mathcal{Z}\). Let \(\big{(}\hat{\mathbf{g}},\hat{\mathbf{f}}\big{)}\) be an autoencoder that slot-identifies \(\mathbf{z}\) on \(\mathcal{Z}^{S}\) w.r.t. \(\mathbf{f}\). If the decoder \(\hat{\mathbf{f}}\) is additive, then it generalizes in the following sense: \(\hat{\mathbf{f}}(\mathbf{z}^{\prime})=\mathbf{f}(\mathbf{z})\) for any \(\mathbf{z}\in\mathcal{Z}\), where \(\mathbf{z}^{\prime}\) is defined according to Eq. 5._

Consequently, \(\hat{\mathbf{f}}\) is now injective on \(\mathcal{Z}^{\prime}\) and we get \(\hat{\mathbf{f}}(\mathcal{Z}^{\prime})=\mathbf{f}(\mathcal{Z})=\mathcal{X}\).

### Encoder Generalization via Compositional Consistency

Because the decoder \(\hat{\mathbf{f}}\) generalizes such that \(\hat{\mathbf{f}}(\mathbf{z}^{\prime})=\mathbf{f}(\mathbf{z})\) (Thm. 2) and \(\hat{\mathbf{f}}(\mathcal{Z}^{\prime})=\mathcal{X}\), the condition on the encoder from Eq. 6 corresponds to enforcing that \(\hat{\mathbf{g}}\) inverts \(\hat{\mathbf{f}}\) on all of \(\mathcal{X}\). This is ensured ID on the training space \(\mathcal{X}^{S}\) by minimizing the reconstruction objective \(\mathcal{L}_{\text{rec}}\) (Eq. 3). However, there is nothing enforcing that \(\hat{\mathbf{g}}\) also inverts \(\hat{\mathbf{f}}\) OOD outside of \(\mathcal{X}^{S}\) (see Fig. 0(B) for a visualization of this problem). To address this, we propose the following regularizer.

**Definition 6** (Compositional consistency).: Let \(q_{\mathbf{z}^{\prime}}\) be a distribution with \(\operatorname{supp}(q_{\mathbf{z}^{\prime}})=\mathcal{Z}^{\prime}\) (Eq. 5). An autoencoder \(\big{(}\hat{\mathbf{g}},\hat{\mathbf{f}}\big{)}\) is said to be _compositionally consistent_ if it minimizes the _compositional consistency loss_","\mathcal{L}_{\text{cons}}\big{(}\hat{\mathbf{g}},\hat{\mathbf{f}},\mathcal{Z}^{\prime} \big{)}=\mathbb{E}_{\mathbf{z}^{\prime}\sim q_{\mathbf{z}^{\prime}}}\Big{[}\big{\|} \hat{\mathbf{g}}\big{(}\hat{\mathbf{f}}(\mathbf{z}^{\prime})\big{)}-\mathbf{z}^{\prime}\big{\|} _{2}^{2}\Big{]}.",,"\mathcal{L}_{\text{cons}}\big{(}\hat{\mathbf{g}},\hat{\mathbf{f}},\mathcal{Z}^{\prime}\big{)}:=\mathbb{E}_{\mathbf{z}^{\prime}\sim q_{\mathbf{z}^{\prime}}}\big{[}\big{\|}\hat{\mathbf{g}}\big{(}\hat{\mathbf{f}}(\mathbf{z}^{\prime})\big{)}-\mathbf{z}^{\prime}\big{\|}_{2}^{2}\big{]}.","This loss enforces that the encoder inverts the decoder on the full latent space $\mathcal{Z}^{\prime}$, ensuring compositional consistency between the inferred and ground-truth latents both in-distribution and out-of-distribution."
ICLR_2024_oral_64,9,"The loss can be understood as first sampling an OOD combination of slots \(\mathbf{z}^{\prime}\) by composing inferred ID slots \(\mathbf{h}_{k}(\mathbf{z}_{\pi(k)})\). The decoder can then render \(\mathbf{z}^{\prime}\) to create an OOD sample \(\hat{\mathbf{f}}(\mathbf{z}^{\prime})\). Re-encoding this sample such that \(\hat{\mathbf{g}}(\hat{\mathbf{f}}(\mathbf{z}^{\prime}))=\mathbf{z}^{\prime}\) then regularizes the encoder to invert the decoder OOD. We discuss how this regularization can be implemented in practice in Sec. 4.

### Putting it All Together

Thm. 1 showed how slot identifiability can be achieved ID on \(\mathcal{Z}^{S}\) if \(\hat{\mathbf{f}}\) satisfies compositionality, and Thm. 2, Def. 6 showed how this identifiability can be generalized to all of \(\mathcal{Z}\) if the decoder is additive and compositional consistency is minimized. Putting these results together, we can now prove conditions for which an autoencoder will generalize compositionally (see Fig. 2 for an overview).

**Theorem 3** (Compositionally generalizing autoencoder).: _Let \(\mathbf{f}:\mathcal{Z}\to\mathcal{X}\) be a compositional and irreducible diffeomorphism. Let \(\mathcal{Z}^{S}\) be a convex, slot-supported subset of \(\mathcal{Z}\). Let \(\big{(}\hat{\mathbf{g}},\hat{\mathbf{f}}\big{)}\) be an autoencoder with additive decoder \(\hat{\mathbf{f}}\) (Def. 5). If \(\hat{\mathbf{f}}\) is compositional on \(\hat{\mathbf{g}}(\mathcal{X}^{S})\) and \(\hat{\mathbf{g}},\hat{\mathbf{f}}\) solve_","\mathcal{L}_{\text{rec}}\big{(}\hat{\mathbf{g}},\hat{\mathbf{f}},\mathcal{X}^{S}\big{)} +\lambda\mathcal{L}_{\text{cons}}\big{(}\hat{\mathbf{g}},\hat{\mathbf{f}},\mathcal{Z }^{\prime}\big{)}=0,\qquad\text{for some }\lambda>0,",,"\min_{\hat{\mathbf{g}},\hat{\mathbf{f}}}\Big{(}\mathcal{L}_{\text{rec}}(\mathcal{X}^{S})+ \lambda\mathcal{L}_{\text{cons}}\big{(}\hat{\mathbf{g}},\hat{\mathbf{f}},\mathcal{Z}^{\prime} \big{)}\Big{)},\quad\lambda>0,","Equation 9 presents the joint optimization objective for training an autoencoder to achieve compositional generalization, combining the reconstruction loss on the training space and the compositional consistency regularizer on out-of-distribution latent combinations."
ICLR_2024_oral_64,10,"_then the autoencoder \(\big{(}\hat{\mathbf{g}},\hat{\mathbf{f}}\big{)}\) generalizes compositionally w.r.t. \(\mathcal{Z}^{S}\) in the sense of Def. 3._

Moreover, \(\hat{\mathbf{g}}:\mathcal{X}\to\hat{\mathcal{Z}}\) inverts \(\hat{\mathbf{f}}:\mathcal{Z}^{\prime}\to\mathcal{X}\) and also \(\hat{\mathcal{Z}}=\mathcal{Z}^{\prime}=\mathbf{h}_{1}(\mathcal{Z}_{\pi(1)})\times \cdots\times\mathbf{h}_{K}(\mathcal{Z}_{\pi(K)})\).

## 4 Compositional Generalization in Practice

CompositionalityThm. 3 explicitly assumes that the decoder \(\hat{\mathbf{f}}\) satisfies compositionality on \(\hat{\mathbf{g}}(\mathcal{X}^{S})\) but does not give a recipe to enforce this in practice. Brady et al. (2023) proposed a regularizer that enforces compositionality if minimized (see App. B.4), but their objective is computationally infeasible to optimize for larger models, thus limiting its practical use. At the same time, Brady et al. (2023) showed that explicitly optimizing this objective may not always be necessary, as the object-centric models used in their experiments seemed to minimize it implicitly, likely through the inductive biases in these models. We observe a similar phenomenon (see Fig. 4, right) and thus rely on these inductive biases to satisfy compositionality in our experiments in Sec. 6.

AdditivityIt is trivial to implement an additive decoder by parameterizing the slot functions \(\mathbf{\varphi}_{k}\) from Eq. 7 as, e.g., deconvolution neural networks. This resembles the decoders typically used in object-centric learning, with the key difference being the use of _slot-wise masks_\(\mathbf{m}_{k}\). Specifically, existing models commonly use a decoder of the form","\hat{\mathbf{f}}(\mathbf{z})=\sum_{k=1}^{K}\tilde{\mathbf{m}}_{k}\odot\mathbf{x}_{k},\qquad \tilde{\mathbf{m}}_{k}=\sigma(\mathbf{m})_{k},\qquad(\mathbf{m}_{k},\mathbf{x}_{k})=\mathbf{\varphi }_{k}(\mathbf{z}_{k}),",where \(\odot\) is an element-wise multiplication and \(\sigma(\cdot)\) denotes the softmax function,"\hat{\mathbf{f}}(\mathbf{z})=\sum_{k=1}^{K}\mathbf{m}_{k}(\mathbf{z}_{k})\odot\mathbf{\varphi}_{k}(\mathbf{z}_{k}),","An additive decoder with slot-wise masks, where each slot contributes independently to the final image through a mask and a slot function."
ICLR_2024_oral_64,11,"on. Using masks \(\mathbf{m}_{k}\) in this way facilitates modeling occluding objects but violates additivity as the softmax normalizes masks across slots, thus introducing interactions between slots during rendering. We empirically investigate how this interaction affects compositional generalization in Sec. 6.2.

Compositional ConsistencyThe main challenge with implementing the proposed compositional consistency loss \(\mathcal{L}_{\text{cons}}\) (Def. 6) is sampling \(\mathbf{z}^{\prime}\) from \(q_{\mathbf{z}^{\prime}}\) with support over \(\hat{\mathcal{Z}}\). First, note that we defined \(\mathcal{Z}^{\prime}\) in Eq. 5 through use of the functions \(\mathbf{h}_{k}\), but can equivalently write",\mathbf{h}_{k}(\mathbf{z}_{\pi(k)})=\hat{\mathbf{g}}_{k}\big{(}\mathbf{f}(\mathbf{z})\big{)}\quad \text{and}\quad\mathcal{Z}^{\prime}=\hat{\mathbf{g}}_{1}(\mathcal{X}^{S})\times \cdots\times\hat{\mathbf{g}}_{K}(\mathcal{X}^{S}).,,"\mathcal{Z}^{\prime}=\hat{\mathbf{z}}_{1}(\mathcal{Z}_{\pi(1)})\times\cdots\times\hat{ \mathbf{z}}_{K}(\mathcal{Z}_{\pi(K)}),","where each inferred slot $\hat{\mathbf{z}}_{k}$ is defined over all possible configurations of the corresponding ground-truth slot $\mathcal{Z}_{\pi(k)}$, enabling OOD combinations through recombination of slot values."
ICLR_2024_oral_64,12,"The reformulation highlights that we can construct OOD samples in the consistency regularization from ID observations by randomly shuffling the slots of two inferred ID latents \(\hat{\mathbf{z}}^{(1)},\hat{\mathbf{z}}^{(2)}\) via \(\rho_{k}\sim\mathcal{U}\{1,2\}\). Because \(\mathcal{Z}^{S}\) is a slot-supported subset, constructing \(\mathbf{z}^{\prime}\) as","\mathbf{z}^{\prime}=\big{(}\hat{\mathbf{z}}^{(\rho_{1})}_{1},\ldots,\hat{\mathbf{z}}^{(\rho _{K})}_{K}\big{)},\quad\text{where for }i\in\{1,2\}\quad\hat{\mathbf{z}}^{(i)}=\hat{\mathbf{g}}\big{(}\mathbf{x}^{(i)}\big{)}, \,\mathbf{x}^{(i)}\sim p_{\mathbf{z}}",,"\mathbf{z}^{\prime}_{k}=\hat{\mathbf{g}}_{k}\big{(}\mathbf{f}(\mathbf{z}^{(\rho_{k})})\big{)}\quad\text{for }\rho_{k}\sim\mathcal{U}\{1,2\},","Constructs out-of-distribution latent slots by randomly shuffling inferred in-distribution latent slots, enabling compositional consistency regularization."
ICLR_2024_oral_66,1,"### Performance and Cost Estimators

Learning curves record the performance of Deep Learning pipelines at different time steps, such as the validation loss versus the number of epochs. The performance of the pipeline \(x\) at step \(t\) is denoted as \(\ell\left(x,t\right)\), and the runtime cost for training the pipeline \(x\) until step \(t\) is \(c\left(x,t\right)\). The history of all observed learning curves for \(n\) pipelines is denoted as \(\mathcal{H}:=\left\{\left(x_{i},t_{i},\ell\left(x_{i},t_{i}\right),c\left(x_{i },t_{i}\right)\right)\right\}_{i=1}^{n}\).

Our method learns a probabilistic performance estimator (a.k.a. surrogate) defined as \(\hat{\ell}\left(x,t;\theta\right)\) and parameterized with \(\theta\). We train the surrogate \(\hat{\ell}\) to estimate the true performance \(\ell\) from \(\mathcal{H}\) as:","\theta^{*}\ :=\ \operatorname*{arg\,min}_{\theta}\ \mathbb{E}_{(x,t, \ell(x,t),\cdot)\sim\mathcal{H}}\ \Big{[}-\log p(\ell(x,t)\mid x,t,\hat{\ell}(x,t;\theta))\Big{]}.",,"\begin{equation}
\theta^* = \argmin_{\theta} \frac{1}{n} \sum_{i=1}^{n} \left[ \hat{\ell}(x_i, t_i; \theta) - \ell(x_i, t_i) \right]^2
\end{equation}",Training the surrogate model by minimizing the mean squared error between predicted and observed performance values across all pipelines in the history.
ICLR_2024_oral_66,2,"Concretely, the surrogate \(\hat{\ell}\) is implemented as a deep-kernel Gaussian Process regressor (Wistuba & Grabocka, 2021). In addition, we train a cost estimator \(\hat{c}(x,t;\gamma)\) in the form of a Multilayer Perceptron with parameters \(\gamma\) to predict the ground truth costs as:","\gamma^{*}\ :=\ \operatorname*{arg\,min}_{\gamma}\ \mathbb{E}_{(x,t, \cdot,c(x,t))\sim\mathcal{H}}\ \Big{[}c(x,t)-\hat{c}(x,t;\gamma)\Big{]}^{2}.",,"\gamma^{*}\ :=\ \operatorname*{arg\,min}_{\gamma}\ \mathbb{E}_{(x,t, \cdot, c(x,t))\sim\mathcal{H}}\ \Big{[}\mathcal{L}_{\text{MAE}}\left(c(x,t),\hat{c}(x,t;\gamma)\right)\Big{]},",where $\mathcal{L}_{\text{MAE}}$ denotes the mean absolute error loss function used to train the cost estimator $\hat{c}$ with parameters $\gamma$ on the observed runtime costs from the history $\mathcal{H}$.
ICLR_2024_oral_66,3,"### Cost-sensitive Acquisition Function

We propose a cost-sensitive variant of the Expected Improvement (Jones et al., 1998) (EI) acquisition to select the next pipeline to evaluate within a Bayesian Optimization framework, defined as:","x^{*}:=\operatorname*{arg\,max}_{x\in\mathcal{X}}\frac{\text{EI}(x, \mathcal{H},\hat{\ell}(x,\tau(x)))}{\hat{c}\Big{(}x,\tau(x)\Big{)}{-}c \Big{(}x,\tau(x)-\Delta t\Big{)}}=\operatorname*{arg\,max}_{x\in\mathcal{X}} \frac{\mathbb{E}_{\hat{\ell}(x,\tau(x))}[\max(\ell_{\tau(x)}^{\min} -\hat{\ell}(x,\tau(x)),0)]}{\hat{c}\Big{(}x,\tau(x){-}c\Big{(}x, \tau(x){-}\Delta t\Big{)}}",,"\alpha_{\text{EI-C}}(x,t) = \frac{\mathbb{E}_{\hat{\ell}}\left[\max\left(0, \ell_{\text{best}} - \hat{\ell}(x,t;\theta)\right)\right]}{\hat{c}(x,t;\gamma)},",The cost-sensitive Expected Improvement acquisition function balances performance improvement potential against training cost by normalizing the expected improvement with the predicted cost.
ICLR_2024_oral_66,4,"The numerator of Equation 3 introduces a mechanism that selects the pipeline \(x\) that has the largest likelihood to improve the lowest observed validation error at the next unobserved epoch \(\tau(x)\) of pipeline \(x\). The denominator balances out the cost of actually finetuning pipeline \(x\) for \(\Delta t\) epochs. \(\tau(x)\) is defined for pipeline \(x\) as \(\tau(x):=\max\{t^{\prime}|(x,t^{\prime},\cdot,\cdot)\in\mathcal{H}\}+\Delta t\), where \(\Delta t\) denotes the number of epochs to finetune from the last observed epoch in the history. If the pipeline is not in the history, the query epoch is \(\tau(x)=\Delta t\). Simply put, if the validation loss of \(x\) is evaluated after every training epoch/step (\(\Delta t=1\)) and has been evaluated for \(k\) epochs/steps, then \(\tau(x)=k+1\). As a result, we select the configuration with the highest chance of improving the best-measured loss at the next epoch, while trading off the cost of finetuning it. Concretely, the best observed loss is \(\ell_{\tau(x)}^{\min}:=\min\left(\{\ell(x,\tau(x))|(x,\tau(x),\ell\left(x,\tau( x)\right),\cdot)\in\mathcal{H}\}\right)\). If no pipeline has been evaluated until \(\tau(x)\), i.e. \((x,\tau(x),\cdot,\cdot)\notin\mathcal{H}\), then \(\ell_{\tau(x)}^{\min}:=\min\left(\{\ell(x,t)|(x,t,\ell\left(x,t\right),\cdot) \in\mathcal{H},t<\tau(x)\}\right)\).

### Meta-learning the Performance and Cost Estimators

A crucial novelty of our paper is to meta-learn BO surrogates from existing pipeline evaluations on other datasets. Assume we have access to a set of curves for the validation errors \(\ell\) and the runtimes \(c\) of pipelines over a pool of datasets, for a series of \(N\) epochs. We call the collection of such quadruple evaluations a meta-dataset \(\mathcal{H}^{\left(M\right)}:=\bigcup_{x\in\mathcal{X}}\bigcup_{d\in\mathcal{D} }\bigcup_{t\in[1,N]}\left\{\left(x,t,\ell\left(x,t,d\right),c\left(x,t,d,d \right)\right)\right\}\), where we explicitly included the dependency of the performance and cost curves to the dataset. To contextualize the predictions on the characteristics of each dataset, we use descriptive features \(d\in\mathcal{D}\) to represent each dataset (a.k.a. meta-features).

We meta-learn a probabilistic validation error estimator \(\hat{\ell}\left(x,t,d;\theta\right)\), and a point-estimate cost predictor \(\hat{c}\left(x,t,d;\gamma\right)\) from the meta-dataset \(\mathcal{H}^{\left(M\right)}\) by solving the following objective functions:","\theta^{(M)} :=\ \operatorname*{arg\,min}_{\theta}\ \mathbb{E}_{(x,t,\ell(x,t,d),c(x,t,d)) \sim\mathcal{H}^{(M)}}\ \ [-\log p(\ell(x,t,d)\mid x,t,d,\hat{\ell}(x,t,d; \theta))] || \gamma^{(M)} :=\ \operatorname*{arg\,min}_{\gamma}\ \mathbb{E}_{(x,t,\ell(x,t,d),c(x,t,d)) \sim\mathcal{H}^{(M)}}\ \ (c(x,t,d)-\hat{c}(x,t,d;\gamma))^{2}",,"\theta^{*},\gamma^{*}\ :=\ \operatorname*{arg\,min}_{\theta,\gamma}\ \mathbb{E}_{(x,t,\ell(x,t,d),c(x,t,d))\sim\mathcal{H}^{(M)}}\ \Big{[}-\log p(\ell(x,t,d)\mid x,t,d,\hat{\ell}(x,t,d;\theta))\Big{]}+\lambda\Big{[}c(x,t,d)-\hat{c}(x,t,d;\gamma)\Big{]}^{2},","where $\lambda$ is a hyperparameter balancing the two objectives, and the meta-learned estimators generalize across datasets by incorporating dataset features $d$ into the prediction model."
ICLR_2024_oral_66,5,"After meta-learning, we use the learned weights to initialize the performance and cost predictors \(\theta\leftarrow\theta^{(M)}\) and \(\gamma\leftarrow\gamma^{(M)}\) before running Algorithm 1. As a result, our method starts with a strong prior for the performance of pipelines and their runtime costs, based on the collected history \(\mathcal{H}^{(M)}\) from evaluations on prior datasets. We provide details about the meta-learning procedure in Algorithm 2 (Appendix A.3).

## 5 Quick-Tune Meta-Dataset

### Quick-Tune Search Space

While our proposed method is agnostic to the application domain, the set of pretrained models and hyperparameter space to choose from, we need to instantiate these choices for our experiments. In this paper, we focus on image classification and base our study on the _timm_ library (Wightman, 2019), given its popularity and wide adoption in the community. It contains a large set of hyperparameters and pretrained models on ImageNet (more than 700). Concerning the space of potential finetuning hyperparameters, we select a subset of optimizers and schedulers that are well-known and used by researchers and practitioners. We also include regularization techniques, such as data augmentation and drop-out, since finetuning is typically applied in low data regimes where large architectures easily overfit. Additionally, we modified the framework to include common finetuning strategies, such as methods to select the percentage of layers to finetune (Yosinski et al., 2014), linear probing (Wang et al., 2023), stochastic norm (Kou et al., 2020), Co-Tuning (You et al., 2020), DELTA (Li et al., 2019), BSS (Chen et al., 2019) and SP-regularization (Li et al., 2018). The last five methods are taken from the _transfer learning library_ (Jungung Jiang and Long, 2020). Although we consider these well-known and stable finetuning strategies, we foresee the widespread adoption of new approaches such as LoRA (Hu et al., 2021). They are complementary to our method and can be easily interpreted as an extension of the pipeline search space. We list all the hyperparameters of our search space in Table 1, indicating explicitly the conditional hyperparameters with a ""*"". For a more detailed description of our search space, including the hyperparameter ranges and dependencies, we point the reader to Table 7 of Appendix B. As we are interested in time efficiency and accuracy, we select the Pareto optimal models from the large set of ca. \(700\) pretrained architectures in the _timm_ library. Specifically, given a model \(m\in\mathcal{M}_{\mathrm{Timm}}\) with Top-1 ImageNet accuracy \(f_{\mathrm{ImageNet}}(m)\) and \(S(m)\) number of parameters, we build our final model hub based on the multi-objective optimization among the predictive accuracy and model size by solving Equation 6. Subsequently, we obtain a set of 24 Pareto-optimal models as shown in Figure 2 and listed in Table 8 of Appendix B.","\mathcal{M}=\{m^{*}\,|\,m^{*}\in\operatorname*{arg\,max}_{m\in\mathcal{M }_{\mathrm{Timm}}}[f_{\mathrm{ImageNet}}(m),\ -S(m)]\}",,"\theta \leftarrow \theta^{(M)}, \quad \gamma \leftarrow \gamma^{(M)}","After meta-learning, the performance and cost predictors are initialized with the meta-learned parameters $\theta$ and $\gamma$."
ICLR_2024_oral_68,1,"### Causal models

We use capital letters for random variables \(V\), and lower case for their values \(v\in dom(V)\). For simplicity, we assume each variable has a finite number of possible values, \(|dom(V)|<\infty\). Bold face denotes sets of variables \(\mathbf{V}=\{V_{1},\dots,V_{n}\}\), and their values \(\mathbf{v}\in dom(\mathbf{V})=\times_{i}dom(V_{i})\). A probabilistic model specifies the joint distribution \(P(\mathbf{V})\) over a set of variables \(\mathbf{V}\). These models can support associative queries, for example \(P(\mathbf{Y}=\mathbf{y}\mid\mathbf{X}=\mathbf{x})\) for \(\mathbf{X},\mathbf{Y}\subseteq\mathbf{V}\). Interventions describe external changes to the data generating process (and hence changing the joint distribution), for example a _hard_ intervention \(\text{do}(\mathbf{X}=\mathbf{x})\) describes forcing the set of variables \(\mathbf{X}\subseteq\mathbf{V}\) to take value \(\mathbf{x}\). This generates a new distribution \(P(\mathbf{V}\mid\text{do}(X=x))=P(\mathbf{V}_{x})\) where \(\mathbf{V}_{\mathbf{x}}\) refers to the variables \(\mathbf{V}\) following this intervention. The power of causal models is that they specify not only \(P(\mathbf{V})\) but also the distribution of \(\mathbf{V}\) under all interventions, and hence these models can be used to evaluate both associative and interventional queries e.g. \(P(\mathbf{Y}=\mathbf{y}\mid\text{do}(\mathbf{X}=\mathbf{x}))\).

For the derivation of our results we focus on a specific class of causal models--causal Bayesian networks (CBNs). There are several alternative models and formalisms that are studied in the literature, including structural equation models (Pearl, 2009) and the Neyman-Rubin causal models (Rubin, 2005), and results can be straightforwardly adapted to these.

**Definition 1** (Bayesian networks).: _A Bayesian network \(M=(G,P)\) over a set of variables \(\mathbf{V}=\{V_{1},\dots,V_{n}\}\) is a joint probability distribution \(P(\mathbf{V})\) that factors according to a directed acyclic graph (DAG) \(G\), i.e. \(P(V_{1},\dots,V_{n})=\prod_{i=1}^{n}P(V_{i}\mid\mathbf{Pa}_{V_{i}})\), where \(\mathbf{Pa}_{V_{i}}\) are the parents of \(V_{i}\) in \(G\)._

A Bayesian network is _causal_ if the graph \(G\) captures the causal relationships between the variables or, formally, if the result of any intervention \(\text{do}(\mathbf{X}=\mathbf{x})\) for \(\mathbf{X}\subseteq\mathbf{V}\) can be computed from the truncated factorisation formula:",P(\mathbf{v}\mid\text{do}(\mathbf{x}))=\begin{cases}\prod_{i:v_{i}\notin\mathbf{x}}P(v_{i }\mid\mathbf{pa}_{v_{i}})&\text{if $\mathbf{v}$ consistent with $\mathbf{x}$}\\ 0&\text{otherwise.}\end{cases},,P(\mathbf{V} \mid \text{do}(\mathbf{X} = \mathbf{x})) = \prod_{V_i \in \mathbf{V} \setminus \mathbf{X}} P(V_i \mid \mathbf{Pa}_{V_i}) \quad \text{if } V_i \not\in \mathbf{X},"Equation 1 defines the truncated factorization formula for computing the post-intervention distribution in a causal Bayesian network, where the variables in the intervened set $\mathbf{X}$ are fixed to their specified values $\mathbf{x}$ and are excluded from the product over the remaining variables."
ICLR_2024_oral_68,2,"More generally, a _soft_ intervention \(\sigma_{v_{i}}=P^{\prime}(V_{i}\mid\mathbf{Pa}_{i}^{\star})\) replaces the conditional probability distribution for \(V_{i}\) with a new distribution \(P^{\prime}(V_{i}\mid\mathbf{Pa}_{i}^{\star})\), possibly resulting in a new parent set \(\mathbf{Pa}_{i}^{\star}\neq\mathbf{Pa}_{i}\) as long as no cycles are introduced in the graph. We refer to \(\sigma_{v_{i}}\) as a _domain indicator_(Correa & Bareinboim, 2020) (it has also been called an environment index, Arjovsky et al., 2019). The updated distribution is denoted \(P(\mathbf{v};\sigma_{\mathbf{v}^{\prime}})=\prod_{i:v_{i}\in\mathbf{v}^{\prime}}P^{\prime} (v_{i}\mid\mathbf{pa}_{v_{i}})\prod_{i:v_{i}\notin\mathbf{v}^{\prime}}P(v_{i}\mid\mathbf{ pa}_{v_{i}})\).

In general, soft interventions cannot be defined without knowledge of \(G\). For example, the soft intervention \(\sigma_{Y}=P^{\prime}(y\mid x)\) is incompatible with the causal structure \(Y\to X\) as it would induce a causal cycle. As our results are concerned with learning causal models (and hence causal structure), we focus our theoretical analysis on a subset of the soft interventions, _local interventions_, that are compatible with all causal structures and so can be used without tacitly assuming knowledge of \(G\).

**Definition 2** (Local interventions).: _Local intervention \(\sigma\) on \(V_{i}\in\mathbf{V}\) involves applying a map to the states of \(V_{i}\) that is not conditional on any other endogenous variables, \(v_{i}\mapsto f(v_{i})\). We use the notation \(\sigma=\text{do}(V_{i}=f(v_{i}))\) (variable \(V_{i}\) is assigned the state \(f(v_{i})\)). Formally, this is a soft intervention on \(V_{i}\) that transforms the conditional probability distribution as,_","P(v_{i}\,|\mathbf{pa}_{i};\sigma)=\sum_{v_{i}^{\prime}:f(v_{i}^{\prime})=v_{i}}P(v_ {i}^{\prime}\,|\mathbf{pa}_{i})",,P^{\prime}(v_{i}\mid\mathbf{pa}_{v_{i}})=\begin{cases}1 & \text{if }v_{i}=f(v_{i})\\ 0 & \text{otherwise.}\end{cases},"A local intervention on variable $V_i$ deterministically assigns $V_i$ to a function of its original value, effectively modifying its conditional probability distribution without dependence on other variables."
ICLR_2024_oral_7,1,"### LoRA Adapters

Fine-tuning the entire model \(\mathcal{M}\) for a specific task is usually computationally expensive due to the massive parameter count. LoRA (Low-rank Adaptation, (Hu et al., 2021)) was introduced to facilitate domain-specific adaptations with a significantly reduced parameter footprint, with the hypothesis that low-rank adaptation is sufficient for fine-tuning domain specific foundation models.

Given the pre-trained weight matrix \(W_{0}\in\mathbb{R}^{d\times k}\), LoRA posits that the weight matrix of the adapted foundation model can be expressed as \(W_{0}+\Delta W=W_{0}+BA\), where \(\Delta W\) has a low-rank structure. This matrix \(\Delta W\) is factorized into two smaller, trainable matrices: \(B\in\mathbb{R}^{d\times r}\) and \(A\in\mathbb{R}^{r\times k}\), such that \(\Delta W=BA\) where \(r\) stands for the rank. For a given input \(\mathbf{x}_{i}\), the output \(\mathbf{y}_{i}\) is given by:","\mathbf{y}_{i}=\mathcal{M}(\mathbf{x}_{i}|\Delta W,W_{0},\theta)",,\mathbf{y}_i = (W_0 + BA)\mathbf{x}_i,"This equation represents the output computation in LoRA-adapted models, where the adapted weight matrix $W_0 + BA$ (composed of the pre-trained weights $W_0$ and low-rank matrices $B,A$) transforms input $\mathbf{x}_i$ into output $\mathbf{y}_i$."
ICLR_2024_oral_7,2,"### Batching & throughput

Batching is a common practice where multiple data points (\(\mathbf{x}_{1},\mathbf{x}_{2},\ldots,\mathbf{x}_{m}\)) are aggregated into a single batch \(\mathcal{B}\). Consequently, the forward passes of these data points are processed concurrently rather than individually. This practice leverages the parallel processing capability of a modern GPU, thereby significantly improving the throughput \(T\), i.e., the number of data points processed per unit of time. In the context of foundation models, throughput of a batch \(\mathcal{B}\) can be defined as \(T=\sum_{i=1}^{m}|\mathbf{y}_{i}|/\Delta t\), where \(|\mathbf{y}_{i}|\) is the number of tokens generated for each example \(\mathbf{x}_{i}\) in the batch, \(\Delta t\) is the total time taken to process the batch, and \(m\) is the number of examples in the batch. Note that batching incurs minimal latency penalties. However, given its substantial increase in throughput, batching and its variants are widely used in the state-of-the-art foundation models serving framework such as vLLM (Kwon et al., 2023) to achieve the best balance between throughput and latency.

### Objective

Batching typically assumes the same model parameters are utilized for every input example within a minibatch. Hence, a straightforward application of batching in LoRA requires that the adapter matrix \(\Delta W\) be shared across all inputs in the batch \(\mathcal{B}\). The challenge arises when considering a scenario where each input example in the batch might originate from a different task. Sharing the same \(\Delta W\) for all \(\mathbf{x}_{i}\) in \(\mathcal{B}\) becomes suboptimal where each input potentially demands a unique adapter. The limitation is particularly acute when the model is expected to serve a world-wide user base with diverse incoming requests.

Given the limitations of LoRA in batching, our objective is to maximize the throughput \(T\) in global user serving scenarios by maintaining the batching mechanism. Formally, for each \(\mathbf{x}_{i}\in\mathcal{B}\), we aim to compute \(\mathbf{y}_{i}=\mathcal{M}(\mathbf{x}_{i}|\Delta W_{i},W_{0},\theta)\), where \(\Delta W_{i}\) is the adapter matrix corresponding to the input example \(x_{i}\). Therefore, \(\Delta W_{i}\) can be unique across \(\mathcal{B}\) and specific to a domain or user preference.

## 3 Flora: fast low rank adaptation

As shown in SS2.3, adapter sharing is often impractical in real-world serving scenarios. The innovation of FLORA is the introduction of example-specific adapter \(\Delta W_{i}\) for each \(\mathbf{x}_{i}\) in a mini-batch. In FLORA, the weight matrix \(W_{i}\) for each example \(\mathbf{x}_{i}\) in the minibatch is calculated as \(W_{i}=\Delta W_{i}\circ W_{0}\), where \(\circ\) denotes element-wise multiplication, \(W_{0}\) is the pre-trained weight matrix and \(\Delta W_{i}\) is a low-rank adaptation specifically designed for \(\mathbf{x}_{i}\). Similar to Hu et al. (2021), \(\Delta W_{i}\) is decomposed into two trainable matrices: \(B_{i}\in\mathbb{R}^{d\times r}\) and \(A_{i}\in\mathbb{R}^{r\times k}\), such that \(\Delta W_{i}=B_{i}A_{i}\), as shown in Fig. 1. Note that FLORA has the same expressive power as LoRA by its construction.

### Forward Pass

The advantage of FLORA is that computations on a minibatch can be written in terms of matrix multiplications. This enables efficient batched implementations on modern accelerators such as GPUs. Let \(x_{i}\) denote the activations in one layer of a neural net, which is a vertical vector of length \(d\). The next layer's activations are given by",y_{i} =\phi(W_{i}^{T}x_{i}) || =\phi\big{(}(W_{0}^{T}\circ\Delta W_{i}^{T})x_{i}\big{)},,\mathbf{y}_{i} = (B_{i} A_{i}) \mathbf{x}_{i} = W_{i} \mathbf{x}_{i},"FLORA computes the output activations for each example in a minibatch using example-specific adapter matrices $B_{i}$ and $A_{i}$, enabling efficient batched computation while maintaining task-specific adaptations."
ICLR_2024_oral_7,3,(3),=\phi\big{(}(W_{0}^{T}\circ(B_{i}A_{i})^{T})x_{i}\big{)},,y_{i} = \phi\left( W_{0}^{T} \circ (A_{i}^{T} B_{i}^{T}) x_{i} \right),"The output activations of a neural network layer in FLORA, where the adapted weight matrix is expressed in terms of the pre-trained weights and example-specific low-rank adapter matrices."
ICLR_2024_oral_7,4,(4),=\phi\Big{(}A_{i}\circ\big{(}W_{0}^{T}(B_{i}\circ x_{i})\big{)} \Big{)},,\mathbf{y}_{i} = \phi\left(\left(W_0^T \circ (B_i A_i)^T\right) \mathbf{x}_i\right),"Equation 4 reformulates the forward pass in FLORA by substituting the low-rank decomposition of the example-specific adapter matrix $\Delta W_i = B_i A_i$ into the activation computation, emphasizing the element-wise multiplication with the pre-trained weight matrix $W_0$."
ICLR_2024_oral_7,5,"When the rank is greater than one, we extend the use of the symbol ""o"" to denote potential broadcasting. Additionally, a dimension reduction operation such as torch.mean is required prior to applying the activation function \(\phi\).

The key to fLoRA's flexibility lies in the low rank decomposition enables the incorporation of example-specific adapters directly into the forward pass, as demonstrated in the equations above. Crucially, each of these operations--the element-wise multiplication between \(A_{i}\) and \(x_{i}\), and between \(B_{i}\) and \(y_{i}\) -- is inherently batch-friendly. Consequently, fLoRA allows for simultaneous processing of multiple requests, each requiring its own adapter, within a single minibatch. To vectorize all adapters in the minibatch, we define matrices \(\mathbf{A}\) and \(\mathbf{B}\) whose rows correspond to the adapters \(A_{i}\) and \(B_{i}\) for all examples in the minibatch. The above equation is vectorized as:",\mathbf{Y}=\phi\Big{(}\mathbf{A}\circ\big{(}(\mathbf{B}\circ\mathbf{X})W_{0} \big{)}\Big{)},,\mathbf{Y} = \phi\left(\text{Reduce}\left(\mathbf{A} \circ \left(W_0^T (\mathbf{B} \circ \mathbf{X})\right)\right)\right),"Vectorized forward pass in FLORA, where $\mathbf{A}$ and $\mathbf{B}$ contain example-specific adapters, $\mathbf{X}$ is the batch of inputs, and $\text{Reduce}$ denotes a dimension reduction operation such as torch.mean."
ICLR_2024_oral_7,6,"### Computational Efficiency

The computational analysis primarily concentrates on the examination of fully connected layers within a transformer architecture, given that LoRA is specifically applied to these layers, such as query and key projections. To begin, we analyze a baseline that leverages batch matrix multiplication to facilitate the serving of LoRA with multiple adapters. This operation is possible under the assumption that every adapter required by the input examples in the minibatch shares the same shape, specifically, the same rank. The batch matrix multiplication (BMM) can be implemented using the torch.bmm operator in deep learning frameworks such as PyTorch (Paszke et al., 2019). Note that the BMM operator is typically unfavorable in practical settings due to the significant overhead it introduces (Abdelfattah et al., 2016). This overhead diminishes the throughput and increases latency, which is detrimental in serving scenarios where response times are crucial for maintaining a good user experience.

Let \(b\) and \(l\) denote the batch size and the maximum sequence length in the input batch \(\mathcal{B}\). Revisiting the notation introduced in SS3, where \(W_{0}\in\mathbb{R}^{d\times k}\), \(B_{i}\in\mathbb{R}^{d\times r}\) and \(A_{i}\in\mathbb{R}^{r\times k}\), the operations required to compute the pre-activation for an input batch \(\mathcal{B}\) with dimensions \([b,l,d]\) consist of one matrix multiplication and two BMMs. The matrix multiplication occurs between the input batch \(\mathbf{X}\) and the pre-trained weight \(W_{0}\). The two BMM operations are conducted firstly between the input batch \(\mathbf{X}\) and \(\mathbf{B}\), and secondly between the result of the prior computation and \(\mathbf{A}\), where \(\mathbf{A}\) and \(\mathbf{B}\) are matrices whose rows correspond to the adapters \(A_{i}\) and \(B_{i}\) for all examples in the minibatch respectively. Assuming for simplicity that the layer neither upscales nor downscales the hidden dimension (i.e. \(d=k\)), the upper bound complexity of this layer is discerned as \(2c_{1}(dblr)+c_{2}(bld^{2})\), with \(c_{1}\) and \(c_{2}\) representing the computational coefficients of BMM and matrix multiplication respectively. Note that \(c_{1}>>c_{2}\) because the BMM operator is more expensive than matrix multiplication.

For fLoRA, the cost is one matrix multiplication which is \(c_{2}(rbld^{2})\) where \(r\) denotes the rank of the adapters. We omit the cost of element-wise multiplication in this analysis because it is negligible to the matrix multiplication cost. Comparing the computational cost of fLoRA and LoRA boils down to the following inequality",\frac{2c_{1}}{dc_{2}}+\frac{1}{r}\geq 1,,\mathbf{c_{2}(r b l d^{2}) < 2c_{1}(d b l r) + c_{2}(b l d^{2})},"The inequality comparing the computational cost of fLoRA and LoRA, where $c_1$ and $c_2$ denote the computational coefficients of batch matrix multiplication and standard matrix multiplication, respectively, and $r$, $b$, $l$, $d$ represent rank, batch size, sequence length, and hidden dimension."
ICLR_2024_oral_73,1,"## 2 Task and network architecture

**Task structure.** The task structure is based on a common ICL formulation. The network is trained to predict the label of a target \(x_{q}\) given an alternating sequence of \(N\) items and \(N\) labels: \(x_{1},\ell_{1},x_{2},\ell_{2},\dots,x_{N},\ell_{N},x_{q},?\) (Figure 1a). We embed the items and labels in \(P+D\) dimensions. The first \(P\) dimensions encode positional information and the latter \(D\) dimensions encode content. Position is encoded by a one-hot \(P\)-dimensional vector (we use \(P=65\) throughout). The input sequence occupies a random window of length \(2N+1\) between 0 and \(P-1\). This choice of positional encoding biases the network to learn a translation-invariant computation.

The items are sampled from a gaussian mixture model with \(K\) classes. Each class \(k\) is defined by a \(D\)-dimensional vector \(\mu_{k}\) whose components are sampled i.i.d from a normal distribution with mean zero and variance \(1/D\). The content of item \(x_{i}\), \(\tilde{x}_{i}\), is given by","\tilde{x}_{i}=\frac{\mu_{k}+\varepsilon\eta}{\sqrt{1+\varepsilon^{2}}},",where \(\eta\) is drawn from the same distribution as the \(\mu_{k}\)'s and \(\varepsilon\) sets the within-class variability,"\tilde{x}_{i} \sim \mathcal{N}(\mu_{k}, \sigma^{2}I_{D})","The content of item $x_i$, $\tilde{x}_i$, is sampled from a Gaussian distribution centered at the class mean $\mu_k$ with covariance $\sigma^2 I_D$."
ICLR_2024_oral_73,2,"ty. The re-scaling with \(\sqrt{1+\varepsilon^{2}}\) ensures that \(||\tilde{x}_{i}||\approx 1\). Each class is assigned to one of \(L\) labels (\(L\leq K\)). The contents of the labels are drawn prior to training from the same distribution as the \(\mu_{k}\)'s. Each label in an input sequence appears the same number of times as every other label in that sequence.

Importantly, at least one item in the context belongs to the target's class. The network is trained to classify the target \(x_{q}\) into one of the \(L\) labels using a cross-entropy loss. The network can thus achieve zero loss by either learning to classify targets from the \(K\) classes as in a standard in-weights classification task (IWL), or by learning a more general in-context solution (ICL).

**Parameterizing the data distribution.** The data distribution is modulated by tuning various parameters in addition to \(K\) and \(\varepsilon\). The burstiness \(B\) is the number of occurrences of items from a particular class in an input sequence (\(N\) is a multiple of \(B\)). \(p_{B}\) is the fraction of bursty sequences. Specifically, the burstiness is \(B\) for a fraction \(p_{B}\) of the training data. The classes (including the target) are sampled i.i.d for the remaining fraction \(1-p_{B}\). The rank-frequency distribution over the classes is \(f(k)\sim k^{-\alpha}\). We use \(L=32,N=8,D=63,\varepsilon=0.1,\alpha=0\) unless otherwise specified.

**Metrics for tracking in-context and in-weights learning.** To track IWL, we measure the prediction accuracy on input sequences. The target and item classes are sampled independently from the rank-frequency distribution used during training (Figure 1b). Since \(K\gg N\) in our experiments, it is unlikely that the target's class appears in the context. The network therefore has to rely on IWL to correctly predict the target's class label.

The primary metric for tracking ICL is the prediction accuracy on input sequences. The target and items belong to novel classes (the \(\mu_{k}\)'s are drawn anew). The novel classes are randomly assigned one of the existing \(L\) labels (Figure 1b). \(B\) copies of the target (within variability \(\varepsilon\)) are included in the context. Since the classes are novel, the network has to rely on ICL for accurate prediction. We introduce a secondary metric for tracking ICL using input sequences where the items' labels are different from those presented during training. We measure the accuracy of the network on predicting the _swapped_ label. That is, the network has to rely on ICL rather than IWL.

**Network architecture.** The inputs are passed through a two-layer attention-only network followed by a classifier. Each attention layer has one attention head with a causal mask. Given a sequence of inputs \(u_{1},u_{2},\dots,u_{n}\), the outputs of the first (\(v_{i}\)) and second (\(w_{i}\)) layers are","v_{i}=u_{i}+V_{1}\sum_{j\leq i}p_{ij}^{(1)}u_{j},\quad w_{i}=v_{i}+V_{2}\sum_{ j\leq i}p_{ij}^{(2)}v_{j} || p_{ij}^{(\mu)}=\frac{e^{(K_{\mu}u_{j})^{T}(Q_{\mu}u_{i})}}{\sum_{k\leq i}e^{(K _{\mu}u_{k})^{T}(Q_{\mu}u_{i})}}","where

\[p_{ij}^{(\mu)}=\frac{e^{(K_{\mu}u_{j})^{T}(Q_{\mu}u_{i})}}{\sum_{k\leq i}e^{(K _{\mu}u_{k})^{T}(Q_{\mu}u_{i})}} \tag{3}\]

is the attention paid by query \(i\) on key \(j\) in the \(\mu\)th layer","v_{i} = \text{softmax}\left(\frac{q_{i}k_{1:i}^{T}}{\sqrt{d}}\right) \cdot v_{1:i},","where $v_{i}$ is the output of the first attention layer computed using queries, keys, and values derived from the inputs $u_{i}$, with a causal mask ensuring that predictions depend only on prior elements in the sequence."
ICLR_2024_oral_73,3,"is the attention paid by query \(i\) on key \(j\) in the \(\mu\)th layer. \(Q_{\mu},K_{\mu},V_{\mu}\) are the query, key and value matrices, respectively. The classifier receives \(w_{n}\) as input.

The classifier is a three-layer MLP with ReLU activations and a softmax layer which predicts the probabilities of the \(L\) labels. We use a deep classifier to ensure perfect IWL is feasible. At least threelayers were necessary to achieve perfect classification accuracy for the parameter ranges considered in this paper (since \(K\gg L\)). The query/key dimension and the MLP hidden layer dimension are both 128. We repeat every experiment with six seeds (with random initializations and training/test sets). For training, we use a batch size of 128 and vanilla SGD with learning rate 0.01. Figure 1d shows sample loss and accuracy curves, including the measures used to track IWL and ICL.

## 3 Results

**Recapitulating data distributional dependencies.** In Figure 2, we quantify how IWL and ICL depend on the parameters of the data distribution. The upshot is that the highly simplified input statistics and network architecture considered here reproduce the core distributional dependencies observed in past work. The results are summarized below.

Increasing the burstiness \(B\) and the number of classes \(K\) promotes ICL while decreasing IWL (Figure 2a), highlighting the trade-off between ICL and IWL. Recall that the target and item classes are randomly sampled when \(B=0\). This implies that the network can indeed learn a perfect IWL solution for the corresponding \(K\). Similarly, within-class variation (\(\varepsilon\)) promotes ICL and decreases IWL (Figure 2b). We find that the network always converges to an IWL solution when the fraction of bursty sequences \(p_{B}<1\). This is expected as the ICL solution is not a global minimum when \(p_{B}<1\).

A striking result is that a Zipfian rank-frequency distribution (\(\alpha=1\)) overcomes the trade-off between IWL and ICL, and promotes both forms of learning. This is recapitulated in our experiments (Figure 2c). Note, however, that while the network learns the IWL solution for the most common classes, it does not learn the less frequent classes even for \(\alpha=1\).

Moreover, we find that the network can support both ICL and IWL simultaneously. To show this, we train the network on IC sequences, where the items are all drawn from novel classes randomly assigned to one of the \(L\) labels. The parameter \(p_{C}\) is the fraction of the training data containing IC sequences. The remaining fraction of the training data is drawn as described previously. When \(0<p_{C}<1\) and \(0\leq p_{B}<1\), the network can only achieve zero loss if it learns both the in-context and in-weights solutions. Figure A.1 shows that the network is capable of learning both solutions simultaneously.

One potential explanation for the results in Figure 2 and Figure A.1 is that the network _independently_ learns the in-weights and in-context solutions at different rates until it achieves zero loss. The relative rates at which the network achieves ICL and IWL will then determine the fraction of loss explained by each mechanism after convergence. The rates of ICL and IWL depend on \(K,\varepsilon,B\). Specifically,

Figure 2: In-weights (top row) and in-context accuracy (bottom row) against the number of classes (\(K\)), burstiness (\(B\)), within-class variability (\(\varepsilon\)) and the exponent of the rank-frequency distribution (\(\alpha\)). Here \(K=1024,\alpha=0,B=1,\varepsilon=0.1\) except when that parameter is varied.

increasing \(K,\varepsilon\) decreases the rate of IWL (as the classification task is harder) whereas increasing \(B\) increases the rate of ICL (as there are more demonstrations in the context). The Zipfian case of \(\alpha=1\) further highlights the dynamic balance between ICL and IWL. Frequent occurrences of common classes allow the network to learn to classify them using IWL. On the other hand, the large number of rare classes promotes learning of a more general in-context solution. Once the in-context solution is learned, IWL freezes as the network incurs near-zero loss on all classes. When \(\alpha>1\), the tail of the rank-frequency distribution falls off rapidly and the rare classes do not contribute sufficiently to the loss to promote ICL. Conversely, when \(\alpha<1\), the network learns the in-context mechanism if \(K\) is large enough such that IWL takes longer than ICL (see Figure 2a for \(\alpha=0\) and varying \(K\)).

Attention maps and progress measures.We now examine the dynamics of ICL. We set \(p_{C}>0,p_{B}=1\) as the IC sequences promote rapid convergence to the in-context solution and allow for more experiments. Figure 3a shows the IC accuracy, which displays a slow learning phase followed by an abrupt transition to perfect accuracy. To investigate network behavior at the transition, we examine the attention maps (for a randomly chosen input sequence) before and after the transition (Figure 3b). Before the transition, the attention map of the first layer \(p^{(1)}\) shows queries paying uniform attention to the keys. For the second layer, we visualize the attention paid by the target \(p^{(2)}_{q}\) on the other tokens (as the other attention patterns do not influence classifier output), which also shows no clear pattern. After the transition, however, the attention heads show clear structure: queries in the first layer pay attention to keys that immediately precede them and the target pays attention to one particular key (here, the target's correct label).

Another curious feature of the IC accuracy curves is the slow learning phase that precedes the abrupt transition (Figure 3a). This phase leads to a non-negligible increase in IC accuracy despite the unstructured attention maps. What drives this slow learning? We hypothesize that the network learns to extract useful information from the context despite not learning the optimal ICL solution. Specifically, the total number of labels (\(L\)) is larger than the number of labels represented in the context (\(N\)). The network can thus randomly pick one of the \(N\) contextual labels to increase its accuracy from \(1/L\) to \(1/N\). This picture suggests that the target pays attention to the \(N\) labels in the second layer.

Figure 4: Progress measures for six seeds aligned based on when the IC accuracy crosses 50%. The color-progress measure pairings are orange: (ILA1), green: (TILA2), blue: (CLA), red: (TLA2), black: IC accuracy. See text for more details.

Figure 3: (a) IC accuracy curve (\(p_{C}=0.8,B=1,K=256\)) shows a slow learning phase followed by the abrupt transition to zero loss. (b) The layer 1 and 2 attention maps \(p^{(1)}\) (top matrices) and \(p^{(2)}_{q}\). (bottom vectors) before and after the abrupt transition (marked in the IC curve in panel (a)).

To test this hypothesis and quantify the patterns visualized in the attention maps, we define four progress measures. Item-label association (ILA1): the attention paid by a token to its previous one in the first layer. Target-item-label association (TILA2): the attention paid by the target to the correct label in the second layer. Context-label accuracy (CLA): the probability that the network predicts a label present in the context. Target-labels association (TLA2): the total attention paid by the target to the \(N\) labels in the second layer. (ILA1) and (TILA2) quantify the changes that occur during the abrupt transition whereas (CLA) and (TLA2) quantify the changes expected during the slow learning phase. Each progress measure is obtained by averaging over 1000 test input sequences.

Figure 4 shows aligned progress measures (based on when IC accuracy reaches 50%). The dynamics of IC accuracy and the progress measures are remarkably reproducible across seeds. Figure 4 confirms the hypothesis that the network learns to randomly pick a contextual label in the slow learning phase (blue curve in Figure 4). Moreover, this is accompanied by the target paying attention to the labels (red curve in Figure 4). As visualized in Figure 2(b), the item-label associations of the first layer and target-item-label associations of the second layer appear precisely at the transition (green and orange curves in Figure 4).

**Induction head formation drives the abrupt transition during ICL.** The dynamics of the progress measures raises various hypotheses regarding the factors that lead to ICL. Specifically, we are interested in whether learning (CLA) or (TLA2) is _necessary_ for the abrupt transition (tracked by (ILA1),(TILA2)). We consider various hypotheses and design experiments to test them: H1. (CLA) \(\rightarrow\) (TLA2) \(\rightarrow\) (ILA1), (TILA2). H2. (TLA2) \(\rightarrow\) (ILA1), (TILA2). H3. (CLA) \(\rightarrow\) (ILA1), (TILA2). It is also possible that none of these factors or a progress measure that we have no considered leads to ICL.

We first note that progress measures (ILA1) and (TILA2) strongly suggest the formation of an induction head. Recall that an induction head enables zero-shot copying: given an input sequence \(\ldots x,\ell\ldots x,?\), an induction head allows for predicting \(\ell\) even if \(x,\ell\) never appear together during training. Clearly, this is a mechanism that plausibly solves our task in-context. An induction head implemented by a two-layer attention-only network executes the following sequence of operations (visualized in Figure 5): (i) A token (say, \(\ell\)) pays attention to the token immediately preceding it (here, \(x\)) using positional information. (ii) The value matrix of the first layer now writes the _content_ of \(x\) into \(\ell\). Importantly, this is written to a ""buffer"" subspace orthogonal to the content of \(\ell\). (iii) The _target_\(x\) pays attention to \(\ell\) by matching its content to \(\ell\)'s buffer, which now contains the content of the _contextual_\(x\) that preceded it. (iv) The value matrix of the second layer writes the content of \(\ell\) to the target \(x\), which is then passed to the classifier. The classifier in turn uses this information to predict \(\ell\).

We construct a minimal three-parameter model of the two-layer induction head that emulates these core computations and also captures the four progress measures. We assume that the input embedding space can be decomposed into two orthogonal \(D\)-dimensional subspaces. For a token \(u_{i}\), these orthogonal subspaces encode content \(u_{i}^{(c)}\) and a buffer \(u_{i}^{(b)}\) (initially empty). Given a sequence \(u_{1},u_{2},\ldots,u_{n}\), the first and second layers of our minimal model compute","v_{i}^{(b)}=\sum_{j\leq i}q_{ij}^{(1)}u_{j}^{(c)},\quad v_{i}^{(c )}=u_{i}^{(c)} || w_{i}^{(b)}=\sum_{j\leq i}q_{ij}^{(2)}v_{j}^{(c)},\quad w_{i}^{( c)}=v_{i}^{(c)}",,"\begin{aligned} v_{i}^{(c)} &= u_{i}^{(c)}, \quad v_{i}^{(b)} = u_{i}^{(b)} + \sum_{j \leq i} p_{ij}^{(1)} u_{j}^{(c)}, \\ w_{i}^{(c)} &= v_{i}^{(c)}, \quad w_{i}^{(b)} = v_{i}^{(b)} + \sum_{j \leq i} p_{ij}^{(2)} v_{j}^{(b)}. \end{aligned}","Minimal model of the two-layer induction head, where content and buffer subspaces are updated through attention mechanisms across layers."
ICLR_2024_oral_73,4,"Figure 5: An illustration of the four operations performed by an induction head.

where","q_{ij}^{(1)}=\frac{e^{\beta_{1}\delta_{i-1,j}}}{\sum_{k\leq i}e^{\beta_{1}\delta_ {i-1,k}}},\quad q_{ij}^{(2)}=\frac{e^{\alpha v_{j}^{(k)},v_{i}^{(c)}+\beta_{2} \Delta_{i,j}}}{\sum_{k\leq i}e^{\alpha v_{k}^{(k)},v_{i}^{(c)}+\beta_{2}\Delta_ {i,k}}}.",,q_{ij}^{(\mu)}=\frac{e^{\left(u_{i}^{(c)}\right)^{T}u_{j}^{(b)}}}{\sum_{k\leq i}e^{\left(u_{i}^{(c)}\right)^{T}u_{k}^{(b)}}},"is the attention paid by query $i$ on key $j$ in the $\mu$th layer, based on content-to-buffer matching"
ICLR_2024_oral_74,1,"## 2 Preliminaries

We begin by introducing the key concepts of quantum computing, TDA and quantum TDA (QTDA).

**Quantum computing:** Quantum computing is characterized by operations on the quantum state of \(n\) quantum bits or qubits, representing a vector in \(2^{n}\) dimensional complex vector (Hilbert) space. The quantum operations or measurements correspond to multiplying the quantum state vector by certain \(2^{n}\times 2^{n}\) matrices. Quantum circuits represent these operations in terms of a set of quantum gates operating on the qubits. The number of these gates and the depth of the circuit define the circuit complexity of a given quantum algorithm. Quantum computers are difficult to build (preparing and maintaining the quantum states is extremely hard) and are very noisy. Therefore, the principles of quantum error correction were proposed to protect the quantum system from information loss and other damages (Gottesman, 2010). A (large-scale) quantum computer with many qubits is said to be fault-tolerant if the device is capable of such quantum error correction. However, realization of such fault-tolerant quantum systems is likely several years away. Currently available quantum computers are termed ""Noisy Intermediate-Scale Quantum"" (NISQ) (Preskill, 2018), and these devices are prone to considerable error rates and are limited in size by the number of logical qubits available in the system. In order to obtain results with reasonable accuracies on a NISQ device, the quantum circuit implementing a given algorithm needs to be of short depth.

**Topological data analysis:** TDA represents one of the few data analysis methodologies that can process high-dimensional datasets and reduce them to a small set of local and global signature values that are interpretable and laden with predictive and analytical value. Given a set of \(n\) data-points \(\{x_{i}\}_{i=0}^{n-1}\) in some space together with a distance metric \(\mathcal{D}\), a Vietoris-Rips (Ghrist, 2008) simplicial complex is constructed by selecting a resolution/grouping scale \(\varepsilon\) that defines the ""closeness"" of the points with respect to the distance metric \(\mathcal{D}\), and then connecting the points that are a distance of \(\varepsilon\) from each other (i.e., connecting points \(x_{i}\) and \(x_{j}\) whenever \(\mathcal{D}(x_{i},x_{j})\leq\varepsilon\), forming a so-called 1-skeleton). A \(k\)-simplex is then added for every subset of \(k+1\) data-points that are pair-wise connected (i.e., for every \(k\)-clique, the associated \(k\)-simplex is added).

Let \(S_{k}\) denote the set of \(k\)-simplices in the Vietoris-Rips complex \(\Gamma=\{S_{k}\}_{k=0}^{n-1}\), with \(s_{k}\in S_{k}\) written as \(\{j_{0},\ldots,j_{k}\}\) where \(j_{i}\) is the \(i\)th vertex of \(s_{k}\). Let \(\mathcal{H}_{k}\) denote an \(\binom{n}{k+1}\)-dimensional Hilbert space, with basis vectors corresponding to each of the possible \(k\)-simplices (all subsets of size \(k+1\)). Further let \(\tilde{\mathcal{H}}_{k}\) denote the subspace of \(\mathcal{H}_{k}\) spanned by the basis vectors corresponding to the simplices in \(S_{k}\), and let \(\left|s_{k}\right\rangle\) denote the basis state corresponding to \(s_{k}\in S_{k}\). Then, the \(n\)-qubit Hilbert space \(\mathbb{C}^{2^{n}}\) is given by \(\mathbb{C}^{2^{n}}\cong\bigoplus_{k=0}^{n}\mathcal{H}_{k}\). The boundary map (operator) on \(k\)-dimensional simplices \(\partial_{k}:\mathcal{H}_{k}\rightarrow\mathcal{H}_{k-1}\) is a linear operator defined by its action on the basis states as follows:","\partial_{k}|s_{k}\rangle=\sum_{l=0}^{k-1}(-1)^{l}|s_{k-1}(l) \rangle,",where \(\left|s_{k-1}(l)\right\rangle\) is the _lower_ simplex obtained by leaving out vertex \(l\) (i,"\partial_{k}\left|s_{k}\right\rangle = \sum_{i=0}^{k}(-1)^{i}\left|s_{k}\setminus j_{i}\right\rangle,","Definition of the boundary map operator acting on a k-dimensional simplex, which computes the alternating sum of its (k-1)-dimensional faces."
ICLR_2024_oral_74,2,"(i.e., \(s_{k-1}\) has the same vertex set as \(s_{k}\) except without \(j_{l}\)), and \(s_{k-1}\) is \(k-1\)-dimensional, a dimension less than \(s_{k}\). The factor \((-1)^{l}\) produces the _oriented_(Ghrist, 2008) sum of boundary simplices, which keeps track of neighbouring simplices so that \(\partial_{k-1}\partial_{k}\left|s_{k}\right\rangle=0\), given that the boundary of the boundary is empty.

The boundary map \(\tilde{\partial}_{k}:\tilde{\mathcal{H}}_{k}\rightarrow\tilde{\mathcal{H}}_{k -1}\) restricted to a given Vietoris-Rips complex \(\Gamma\) is given by \(\tilde{\partial}_{k}=\partial_{k}\tilde{P}_{k}\), where \(\tilde{P}_{k}\) is the projector onto the space \(S_{k}\) of \(k\) simplices in \(\Gamma\). The full boundary operator on the fully connected complex (the set of all subsets of \(n\) points) is the direct sum of the \(k\)-dimensional boundary operators, namely \(\partial=\bigoplus_{k}\partial_{k}\). The \(k\)_-homology group_ is the quotient space \(\mathbb{H}_{k}:=\ker(\tilde{\partial}_{k})/{\rm img}(\tilde{\partial}_{k+1})\), representing all \(k\)-holes which are not ""filled-in"" by \(k+1\) simplices and counted once when connected by \(k\) simplices (e.g., the two holes at the ends of a tunnel count once). Such global structures moulded by local relationships is what is meant by the ""shape"" of data. The \(k\)_th Betti Number_\(\beta_{k}\) is the dimension of this \(k\)-homology group, namely \(\beta_{k}:=\dim\mathbb{H}_{k}\).

These Betti numbers therefore count the number of holes at scale \(\varepsilon\), as described above. By computing the Betti numbers at different scales \(\varepsilon\), we can obtain the _persistence barcodes/diagrams_(Ghrist, 2008), i.e., a set of powerful interpretable topological features that account for different scales while being robust to small perturbations and invariant to various data manipulations. These stable persistence diagrams not only provide information at multiple resolutions, but they also help identify, in an unsupervised fashion, the resolutions at which interesting structures exist. The _Combinatorial Laplacian_, or Hodge Laplacian, of a given complex is defined as \(\Delta_{k}:=\partial_{k}^{\dagger}\partial_{k}+\partial_{k+1}\partial_{k+1}^{ \dagger}\). From the Hodge theorem (Friedman, 1998; Lim, 2019), we can compute the \(k\)th Betti number as",\beta_{k}:=\dim\ker(\Delta_{k}).,,\beta_{k} = \dim \ker \Delta_{k}.,The $k$th Betti number is equal to the dimension of the kernel of the $k$th Hodge Laplacian operator.
ICLR_2024_oral_74,3,"Therefore, computing Betti numbers for TDA can be viewed as a rank estimation problem (i.e., \(\beta_{k}=\dim\tilde{\mathcal{H}}_{k}-\mathrm{rank}(\Delta_{k})\)). Additional TDA details can be found in Appendix A.2. The problem of normalized Betti number estimation (BNE) is defined as (Gyurik et al., 2020): Given a set of \(n\) points, its corresponding Vietoris-Rips complex \(\Gamma\), an integer \(0\leq k\leq n-1\), and the parameters \((\epsilon,\eta)\in(0,1)\), find the value \(\chi_{k}\in[0,1]\) that satisfies with probability \(1-\eta\) the condition","|\chi_{k}-\frac{\beta_{k}}{|S_{k}|}|\leq\epsilon,","where \(|S_{k}|\) is the the number of \(k\)-simplices \(S_{k}\in\Gamma\) or \(\dim\tilde{\mathcal{H}}_{k}\), the dimension of the Hilbert space spanned by the set of \(k\)-simplices in the complex",\left|\chi_{k}-\frac{\beta_{k}}{\dim\tilde{\mathcal{H}}_{k}}\right|\leq\epsilon.,"This equation defines the normalized Betti number estimation problem, where the goal is to approximate the ratio of the $k$th Betti number to the dimension of the corresponding Hilbert space within a specified precision and confidence level."
ICLR_2024_oral_74,4,"ex.

**Quantum TDA:** Lloyd et al. (2016) proposed Quantum TDA (QTDA), an algorithm for solving an approximation of TDA in polynomial time for a class of simplicial complexes. Recent works have shown, e.g., (Gyurik et al., 2020; Schmidhuber and Lloyd, 2022), that the problem QTDA solves approximately is intractable classically for certain classes of complexes. The TDA problem of computing Betti numbers exactly has been shown to be intractable for even quantum computers as decision clique homology has been proven to be QMA1-hard (Crichigno and Kohler, 2022) for clique complexes; and promise weighted clique homology has been shown to be QMA1-hard and contained in QMA (King and Kohler, 2023). The approximative version that QTDA actually solves involves a different computational class: DQC1-hard. This normalized Betti number estimation problem has been shown to be DQC1-hard for general chain complexes (Cade and Crichigno, 2021) and is conjectured to hold for clique complexes (Cade and Crichigno, 2021; King and Kohler, 2023).

QTDA involves two main steps, namely: (a) repeatedly constructing the simplices in the given simplicial complex as a mixed quantum state using Grover's search algorithm (Boyer et al., 1998); and (b) projecting this onto the eigenspace of \(\Delta_{k}\) in order to calculate the _Betti numbers_ of the complex, using quantum phase estimation (QPE) (Nielsen and Chuang, 2010) (details are provided in the Appendix A). The computational complexity is \(O(n^{5}/(\delta_{k}\sqrt{\zeta_{k}}))\) where \(n\) is the number of data points, \(\delta_{k}\) denotes the smallest nonzero eigenvalue of \(\Delta_{k}\), and \(\zeta_{k}\) is the fraction of all simplices of order \(k\) in the given complex, resulting in significant speedup over known classical algorithms. However, QTDA requires long-lasting quantum coherence to store the loaded data for the length of the long-depth circuits thus requiring fault-tolerant quantum computing. In particular, Grovers and QPE require precise phase information where any errors would accumulate multiplicatively.

## 3 NISQ-TDA

We now present our proposed quantum algorithm, NISQ-TDA, for estimating the (normalized) Betti numbers of datasets (simplicial complexes) defined through vertices and edges. The algorithm involves three key components, namely: (a) an efficient representation of the full boundary operator as a sum of Pauli operators; (b) a quantum rejection sampling technique to project onto the data-defined simplicial complex; and (c) a stochastic rank estimation method to estimate the output signature _Betti_ numbers. In order to calculate the Betti numbers, the first of two major tasks is to construct a quantum circuit that applies the data-defined Laplacian to _any_ input set of simplices. In our algorithm, this involves three main sub-components.

The first is a quantum representation of the complete (not data-defined) boundary map operator (say \(B\)), called the **Fermionic boundary operator**(Cade and Crichigno, 2021; Akhalwaya et al., 2022). It acts on all possible simplices with \(n\) points and returns their corresponding boundary simplices. The representation involves only unitary operators written as a sum of Pauli (fermionic) operators. The Hermitian boundary operator \(B\) is written as \(B=\sum_{i=0}^{n-1}a_{i}+a_{i}^{\dagger}\), where the \(a_{i}\) are the Jordan-Wigner (Jordan and Wigner, 1928) Pauli embeddings corresponding to the \(n\)-spin fermionic annihilation operators. The implementation of this fermionic boundary operator \(B\) on a quantum computer requires only \(n\) qubits, \(O(n^{2})\) gates, and an \(O(n)\)-depth circuit; see Appendix B for details.

The second sub-component, which we call **Projection onto simplices**, consists of projecting onto the simplicial complex (\(\Gamma\)) corresponding to the given data by implementing the projector (\(P_{\Gamma}\)) using multi-qubit gates and auxiliary flag-registers. A series of multi-control-NOT gates, one for each edge in the complement of the \(\varepsilon\)-close graph (precomputed classically), checks if the edges of the input simplices (in superposition) are not present in the data. The result is stored in a _flag_ register which can either be measured (causing a collapse) or used reversibly allowing for uncomputation. Since there are \(\binom{n}{2}\sim O(n^{2})\) potential edges, this seems to require \(O(n^{2})\) depth. However the checks can be run in parallel and in batches using a round-robin procedure for a depth of \(O(n)\).

The result of the checks on a maximum of \(n/2\) pairs of vertices at a time, needs \(n/2\) flag qubits in \(n-1\) rounds, thereby covering all possible \(\binom{n}{2}\) pairs of vertices not \(\varepsilon\)-close. The \(n/2\) pairs are chosen such that the C-C-NOT (Toffoli) gates, controlling on pairs of vertex qubits, targeting the flag register, are executed in parallel.

If the flag registers are measured, then just \(n/2\) flag qubits suffice in total and can be reused using the power of mid-circuit measure and reset. In each round, we measure the flag register and proceed only if we receive all zeros. This collapses the simplex superposition into those simplices that only have pairs which are not missing from the adjacency graph.

If the flag registers are not measured due to the need for reversibility (which is our case when performing subitization), then the auxiliary qubits cannot be measured and reused and \(O(n^{2})\) independent auxiliary qubits are needed.

The 'all-orders' data-defined **Laplacian** can now be expressed as \(\Delta=P_{\Gamma}BP_{\Gamma}BP_{\Gamma}\).

Although this simple linear-depth circuit implementation of \(P_{\Gamma}\) suggests a requirement of quantum computers with all-to-all connectivity (as used in our experiments), we can indeed implement it on quantum computers with only linear qubit connectivity using a sorting network approach in \(O(n)\) depth (Beals et al., 2013; O'Gorman et al., 2019). The network uses nearest-neighbor SWAP gates and with \(n\) layers of such 'qubit swaps', all \(\binom{n}{2}\) pair of qubits become nearest-neighbors at some layer; see O'Gorman et al. (2019) for details.

Most importantly, the ability to write the Laplacian in terms of a circuit that does not require accessing stored quantum data is one of the key enabling innovations of NISQ-TDA. The input edge data is not stored on the quantum computer but enters through the presence or absence of the multi-qubit control gates of the projector. Every time the complex projection is called, the data is freshly and accurately injected into the quantum computer. This suggests that NISQ-TDA is partially self-correcting, and under noise presence, the last application of \(P_{\Gamma}\) mitigates the noise. When noise-levels only allow for one coherent application of \(P_{\Gamma}\), this application meaningfully represents the data and can be used for alternate machine learning tasks.

The third sub-component, which we call **Projection to a simplicial order**, is the construction of the projector (\(P_{k}\)) onto the \(k\)-simplex subspace. The circuit is a sequence of control-'add one' sub-circuits that conditions on each vertex qubit of the simplex register and increments a \(\log(n)\)-sized count register. The operation is equivalent to implementing conditional-permutation, and can be efficiently implemented using diagonalization (Shende et al., 2006) in the Fourier basis. Similarly to \(P_{\Gamma}\), the projection can be executed by measurement collapse (by measuring the count register) or reversibly, enabling uncomputation. The cost in depth is \(O(n)\), since each vertex qubit must take a turn to control onto a count register qubit. The data-defined Laplacian corresponding to simplicial order \(k\) can thus be written as \(\Delta_{k}=P_{k}\Delta P_{k}\).

The second major part of the NISQ-TDA algorithm, which we call the **Stochastic Chebyshev method**, consists of using the above quantum circuit in a larger classically controlled framework, making NISQ-TDA a hybrid quantum-classical algorithm. The classical framework is a stochastic _rank_ estimation using the _Chebyshev_ polynomials (Ubaru & Saad, 2016; Ubaru et al., 2017). Once we obtain the rank of the Laplacian, we have the Betti numbers \(\beta_{k}=\dim(\ker(\Delta_{k}))=|S_{k}|-\rank(\Delta_{k})\), where \(S_{k}\subseteq\Gamma\) is the set of \(k\)-simplices in the given complex \(\Gamma\). Stochastic rank estimation recasts the eigen-decomposition problem into the estimation of the matrix function trace.

Assuming the smallest nonzero eigenvalue of \(\tilde{\Delta}_{k}=\Delta_{k}/n\) is greater than or equal to \(\delta\), we have","\rank(\Delta_{k})\stackrel{{\text{\tiny{def}}}}{{=}}\trace(h( \tilde{\Delta}_{k})),\text{ where }h(x)=\{\begin{array}{ll}1&\text{ if }\ x>\delta\\ 0&\text{ otherwise}\end{array}..",,"\text{rank}(\Delta_{k})=\text{Tr}\left[f_{\delta}(\tilde{\Delta}_{k})\right],","where $f_{\delta}(\cdot)$ is a smooth approximation to the indicator function that identifies non-zero eigenvalues, enabling rank estimation via trace estimation."
ICLR_2024_oral_74,5,"Supposing \(\tilde{\Delta}_{k}=\sum_{i}\lambda_{i}|u_{i}\rangle\langle u_{i}|\) is the eigen-decomposition, we have \(h(\tilde{\Delta}_{k})=\sum_{i}h(\lambda_{i})|u_{i}\rangle\langle u_{i}|\), where the step function \(h(\cdot)\) takes a value of \(1\) above the threshold \(\delta>0\) and the eigenvalues of \(\tilde{\Delta}_{k}\) are in the interval \(\{0\}\cup[\delta,1]\). Next, \(h(\tilde{\Delta}_{k})\) is approximated using a truncated Chebyshev polynomial series (Trefethen, 2019) as \(h(\tilde{\Delta}_{k})\approx\sum_{j=0}^{m}c_{j}T_{j}(\tilde{\Delta}_{k}),\) where \(T_{j}(\cdot)\) is the \(j\)th-degree Chebyshev polynomial of the first kind and \(c_{j}\) are the coefficients with closed-form expressions. The trace is approximated using the stochastic trace estimation method (Hutchinson, 1990) given by \(\trace(A)\approx\frac{1}{n}\sum_{l=1}^{n_{\text{v}}}\langle v_{l}|A|v_{l}\rangle\), where \(|v_{l}\rangle,l=1,\ldots,n_{\text{v}}\), are random vectors with zero mean and uncorrelated coordinates. It can be shown that a set of random columns of the Hadamard matrices works well as a choice for \(\left|v_{l}\right\rangle\), both in theory and practice (see the supplementary material). Sampling a random Hadamard state vector in a quantum computer can be conducted with a short-depth circuit. Given an initial state \(\left|0\right\rangle\), we randomly flip the \(n\) qubits (by applying a NOT gate as determined by a random \(n\)-bit binary number generated classically). Thereafter, we apply the \(n\)-qubit Hadamard gate to produce a state corresponding to a random column of the \(2^{n}\times 2^{n}\) Hadamard matrix. Therefore, the rank of \(\Delta_{k}\) can be approximately estimated as \(\operatorname{rank}(\Delta_{k})\approx\frac{1}{n_{\text{v}}}\sum_{l=1}^{n_{ \text{v}}}\left[\sum_{j=0}^{m}c_{j}\langle v_{l}|T_{j}(\tilde{\Delta}_{k})|v_{ l}\rangle\right]\), where the \(c_{j}\) are Chebyshev coefficients for approximating the step function. Given a circuit that block-encodes \(\tilde{\Delta}_{k}\), we can block-encode a \(j\)-degree Chebyshev polynomial \(T_{j}(\tilde{\Delta}_{k})\) using the idea of quitization (Low and Chuang, 2019; Gilyen et al., 2019). Details are given in Appendix B.

**NISQ-TDA Algorithm:**

```
Input: Pairwise distances of \(n\) data points and encoding of the \(\varepsilon\)-close pairs; parameters \(\epsilon,\delta\), and \(\operatorname{n_{v}}=O(\epsilon^{-2})\); and \(\operatorname{n_{v}}\)\(n\)-bit random binary numbers. Output: Betti number estimates \(\chi_{k},\ k=0,\ldots,n-1\). for\(l=1,\ldots,\operatorname{n_{v}}=O(\epsilon^{-2})\)do for\(j=0,\ldots,m=O(\log(1/\epsilon))\)do 1. Prepare a random Hadamard state vector \(\left|v_{l}\right\rangle\) from \(\left|0\right\rangle\) using the \(l\)-th random number. 2. Use the circuits for \(P_{k}\), \(P_{\Gamma}\), and \(\tilde{B}=B/\sqrt{n}\) to compute \(\left|\phi_{l}\right\rangle=\left|0^{\eta}\right\rangle\tilde{\Delta}_{k} \left|v_{l}\right\rangle+\left|\tilde{\bot}\right\rangle\), where \(q=\#\)auxiliary qubits needed for projections. 3. Use qubitization to form: \(\left|\psi_{l}^{(j)}\right\rangle=\left|0^{\sigma+1}\right\rangle T_{j}( \tilde{\Delta}_{k})\left|v_{l}\right\rangle+\left|\bot\right\rangle\) from \(\left|\phi_{l}\right\rangle\). 4. Compute the Chebyshev moments \(\theta_{l}^{(j)}=\left\langle v_{l}\right|T_{j}(\tilde{\Delta}_{k})\left|v_{ l}\right\rangle\) from \(\left|\psi_{l}^{(j)}\right\rangle\). endfor  For \(j=0\), estimate \(\left|S_{k}\right|\) using the average norm of the \(P_{\Gamma}P_{k}\left|v_{l}\right\rangle\). endfor  Estimate \(\chi_{k}=1-\frac{1}{\operatorname{n_{v}}}\sum_{l=1}^{\operatorname{n_{v}}} \left[\sum_{j=0}^{m}c_{j}\theta_{l}^{(j)}\right]\). Repeat for \(k=0,\ldots,n-1\).
```

**Algorithm 1** NISQ-TDA Algorithm

**Analyses:** Our _NISQ-TDA_ algorithm returns the estimates \(\chi_{k}\) for the normalized Betti numbers \(\beta_{k}/|S_{k}|\), for each order \(k=0,\ldots,n-1\), where \(|S_{k}|\) is the number of \(k\)-simplices in the given \(\Gamma\). We discuss potential scientific machine learning and AI applications of NISQ-TDA in the Appendix. The remainder of this section focuses on theoretical analyses of our _NISQ-TDA_ algorithm, with the formal details and proofs provided in Appendix C. We begin with the following main result.

**Theorem 1**.: _Assume we are given the pairwise distances of any \(n\) data points and the encoding of the corresponding \(\varepsilon\)-close pairs, together with an integer \(0\leq k\leq n-1\) and the parameters \((\epsilon,\delta,\eta)\in(0,1)\). Further assume the eigenvalues of the scaled Laplacian \(\tilde{\Delta}_{k}\) are in the interval \(\{0\}\cup[\delta,1]\), and choose \(\operatorname{n_{v}}\) and \(m\) such that_",\operatorname{n_{v}}=O(\frac{\log(1/\eta)}{\epsilon^{2}})\qquad \qquad\text{and}\qquad\qquad m>\frac{\log(1/\epsilon)}{\sqrt{\delta}}.,,"h(\tilde{\Delta}_{k})\approx\sum_{j=0}^{m}c_{j}T_{j}(\tilde{\Delta}_{k}),","where $T_{j}(\cdot)$ denotes the $j$th-degree Chebyshev polynomial of the first kind and $c_{j}$ are closed-form coefficients approximating the step function $h(\cdot)$, enabling stochastic trace estimation for $\rank(\Delta_{k})$ via quantum-classical hybrid computation."
ICLR_2024_oral_74,6,"_Then, the Betti number estimation \(\chi_{k}\in[0,1]\) by NISQ-TDA, with probability at least \(1-\eta\), satisfies_",|\chi_{k}-\frac{\beta_{k}}{|S_{k}|}|\leq\epsilon.,,"|\chi_{k}-\frac{\beta_{k}}{|S_{k}|}|\leq\epsilon,","where $\chi_{k}$ is the estimated normalized Betti number, $\beta_{k}$ is the true $k$th Betti number, and $|S_{k}|$ is the number of $k$-simplices in the given simplicial complex $\Gamma$."
ICLR_2024_oral_74,7,"Our analysis accounts for errors due to (a) polynomial approximation of the step function; (b) stochastic trace estimator; and (c) also shot noise, i.e., errors in Chebyshev moments estimation and their propagation in classical computation; for details, see Appendix C.

We next discuss the circuit and computational complexities of our proposed algorithm and show that it is NISQ implementable under certain conditions, such as the requirement for simplices-dense complexes, which commonly occur for large resolution scale. The main quantum component of the algorithm comprises the computation of \(\theta_{l}^{(j)}=\left\langle v_{l}\right|T_{j}(\tilde{\Delta}_{k})\left|v_{l}\right\rangle\), for \(j=0,\ldots,m\sim O(\log(1/\epsilon)/\sqrt{\delta})\)with \(\mathrm{n_{v}}\sim O(\epsilon^{-2})\) random Hadamard vectors. The random Hadamard state preparation requires \(n\) single-qubit Hadamard gates in parallel and \(O(1)\) time. For a given \(k\), constructing \(\tilde{\Delta}_{k}\) involves implementing the boundary operator \(\tilde{B}\) and the projectors \(P_{\Gamma}\) and \(P_{k}\). The operator \(B\), involving the sum of \(n\) Pauli operators, can be implemented using a circuit with \(O(n)\) gates. \(P_{k}\) requires \(O(n\log^{2}n)\) gates, while \(P_{\Gamma}\) requires \(O(n^{2})\) gates, but with both fitting within \(O(n)\) depth due to parallelization. Our implementation of \(P_{\Gamma}\) requires \(n/2\) auxiliary qubits for measured projection but \(O(n^{2})\) auxiliary qubits when uncomputation is needed.

The total time complexity of our algorithm is","O(\frac{1}{\epsilon^{2}}\max\{\frac{n\log(1/\epsilon)}{\sqrt{\delta} },\frac{n}{\zeta_{k}}\}).",,O\left(\frac{n^{2}}{\epsilon^{2}\delta^{1/2}}\log\frac{1}{\eta}\right).,
ICLR_2024_oral_82,1,"### BiasNorm

Conformer (Gulati et al., 2020) utilizes LayerNorm (Ba et al., 2016) to normalize the module activations. Given \(\mathbf{x}\) with \(D\) channels, LayerNorm is formulated as:",\mathrm{LayerNorm}(\mathbf{x})=\frac{\mathbf{x}-\mathrm{E}[\mathbf{x}]}{\sqrt {\mathrm{Var}[\mathbf{x}]+\epsilon}}\odot\mathbf{\gamma}+\mathbf{\beta}.,,"\begin{equation}
    \text{LayerNorm}(\mathbf{x}) = \gamma \cdot \frac{\mathbf{x} - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta
\end{equation}","LayerNorm normalizes the input $\mathbf{x}$ by subtracting the mean $\mu$ and dividing by the standard deviation $\sqrt{\sigma^2 + \epsilon}$, followed by scaling with $\gamma$ and shifting with $\beta$."
ICLR_2024_oral_82,2,"Specifically, it first computes the mean \(\mathrm{E}[\mathbf{x}]\) and the standard-deviation \(\sqrt{\mathrm{Var}[\mathbf{x}]}\) for normalizing, scaling the vector length to \(\sqrt{D}\). Then it uses the learnable channel-wise scale \(\mathbf{\gamma}\) and bias \(\mathbf{\beta}\) for transformation, which helps to adjust the size of activations and balance the relative contributions of specific modules. However, we observe that the trained Conformer using LayerNorm suffers from two problems: 1) It sometimes sets one channel to a large constant value, e.g. 50. We argue that it aims to ""defeat"" the LayerNorm which fully removes the vector length, functioning as a very large value so that length information could be retained after normalization. 2) Some modules (typically feed-forward or convolution) are ""dead"" as they have extremely small output values, e.g., \(10^{-6}\). We argue that early in training, the un-trained modules are not useful so they are ""turned off"" by the LayerNorm scale \(\mathbf{\gamma}\) approaching zero. If the scale \(\mathbf{\gamma}\) oscillates around zero, the inconsistent sign constantly reverses the gradient directions back-propagating to the modules. Because of the inconsistent gradient sign, the modules never learn anything useful, since this is a bad local optimum which is hard to escape because of the dynamics of stochastic gradient descent-like updates.

To address above problems, we propose the _BiasNorm_ which is intended to be a simpler replacement of LayerNorm. Specifically, _BiasNorm_ is formulated as:","\text{{BiasNorm}}(\mathbf{x})=\frac{\mathbf{x}}{\mathrm{RMS}[\mathbf{x}- \mathbf{b}]}\cdot\exp(\gamma),","where \(\mathbf{b}\) is the learnable channel-wise bias, \(\mathrm{RMS}[\mathbf{x}-\mathbf{b}]\) is the root-mean-square value taken over channels, and \(\gamma\) is a scalar",\mathrm{BiasNorm}(\mathbf{x}) = \frac{\mathbf{x}}{\sqrt{\mathrm{Var}[\mathbf{x}] + \epsilon}} \odot \mathbf{\gamma} + \mathbf{\beta}.,"BiasNorm normalizes the input by subtracting only the mean and scaling by the standard deviation, while retaining learnable scale and shift parameters to adjust activation magnitudes and module contributions."
ICLR_2024_oral_82,3,"ar. We first remove the operation of mean subtraction since it is a waste of time unless it follows a non-linearity. The bias \(\mathbf{b}\) serves as the large constant value which allows to retain the vector length information after normalization. Since the scale \(\exp(\gamma)\) is always positive, it avoids the gradient oscillation problem.

### Swooshr and SwooshL activation functions

Conformer (Gulati et al., 2020) adopts Swish (Ramachandran et al., 2017) activation function with the following formula:",\mathrm{Swish}(x)=x\cdot(1+\exp(-x))^{-1}.,,"\text{{Swish}}(x) = x \cdot \sigma(\beta x),",where $\sigma(\cdot)$ is the sigmoid function and $\beta$ is a learnable parameter.
ICLR_2024_oral_82,4,"In this work, we propose two new activation functions respectively called _SwooshR_ and _SwooshL_ as replacements of Swish:","\begin{split}\textit{SwooshR}(x)&=\log(1+\exp(x-1))-0.08x-0.313261687,\\ \textit{SwooshL}(x)&=\log(1+\exp(x-4))-0.08x-0.035. \end{split}",,"\mathrm{SwooshR}(x) = \frac{x + \exp(-x)}{1 + \exp(-x)}, \quad \mathrm{SwooshL}(x) = \frac{x + \max(0, x)}{1 + \max(0, x)}.","_SwooshR_ is a smooth variant inspired by the Swish function, while _SwooshL_ is a piecewise linear approximation designed for efficiency and stability."
ICLR_2024_oral_82,5,"In _SwooshR_, the offset 0.313261687 is to make it pass through the origin; in _SwooshL_, the offset 0.035 was tuned, which slightly outperformed the value exactly making the curve pass through the origin. We present the curves of Swish, _SwooshR_, and _SwooshL_ in Appendix Section A.2. _SwooshL_ is roughly a right shifted version of _SwooshR_. Note that the suffix ""L"" or ""R"" represents whether the left or right zero-crossing is at or around \(x=0\). Similar to Swish, _SwooshR_ and _SwooshL_ have lower bounds and are non-monotonic. Compared to Swish, the most striking difference is that _SwooshR_ and _SwooshL_ have non-vanishing slopes for negative inputs, which helps to escape from situations where the input is always negative and prevents the denominator term in Adam-type updates from getting dangerously small. When replacing Swish with _SwooshR_, we observe that the modules with bypass connections, such as feed-forward and ConvNeXt, tend to learn a large negative bias in the preceding linear layer to learn ""normally-off"" behavior. Therefore, we use _SwooshL_ for these ""normally-off"" modules and use _SwooshR_ for convolution modules and the rest of _Conv-Embed_.

### ScaledAdam optimizer

We propose a parameter-scale-invariant version of Adam (Kingma & Ba, 2014) called _ScaledAdam_, which enables faster convergence and better performance. _ScaledAdam_ scales each parameter's update proportional to the scale of that parameter, and also explicitly learns the parameter scale. Algorithm 1 in Appendix Section A.1.1 presents the pseudo-code of the _ScaledAdam_.

Let \(f(\mathbf{\theta})\) be the loss function that we aim to minimize, which is differentiable w.r.t. the learnable parameter \(\mathbf{\theta}\). At each step \(t\), Adam computes the parameter gradient \(\mathbf{g}_{t}=\nabla\mathbf{\theta}f(\mathbf{\theta}_{t-1})\), and updates the first moment \(\mathbf{m}_{t}=\beta_{1}\cdot\mathbf{m}_{t-1}+(1-\beta_{1})\cdot\mathbf{g}_{t}\) and the second moment \(\mathbf{v}_{t}=\beta_{2}\cdot\mathbf{v}_{t-1}+(1-\beta_{2})\cdot\mathbf{g}_{t}^ {2}\) of gradients, where \(\beta_{1},\beta_{2}\in[0,1)\) are coefficients used to compute the moving averages. The parameter update \(\mathbf{\Delta}_{t}\) at step \(t\) is formulated as:","\mathbf{\Delta}_{t}=-\alpha_{t}\cdot\frac{\sqrt{1-\beta_{t}^{2}}}{1-\beta_{1}^{t}} \cdot\frac{\mathbf{m}_{t}}{\sqrt{\mathbf{v}_{t}}+\epsilon},","where \(\alpha_{t}\) is the learning rate typically specified by an external schedule, \(\frac{\sqrt{1-\beta_{t}^{2}}}{1-\beta_{1}^{2}}\) is the bias-correction term, and \(\epsilon=10^{-8}\)","\mathbf{\Delta}_{t}=-\frac{\alpha}{\sqrt{\hat{\mathbf{v}}_{t}}+\epsilon}\odot\hat{\mathbf{m}}_{t},","where $\alpha$ is the learning rate, $\hat{\mathbf{m}}_{t}$ and $\hat{\mathbf{v}}_{t}$ are the bias-corrected first and second moments, and $\epsilon$ is a small constant to prevent division by zero."
ICLR_2024_oral_82,6,"\). Whilst Adam is invariant to gradient scale of each parameter, we argue that it still suffers from two limitations: 1) The update \(\mathbf{\Delta}_{t}\) in Equation 5 does not take into account the parameter scale (denoted as \(r_{t-1}\)). Considering the relative parameter change \(\mathbf{\Delta}_{t}/r_{t-1}\), Adam might cause learning in relative terms too slowly for parameters with large scales, or too fast for parameters with small scales. 2) It is difficult to learn the parameter scale directly, as the direction of growing or shrinking the parameter tensor is a very specific direction in a large-dimensional space. It's particularly difficult to shrink a parameter, since each gradient step \(\mathbf{g}_{t}\) adds noise which tends to grow the parameter norm.

**Scaling update.** To keep the relative change \(\mathbf{\Delta}_{t}/r_{t-1}\) over parameters of varying scales about the same, we scale the update \(\mathbf{\Delta}_{t}\) in Equation 5 by the parameter scale \(r_{t-1}\):",\mathbf{\Delta}^{\prime}_{t}=-\alpha_{t}\cdot r_{t-1}\cdot\frac{\sqrt{1-\beta_{t}^ {2}}}{1-\beta_{1}^{t}}\cdot\frac{\mathbf{m}_{t}}{\sqrt{\mathbf{v}_{t}}+ \epsilon}.,,\mathbf{\Delta}_{t} = -\alpha_{t} \cdot r_{t-1} \cdot \frac{\sqrt{1-\beta_{t}^{2}}}{1-\beta_{1}^{t}} \cdot \frac{\mathbf{m}_{t}}{\sqrt{\mathbf{v}_{t}}+\epsilon}.,"This equation modifies the Adam update rule by scaling the parameter update with the parameter scale $r_{t-1}$, ensuring consistent relative changes across parameters of different scales."
ICLR_2024_oral_82,7,"We compute the parameter scale \(r_{t-1}\) as the root-mean-square value \(\mathrm{RMS}[\mathbf{\theta}_{t-1}]\). Because the ScaledAdam update is less prone to divergence than Adam, we use a learning rate schedule called _Eden_ that does not have a long warm-up period; we also use absolutely larger learning rate values because the parameter RMS value is normally much less than one.

**Learning parameter scale.** To explicitly learn the parameter scale, we treat it as a regular parameter to be learned, as if we have factored each parameter as \(\mathbf{\theta}=r\cdot\mathbf{\theta}^{\prime}\), and we are doing gradient descent on the parameter scale \(r\) and the underlying parameter \(\mathbf{\theta}^{\prime}\). Let \(h\) be the gradient of the parameter scale \(r\), at step \(t\) we get \(h_{t}=\nabla_{r}f(\mathbf{\theta}_{t-1})=\mathbf{g}_{t}\cdot\mathbf{\theta}^{\prime}_{t -1}\). Since Adam is nearly invariant to changes in the gradient scale, for simplicity we replace this with \(h_{t}=\mathbf{g}_{t}\cdot(r_{t-1}\odot\mathbf{\theta}^{\prime}_{t-1})=\mathbf{g}_{t} \cdot\mathbf{\theta}_{t-1}\). Following the Adam algorithm, we maintain the first moment \(n_{t}=\beta_{1}\cdot n_{t-1}+(1-\beta_{1})\cdot h_{t}\) and the second moment \(w_{t}=\beta_{2}\cdot w_{t-1}+(1-\beta_{2})\cdot h_{t}^{2}\) of the scale gradients \(h_{t}\). The parameter change on \(\mathbf{\theta}\) causedby updating parameter scale from \(r_{t-1}\) to \(r_{t}\) is \(\mathbf{\Delta}^{\prime}_{t,r}=(r_{t}-r_{t-1})\odot\mathbf{\theta}^{\prime}_{t-1}\). Similar to Equation 6, we also integrate the parameter scale \(r_{t-1}\) into the update \(\mathbf{\Delta}^{\prime}_{t,r}\):","\begin{split}\mathbf{\Delta}^{\prime}_{t,r}&=-\eta\cdot \alpha_{t}\cdot r_{t-1}\cdot\frac{\sqrt{1-\beta_{t}^{2}}}{1-\beta_{1}^{2}} \cdot\frac{n_{t}}{\sqrt{w_{t}}+\epsilon}\odot\mathbf{\theta}^{\prime}_{t-1}\\ &=-\eta\cdot\alpha_{t}\cdot\frac{\sqrt{1-\beta_{t}^{2}}}{1-\beta _{1}^{t}}\cdot\frac{n_{t}}{\sqrt{w_{t}}+\epsilon}\odot\mathbf{\theta}_{t-1}.\end{split}","where \(\eta\) is a scaling factor on learning rate \(\alpha_{t}\), and we found that setting \(\eta=0","\mathbf{\Delta}^{\prime}_{t,r} = \left( -\alpha_{t} \cdot \frac{\sqrt{1 - \beta_{2}^{t}}}{1 - \beta_{1}^{t}} \cdot \frac{n_{t}}{\sqrt{w_{t}} + \epsilon} \right) \odot \mathbf{\theta}^{\prime}_{t-1}.","This equation formulates the parameter change caused by updating the parameter scale $r$ in the ScaledAdam optimizer, incorporating the first and second moments of the scale gradients and maintaining scale invariance during optimization."
ICLR_2024_oral_82,8,"=0.1\) helps to stabilize the training. Now the update \(\mathbf{\Delta}^{\prime}_{t}\) is replaced with \(\mathbf{\Delta}^{\prime}_{t,r}+\mathbf{\Delta}^{\prime}_{t}\), which amounts to adding an extra gradient term in the direction of growing or shrinking each parameter. This also allows to simplify the network structure by removing most of normalization layers in our _Zipformer_ Block (described in Section 3.2), since the modules now can easily learn to scale the activations in a suitable range. One similar method called weight normalization (Salimans and Kingma, 2016) decouples the parameter norm from its direction to speed up the convergence. It replaces each parameter with two parameters, respectively specifying the direction and the magnitude. However, ScaledAdam learns the parameter scales by adding an extra update term \(\mathbf{\Delta}^{\prime}_{t,r}\), which makes writing the modeling code simpler.

**Eden schedule.** The proposed _Eden_ learning rate schedule is formulated as:","\alpha_{t}=\alpha_{\mathrm{base}}\cdot(\frac{t^{2}+\alpha_{\mathrm{step }}^{2}}{\alpha_{\mathrm{step}}^{2}})^{-0.25}\cdot(\frac{e^{2}+ \alpha_{\mathrm{epoch}}^{2}}{\alpha_{\mathrm{epoch}}^{2}})^{-0.25} \cdot\mathrm{linear}(\alpha_{\mathrm{start}},t_{\mathrm{warmup}},t).",,"\alpha_{t}=\begin{cases} \alpha_{0} & \text{if } t\leq T_{0},\\ \alpha_{0}\cdot\left(1-\frac{t-T_{0}}{T_{1}-T_{0}}\right) & \text{if } T_{0}<t\leq T_{1},\\ \alpha_{0}\cdot\left(\frac{T_{1}-T_{0}}{T_{2}-T_{1}}\right)\cdot\left(\frac{T_{2}-t}{T_{2}-T_{1}}\right) & \text{if } T_{1}<t\leq T_{2},\\ 0 & \text{if } t>T_{2}, \end{cases}","The _Eden_ learning rate schedule linearly decreases the learning rate from $\alpha_0$ to zero over three distinct phases defined by steps $T_0$, $T_1$, and $T_2$, enabling faster convergence without requiring a long warm-up period."
ICML_2024_oral_1,1,"## 2 Preliminary

Reinforcement Learning (RL).Within the standard framework of the Markov decision process (MDP), RL can be formulated as \(\mathcal{M}=\langle\mathcal{S},\mathcal{A},\mathcal{P},\mathcal{R},\gamma\rangle\). Here, \(\mathcal{S}\) denotes the state space, \(\mathcal{A}\) the action space, \(r:\mathcal{S}\times\mathcal{A}\in[-R_{max},R_{max}]\) the reward function, and \(\gamma\in(0,1)\) the discount factor, and \(P(\cdot\mid s,a)\) stands for transition dynamics. The objective of an RL agent is to learn an optimal policy \(\pi\) that maximizes the expected discounted sum of rewards, formulated as \(\mathbb{E}_{\pi}[\sum_{t=0}^{\infty}\gamma^{t}r_{t}]\). For any \(s\in\mathcal{S}\) and action \(a\in\mathcal{A}\), the value of action \(a\) under state \(s\) is given by the action-value function \(Q^{\pi}(s,a)=\mathbb{E}[\sum_{t=0}^{\infty}\gamma^{t}\mathcal{R}(s_{t},a_{t})]\). In deep RL, the policy \(\pi\) and the value function \(Q\) are represented by neural network function approximations.

Soft Actor-Critic (SAC).Soft actor-critic (SAC) is a popular off-policy maximum entropy deep reinforcement learning algorithm based on soft policy iteration that maximizes the entropy-augmented policy objective function. Its objective includes a policy entropy regularization term in the objective function with the aim of performing more diverse actions for each given state and visiting states with higher entropy for better exploration, stated as below,","J(\pi)=\sum_{t=0}^{\infty}\mathbb{E}_{(\mathbf{s}_{t},\mathbf{a}_{t})\sim\rho (\pi)}[\gamma^{t}(r(\mathbf{s}_{t},\mathbf{a}_{t})+\alpha\mathcal{H}(\pi( \cdot|\mathbf{s}_{t})))]",,"J(\pi) = \sum_{t=0}^{\infty} \mathbb{E}_{(s_t, a_t) \sim \rho_\pi} \left[ \mathcal{R}(s_t, a_t) + \alpha \mathcal{H}(\pi(\cdot \mid s_t)) \right],","where $\alpha$ is the temperature coefficient that determines the stochasticity of the policy, and $\mathcal{H}(\pi(\cdot \mid s_t))$ denotes the entropy of the policy at state $s_t$."
ICML_2024_oral_1,2,"## 3 Off-policy Actor-critic with Causality-aware Entropy regularization

Overview.Our approach builds upon maximum entropy RL (Haarnoja et al., 2018), with the primary aim of enhancing the sample efficiency of off-policy RL algorithms. Initially, we construct a causal policy-reward structural model, providing a theoretical guarantee for its identifiability. Subsequently, based on this causal model, we introduce causality-aware entropy to present a causality-aware variant of SAC named CausalSAC. To address potential overfitting and further enhance exploration efficiency, we analyze the gradient dormancy phenomenon and propose a gradient-dormancy-guided reset mechanism. The integration of this reset mechanism with CausalSAC constitutes our proposed **ACE**: Off-policy **A**tor-critic with **C**ausality-aware **E**ntropy regularization algorithm.

### Causal Discovery on Policy-Reward Relationship

To explore the causal relationships between each action dimension \(\mathbf{a}_{i}\) and its potential impact on reward gains \(r\), we first establish a causal policy-reward structural model and provide theoretical analyses to ensure the identifiability of the causal structure.

Causal policy-reward structural modeling.Suppose we have sequences of observations \(\{\mathbf{s}_{t},\mathbf{a}_{t},r_{t}\}_{t=1}^{T}\), where \(\mathbf{s}_{t}=(s_{1,t},...,s_{\mathrm{dim}S,t})^{T}\subseteq\mathcal{S}\) denote the perceived \(\mathrm{dim}\mathcal{S}\)-dimensional states at time \(t\), \(\mathbf{a}_{t}=(a_{1,t},...,a_{\mathrm{dim}A,t})^{T}\subseteq\mathcal{A}\) are the executed \(\mathrm{dim}\mathcal{A}\)-dimensional actions and \(r_{t}\) is the reward. Note that the reward variable \(r_{t}\) may not be influenced by every dimension of \(\mathbf{s}_{t}\) or \(\mathbf{a}_{t}\), and there are causal structural relationships between \(\mathbf{s}_{t}\), \(\mathbf{a}_{t}\) and \(r_{t}\)(Huang et al., 2022). To integrate such relationships in MDP, we explicitly encode the causal structures over variables into the reward function","r_{t}=r_{\mathcal{M}}(\mathbf{B}_{\mathbf{s}\to r|\mathbf{a}}\odot \mathbf{s}_{t},\mathbf{B}_{\mathbf{a}\to r|\mathbf{s}}\odot\mathbf{a}_{t}, \epsilon_{t}),","where \(\mathbf{B}_{\mathbf{s}\to r|\mathbf{a}}\in\mathbb{R}^{\mathrm{dim} \mathcal{S}\times 1}\) and \(\mathbf{B}_{\mathbf{a}\to r|\mathbf{s}}\in\mathbb{R}^{\mathrm{dim}\mathcal{A} \times 1}\) are vectors that represent the graph structure 1 from \(\mathbf{s}_{t}\) to \(r_{t}\) given \(\mathbf{a}_{t}\) and from \(\mathbf{a}_{t}\) to \(r_{t}\) given \(\mathbf{s}_{t}\), respectively","r_{t} = f_{r}(\mathbf{s}_{t}, \text{do}(\mathbf{a}_{t}), \varepsilon_{r})","The reward at time $t$ is determined by the state $\mathbf{s}_{t}$, the intervened action $\text{do}(\mathbf{a}_{t})$, and exogenous noise $\varepsilon_{r}$ through the function $f_r$."
ICML_2024_oral_1,3,"ly. Here \(\odot\) denotes the element-wise product while \(\epsilon_{t}\) are i.i.d. noise

Figure 2: **Motivating example. This task involves a robotic arm hammering a screw into a wall. \(\blacklozenge\) Initially, the robotic arm approaches the desk moving on the 2-axis and struggles with torque grasping, making z-axis positioning \(\uparrow\) and torque exploration \(\uparrow\) a priority. \(\blacktriangle\) As the training advances, the agent’s focus shifts to optimizing movement, prioritizing end-effector position (x-axis \(\uparrow\) and y-axis \(\uparrow\)). \(\bigstar\) Finally, potential improvements lie in the stable and swift hammering, shifting focus back to torque \(\uparrow\) and placing down the object \(\uparrow\). The evolving causal weights, depicted on the left, reflect these changing priorities. See more examples in Appendix D.**terms.

Specifically, under the causal Markov condition and faithfulness assumption Pearl (2009), we establish conditions for the causal relationship existence in Proposition 3.3, then the true causal graph \(\mathbf{B}_{\mathbf{a}\to r|\mathbf{s}}\) could be identified from observational data alone, as guaranteed in Theorem 3.4.

**Assumption 3.1** (Global Markov Condition Spirtes et al. (2000); Pearl (2009)).: The distribution \(p\) over a set of variables \(\mathbf{V}=(s_{1,t},...,s_{\dim\mathcal{S},t},a_{1,t},...,a_{\dim\mathcal{A},t },r_{t})^{T}\) satisfies the global Markov condition on the graph if for any partition \((\mathbf{S},\mathbf{A},\mathbf{R})\) in \(\mathbf{V}\) such that if \(\mathbf{A}\) d-separates \(\mathbf{S}\) from \(\mathbf{R}\), then \(p(\mathbf{S},\mathbf{R}|\mathbf{A})=p(\mathbf{S}|\mathbf{A})p(\mathbf{R}| \mathbf{A})\).

**Assumption 3.2** (Faithfulness Assumption Spirtes et al. (2000); Pearl (2009)).: For a set of variables \(\mathbf{V}=(s_{1,t},...,s_{\dim\mathcal{S},t},a_{1,t},...,a_{\dim\mathcal{A},t },r_{t})^{T}\), there are no independencies between variables that are not entailed by the Markovian Condition.

With these two assumptions, we provide the following proposition to characterize the condition of the causal relationship existence so that we are able to uncover those key actions from conditional independence relationships.

**Proposition 3.3**.: _Under the assumptions that the causal graph is Markov and faithful to the observations, there exists an edge from \(a_{i,t}\) to \(r_{t}\) if and only if \(a_{i,t}\not\models r_{t}|\mathbf{s}_{t},\mathbf{a}_{-i,t}\), where \(\mathbf{a}_{-i,t}\) are states of \(\mathbf{a}_{t}\) except \(a_{i,t}\)._

We next provide the theorem to guarantee the identifiability of the proposed causal structure.

**Theorem 3.4**.: _Suppose \(s_{t}\), \(a_{t}\), and \(r_{t}\) follow the MDP model with no unobserved confounders, as in Eq.(1). Under the Markov condition and faithfulness assumption, the structural vectors \(\mathbf{B}_{\mathbf{a}\to r|\mathbf{s}}\) are identifiable._

Note that such a theorem guarantees the identifiability of the correct graph in an asymptotic manner. Additionally, by imposing further assumptions on the data generation mechanism, we could uniquely identify the causal effects. See the proof in Appendix A.1 for details.

### Causality-aware Bellman Operator

By infusing the explainable causal weights \(\mathbf{B}_{\mathbf{a}\to r|\mathbf{s}}\) into policy entropy, we propose the causality-aware entropy \(\mathcal{H}_{c}\) for enhanced exploration. \(\mathcal{H}_{c}\) is defined as","\begin{split}\mathcal{H}_{c}(\pi(\cdot|\mathbf{s}))& =-\mathbb{E}_{\mathbf{a}\in\mathcal{A}}[\sum_{i=1}^{\dim \mathcal{A}}\mathbf{B}_{a_{i}\to r|\mathbf{s}}\pi(a_{i}|\mathbf{s}) \log\pi(a_{i}|\mathbf{s})],\\ &\mathbf{a}=(a_{1},\ldots,a_{\dim\mathcal{A}}).\end{split}",,\mathcal{H}_{c}(\pi) = \mathbb{E}_{\mathbf{s}_t \sim \rho(\pi)} \left[ \sum_{i=1}^{\mathrm{dim,
ICML_2024_oral_1,4,"Based on the causality-aware entropy, then the \(Q\)-value for a fixed policy \(\pi\) could be computed iteratively by applying a modified Bellman operator \(\mathcal{T}_{c}^{\pi}\) with \(\mathcal{H}_{c}(\pi(\cdot|\mathbf{s}))\) term as stated below,","\begin{split}\mathcal{T}_{c}^{\pi}Q(\mathbf{s}_{t},\mathbf{a}_{t })\triangleq& r(\mathbf{s}_{t},\mathbf{a}_{t})+\gamma\mathbb{E}_{ \mathbf{s}_{t+1}\sim P}[\mathbb{E}_{\mathbf{a}_{t}\sim\pi}[Q(\mathbf{s}_{t+1},\mathbf{a}_{t+1})\\ &+\alpha\mathcal{H}_{c}(\pi(\mathbf{a}_{t+1}|\mathbf{s}_{t+1}))]].\end{split}",,"\mathcal{T}_{c}^{\pi}Q(s,a)=r(s,a)+\gamma\mathbb{E}_{s'\sim P(\cdot|s,a)}\left[\mathbb{E}_{a'\sim\pi(\cdot|s')}[Q(s',a')]+\alpha\mathcal{H}_{c}(\pi(\cdot|s'))\right].",The modified Bellman operator incorporates the causality-aware entropy term to update the Q-value for improved policy evaluation.
ICML_2024_oral_1,5,"For a better understanding of our operator, we conduct a theoretical analysis of its dynamic programming properties in the tabular MDP setting, covering policy evaluation, policy improvement, and policy iteration. All proofs are included in Appendix A.2.

**Proposition 3.5** (Policy evaluation).: _Consider an initial \(Q_{0}:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}\) with \(|\mathcal{A}|<\infty\), and \(Q\)-value iterates by \(Q_{k+1}=\mathcal{T}_{c}^{\pi}Q_{k}\). Then the sequence \(\{Q_{k}\}\) converges to a fixed point \(Q^{\pi}\) as \(k\rightarrow\infty\)._

**Proposition 3.6** (Policy improvement).: _Let \(\pi_{k}\) be the policy at iteration \(k\), and \(\pi_{k+1}\) be the updated policy ( maximize of the \(Q\)-value). Then for all \((s,a)\in\mathcal{S}\times\mathcal{A}\), \(|\mathcal{A}|<\infty\), we have \(Q^{\pi_{k+1}}(\mathbf{s},\mathbf{a})\geq Q^{\pi_{k}}(\mathbf{s},\mathbf{a})\)._

**Proposition 3.7** (Policy iteration).: _Assume \(|\mathcal{A}|<\infty\), by repeating iterations of the policy evaluation and policy improvement, any initial policy converge to the optimal policy \(\pi^{*}\), s.t. \(Q^{\pi^{*}}(\mathbf{s}_{t},\mathbf{a}_{t})\geq Q^{\pi}(\mathbf{s}_{t}, \mathbf{a}_{t}),\forall\pi\in\Pi,\forall(\mathbf{s}_{t},\mathbf{a}_{t})\)._

**Causality-aware off-policy actor-critic (CausalSAC).** Our causality-aware entropy provides a flexible solution that can be seamlessly incorporated into any Max-Entropy RL framework. For example, as a plug-and-play component, an algorithm instantiation CausalSAC can be implemented within SAC Haarnoja et al. (2018) by integrating our \(\mathcal{H}_{c}\) into the policy optimization objective, \(J(\pi)=\sum_{t=0}^{\infty}\mathbb{E}_{(\mathbf{s}_{t},\mathbf{a}_{t})\sim p( \pi)}\left[\gamma^{t}(r(\mathbf{s}_{t},\mathbf{a}_{t})+\alpha\mathcal{H}_{c}( \pi(\cdot|\mathbf{s}_{t}))\right]\).

### Gradient-dormancy-guided Reset

Guided by causality-aware entropy, the agent efficiently explores and masters primitive behaviors for different learning stages. However, causality-aware exploration introduces the risk of getting stuck in local optima and overfitting to specific primitive behaviors. To address this challenge, we analyze the gradient dormancy phenomenon during RL training and introduce a soft reset mechanism. This mechanism, guided by gradient dormancy, regularly perturbs the agents' neural networks to maintain network expressivity, thereby improving the agent's performance.

The dormancy phenomenon of neural networks in RL, signifying a loss of expressive capacity, has been previously discussed in existing works Sokar et al. (2023); Xu et al. (2023). However, the dormant phenomenon defined in these works is not evident in state-based RL, and it cannot effectively enhance our algorithm in exploration scheduling. We are the first to investigate dormancy from the perspective of gradients. Here, we introduce definitions for gradient -dormant neurons and the gradient dormancy degree of a neural network.

**Definition 3.8** (Gradient-dormant Neurons).: For a fully connected layer in a neural network, where \(N^{l}\) represents the number of neurons in layer \(l\), the L2 norm of gradients of the weights for neuron \(i\) is denoted as \(n_{i}^{l}\). Neuron \(i\) is classified as a gradient-dormant neuron if it satisfies","\frac{n_{i}^{l}(x)}{\frac{1}{N^{l}}\sum_{k\in l}n_{k}^{l}}\leq\tau,",where \(\tau\) is a constant serving as a threshold to determine the gradient dormancy of neurons in each layer,"n_{i}^{l} < \tau \cdot \frac{1}{N^{l}} \sum_{j=1}^{N^{l}} n_{j}^{l},","where $\tau \in (0,1)$ is a threshold hyperparameter that determines the sensitivity for identifying dormant neurons based on the average gradient magnitude in the layer."
ICML_2024_oral_1,6,"er.

**Definition 3.9** (\(\tau\)-Dormancy Degree \(\alpha_{\tau}\)).: Denote the number of all neurons in the neural network identified as gradient-dormant neurons as \(N_{\tau}^{l}\). The \(\alpha_{\tau}\) for the neural network is defined as:",\alpha_{\tau}=\frac{\sum_{l\in\phi}N_{\tau}^{l}}{\sum_{l\in\phi}N^{l}}.,,"\alpha_{\tau} = \frac{1}{L}\sum_{l=1}^{L}\frac{N_{\tau}^{l}}{N^{l}},","The $\tau$-Dormancy Degree $\alpha_{\tau}$ quantifies the proportion of gradient-dormant neurons across all layers in a neural network, normalized by layer size and total layers $L$."
ICML_2024_oral_1,7,"The \(\tau\)-dormancy degree \(\alpha_{\tau}\) indicates the percentage of gradient-dormant neurons with the \(\tau\) threshold in the fully-connected neural network.

In Figure 3, we depict the occurrence of the gradient dormancy phenomenon at the initial learning stages of SAC. However, in algorithms with superior sample efficiency like CausalSAC and **ACE**, there is a notable reduction in the dormancy degree during training. In tasks that CausalSAC can effectively solve, the gradient dormancy degree can decrease to near-zero levels, as shown in Figure 3. Particularly, when addressing challenging tasks such as pick place walls, which were previously unsolved by existing baselines, dormancy degrees persist at high levels without reset interventions in SAC and CausalSAC. Hence, we speculate that this may represent a potential local optimum for causal-aware exploration. We consider that dormancy degrees may impact sample efficiency and employ a soft reset method (Xu et al., 2023; Ash and Adams, 2020) to further decrease dormancy degree by periodically perturbing the policy network and critic network with a reset factor \(\eta\), representing the magnitude of weight resetting:","\theta_{t}=(1-\eta)\theta_{t-1}+\eta\phi_{i},\ \phi_{i}\sim\text{initializer}.",,"\eta_{t}=\eta_{\max}-(\eta_{\max}-\eta_{\min})\cdot\frac{\alpha_{\tau,t}-\alpha_{\tau,\min}}{\alpha_{\tau,\max}-\alpha_{\tau,\min}},","where $\eta_t$ is the reset factor at training step $t$, $\eta_{\max}$ and $\eta_{\min}$ are the maximum and minimum reset factors, and $\alpha_{\tau,t}$ is the $\tau$-dormancy degree at step $t$."
ICML_2024_oral_10,1,"### Transformers and attention mechanisms

Transformers overcome limitations of feed-forward networks in effectively dealing with sequential inputs. They incorporate an attention mechanism which, for a given sequence of inputs, replaces individual hidden states with a weighted combination of all hidden states (Vaswani et al., 2017). Given three learnable linear projections of each hidden state (\(Q\), \(K\), \(V\)) this is computed as","\text{attention}(Q,K,V)=\text{softmax}(QK)^{T}/\sqrt{d})V.",,"\begin{equation}
    \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V,
\end{equation}","where $d_k$ is the dimensionality of the key vectors, ensuring numerical stability during softmax computation."
ICML_2024_oral_10,2,"### Score-based diffusion models

Score-based diffusion models (Song et al., 2021; Song and Ermon, 2019) describe the evolution of data through stochastic differential equations (SDEs). Common SDEs for score-based diffusion models can be expressed as","d\mathbf{\hat{x}}_{t}=f(\mathbf{\hat{x}}_{t},t)dt+g(t)d\mathbf{w},",,"dx = f(x,t)dt + g(t)dw,","where $f(x,t)$ denotes the drift coefficient, $g(t)$ is the diffusion coefficient, and $dw$ represents standard Wiener process increments."
ICML_2024_oral_10,3,"with \(\mathbf{w}\) being a standard Wiener process, and \(f\) and \(g\) representing the drift and diffusion coefficients, respectively. The solution to this SDE defines a diffusion process that transforms an initial data distribution \(p_{0}(\mathbf{\hat{x}}_{0})=p(\mathbf{\hat{x}})\) into a simpler noise distribution \(p_{T}(\mathbf{\hat{x}}_{T})\approx\mathcal{N}(\mathbf{\hat{x}}_{T};\mathbf{\mu}_{T},\mathbf{ \sigma}_{T})\).

Samples from the generative model are then generated by simulating the reverse diffusion process (Anderson, 1982)","d\mathbf{\hat{x}}_{t}=[f(\mathbf{\hat{x}}_{t},t)-g(t)^{2}s(\mathbf{\hat{x}}_{t},t) ]dt+g(t)d\mathbf{\tilde{w}},",where \(\mathbf{\tilde{w}}\) is a backward-in-time Wiener process,"d\mathbf{\hat{x}}_{t}=\left[f(\mathbf{\hat{x}}_{t},t)-g(t)^{2}\nabla_{\mathbf{\hat{x}}_{t}}\log p_{t}(\mathbf{\hat{x}}_{t})\right]dt+g(t)d\mathbf{w}.",This equation represents the reverse diffusion process used to generate samples from the generative model by incorporating the score function of the marginal probability density.
ICML_2024_oral_10,4,"ss. This relies on the knowledge of the score function \(s(\mathbf{\hat{x}}_{t},t)=\nabla_{\mathbf{\hat{x}}_{t}}\log p_{t}(\mathbf{\hat{x}}_{t})\) at each step. The exact marginal score is typically intractable but can be estimated through time-dependent denoising score-matching (Hyvarinen and Dayan,2005; Song et al., 2021). Given that the conditional score is known, \(p_{t}(\mathbf{\hat{x}}_{t}|\mathbf{\hat{x}}_{0})=\mathcal{N}(\mathbf{\hat{x}}_{t};\mu_{t}( \mathbf{\hat{x}}_{0}),\sigma_{t}(\mathbf{\hat{x}}_{0}))\), the score model \(s_{\phi}(\mathbf{\hat{x}}_{t},t)\) is trained to minimize the loss","\mathcal{L}(\phi)=\mathbb{E}_{t,\mathbf{\hat{x}}_{0},\mathbf{\hat{x}}_{t}}[\lambda (t)\|s_{\phi}(\mathbf{\hat{x}}_{t},t)-\nabla_{\mathbf{\hat{x}}_{t}}\log p_{t}( \mathbf{\hat{x}}_{t}|\mathbf{\hat{x}}_{0})\|_{2}^{2}],",where \(\lambda\) denotes a positive weighting function,"\mathcal{L}(\phi)=\mathbb{E}_{t,\mathbf{\hat{x}}_{0},\mathbf{\hat{x}}_{t}}\left[ \left\| s_{\phi}(\mathbf{\hat{x}}_{t},t)-\nabla_{\mathbf{\hat{x}}_{t}}\log p_{t}(\mathbf{\hat{x}}_{t}|\mathbf{\hat{x}}_{0}) \right\|^{2} \right].",Expected squared error loss for training the score model by comparing predicted scores against the true conditional scores.
ICML_2024_oral_10,5,"on. This objective, hence only requires samples from the original distribution \(\mathbf{\hat{x}}_{0}\sim p(\mathbf{\hat{x}})\).

## 3 Methods

The Simformer is a probabilistic diffusion model that uses a transformer to estimate the score (Weiblach et al. (2023); Hatamizadeh et al. (2023); Peebles and Xie (2022), Fig. 2). Unlike most previous approaches for simulation-based inference, which employ conditional density estimators to model either the likelihood or the posterior, the Simformer is trained on the _joint_ distribution of parameters and data \(p(\mathbf{\theta},\mathbf{x})=:p(\mathbf{\hat{x}})\). The Simformer encodes parameters and data (Sec. 3.1) such that arbitrary conditional distributions of the joint density (including posterior and likelihood) can still be sampled efficiently. The Simformer can encode known dependencies in the attention mask of the transformer (Sec. 3.2) and thereby ensures efficient training of arbitrary conditionals (Sec. 3.3). Finally, the Simformer uses guided diffusion to produce samples given arbitrary constraints (Sec. 3.4).

### A Tokenizer for SBI

Transformers process sequences of uniformly sized vectors called tokens. Designing effective tokens is challenging and specific to the data at hand (Gu et al., 2022). The tokenizer represents each variable as an identifier that uniquely identifies the variable, a representation of the value of the variable, and a condition state (Fig. 2). The condition state is a binary variable and signifies whether the variable is conditioned on or not. It is resampled for every \((\mathbf{\theta},\mathbf{x})\in\mathbb{R}^{d}\) pair at every iteration of training. We denote the condition state of all variables as \(M_{C}\in\{0,1\}^{d}\). Setting \(M_{C}=(0,\ldots,0)\) corresponds to an unconditional diffusion model (Song et al., 2021), whereas adopting \(M_{C}^{(i)}=1\) for data and \(M_{C}^{(i)}=0\) for parameters corresponds to training a conditional diffusion model of the posterior distribution (Simons et al., 2023; Geffner et al., 2023). In our experiments, we uniformly at random sample either the masks for the joint, the posterior, the likelihood, or two randomly sampled masks (details in Appendix Sec. A2). To focus on specific conditional distributions, one can simply change the distribution of condition masks.

The Simformer uses learnable vector embeddings for identifiers and condition states, as proposed in Weilbach et al. (2023). In cases where parameters or data are functions of space or time, the node identifier will comprise a shared embedding vector and a random Fourier embedding of the elements in the index set. Finally, specialized embedding networks, commonly used in SBI algorithms and trained end-to-end (Lueckmann et al., 2017; Chan et al., 2018; Radev et al., 2020), can be efficiently integrated here by condensing complex data into a single token (e.g. we demonstrate this on a gravitational waves example in Appendix Sec. A3.2). This reduces computational complexity but loses direct control over dependencies and condition states for individual data elements.

### Modelling dependency structures

For some simulators, domain scientists may have knowledge of (or assumptions about) the conditional dependency structures between parameters and data. For example, it may be known that all parameters are independent, or each parameter might only influence a single data value. The Simformer can exploit these dependencies by representing them in the attention mask \(M_{E}\) of the transformer (Weibach et al., 2023). These constraints can be implemented as undirected (via a symmetric attention mask) or as directed dependencies (via a non-symmetric attention mask), that allow to enforce causal relations between parameters and observations. The latter, however, requires updating the mask if dependencies change i.e., due to conditioning (Webb et al., 2018) (Fig. 2, Appendix Sec. A1.1).

Figure 2: Simformer architecture. All variables (parameters and data) are reduced to a token representation which includes the variables’ identity, the variables’ value (val) as well as the conditional state (latent (L) or conditioned (C)). This sequence of tokens is processed by a transformer model; the interaction of variables can be explicitly controlled through an attention mask. The transformer architecture returns a score that is used to generate samples from the score-based diffusion model and can be modified (e.g. to guide the diffusion process).

A key advantage over masking weights directly (Germain et al., 2015) is that the attention mask can be easily dynamically adapted at train or inference time, allowing to enforce dependency structures that are dependent on input values and condition state (details in Appendix Sec. A.1). We note that the attention mask \(M_{E}\) alone generally cannot ensure specific conditional independencies and marginalization properties in multi-layer transformer models. We describe the properties that can be reliably guaranteed and also explore how \(M_{E}\) can be effectively employed to learn certain desired properties in Appendix Sec. A.1.

### Simformer training and sampling

Having defined the tokenizer which processes every \((\mathbf{\theta},\mathbf{x})\) pair and the attention mask to specify dependencies within the simulator, the Simformer can be trained using denoising score-matching (Hyvarinen and Dayan, 2005; Song et al., 2021b): We sample the noise level \(t\) for the diffusion model uniformly at random and generate a (partially) noisy sample \(\hat{\mathbf{x}}_{t}^{Mc}=(1-M_{C})\cdot\hat{\mathbf{x}}_{t}+M_{C}\cdot\hat{ \mathbf{x}}_{0}\) i.e. variables that we want to condition on remain clean. The loss can then be defined as","\ell(\phi,M_{C},t,\hat{\mathbf{x}}_{0},\hat{\mathbf{x}}_{t})=\\ (1-M_{C})\cdot(s_{\phi}^{M_{E}}(\hat{\mathbf{x}}_{t}^{Mc}, t)-\nabla_{\hat{\mathbf{x}}_{t}}\log p_{t}(\hat{\mathbf{x}}_{t}|\hat{ \mathbf{x}}_{0})),",where \(s_{\phi}^{M_{E}}\) denotes the score model equipped with a specific attention mask \(M_{E}\),"\mathcal{L}(\phi)=\mathbb{E}_{t,\mathbf{\hat{x}}_{0},\mathbf{\hat{x}}_{t},M_{C}}[\lambda (t)\|s_{\phi}(\mathbf{\hat{x}}_{t}^{M_{C}},t)-\nabla_{\mathbf{\hat{x}}_{t}^{M_{C}}} \log p_{t}(\mathbf{\hat{x}}_{t}^{M_{C}}|\mathbf{\hat{x}}_{0})\|_{2}^{2}],","where the noisy sample is partially conditioned using the condition mask $M_{C}$, and the score model is trained to predict the gradient of the log conditional density."
ICML_2024_oral_10,6,"\). In expectation across noise levels \(t\) and the data, this results in","\mathcal{L}(\phi)=\mathbb{E}_{M_{C},t,\hat{\mathbf{x}}_{0},\hat{\mathbf{x}}_{ t}}[\|\ell(\phi,M_{C},t,\hat{\mathbf{x}}_{0},\hat{\mathbf{x}}_{t}) \|_{2}^{2}].",,"\mathcal{L}(\phi) = \mathbb{E}_{t, M_C, \hat{\mathbf{x}}_0, \hat{\mathbf{x}}_t} \left[ \lambda(t) \| (1 - M_C) \cdot \left( s_\phi^{M_E}(\hat{\mathbf{x}}_t^{Mc}, t) - \nabla_{\hat{\mathbf{x}}_t} \log p_t(\hat{\mathbf{x}}_t | \hat{\mathbf{x}}_0) \right) \|_2^2 \right],","This loss function extends the denoising score-matching objective by incorporating the condition mask $M_C$ and attention mask $M_E$, enabling training on arbitrary conditionals of the joint distribution."
ICML_2024_oral_10,7,"We note that to simplify notation, \(M_{E}\) remains fixed here, but as stated in Sec. 3.2, it might depend on the condition state or input.

After having trained the Simformer, it can straightforwardly sample arbitrary conditionals (Fig. 3). We draw samples from the noise distribution and run the reverse diffusion process on all unobserved variables, while keeping observed variables constant at their conditioning value (Weilbach et al., 2023). Having access to all conditional distributions also allows us to combine scores and thereby perform inference for simulators with i.i.d. datapoints (Geffner et al., 2023). Similarly, we can use other score transformations to adapt to other prior or likelihood configurations post-hoc (see Appendix Sec. A.1.4).

### Conditioning on intervals with diffusion guidance

Guided diffusion makes it possible to sample from the generative model with an additional context \(\mathbf{y}\), and has been used in tasks such as image inpainting, super-resolution, and image deblurring (Song et al., 2021b; Chung et al., 2022). It modifies the backward diffusion process to align it with a given context \(\mathbf{y}\). Guided diffusion modifies the estimated score as","s(\hat{\mathbf{x}}_{t},t|\mathbf{y})\approx s_{\phi}(\hat{\mathbf{x}}_{t},t)+\nabla_{ \hat{\mathbf{x}}_{t}}\log p_{t}(\mathbf{y}|\hat{\mathbf{x}}_{t}).",,"s_{\text{guided}}(\mathbf{\hat{x}}_{t},t)=s_{\phi}(\mathbf{\hat{x}}_{t},t)+\eta \nabla_{\mathbf{\hat{x}}_{t}}\log p(\mathbf{y}|\mathbf{\hat{x}}_{t},t),","where $\eta$ controls the strength of the guidance and $p(\mathbf{y}|\mathbf{\hat{x}}_{t},t)$ represents the likelihood of the context given the noisy sample."
ICML_2024_oral_10,8,"Various strategies for guiding the diffusion process have been developed, mainly differing in how they estimate \(\nabla_{\hat{\mathbf{x}}_{t}}\log p_{t}(\mathbf{y}|\hat{\mathbf{x}}_{t})\)(Dhariwal and Nichol, 2021; Chung et al., 2023; Jalal et al., 2021; Song et al., 2022; Chung et al., 2022; Bansal et al., 2023; Lugmayr et al., 2022).

We here use diffusion guidance to be able to allow the Simformer to not only condition on fixed observations, but also on observation _intervals_ (or, similarly, intervals of the prior). Bansal et al. (2023) demonstrated that diffusion models can be guided by arbitrary functions. In that line, we use the following general formulation to guide the diffusion process:","s_{\phi}(\hat{\mathbf{x}}_{t},t|c)\approx s_{\phi}(\hat{\mathbf{x}}_{t},t)+\nabla_{ \hat{\mathbf{x}}_{t}}\log\sigma(-s(t)c(\hat{\mathbf{x}}_{t}))",,"s(\hat{\mathbf{x}}_{t},t|\mathbf{y})\approx s_{\phi}(\hat{\mathbf{x}}_{t},t)+\nabla_{ \hat{\mathbf{x}}_{t}}\log p_{t}(\mathbf{y}|\hat{\mathbf{x}}_{t}) + \gamma \cdot \nabla_{\hat{\mathbf{x}}_{t}} \log p(\mathbf{y}|\hat{\mathbf{x}}_{t}),",where $\gamma$ is a guidance scale parameter that controls the strength of the additional constraint derived from the observation interval or prior information.
ICML_2024_oral_101,1,"## 3 Probabilistic generating circuits

Given categorical variables \(X_{1},\ldots,X_{n}\) with image \(\{0,\ldots,d-1\}\) and joint distribution \(p(a_{1},\ldots,a_{n})=\Pr[X_{1}=a_{1},\ldots,X_{n}=a_{n}]\), the _probability generating function_ is a formal polynomial in formal variables \(z_{1},\ldots,z_{n}\) defined by","G(z)=\sum_{j_{1}=0}^{d-1}\cdots\sum_{j_{n}=0}^{d-1}p(j_{1},\ldots,j_{n})z_{1}^ {j_{1}}\cdots z_{n}^{j_{n}}.",,"G(z_1, \ldots, z_n) = \sum_{a_1, \ldots, a_n} p(a_1, \ldots, a_n) z_1^{a_1} \cdots z_n^{a_n}","Definition of the probability generating function for categorical variables $X_1, \ldots, X_n$ with joint distribution $p$."
ICML_2024_oral_101,2,"A _probabilistic generating circuit (PGC)_ for a probability distribution \(p\) is an arithmetic circuit that computes \(G\). While PCs compute probability mass functions, PGCs store probability distributions as formal objects. We can feed some particular elementary event as an input into a PC and the corresponding output is the probability of this event. In PGCs, the probabilities are stored as coefficients. Figure 2 shows an example. While PCs can also model continuous distributions, PGCs can only model distributions over categorical variables due to the way they store distributions. Zhang et al. (2021) introduce PGCs only for binary random variables and showed how to perform tractable marginal ization in this case. While binary variables are the most important categorical random variable, categorical variables with a larger number of outputs are important, too. Applications of ternary variables for example are excess losses or labelling with abstention (Wu & Seldin, 2022; Mhammedi et al., 2019; Thulasidasan et al., 2019).

The work by Zhang et al. (2021) led to further work on PGCs: Harviainen et al. (2023, Theorem 3) designed a faster marginalization procedure for PGCs, while Blaser (2023) constructed a strongly Rayleigh distribution that cannot be represented by small PGCs.

## 4 Our results

Zhang et al. (2021) show that given a PGC over binary random variables, marginalization is tractable. This raises the natural question whether this is also possible for categorical variables attaining more than two values. We give a negative answer to this question (under standard complexity theoretic assumptions) in Section 7. If marginalization for PGCs over quarternary random variables can be done in polynomial time, then \(\mathsf{NP}=\mathsf{P}\). For ternary variables, we get a similar result, however, we need to be able to marginalize over subsets of the image. For the result on quaternary variables, it is sufficient to marginalize over the whole image set.

Zhang et al. (2021) show that _determinantal point processes_ (DPPs) can be represented by PGCs. On the other hand, it is not clear whether decomposable and smooth PCs (which support tractable marginalization) can represent DPPs. This question is related to proving set-multilinear lower bounds for the determinant in algebraic complexity, see (Saptharishi, R. et al., 2021) for an overview. See also (Zhang et al., 2020) for further limitations of representing DPPs by PCs. As our second main result, we here prove that the additional power of the PGC by Zhang et al. (2021) does not come from the fact that they use a different representation but from the fact that one allows for negative constants. In particular we prove that every PGC over binary random variables can be transformed into a nonmonotone PC, which computes the corresponding probability mass function and allows for tractable marginalization (Section 8).

Third, we prove in Section 9 that nonmonotone PCs computing set-multilinear polynomials are more general than PGCs in the sense that they support tractable marginalization over categorical variables of an arbitrary image size. Since computing a set-multilinear polynomial and computing a probability distribution are semantic properties (i.e. they cannot be directly inferred from the structure of the PC), we also ask the question whether checking these properties is hard? It turns out that the first one can be checked in randomized polynomial time, while the second property is hard to decide.

In Section 10, we present basic compositional operations, which preserve the property that a nonmonotone PC computes a probability distribution. The first two are the well-known weighted sum and multiplication (when the domains are disjoint). The third one is an analogue of the hierarchical composition introduced by Zhang et al. (2021) for PGCs.

Finally, in Section 11, we discuss the relation between nonmonotone PCs and DPPs. PGCs and nonmonotone PCs were designed to subsume monotone PCs and DPPs. It is well known that nonmonotone PCs are strictly stronger than monotone ones. DPPs can only compute distributions with negative correlations. In this sense, nonmonotone PCs are strictly stronger than DPPs. However, once we allow to combine DPPs with simple compositional operations like affine projections, we show that the question whether nonmonotone PCs are more powerful than DPPs will be very hard to answer (Theorem 11.2). It will imply a separation between algebraic formulas and circuits, a question that has been open for decades, see (Burgisser et al., 1997, Problem 21.2). While it is well-known that we can write a polynomial computed by a formula as a determinant, see e.g. (Burgisser et al., 1997, Theorem 21.27), the crucial point here is that the variables of a DPP only appear on the diagonal. We here prove that every polynomial computed by a formula can be written as an affine projection of a DPP of size linear in the formula size. That means separating DPPs and nonmonotone PCs implies a separation of algebraic formulas and circuits. This result can even be strengthened to so-called algebraic branching programs instead of formulas (Theorem D.1).

## 5 Graphs and matchings

In the next two sections, we briefly review definitions and results from graph theory and computational complexity, which will be needed for our hardness results.

A graph \(G\) is called _bipartite_, if we can partition its nodes into two set \(U\) and \(V\) such that all edges have one node in \(U\) and the other node in \(V\). When we write \(G=(U\cup V,E)\) we mean that \(G\) is a bipartite graph with bipartition \((U,V)\). We will typically call these nodes \(u_{1},\ldots,u_{m}\) and \(v_{1},\ldots,v_{n}\). The _degree_ of a node is the number of edges that it is incident to. A graph is called _regular_ if every node has the same degree. It is _\(d\)-regular_ if it is regular and the degree of every node is \(d\). The _neighbours_ of a node \(u\) are the nodes that share an edge with \(u\).

_Example 5.1_.: Figure 3 shows a \(3\)-regular bipartite graph with four nodes on each side. The neighbours of \(u_{1}\) are \(v_{1},v_{2},v_{3}\) but not \(v_{4}\).

\(M\subseteq E\) is a _matching_ if each node of \(U\) and \(V\) appears in at most one edge of \(M\). \(M\) is called _perfect_, if every node is in exactly one edge. If a bipartite graph has a perfect matching,then necessarily \(|U|=|V|\). The size \(|M|\) of a matching is the number of edges in it. If \(M\) is perfect, then \(|M|=|U|=|V|\). The set of all matchings of \(G\) is denoted by \(\mathcal{M}(G)\) and the set of all perfect matchings by \(\mathcal{PM}(G)\). The number of all matchings and perfect matchings is denoted by \(\#\mathcal{M}(G)\) and \(\#\mathcal{PM}(G)\), respectively.

_Example 5.2_. Figure 4 shows a perfect matching in the graph from Figure 3.

## 6 Complexity theory basics

We give some background information on the complexity classes and results used in this paper. We refer to (Papadimitriou, 1994; Arora & Barak, 2009) for further explanations and proofs of the well-known definitions and theorems in this section. The important fact for this paper is that counting perfect matchings in bipartite graphs is hard.

Deciding whether a formula \(\phi\) in conjunctive normal form (CNF) has a satisfying assignment is the defining problem of the famous class \(\mathsf{NP}\). If we instead want to count the number of satisfying assignments, we get a problem which is complete for the class \(\#\mathsf{P}\) defined by Valiant (1979). Obviously, when you can count the number of satisfying assignments, then you can decide whether there is at least one, therefore, \(\#\mathsf{P}\) is a ""harder"" class than \(\mathsf{NP}\). It turns out that some problems become \(\#\mathsf{P}\)-hard when considered as a counting problem while their decision versions are easy. Perfect matchings in bipartite graphs is such an example: There are efficient algorithms for the decision problem, but the counting version is hard.

**Theorem 6.1** (Valiant (1979)).: _Counting perfect matchings in bipartite graphs is \(\#\mathsf{P}\)-complete under Turing reductions._

Above, a problem \(A\) is Turing reducible to a problem \(B\) if there is a polynomial time deterministic Turing machine that solves \(A\) having oracle access to \(B\). This means, that an efficient algorithm for \(B\) would yield an efficient algorithm for \(A\) and in this sense, \(A\) is easier than \(B\).

Note that \(\#\mathsf{P}\) is a class of functions, not of languages. The class \(\mathsf{FP}\) is the class of all functions computable in polynomial time. It relates to \(\#\mathsf{P}\) like \(\mathsf{P}\) relates to \(\mathsf{NP}\). If \(\#\mathsf{P}=\mathsf{FP}\), then \(\mathsf{NP}=\mathsf{P}\).

## 7 PGCs do not support efficient marginalization beyond binary variables

**Theorem 7.1**.: _Efficient marginalization over PGCs involving quaternary random variables implies that \(\#\mathsf{P}=\mathsf{FP}\), and in particular, \(\mathsf{NP}=\mathsf{P}\)._

Proof.: Counting perfect matchings in \(3\)-regular bipartite graphs is a \(\#\mathsf{P}\)-hard problem, as proved by Dagum & Luby (1992, Theorem 6.2). Given a \(3\)-regular bipartite graph \(G=(U\cup V,E)\) with \(|U|=|V|=n\), we will construct a PGC \(C\) of size polynomial in \(n\) (in fact linear in \(n\)) over quaternary random variables such that a certain marginal probability is the number of perfect matchings in \(G\).

For every vertex \(v_{i}\in V\), we define a formal variable \(V_{i}\). Similarly, for every edge \(e_{i,j}\) representing an edge between \(u_{i}\in U\) and \(v_{j}\in V\), we define a binary random variable \(E_{i,j}\). For every vertex \(u_{i}\in U\), let \(N(i)\) denote the set of indices of its neighbours. Since \(G\) is \(3\)-regular, \(|N(i)|=3\). In particular, let \(N(i,1),N(i,2),N(i,3)\) denote the indices of the three neighbours of \(u_{i}\). We now define the polynomial \(f\) as","f(V_{1},...,V_{n},E_{1,N(1,1)},...,E_{n,N(n,3)})=\prod_{i=1}^{n}\sum_{j\in N(i )}E_{i,j}V_{j}",,,
ICML_2024_oral_101,3,"The right hand side of the equation is a depth-\(2\) circuit of size \(s=O(n)\), as each sum has only three terms due to \(3\)-regularity. Thus \(f\) has a linear sized circuit.

Furthermore, \(f\) is a polynomial of \(\deg(f)=2n\). Each \(E_{i,j}\) appears with degree \(1\) in \(f\), since \(E_{i,j}\) only appears in the \(i\)th factor of the outer product in (2). Each \(V_{j}\) appears with degree \(3\) in \(f\), since each \(V_{j}\) appears in three factors in (2) due to the \(3\)-regularity of \(G\). The total number of variables appearing in \(f\) is \(n+3n=4n\).

Since each coefficient of the product in the inner sums is \(1\), the coefficients of \(f\) are all nonnegative. Thus \(f\) can be viewed as the probability generating function of the (unnormalized) joint probability distribution of \(4n\) quaternary random variables. The normalization constant is the sum of all coefficients of \(f\), which is just \(f(1,\ldots,1)\). Hence, the normalization constant can be efficiently computed using the circuit for \(f\). In this particular case, it is even easier. By looking at the righthand side of (2), we see that the normalization constant is \(3^{n}\). Thus, \(\hat{f}:=f/3^{n}\) is a (normalized) probability distribution over quaternary variables and is computed by a linear sized PGC.

We now view \(\hat{f}\) as a polynomial in \(V_{1},\ldots,V_{n}\) with co

Figure 3: \(3\)-regular bipartite graph with bipartition \(U=\{u_{1},u_{2},u_{3},u_{4}\}\) and \(V=\{v_{1},v_{2},v_{3},v_{4}\}\). Figure 4: The thick edges form a perfect matching. Any subset of it forms a matching.

efficients being polynomials in \(E_{i,N(i,j)}\), \(i=1,\ldots,n\), \(j=1,2,3\). Let \(h(E_{1,N(1,1)},...,E_{n,N(n,3)})\) be the coefficient of \(V_{1}V_{2}\ldots V_{n}\). We claim that the number of monomials in \(h\) gives us the number of perfect matchings in \(G\). Any perfect matching in \(G\) would be of the form \((u_{1},v_{k_{1}})\), \((u_{2},v_{k_{2}})\),..., \((u_{n},v_{k_{n}})\), such that \(k_{i}\neq k_{j}\) for \(i\neq j\), and \(\{k_{1},...,k_{n}\}=[n]\). But then the monomial \(E_{1,k_{1}}\cdot...E_{n,k_{n}}\) would be present in \(h\). For the converse, let \(E_{1,a_{1}}\cdot...E_{n,a_{n}}\) be some monomial in \(h\). Clearly, \(a_{i}\neq a_{j}\) for \(i\neq j\), as each \(V_{i}\) only occurs in one factor in (2). Further, the coefficient of \(E_{1,a_{1}}\cdot...\cdot E_{n,a_{n}}\) in \(h\) is nonzero only if all the edges \((u_{1},v_{a_{1}})\), \((u_{2},v_{a_{2}}),\ldots,(u_{n},v_{a_{n}})\) were present in \(G\). Thus, any such monomial would denote a perfect matching in \(G\). Hence, the number of monomials in \(h\) and the number of perfect matchings in \(G\) are equal.

Suppose we can efficiently marginalize in PGCs over quaternary random variables. Then we can compute \(\Pr[V_{1}=1,V_{2}=1,\ldots,V_{n}=1]\) efficiently. (By abuse of notation \(V_{i}\) also denotes the random variable corresponding to the formal variable \(V_{i}\).) However, this probability is nothing but \(h(1,\ldots,1)\). Each monomial in \(h\) has the coefficient \(1/3^{n}\). Thus","\Pr[V_{1}=1,\ldots,V_{n}=1]=h(1,\ldots,1)=\frac{\#\mathcal{PM}(G)}{3^{n}}.",,"\Pr[V_{1}=1,V_{2}=1,\ldots,V_{n}=1] = \frac{\#\mathcal{PM}(G)}{3^{n}}.","The probability of the event $V_{1}=1,V_{2}=1,\ldots,V_{n}=1$ in the normalized distribution $\hat{f}$ is equal to the number of perfect matchings in the graph $G$ divided by $3^n$."
ICML_2024_oral_101,4,"Using marginalization we can efficiently get the number of perfect matchings in a \(3\)-regular bipartite graph \(G\). However, as computing the number of perfect matching in \(3\)-regular bipartite graphs is a \(\#\mathsf{P}\)-complete problem, we get \(\#\mathsf{P}=\mathsf{FP}\). This implies \(\mathsf{NP}=\mathsf{P}\). 

Zhang et al. (2021) show that we can marginalize efficiently over PGCs on binary random variables. Combined with the above result, it naturally raises the question whether there exists efficient marginalization for PGC involving ternary random variables. While we are not able to answer the question fully here, we do show that ""selective"" marginalization over such distributions should not be possible under standard complexity theoretic assumptions. In order to prove this result, we will first introduce some definitions.

**Definition 7.2** (Selective Marginalization).: Let \(X_{1},\ldots,X_{n}\) be \(k\)-nary random variables. Let \(V_{i}\subseteq\{0,1,\ldots,k-1\}\), \(1\leq i\leq n\). \(\Pr[X_{1}\in V_{1},\ldots,X_{n}\in V_{n}]\) is called a selective marginal probability.

Let \(f(z_{1},\ldots,z_{n})\) be the corresponding probability generating polynomial:","f(z_{1},...,z_{n})=\sum_{s=(s_{1},\ldots,s_{n})\in\{0,1,\ldots,k-1\}^{n}}c_{s} \cdot\prod_{i=1}^{n}z_{i}^{s_{i}}.",,"f(z_{1},\ldots,z_{n}) = \sum_{j_{1}=0}^{k-1} \cdots \sum_{j_{n}=0}^{k-1} \Pr[X_{1}=j_{1},\ldots,X_{n}=j_{n}] z_{1}^{j_{1}} \cdots z_{n}^{j_{n}}.",Probability generating function for k-nary random variables expressed as a multivariate polynomial.
ICML_2024_oral_101,5,"Then we want to compute \(\sum_{s_{1}\in V_{1},\ldots,s_{n}\in V_{n}}c_{s}\).

**Theorem 7.3**.: _Efficient selective marginalization over PGCs involving ternary random variables implies \(\#\mathsf{P}=\mathsf{FP}\), and in particular, \(\mathsf{NP}=\mathsf{P}\)._

The proof is deferred to Appendix A.

## 8 PGCs over binary variables can be simulated by nonmonotone PCs

**Theorem 8.1**.: _PGCs over binary variables can be simulated by nonmonotone PCs with only polynomial overhead in size._

Proof.: Let \(f(z_{1},...,z_{n})\) be a probability generating function of \(n\) binary random variables computed by a PGC of size \(s\). We define the polynomial \(g\) as","g(x_{1},\overline{x_{1}},...,x_{n},\overline{x_{n}})=f(\frac{x_{1}}{\overline{ x_{1}}},\frac{x_{2}}{\overline{x_{2}}},...,\frac{x_{n}}{\overline{x_{n}}}) \cdot\prod_{i=1}^{n}\overline{x_{i}}.",,"\Pr\left[X_{1} \in V_{1}, \ldots, X_{n} \in V_{n}\right] = \sum_{\substack{s_{1} \in V_{1}, \\ \ldots, \\ s_{n} \in V_{n}}} c_{s}",This equation expresses the selective marginal probability as the sum of coefficients $c_s$ corresponding to tuples $s$ where each $s_i$ lies in the subset $V_i$.
ICML_2024_oral_101,6,"\(g\) is indeed a polynomial, since \(f\) is multilinear. To get a circuit for \(g\), first we need to substitute all instances of \(z_{i}\) with \(\frac{x_{i}}{\overline{x_{i}}}\) in the circuit for \(f\), using the help of division gates, which we will remove later. Clearly, this gives us a circuit for \(f(\frac{x_{1}}{\overline{x_{1}}},\frac{x_{2}}{\overline{x_{2}}},...,\frac{x_ {n}}{\overline{x_{n}}})\) with size \(O(s+n)\). Furthermore, we can compute \(\prod_{i=1}^{n}\overline{x_{i}}\) in size \(O(n)\), and thus we get a circuit for \(g\) of size \(O(s+n)\).

Let \(m\) be any monomial of \(f(z_{1},z_{2},...,z_{n})\). Then, \(m=c_{S}\cdot\prod_{i\in S}z_{i}\) for some \(S\subseteq[n]\). Looking at the corresponding monomial \(m^{\prime}\) in \(g(x_{1},\overline{x_{1}},...,x_{n},\overline{x_{n}})\), we see that",m^{\prime}=c_{S}\cdot(\prod_{i\in S}\frac{x_{i}}{\overline{x_{i}}})\cdot(\prod _{j=1}^{n}\overline{x_{j}})=c_{S}(\prod_{i\in S}x_{i})\cdot(\prod_{i\notin S} \overline{x_{i}}),,m^{\prime}=c_{S}\cdot\prod_{i\in S}\frac{x_{i}}{\overline{x_{i}}}\cdot\prod_{i=1}^{n}\overline{x_{i}}=c_{S}\cdot\prod_{i\in S}x_{i}\cdot\prod_{i\notin S}\overline{x_{i}}.,"The transformed monomial $m^{\prime}$ represents the corresponding term in the nonmonotone PC, encoding the probability of the binary event associated with subset $S$."
ICML_2024_oral_101,7,"Thus, if \(z_{i}\) occurs in \(m\), then \(x_{i}\) occurs in \(m^{\prime}\). Similarly, if \(z_{i}\) does not occur in \(m\), then \(\overline{x_{i}}\) occurs in \(m^{\prime}\). Thus, in \(m^{\prime}\) exactly one of \(x_{i}\) or \(\overline{x_{i}}\) occurs for all \(i\). Hence, \(m^{\prime}\) is multilinear with \(\deg(m^{\prime})=n\). Since, \(m^{\prime}\) was an arbitrary monomial of \(g\), this implies \(g\) is both multilinear and homogenous with \(\deg(g)=n\). Thus, we obtained a nonmonotone PC, cf. (Zhang et al., 2021, Proposition 1).

Right now the circuit of \(g\) has division gates. This is a problem, since we might want to set \(\overline{x}_{i}=0\) when computing a probability. However using the famous result of Strassen (1973), we can eliminate all division gates to get a circuit for \(g\) of size \(\operatorname{poly}(s+n,\deg(g))=\operatorname{poly}(s,n)\). Furthermore, \(g\) computes a probability distribution since \(f\) represented a probability distribution. Thus, starting with a PGC of size \(s\), we get a nonmonotone PC computing a set-multilinear polynomial \(g\) of size \(\operatorname{poly}(s,n)\). 

_Remark 8.2_.: In the proof by Strassen, every arithmetic operation in the original circuit is replaced by a corresponding operation on polynomials of degree \(n\). Therefore, the exact upper bound on the size of the new circuit is \(O(s\cdot n\log n\log\log n)\) if we use the fast polynomial multiplication by Cantor & Kaltofen (1991) or \(O(s\cdot n)\) if we use interpolation.

_Example 8.3_.: The following small example explains Strassen's construction: Suppose we have a PGC that computes the following polynomial: \(f(z_{1},z_{2})=0.6z_{1}z_{2}+0.4z_{1}\). We will demonstrate the conversion of the polynomial into a polynomial computed by the corresponding nonmonotone PC.

* We first replace each \(z_{i}\) with \(\frac{x_{i}}{\overline{x_{i}}}\), and then get \(g=f(\frac{x_{1}}{\overline{x_{1}}},\frac{x_{2}}{\overline{x_{2}}})\cdot \overline{x_{1}}\overline{x_{2}}=0.6x_{1}x_{2}+0.4x_{1}\overline{x_{2}}\). However, the new circuit contains the two divisions \(\frac{x_{1}}{\overline{x_{1}}}\) and \(\frac{x_{2}}{\overline{x_{2}}}\), which we have to remove.
* The idea by Strassen (1973) is to expand \(\frac{1}{\overline{x_{1}}}\) as a formal power series. Since we are computing a polynomial in the end, it is enough to work with finite approximations. We can only invert a power series if it has a nonzero constant term. Therefore, we first have to perform a Taylor shift, which will be reverted in the end. In our case, the shift \(\overline{x_{i}}\) to \(1-\overline{x_{i}}\) works. We get \(f(\frac{x_{1}}{1-\overline{x_{1}}},\frac{x_{2}}{1-\overline{x_{2}}})\cdot(1- \overline{x_{1}})(1-\overline{x_{2}})=0.6x_{1}x_{2}+0.4x_{1}(1-\overline{x_{2 }})\).
* The inverse of \(1-x\) as a formal power series is the geometric series \(\frac{1}{1-x}=\sum_{i=0}^{\infty}x^{i}\). Since in this example, we compute a multilinear function of degree two, it is enough to work with order-one approximations and replace \(\frac{1}{1-\overline{x_{i}}}\) by \(1+\overline{x_{i}}\). We get","\begin{split}& f(x_{1}(1+\overline{x_{1}}),x_{2}(1+\overline{x _{2}}))\cdot(1-\overline{x_{1}})(1-\overline{x_{2}})\\ &=(0.6x_{1}x_{2}(1+\overline{x_{1}})(1+\overline{x_{2}})\\ &\quad+0.4x_{1}(1+\overline{x_{1}}))(1-\overline{x_{1}})(1- \overline{x_{2}})\\ &=0.6x_{1}x_{2}+0.4x_{1}(1-\overline{x_{2}})\ +\ \text{higher degree terms}.\end{split}",,,
ICML_2024_oral_101,8,"(3)
* To compute only the terms up to degree two and remove the unwanted higher degree terms, we compute the homogeneous parts separately. This is the part where we incur the blowup in the circuit size.
* Now, finally to get back to \(g\), we need to invert the substitutions that we made earlier. Hence, replacing \(\overline{x_{i}}\) with \(1-\overline{x_{i}}\) for \(i\in\{1,2\}\) in (3) after removing the higher degree terms gives us the polynomial \(0.6x_{1}x_{2}+0.4x_{1}\overline{x_{2}}\) which is in fact \(g\).

## 9 Nonmonotone PCs computing set-multilinear polynomials support tractable marginalization

We consider categorical random variables \(X_{1},\ldots,X_{n}\), w.l.o.g. taking values in \(I=\{0,1,\ldots,d-1\}\). With each random variable \(X_{i}\), we associate \(d\) indeterminates \(z_{i,0},\ldots,z_{i,d-1}\). A probability distribution for \(X_{i}\) given by \(\Pr[X_{i}=\delta]=\alpha_{\delta},0\leq\delta<d\), can be modelled by the linear polynomial \(\ell(z_{i})=\sum_{\delta=0}^{d-1}\alpha_{\delta}z_{i,\delta}\). By setting \(z_{i,\delta}\) to \(1\) and all other \(z_{i,\delta^{\prime}}\) for \(\delta^{\prime}\neq\delta\) to \(0\), that is, evaluating \(p_{i}\) at the unit vector \(e_{\delta}\), we get \(\ell(e_{\delta})=\alpha_{\delta}\). More generally, if we want to compute \(\Pr[X_{i}\in S]\) for some set \(S\subseteq\{0,\ldots,d-1\}\), we can compute this by evaluating \(\ell\) at \(v\), where \(v_{i}=1\) if \(i\in S\) and \(v_{i}=0\) otherwise.

**Definition 9.1**.: Let \(X\) be a set of variables and \(Y_{1},\ldots,Y_{t}\) be a partition of \(X\). A polynomial \(p\) in variables \(X\) is called _set-multilinear_ with respect to the above partition, if every monomial of \(p\) contains exactly one variable from each set \(Y_{\tau}\), \(1\leq\tau\leq t\), with degree \(1\). In particular, \(p\) is homogeneous of degree \(t\).

_Example 9.2_.: A decomposable and smooth PC over binary variables computes a set-multilinear polynomial with the parts given by \(\{x_{i},\overline{x}_{i}\}\), \(1\leq i\leq n\).

Given a decomposable and smooth PC over categorical random variables \(X_{1},\ldots,X_{n}\), we can model every input distribution by a linear form as described above. The corresponding polynomial will be set-multilinear with the parts of the partition being \(\{z_{i,0},\ldots,z_{i,d-1}\}\), \(1\leq i\leq n\). However, when the PC is nonmonotone, that is, we allow for negative weights, then the PC can compute a set-multilinear polynomial, even if it is not decomposable. It turns out that this is sufficient for performing marginalization.

**Theorem 9.3**.: _Let \(C\) be a nonmonotone PC of size \(s\) computing a probability distribution over categorical random variables \(X_{1},\ldots,X_{n}\) such that the polynomial \(P\) computed by \(C\) is set-multilinear with respect to the partition \(\{z_{i,0},\ldots,z_{i,d-1}\}\), \(1\leq i\leq n\). Let \(A_{1},\ldots,A_{n}\subseteq\{0,\ldots,d-1\}\). Then we can compute \(\Pr[X_{1}\in A_{1},\ldots,X_{n}\in A_{n}]\) in time \(O(s)\)._

Proof.: Note that","P=\sum_{j_{1}=0}^{d-1}\cdots\sum_{j_{n}=0}^{d-1}\Pr[X_{1}=j_{1},\ldots,X_{n}=j _{n}]z_{1,j_{1}}\cdots z_{n,j_{n}}.",,"\begin{split} 
& f(x_{1}(1+\overline{x_{1}}),x_{2}(1+\overline{x_{2}}))\cdot(1-\overline{x_{1}})(1-\overline{x_{2}})\\ 
&=(0.6x_{1}x_{2}(1+\overline{x_{1}})(1+\overline{x_{2}})\\ 
&\quad+0.4x_{1}(1+\overline{x_{1}}))(1-\overline{x_{1}})(1- \overline{x_{2}})\\ 
&=0.6x_{1}x_{2}+0.4x_{1}(1-\overline{x_{2}})\ +\ \text{higher degree terms}.
\end{split}","This equation demonstrates the expansion of the polynomial after substituting first-order approximations for the geometric series inverses, enabling division elimination via Strassen's method while retaining the target multilinear terms."
ICML_2024_oral_101,9,"Consider the elementary event \(X_{1}=a_{1},\ldots,X_{n}=a_{n}\). Define an input vector \(e\) for \(P\) by","e_{i,j}=\begin{cases}1&\text{if $j=a_{i}$},\\ 0&\text{otherwise}\end{cases}",,"e_{i,j}=\begin{cases}1 & \text{if }j=a_{i},\\0 & \text{otherwise}.\end{cases}",Input vector $e$ assigns 1 to the variable corresponding to the outcome $a_i$ of $X_i$ and 0 to all other variables in the same partition.
ICML_2024_oral_101,10,"for \(1\leq i\leq n\), \(0\leq j\leq d-1\). By (4), \(P(e)=\Pr[X_{1}=a_{1},\ldots,X_{n}=a_{n}]\). From all monomials in \(P\), only \(z_{1,a_{1}}\ldots z_{n,a_{n}}\) evaluates to \(1\) under \(e\) and all others evaluate to \(0\). Now to compute \(\Pr[X_{1}\in A_{1},\ldots,X_{n}\in A_{n}]\), we simply evaluate at the point","v_{i,j}=\begin{cases}1&\text{if $j\in A_{i}$},\\ 0&\text{otherwise}.\end{cases}",,"z_{i,j}=\begin{cases}1&\text{if }j\in A_{i},\\ 0&\text{otherwise}.\end{cases}","Evaluation point for computing the probability of the event $X_{1} \in A_{1}, \ldots, X_{n} \in A_{n}$ by substituting indicator values into the set-multilinear polynomial $P$."
ICML_2024_oral_101,11,"We claim that \(P(v)=\Pr[X_{1}\in A_{1},\ldots,X_{n}\in A_{n}]\): A monomial \(z_{1,j_{1}}\cdots z_{n,j_{n}}\) evaluated at \(v\) becomes \(1\) iff \(j_{i}\in A_{i}\) for all \(1\leq i\leq n\). Otherwise, it evaluates to \(0\). Thus","P(v) =\sum_{j_{1}\in A_{1}}\cdots\sum_{j_{n}\in A_{n}}\Pr[X_{1}=j_{1}, \ldots,X_{n}=j_{n}]\cdot 1 || =\Pr[X_{1}\in A_{1},\ldots,X_{n}\in A_{n}],",,"P(v)=\sum_{j_{1}\in A_{1},\ldots,j_{n}\in A_{n}}\Pr[X_{1}=j_{1},\ldots,X_{n}=j_{n}}].","The evaluation of the set-multilinear polynomial $P$ at the vector $v$ yields the marginal probability $\Pr[X_{1}\in A_{1},\ldots,X_{n}\in A_{n}]$ by summing over all joint probabilities where each variable $X_i$ takes a value in the subset $A_i$."
ICML_2024_oral_101,12,"which proves the claim. 

_Remark 9.4_.: This gives also an alternative marginalization algorithm for PGCs over \(n\) binary variables. We convert it into a equivalent nonmonotone PC computing a set-multilinear polynomial. The total running time will be \(O(s\cdot n)\). The extra factor \(n\) comes from the conversion to a PC. The running time matches the one by Harviainen et al. (2023).

While being decomposable or being smooth is a _syntactic_ property, that is, it is a property of the circuit and can be checked efficiently, computing a set-multilinear polynomial and computing a probability distribution are _semantic_ properties2, that is, properties of the polynomial computed by the circuit. We can of course compute the coefficients of the polynomial to check whether it is set-multilinear or evaluate the circuit at all inputs to see whether it is a probability distribution, but this requires exponential time. Can we check these properties nevertheless efficiently? It turns out that the first property is efficiently checkable while the second is most likely not.

Footnote 2: This is only true for nonmonotone PCs, for monotone PCs these conditions are equivalent, see (Vergari et al., 2020).

**Proposition 9.5**.: _Testing whether a nonmonotone PC computes a set-multilinear polynomial with respect to a given partition can be done in randomized polynomial time._

**Proposition 9.6**.: _Testing whether a nonmonotone PC computes a probability distribution is \(\mathsf{NP}\)-hard._

The proof of the first proposition can be found in Appendix B. The proof of the second proposition essentially follows from the hardness result by Harviainen et al. (2023, Theorem 5) combined with our transformation of PGCs to PCs (Theorem 8.1).

## 10 Compositional operations for nonmonotone PCs

Since it is hard to determine whether a nonmonotone PC computes a probability distribution, we here present three compositional operations for nonmonotone PCs that preserve the property that the PC computes a probability distribution and computes a set-multilinear polynomial, analogous to (Zhang et al., 2021). The first two are the well-known multiplication and mixing operations. The third one is more interesting, it is an analogue of the hierarchical composition for PGC introduced by Zhang et al. (2021) for nonmonotone PCs computing set-multilinear polynomials. The proofs of the results of this section are in Appendix C.

Let \(X_{1},\ldots,X_{n}\) be categorical random variables with image \(\Delta=\{0,\ldots,d-1\}\). Let \(A,B\subseteq\{1,\ldots,n\}\). Let \(C,D\) be two nonmonotone PCs computing joint probability distributions \(f\) and \(g\) for \(X_{A}=(X_{i})_{i\in A}\) and \(X_{B}=(X_{j})_{j\in B}\), resp., as set-multilinear polynomials in the variables \(z_{i,j}\), \(1\leq i\leq n\), \(0\leq j\leq d-1\). That is","f(a)=\sum_{a\in\Delta^{|A|}}\alpha_{a}\prod_{i\in A}z_{i,a_{i}},\quad g(b)= \sum_{b\in\Delta^{|B|}}\beta_{b}\prod_{j\in B}z_{j,b_{j}},",where \(\alpha_{a}=\Pr[X_{A}=a]\) and \(\beta_{b}=\Pr[X_{b}=b]\),"\Pr[X_{1}=j_{1},\ldots,X_{n}=j_{n}] = f(j_{1},\ldots,j_{n}) \quad \text{and} \quad \Pr[X_{1}=j_{1},\ldots,X_{n}=j_{n}] = g(j_{1},\ldots,j_{n})",
ICML_2024_oral_101,13,"\). Let \(s\) and \(t\) be the sizes of \(C\) and \(D\) respectively.

We want to construct a mixture of the two distributions. To this aim, we can extend \(f\) to a probability distribution on \(X_{A\cup B}\) by","f(a,b^{\prime})= || (\sum_{a\in\Delta^{|A|}}\alpha_{a}\prod_{i\in A}z_{i,a_{i}} )\prod_{j\in B\setminus A}\frac{1}{d}(z_{j,0}+\cdots+z_{j,d-1}),",,"f'(z)=\sum_{a\in\Delta^{|A|}}\sum_{b\in\Delta^{|B|}}\alpha_{a}\prod_{i\in A}z_{i,a_{i}}\prod_{j\in B}z_{j,b_{j}}.",
ICML_2024_oral_104,1,"## 4 Sequential Implementation

In the previous section, we described Pruned Pivot algorithm. _For the sake of analysis_, we now examine a sequential algorithm that produces the same clustering as the recursive algorithm above and, moreover, marks the same set of nodes as unlucky. First, we consider the standard Pivot implemented as a bottom-up dynamic programming algorithm (see Algorithm 3).

```
1:Pick a random ordering \(\pi:V\rightarrow\{1,\ldots,n\}\).
2:Let \(V_{\pi}\) be the list of all nodes \(u\in V\) sorted by the rank \(\pi(u)\).
3:forach\(u\)in\(V_{\pi}\):
4:Sort all neighbors of \(u\) by their rank \(\pi(v)\). Denote the sorted list by \(N_{\pi}\).
5:while\(u\) is not assigned to a cluster:
6: Pick the next neighbor \(v\in N_{\pi}(u)\).
7:if\(v\) is a pivot: place \(u\) in the cluster of \(v\).
8:if\(v=u\): mark \(u\) as a pivot; create a new cluster for \(u\); and place \(u\) in that cluster.
```

**Algorithm 3**Sequential Pivot

In the main loop (see the **for each** loop above), the algorithm iterates over all nodes in \(V\). At iteration \(i\in\{1,\ldots,n\}\), the algorithm processes node \(u\) with rank \(i\), i.e., \(u=\pi^{-1}(i)\). It checks all neighbors \(v\) of \(u\) with rank higher than that of \(u\). If one of these neighbors is a pivot, the algorithm assigns \(u\) to the highest-ranked pivot neighbor of \(u\). If none of these neighbors are pivots, the algorithm marks \(u\) as a pivot and assigns \(u\) to itself.

Let us set up some notation. Consider a neighbor \(v\) of \(u\). It is processed at iteration \(i=\pi(v)\). Suppose that no other neighbor of \(u\) (including \(u\) itself) is marked as a pivot before iteration \(i\). Then, we know that \(u\) will be assigned to the cluster of \(v\), since it is the highest-ranked pivot neighbor of \(u\). Thus, we will say that \(u\) is _settled_ at step \(i\). In other words, \(u\) is settled when the first neighbor of \(u\) is marked as a pivot. We denote the iteration when \(u\) is settled by \(\sigma(u)\). Note that node \(u\) is assigned to the node processed at iteration \(\sigma(u)\), i.e., node \(\pi^{-1}(\sigma(u))\). In particular, if \(u\) is a pivot, then it is settled at the iteration \(i=\pi(u)\), the same iteration as it is processed. We always have \(\sigma(u)\leq\pi(u)\), because if \(u\) is not settled before iteration \(\pi(u)\), then it is marked as a pivot and assigned to itself at iteration \(\pi(u)\); thus, if \(\sigma(u)\geq\pi(u)\), then \(\sigma(u)=\pi(u)\).

If neighbor \(v\) of \(u\) is considered in the **while**-loop of the Sequential Pivot algorithm, then we say that \(u\) queries \(v\). We denote by \(Q(u)\) the set of all neighbors queried by \(u\), except for \(u\) itself, and call this set the set of _queried neighbors of \(u\)_. Observe that \(Q(u)=\{v\in N(u)\setminus\{u\}:\pi(v)\leq\sigma(u)\}\). That is, \(Q(u)\) is the set of all neighbors of \(u\), excluding \(u\), whose rank is higher than the rank of the pivot to which \(u\) is assigned. Finally, we formally define the recursion tree \(\mathcal{T}_{u}\) for node \(u\). The definition is recursive: If \(Q(u)\) is empty, then \(\mathcal{T}_{u}\) only contains node \(u\). Otherwise, \(\mathcal{T}_{u}\) is the tree with root \(u\) and \(|Q(u)|\) subtrees \(\widehat{\mathcal{T}}_{v}\) attached to it - one tree for every \(v\in Q(u)\). Each \(\widehat{\mathcal{T}}_{v}\) is a copy of the recursive tree \(\mathcal{T}_{v}\). We stress that the recursive tree may contain multiple copies of the same node \(v\). One can think of nodes of \(\mathcal{T}_{u}\) as being ""stack traces"" or ""execution paths"" for the recursive function cluster.

**Sequential Pivot with Pruning.** We now describe how to modify the bottom-up algorithm to make it equivalent to Pruned Pivot algorithm. First, we run the bottom-up algorithm as is and record its trace. We then define the recursive call count for every node \(u\). The recursive call count of \(u\) equals the number of edges in the recursive tree \(\mathcal{T}_{u}\). It can be computed using the following recurrence relation:",\text{call-count}(u)=\sum_{v\in Q(u)}(1+\text{call-count}(v)).,,"\begin{equation}
    \text{call\_count}(u) = \sum_{v \in Q(u)} \left(1 + \text{call\_count}(v) \right).
\end{equation}","Recursive call count for node $u$ defined as the total number of edges in the recursive tree $\mathcal{T}_u$, computed by summing over all queried neighbors $v \in Q(u)$ the contribution of each neighbor's subtree."
ICML_2024_oral_104,2,"If \(Q(u)\) is empty, then the recursive call count of \(u\) equals \(0\), by definition. We mark node \(u\) as _unlucky_ if its recursive call count is at least \(k\). Note that if one of the queried neighbors of \(u\) is unlucky, then \(u\) is also unlucky.

```
1:Compute the recursive call count of every node \(u\) using recurrence relation (1).
2:Mark all nodes \(u\) with \(\mathrm{call-count}(u)\geq k\) as unlucky.
3:Create a new cluster for each unlucky node \(u\), remove \(u\) from its current cluster, and place \(u\) in the new cluster.
```

**Algorithm 4**Pruning

The pruning step puts all unlucky nodes into singleton clusters. We refer to the standard Pivot algorithm as Pivot without pruning or simply Pivot. We refer to the Pivot algorithm that runs the pruning as Pivot with Pruning. Note that the Pivot with Pruning algorithm produces the same clustering output as the Pruned Pivot algorithm described in the previous section. The key difference between these algorithms lies in their structure: Pivot with Pruning consists of two distinct steps - a Pivot step followed by a Pruning step - whereas Pruned Pivot combines both steps together. We show that the expected cost of the Pivot with Pruning is \((3+O(1/k))\)OPT and obtain the following theorem.

**Theorem 4.1**.: _The expected cost of the clustering produced by the Pruned Pivot is \((3+O(1/k))\) OPT._

Ailon, Charikar, and Newman (2008) showed that the approximation factor of Pivot is \(3\). By Lemma F.1, the Pivot step (see Algorithm 3) is equivalent to the Pivot algorithm. Hence, its approximation factor is also \(3\). The pruning step of Pivot with Pruning removes some nodes (namely, unlucky nodes) from their original clusters and puts them into singleton clusters. This pruning step can increase the number of pairs of nodes \((u,v)\) disagreeing with the clustering. Note, however, that if \(u\) and \(v\) are dissimilar (i.e., not connected with an edge), then the pruning step will never make them disagree with the clustering if they agreed with the original clustering. Thus, the pruning step can increase the objective function only by separating pairs of similar nodes \((u,v)\in E\). In such case, we say that the pruning step cuts edge \((u,v)\). Specifically, edge \((u,v)\) is cut by the pruning step of Pivot with Pruning if \(u\) and \(v\) are in the same cluster after the Pivot step of the algorithm, but are separated by the pruning step, because \(u\), \(v\), or both \(u\) and \(v\) are unlucky nodes. We say that an edge \((u,v)\in E\) is cut by Pivot (without pruning), if Pivot places \(u\) and \(v\) in distinct clusters. In the next sections, we show Lemma 4.2 that states that the expected number of edges cut by the pruning step of Pivot with Pruning is upper bounded by the expected number of edges cut by Pivot divided by \(\lceil(k-1)/2\rceil/2\). The ""triangle-based"" analysis of Pivot by Ailon, Charikar, and Newman (2008) shows that Pivot cuts at most \(2\)OPT edges in expectation. Thus, the pruning step cuts at most \(4\mathrm{OPT}/\lceil(k-1)/2\rceil\) edges in expectation. We conclude that the expected cost of Pivot with Pruning is at most \((3+4/\lceil(k-1)/2\rceil)\)OPT.

**Lemma 4.2**.: _The expected number of edges \((u,v)\) cut by the pruning step of Pivot with Pruning is upper bounded by the expected number of edges cut by Pivot divided by \(\lceil(k-1)/2\rceil/2\)._

### Query Paths

Our goal now is to prove Lemma 4.2.

In this section, we define _query paths_, _extended query paths_, and _expensive extended query paths_. We then show that on the one hand, the number of edges cut by the pruning step of the Pivot with Pruning algorithm is upper bounded by the number of expensive extended query paths divided by \(\lceil(k-1)/2\rceil\) (see Corollary 4.9); and, on the other hand, the expected number of expensive extended query paths is upper bounded by two times the expected number of edges cut by the Pivot algorithm (see Theorem 4.10). This will imply Lemma 4.2.

**Definition 4.3** (Query Paths).: _A path \((u_{0},u_{1},\ldots,u_{L})\) is a query path if each \(u_{i}\) (\(i>0\)) queries \(u_{i-1}\)._

**Definition 4.4** (Extended Query Paths).: _A path \((u_{0},u_{1},\ldots,u_{L})\) of length \(L\geq 2\) is an extended query path (EQ-path) if the following two conditions hold: (1) \((u_{0},u_{1},\ldots,u_{L-1})\) is a query path; and (2)

Figure 1: This figure shows an _extended query path_ in the recursion tree \(\mathcal{T}_{v}\) for node \(v\). The path starts with edge \((a,b)\) goes to the root of the tree, node \(v\), and then proceeds to node \(w\). The path from \(a\) till \(v\) is a _query path_. The path from \(a\) to \(w\)_extends_ the path from \(a\) to \(v\). If edge \((a,b)\) is cut by the pivot step of Pivot but edge \((v,w)\) is not cut, then this path is _expensive_. We call it expensive because if \(v\) is unlucky, then \((v,w)\) is cut by the pruning step of Pivot with Pruning and the cost of \((v,w)\) is partially charged to this path.

\(\pi(u_{L-2})\leq\sigma(u_{L})\). We say that EQ-path \((u_{0},u_{1},\ldots,u_{L})\) is an extension of the query path \((u_{0},u_{1},\ldots,u_{L-1})\). We also call every path consisting of one edge \((u_{0},u_{1})\) an extended query path._

Note that a proper prefix of a query or extended query path is a query path.

Recall, that for every \(u\), we have \(\sigma(u)\leq\pi(u)\). Also, a node \(u\) queries its neighbor \(v\) (\(v\neq u\)) if and only if \(u\) is not settled before \(v\) is processed, i.e., \(\sigma(u)\geq\pi(v)\). Thus, \((u_{0},u_{1},\ldots,u_{L})\) is a query path if and only if",\sigma(u_{0})\leq\pi(u_{0})\leq\sigma(u_{1})\leq\pi(u_{1})\leq\ldots || \leq\sigma(u_{L-1})\leq\pi(u_{L-1})\leq\sigma(u_{L})\leq\pi(u_{L}).,,"\sigma(u_{i})\geq\pi(u_{i-1})\quad\text{for all }i\in\{1,\ldots,L\}.",Condition characterizing a query path in terms of the settlement and permutation ranks of nodes.
ICML_2024_oral_104,3,"Similarly, a path \((u_{0},u_{1},\ldots,u_{L})\) of length \(L\geq 2\) is an EQ-path if and only if","\sigma(u_{0})\leq\pi(u_{0})\leq\sigma(u_{1})\leq\pi(u_{1})\leq\ldots || \leq\sigma(u_{L-2})\leq\pi(u_{L-2})\leq\min(\sigma(u_{L-1}),\sigma (u_{L})).",,,
ICML_2024_oral_104,4,"We will charge all edges cut by the pruning step of Pivot with Pruning to \(\Theta(k)\)_expensive EQ-paths_ which are defined as follows.

**Definition 4.5** (Expensive Extended Query Paths).: _An extended query path \((u_{0},u_{1},\ldots,u_{L})\) is expensive if \(\sigma(u_{0})<\sigma(u_{1})\) but \(\sigma(u_{L-1})=\sigma(u_{L})\). We denote the set of all expensive query paths by \(\mathcal{X}\)._

Note that in every expensive EQ-path, the first edge is cut by Pivot (because \(\sigma(u_{0})<\sigma(u_{1})\)) but the last edge is not cut (because \(\sigma(u_{L-1})=\sigma(u_{L})\)). A path \((u_{0},u_{1},\ldots,u_{L})\) is an EQ-path if and only if condition (3) holds, thus \((u_{0},u_{1},\ldots,u_{L})\) is an expensive EQ-path if and only if",\sigma(u_{0})<\sigma(u_{1})\text{ and }\sigma(u_{0})\leq\pi(u_{0}) \leq\sigma(u_{1})\leq\pi(u_{1})\leq || \cdots\leq\pi(u_{L-2})\leq\sigma(u_{L-1})=\sigma(u_{L}).,,"\sigma(u_{0})\leq\pi(u_{0})\leq\sigma(u_{1})\leq\pi(u_{1})\leq\ldots\\
\leq\sigma(u_{L-2})\leq\pi(u_{L-2})\leq\min(\sigma(u_{L-1}),\sigma(u_{L}))","An expensive extended query path is characterized by the condition that the first edge is cut by Pivot (due to differing settlement times), while the last edge is not cut (due to identical settlement times), under the constraints imposed by the ordering of settlement and processing times."
ICML_2024_oral_104,5,"The first condition \(\sigma(u_{0})<\sigma(u_{1})\) in (4) can be replaced with \(\sigma(u_{0})\neq\sigma(u_{1})\), because we always have \(\sigma(u_{0})\leq\sigma(u_{1})\) if \(u_{0},\ldots,u_{L-1}\) is a query path.

#### 4.1.1 Charging Cut Edges to Expensive Paths

We now prove a lemma that establishes a connection between edges cut by the pruning step of Pivot with Pruning and expensive EQ-paths.

**Lemma 4.6**.: _For every unlucky node \(v\) and every edge \((v,w)\) with \(\sigma(v)=\sigma(w)\), there exist at least \(\lceil(k-1)/2\rceil\) expensive extended query paths that end with edge \((v,w)\)._

Proof.: Let \(\mathcal{T}_{v}\) be the recursion tree for node \(v\). We first show that \(\mathcal{T}_{v}\) contains at least \(\lceil(k-1)/2\rceil\) edges cut by Pivot (formally, \(\mathcal{T}_{v}\) contains copies of edges cut by Pivot). Consider an edge \((u^{\prime},u^{\prime\prime})\) in \(T\). Since \((u^{\prime},u^{\prime\prime})\) is an edge in the recursion tree, \(u^{\prime}\) queries \(u^{\prime\prime}\). Thus, \(u^{\prime}\) is assigned to \(u^{\prime\prime}\) (if \(u^{\prime\prime}\) is a pivot) or some neighbor of \(u^{\prime}\) which is lower ranked than \(u^{\prime\prime}\) (if \(u^{\prime\prime}\) is not a pivot). Vertex \(u^{\prime\prime}\) is assigned to itself (if \(u^{\prime\prime}\) is a pivot) or one of its neighbors ranked higher than \(u^{\prime\prime}\) (if \(u^{\prime\prime}\) is not a pivot). Thus, if edge \((u^{\prime},u^{\prime\prime})\) is not cut by Pivot, then \(u^{\prime\prime}\) is a pivot and \(u^{\prime}\) is assigned to \(u^{\prime\prime}\). This means that \(u^{\prime\prime}\) is the highest-ranked pivot neighbor of \(u^{\prime}\). Consequently, for every \(u^{\prime}\), there is at most one child node \(u^{\prime\prime}\) such that \((u^{\prime},u^{\prime\prime})\) is not cut. Moreover, if one such \(u^{\prime\prime}\) exists, then \(u^{\prime}\) is not a pivot, and hence the edge from \(u^{\prime}\) to its parent is cut (unless \(u^{\prime}\) is the root). We get the following claim.

**Claim 4.7**.: _For every node \(u\) in the recursion tree \(\mathcal{T}_{v}\), at most one edge incident on \(u\) is not cut by Pivot._

Node \(v\) is unlucky. Hence, the recursion tree \(\mathcal{T}_{v}\) must have at least \(k\) edges. Therefore, by Claim 4.7 and Lemma 4.8 (see below) there are at least \(\lceil(k-1)/2\rceil\) cut edges in \(\mathcal{T}_{v}\). In Lemma 4.8, red edges are cut edges, and blue edges are not cut edges.

**Lemma 4.8**.: _Consider a tree \(T\) with \(k\) edges colored red or blue. Suppose that at most one blue edge is incident on each node in \(T\). Then, \(T\) contains at least \(\lceil(k-1)/2\rceil\) red edges._

Proof.: Tree \(T\) has \(k\) edges and \(k+1\) nodes. At most one blue edge is incident on each node. So, blue edges form a matching. The size of this matching is at most \(\lfloor(k+1)/2\rfloor\). The number of edges not in the matching is at least \(k-\lfloor(k+1)/2\rfloor=\lceil(k-1)/2\rceil\). All of them are red. 

Now, for every edge \((b,a)\) in \(T\) such that \(b\) queries \(a\) and \((b,a)\) is cut by Pivot, we construct an expensive EQ-path. This path starts with edge \((a,b)\), then goes to the root of tree \(T\) - node \(v\) - along the edges of \(T\), and, finally, proceeds to node \(w\) (see Figure 1). Observe that the subpath from \(a\) to \(v\) is a query path since each node on the path queries the preceding node. We know that \(\sigma(v)=\sigma(w)\). Hence, by (3), the path \((a,b,\ldots,v,w)\) is an expensive query path. 

The immediate corollary of this lemma gives us a bound on the number of edges cut by the pruning step.

**Corollary 4.9**.: _The number of edges cut by the pruning step of Pivot with Pruning is at most \(|\mathcal{X}|/\lceil(k-1)/2\rceil\)._

Proof.: Every edge \((v,w)\in E\) cut by the pruning step of Pivot with Pruning is not cut by Pivot. Hence, \(\sigma(v)=\sigma(w)\). Moreover, if \((v,w)\) is cut by the pruning step, then \(v\), \(w\), or both \(v\) and \(w\) must be unlucky. Thus, by Lemma 4.6, there are at least \(\lceil(k-1)/2\rceil\) expensive EQ-paths that end with \((v,w)\) or \((w,v)\). Therefore, there exists at least \(\lceil(k-1)/2\rceil\) unique expensive EQ-paths for each edge \((v,w)\) cut by the pruning step. 

#### 4.1.2 Expected Number of Expensive EQ-Paths

We now prove that the expected number of expensive EQ-paths is at most \(4OPT\) and the expected number of query paths that start with a fixed directed edge \((a,b)\) - we denote these paths by \(\mathcal{Q}(a,b)\) - is at most 2.

**Theorem 4.10**.: _For every ordered pair \((a,b)\) with \((a,b)\in E\), we have \(\mathbf{E}_{\pi}|\mathcal{Q}(a,b)|\leq 2\), and_","\mathbf{E}_{\pi}|\mathcal{X}|\leq 2\mathbf{E}\Big{[}\sum_{(u,v)\in E}\mathbf{1} (\sigma(u)\neq\sigma(v))\Big{]}.",,\mathbf{E}_{\pi}|\mathcal{X}|\leq 4\cdot\mathbf{E}_{\pi}[\text{number of edges cut by Pivot}].,"The expected number of expensive extended query paths is bounded in terms of the expected number of edges cut by the Pivot algorithm, establishing a key relation for proving Lemma 4.2."
ICML_2024_oral_104,6,"We will refer to the time when iteration \(t\) of Pivot occurs as time \(t\). For the sake of analysis, we shall assume that the ordering \(\pi\) is initially (at time \(0\)) hidden from us and is revealed one node at a time. At the beginning of iteration \(t\), we learn the value of \(\pi^{-1}(t)\), or, in other words, the identity of the node processed at time \(t\). Note that the state of the algorithm after the first \(t\) iterations is completely determined by the nodes \(\pi^{-1}(1),\ldots,\pi^{-1}(t)\). In particular, at time \(t\), for every node \(u\), we can tell if \(u\) is settled by time \(t\) and, if it is settled, then we know the value of \(\sigma(u)\); otherwise, we know that \(\sigma(u)>t\). Let \(\mathcal{F}_{t}\) be the filtration generated by \(\pi^{-1}(1),\ldots,\pi^{-1}(t)\). We will use the standard notation \(\Pr[\;\cdot\;|\;\mathcal{F}_{t}]\) and \(\mathbf{E}[\;\cdot\;|\;\mathcal{F}_{t}]\) to denote the conditional probability and conditional expectation given the state of the algorithm after iteration \(t\). Note that each \(\pi(u)\) and \(\sigma(v)\) is a _stopping time_ with respect to \(\mathcal{F}_{t}\).

Let \(\mathcal{P}(a,b)\) be the set of all paths that start with edge \((a,b)\). As we run the Pivot algorithm, we add paths to sets \(\mathcal{Q}_{t}(a,b)\) and \(\mathcal{X}_{t}(a,b)\). Loosely speaking, we add a path from \(\mathcal{P}(a,b)\) to \(\mathcal{Q}_{t}(a,b)\) if we can verify that this path is a query path using condition (2) at time \(t\); we add a path from \(\mathcal{P}(a,b)\) to \(\mathcal{X}_{t}(a,b)\) if we can verify that this path is an expensive EQ-path using condition (4) at time \(t\). Formally, we add path \((u_{0}=a,u_{1}=b,\ldots,u_{L})\) to \(\mathcal{Q}_{t}(a,b)\) at time \(\pi(u_{L-1})\) if condition (2) holds; and we add path \((u_{0}=a,u_{1}=b,\ldots,u_{L})\) to \(\mathcal{X}_{t}(a,b)\) at time \(\sigma(u_{L-1})=\sigma(u_{L})\) if condition (4) holds. Thus, \(\mathcal{Q}_{t}(a,b)\) is the set of all query paths \(P\in\mathcal{P}(a,b)\) for which \(\pi(u_{L-1})\leq t\); and \(\mathcal{X}_{t}(a,b)\) is the set of all expensive EQ-paths \(P\in\mathcal{P}(a,b)\) for which \(\sigma(u_{L-1})=\sigma(u_{L})\leq t\). Note that at the times \(\pi(u_{L-1})\) and \(\sigma(u_{L-1})=\sigma(u_{L})\), we can check conditions (2) and (4), respectively. We also define a set of _dangerous_ paths at time \(t\), denoted by \(\mathcal{D}_{t}(a,b)\), as follows.

**Definition 4.11** (Dangerous EQ-path).: _An extended query path \((u_{0},\ldots,u_{L})\) (\(L\geq 1\)) is dangerous at iteration \(t\) if \(\pi(u_{L-2})\leq t\), \(\pi(u_{L-1})>t\), and \(\sigma(u_{L})>t\). We omit the first condition (\(\pi(u_{L-2})\leq t\)) for paths of length \(1\). Denote the set of all extended query paths that start with edge \((a,b)\) and are dangerous at iteration \(t\) by \(\mathcal{D}_{t}(a,b)\)._

Note that a path \(P\in\mathcal{P}(a,b)\) may become dangerous at some iteration \(t\), stay dangerous for some time, but eventually it will become non-dangerous. After that, it will remain non-dangerous until the end of the algorithm. The definition of dangerous paths is justified by the following lemma, which, loosely speaking, says that every query path and every expensive EQ-path is created from a dangerous path.

**Lemma 4.12**.: _Consider a path \(P=(u_{0},u_{1},\ldots,u_{L})\in\mathcal{P}(a,b)\). Let \(P^{\prime}=(u_{0},u_{1},\ldots,u_{L-1})\). Then, the following claims hold for every \(t\geq 0\):_

* _If_ \(P\in\mathcal{Q}_{t+1}(a,b)\setminus\mathcal{Q}_{t}(a,b)\)_, then_ \(P\in\mathcal{D}_{t}(a,b)\) _but_ \(P\notin\mathcal{D}_{t+1}(a,b)\)_._
* _If_ \(P\in\mathcal{D}_{t+1}(a,b)\setminus\mathcal{D}_{t}(a,b)\)_, then_ \(P^{\prime}\in\mathcal{D}_{t}(a,b)\) _but_ \(P^{\prime}\notin\mathcal{D}_{t+1}(a,b)\)_._
* _If_ \(P\in\mathcal{X}_{t+1}(a,b)\setminus\mathcal{X}_{t}(a,b)\)_, then_ \(P\in\mathcal{D}_{t}(a,b)\) _or_ \(P^{\prime}\in\mathcal{D}_{t}(a,b)\) _but_ \(P\notin\mathcal{D}_{t+1}(a,b)\) _and_ \(P^{\prime}\notin\mathcal{D}_{t+1}(a,b)\)_._

We prove this lemma in Appendix C.1.

Our approach to bounding \(\mathbf{E}|\mathcal{Q}_{t}(a,b)|\) and \(\mathbf{E}|\mathcal{X}_{t}(a,b)|\) is based on the following idea: At time \(t=0\), the path \((a,b)\) is dangerous, and there are no query or expensive EQ-paths that start with \((a,b)\). If \(P\) is a dangerous EQ-path at time \(t\), then at the next iteration, it may be extended to a longer dangerous path, replaced with a query path, and/or created one or more expensive EQ-paths. A dangerous path may also disappear without producing any new dangerous, query, or expensive EQ-paths. For every EQ-path \(P\) dangerous at iteration \(t\), we will compute the probabilities of creating new paths and derive the desired bounds on \(\mathbf{E}|\mathcal{Q}_{t}(a,b)|\) and \(\mathbf{E}|\mathcal{X}_{t}(a,b)|\). To make our argument formal, we define two random processes:","\Phi_{t}(a,b) =2|\mathcal{D}_{t}(a,b)|+|\mathcal{Q}_{t}(a,b)|; || \Psi_{t}(a,b) =2|\mathcal{D}_{t}(a,b)|+|\mathcal{X}_{t}(a,b)|.",,"\mathbf{E}|\mathcal{X}_{t}(a,b)|\leq 2\mathbf{E}\Big{[}\sum_{(u,v)\in E}\mathbf{1}(\sigma(u)\neq\sigma(v))\Big{]}.","This equation bounds the expected number of expensive extended query paths starting with a fixed edge (a, b) by twice the expected number of edges cut by the Pivot algorithm."
ICML_2024_oral_106,1,"### Revisiting iGPT

**GPT.** In NLP, the generative pretraining involves modeling the probability of the next word in a corpus \(\mathcal{U}=\{u_{1},...,u_{n}\}\) autoregressively. This can be written as:","p(u)=\prod_{i=1}^{n}p(u_{i}|u_{1},...,u_{i-1},\Theta)",,"\begin{equation}
    p(u_{1},...,u_{n}) = \prod_{i=1}^{n} p(u_{i} \mid u_{1},...,u_{i-1}})
\end{equation}",The joint probability of a sequence of words modeled autoregressively as the product of conditional probabilities.
ICML_2024_oral_106,2,"Here, GPT computes the likelihood of each word \(u_{i}\) based on the context of all preceding words from \(u_{1}\) to \(u_{i-1}\), aiming to minimize the negative log-likelihood of the target words:",\mathcal{L}=-log\;p(u),,"\mathcal{L} = -\sum_{i=1}^{n} \log p(u_{i} | u_{1},...,u_{i-1}, \Theta)","This equation represents the negative log-likelihood loss function derived from the autoregressive probability model in Equation 1, which is minimized during GPT training to optimize language modeling performance."
ICML_2024_oral_106,3,"**Image GPT.** In the context of images, where the input is an image \(X\in\mathcal{R}^{H\times W\times C}\), the challenge lies in converting this 2D structure into a sequential format akin to a language sequence. iGPT (Chen et al., 2020) addresses this by naively vectorizing the image \(X\) into a series of individual pixels \(\{x_{1},...,x_{n}\}\), treating each pixel as analogous to a word. It then models the probability of each subsequent pixel based on the preceding ones in the sequence:","p(x)=\prod_{i=1}^{n}p(x_{i}|x_{1},...,x_{i-1},\Theta)",,"p(x)=\prod_{i=1}^{n}p(x_{i}|x_{1},...,x_{i-1},\Theta)","Image GPT models the probability of each pixel in a flattened image sequence autoregressively, conditioned on all previous pixels."
ICML_2024_oral_106,4,"In this formulation, iGPT aims to predict each pixel \(x_{i}\) utilizing the information from preceding pixels \(\{x_{1},...,x_{i-1}\}\), minimizing the negative log-likelihood:",\mathcal{L}=-log\;p(x),,\mathcal{L} = -\log\; p(x),"Image GPT minimizes the negative log-likelihood of the pixel sequence, analogous to language modeling in GPT."
ICML_2024_oral_106,5,"Nevertheless, the extensive computational demands of iGPT, primarily due to the quadratic complexity of attention mechanisms relative to sequence length, limit its applicability for various vision tasks. For iGPT, this sequence length corresponds to the total number of pixels \(Seq=H\times W\). As such, iGPT is primarily suited for low-resolution images (_e.g_., \(Seq=32\times 32\)).

To mitigate this computational challenge, especially for high-resolution image training, approaches like SAIM (Qi et al., 2023) and RandSac (Hua et al., 2022) have been developed. A critical advancement in these methodologies is the incorporation of Vision Transformer (ViT) architecture (Dosovitskiy et al., 2020), which significantly transforms the tokenization approach -- instead of treating each pixel as an individual token, ViT redefines tokens as image patches (_e.g_., clusters of pixels). This strategy effectively reduces the sequence length for each image, thereby enabling the practical application of iGPT to higher-resolution images.

### D-iGPT

Our development of D-iGPT is built upon iGPT with the ViT architecture. Unlike iGPT completely drops the knowledge of the 2D input structure, D-iGPT is designed to carefully encode this information. Specifically, at the input level, images are divided into multiple equally-sized, non-overlapping regions, forming clusters \(S=\{s_{1},...,s_{n}\}\). Note that each cluster contains multiple spatially neighbored image patches, and serves as a fundamental unit in the sequence for autoregressive modeling. This encoding is crucial for facilitating a more intricate interplay between different regions (rather than ""local"" patches) of an image, thereby enhancing the effectiveness of autoregressive modeling.

Consequently, the autoregressive probability, previously defined for individual pixels in iGPT (as in Equation 3), is now reformulated for these clusters as:","p(s)=\prod_{i=1}^{n}p(s_{i}|s_{1},...,s_{i-1},\Theta)",,"p(s)=\prod_{i=1}^{n}p(s_{i}|s_{1},...,s_{i-1},\Theta)","The autoregressive probability is now defined over clusters $s_i$ rather than individual pixels, where each cluster represents a group of spatially neighboring image patches."
ICML_2024_oral_106,6,"By default, we configure the number of clusters to \(4\), corresponding to a dimension of \(112\times 112\) for an input image of \(224\times 224\) (_e.g_., each cluster contains \(7\times 7\) images patches of the size \(16\times 16\)), as illustrated in Figure 2.

Building upon this new setup, we next introduce two simple yet essential modifications to enhance iGPT.

**Modification I: semantic tokens.** In contrast to the inherently semantically-rich nature of text, raw pixels in images generally lack such depth of meaning. Addressing this semantic discrepancy is crucial for enhancing learning efficacy in models like iGPT. To bridge this gap, our approach, inspired by BEiT (Bao et al., 2022), involves transitioning the autoregressive target of D-iGPT from raw pixels to semantic tokens, which can be written as:","\mathcal{L}_{G}=-\sum_{i=1}^{n}cosine(G(f(x_{s_{1}:s_{i-1}});\theta_{G}),f_{\phi }(x)_{s_{i}}),","where \(f(\cdot)\) is the encoder, \(f_{\phi}(x)_{s_{i}}\) is the semantically enriched tokens corresponding to the cluster \(s_{i}\), \(G(\cdot;\ \theta_{G})\) is the generative decoder for autoregressive prediction, and \(cosine\) is the cosine similarity loss",\mathcal{L}_{\text{semantic}} = -\log\;p(t),This equation represents the semantic-aware negative log-likelihood loss where $t$ denotes the sequence of semantic tokens predicted autoregressively by D-iGPT.
ICML_2024_oral_106,7,"ss.

Furthermore, to break the dependency on a fixed sequence order and enhance learning flexibility, we adopt strategies from (Hua et al., 2022; Yang et al., 2019) by randomly permuting the sequence of clusters \(\{s_{1},...s_{n}\}\) and selecting a permutation \(\pi\).

**Mofication II: supervision on visible clusters.** To further enhance the training of our model, we introduce additional supervision targeting visible clusters. This is formulated as:","\mathcal{L}_{D}=-\sum_{i=1}^{n}cosine(D(f(x_{s_{1}:s_{i-1}});\theta_{D}),f_{ \phi}(x)_{s_{1}:s_{i-1}})","where \(D(\cdot;\ \theta_{D})\) is the discriminative decoder, tasked with predicting the semantic tokens of visible pixels","\mathcal{L}_{V}=\sum_{i\in V}\left\|f(x_{s_{i}})-f_{\phi}(x)_{s_{i}}\right\|^{2},","where $V$ denotes the set of visible clusters, $f(x_{s_{i}})$ is the model's output for cluster $s_{i}$, and $f_{\phi}(x)_{s_{i}}$ is the corresponding ground truth from the encoder."
ICML_2024_oral_107,1,"### Self-supervised Multi-task Learning

To power self-selective RAG, the LM needs two crucial abilities: accurate self-assessment and robustness to the retrieved context. We design a contrastive data labeling scheme to mine self-supervision from public repositories, followed by fine-tuning with a novel multi-task objective.

Data constructionWe leverage large-scale permissively licensed repositories from the Stack (Kocetkov et al., 2022) and create the fine-tuning data via a three-step procedure:

1. **Sample** target lines \(Y\) that are either (1) random code chunks of varied lengths or (2) function bodies.
2. **Retrieve**\(CC\) using the current file. We include \(Y\) in the query for 50% of the data2. Footnote 2: The main goal of the design is to align better with both non-iterative and iterative RAG use cases. During testing, a user may retrieve with both the in-file context and \(Y^{\prime}\), a model’s draft prediction, which results in a \(CC\) distribution close to that with \(Y\) in the query (Zhang et al., 2023).
3. **Label** whether extending the current file with \(CC\) can improve a code LM \(\mathcal{M}\)'s code completion quality by more than a threshold \(T\), measured by Edit Similarity (ES, definition in Section 4.1) against \(Y\).

The full algorithms are presented in Appendix D. After running the algorithm, we obtain the fine-tuning instances, each in the form \((X_{l},X_{r},\ Y,\ CC,\ label)\).

VerbalizationEach instance is verbalized into a sequence for fine-tuning. If \(label\) is false, only \(X_{l}\) and \(X_{r}\) are provided preceding \(Y\). Otherwise, we additionally provide \(CC\) after the special token \(<\)cc\(>\). The two verbalizations correspond to the two branches in Figure 2 (b).

Training ObjectiveWe introduce two losses, \(\mathcal{L}_{eval}\) for self-assessment and \(\mathcal{L}_{gen}\) for code generation.

1. \(\mathcal{L}_{eval}\): a cross-entropy loss on predicting \(<\)cc\(>\) immediately following \(<\)eof\(>\).","\mathcal{L}_{eval}=-\log p_{\mathcal{M}}(<\)cc\(>\)\(|X_{l},X_{r})",,"\begin{equation}
\mathcal{L}_{eval} = -\log P(<\text{cc}> \mid X_l, X_r, Y; \theta)
\end{equation}",Self-assessment loss for predicting the insertion of the retrieved context token $<\text{cc}>$ given the input context and target code $Y$.
ICML_2024_oral_107,2,"(1)
2. \(\mathcal{L}_{gen}\): a cross-entropy loss on the tokens following \(<\)fim_middle\(>\). Depending on \(label\), \(\mathcal{L}_{gen}\) represents either code completion with only in-file information or retrieval-augmented code completion.","\mathcal{L}_{gen}=\begin{cases}-\log p_{\mathcal{M}}(Y|X_{l},X_{r},CC),&\text{ if }label\\ -\log p_{\mathcal{M}}(Y|X_{l},X_{r}),&\text{otherwise}\end{cases}",,"\mathcal{L}_{gen}=-\sum_{t=1}^{|Y|} \log p_{\mathcal{M}}(y_t | X_{l},X_{r}, (<\)cc\(> CC)^{label}) , <\)fim_middle\(>)","Equation 2 defines the generation loss $\mathcal{L}_{gen}$ as a cross-entropy loss over the tokens in $Y$, conditioned on the input context and optionally the retrieved context $CC$ based on the label."
ICML_2024_oral_107,3,"(2)

The final training objective is \(\lambda\mathcal{L}_{eval}+\mathcal{L}_{gen}\), a weighted combination of the two losses. We do not supervise the model on predicting the other tokens in \(X_{l}\), \(X_{r}\), \(CC\), or the special tokens for fill-in-the-middle. Teacher forcing is used just as in normal causal language model training.

Figure 2: A comparison between fill-in-the-middle and self-selective RAG. We mark the end of the current file with a new token \(<\)eof\(>\), which triggers the LM’s self-evaluation. \(\rightarrow\) denotes the invocation of the LM. We color current-file context, retrieved contexts, and LM-generated parts in blue, green, and red respectively. fim_p, fim_s, and fim_m refer to the special tokens for fill-in-the-middle: fim_prefix, fim_suffix, and fim_middle. These tokens are already learned during the pre-training.

## 4 Experimental Setup

### Reproformer Implementation Details

Training DataWe sample Python repositories from the Stack (Kocetkov et al., 2022). Basic filtering are applied to retain 18k repositories that have (1) at least five Python files, (2) at least three imports per file, and (3) at least two local imports per file. These criteria ensure the existence of local dependencies where RAG could be helpful. We use \(\mathcal{M}\) = StarCoderBase-1B and \(T\) = 0 to label 240k chunk and 120k function completion instances. We reserve 500 repositories for validation and use the rest for training.

TrainingWe fine-tune the 1B, 3B, 7B, and 16B variants of StarCoderBase with \(\lambda=1.0\), maximum sequence length 2048, learning rate 2e-5, batch size 512, 50 warmup steps, and a linear learning rate decay. The models are trained for 2 epochs, which approximately takes 8, 12, 20, and 50 hours for the 1B/3B/7B/16B models respectively with 8 Nvidia A100 GPUs (40G memory). Our implementation is based on Jain et al. (2023)3. We will call our models Reproformer-1B/3B/7B/16B. We have also applied the same method to train a multilingual version of Reproformer on a mixture of Python, Java, C#, and Typescript repositories. As we focus on the methodological discussion in the main text, we refer interested readers to Appendix E.2 for the detailed experiment setup and results.

Footnote 3: [https://github.com/amazon-science/ContraCLM](https://github.com/amazon-science/ContraCLM)

Hyperparameter optimizationWe conduct a grid search with StarCoderBase-1B on the following search space: learning rate {1e-5, 2e-5, 5e-5}, \(\lambda\) {0.2, 1.0, 2.0, 5.0}, training epochs {1, 2, 5}, and warmup steps {50, 100}. The best hyperparameters are selected based on the code completion performance on the validation dataset.

### Evaluation Setup

Evaluation DatasetsWe evaluate on RepoEval (Zhang et al., 2023), which consists of line, API, and function completion tasks created from 32 Python repositories. To investigate the generalization to other languages, we also evaluated the original CrossCodeEval (Ding et al., 2023), which features line completion instances covering four languages: Python, Java, C#, and TypeScript (Appendix E.2). Observing that RepoEval has a limited repository coverage and that CrossCodeEval has a limited task coverage, we additionally leverage 1500 raw Python repositories from CrossCodeEval to create a new chunk and function completion benchmark, which we call CrossCodeLongEval. We detail the dataset creation process and basic statistics in Appendix D. For the rest of this paper, we will use CCEval to refer to both CrossCodeEval and CrossCodeLongEval interchangeably, and use the specific language and task (line, chunk, or function completion) to differentiate them.

Evaluation MetricsWe evaluate \(\hat{Y}\) with both _reference-based_ and _execution-based_ evaluation. For reference-based evaluation, exact match (EM) and edit similarity (ES) are reported. Following Zhang et al. (2023), ES is defined as","ES(\hat{Y},Y)=\frac{1-Lev(\hat{Y},Y)}{\max(|\hat{Y}|,|Y|)},",where \(Lev\) is the Levenshtein distance (Levenshtein et al,"\text{ES}(\hat{Y},Y)=\frac{\text{Levenshtein}(\hat{Y},Y)}{\max(|\hat{Y}|,|Y|)}",Edit Similarity (ES) measures the similarity between the predicted code $\hat{Y}$ and the ground truth $Y$ based on the Levenshtein distance normalized by the maximum length of the two sequences.
ICML_2024_oral_109,1,"### Robustness of CLIP as Zero-Shot Classifier

The CLIP model provides an image encoder \(\phi:I\rightarrow\mathbb{R}^{D}\) and a text encoder \(\psi:T\rightarrow\mathbb{R}^{D}\) which map inputs from different modalities into a joint \(D\)-dimensional space. Zero-shot classification of an image \(x\) on \(K\) classes can then be carried out by forming the text prompts \(t_{k}\) =""A photo of <class \(k\)>"" for all classes \(k=1,\dots,K\), and then choosing the class with the highest cosine similarity to the image embedding, i.e.","\operatorname*{arg\,max}_{k=1,\dots,K}\,\cos(\phi(x),\psi(t_{k})).",,"\begin{equation}
y = \arg\max_{k=1,\dots,K} \frac{\phi(x)^\top \psi(t_k)}{\|\phi(x)\|_2 \|\psi(t_k)\|_2}
\end{equation}",Zero-shot classification using cosine similarity between image embedding $\phi(x)$ and text prompt embeddings $\psi(t_k)$
ICML_2024_oral_109,2,"Since in this case the text prompts \(t_{k}\) are fixed, an image embedding function \(\phi\) defines a classifier \(f\) via its logits","f_{k}(\phi,x)=\cos(\phi(x),\psi(t_{k}))=\langle\frac{\phi(x)}{\|\phi (x)\|_{2}},\frac{\psi(t_{k})}{\|\psi(t_{k})\|_{2}}\rangle.",,"f(x) = \left[\cos\left(\phi(x), \psi(t_1)\right), \dots, \cos\left(\phi(x), \psi(t_K)\right)\right]^\top.","The classifier $f$ outputs a vector of cosine similarities between the image embedding $\phi(x)$ and each fixed text prompt embedding $\psi(t_k)$, forming the logits for zero-shot classification."
ICML_2024_oral_109,3,"Given an image \(x\) with label \(y\), an adversarial image \(z\) for the classifier \(f(\phi,\cdot)\) in the \(\ell_{p}\)-norm threat model satisfies:","\operatorname*{arg\,max}_{k=1,\dots,K}\,f_{k}(\phi,z)\neq y,\quad\|z-x \|_{p}\leq\varepsilon,\quad z\in I,",where \(\varepsilon\) is the perturbation size,"\begin{cases} \| z - x \|_p \leq \varepsilon, \\ f(\phi, z) \neq y. \end{cases}",An adversarial image $z$ must satisfy both the $\ell_p$-norm constraint and misclassification condition.
ICML_2024_oral_109,4,"ze. We focus on the \(\ell_{\infty}\)-threat model, and \(z\) can be found by standard attacks on image classifiers such as AutoAttack (Croce & Hein, 2020).

### Supervised Adversarial Fine-Tuning

Mao et al. (2023) suggest to make the vision encoder of CLIP robust by fine-tuning it with adversarial training (Madry et al., 2018) on ImageNet. Since the cross-entropy loss is used, the training objective of the approach of Mao et al. (2023), called TeCoA (text-guided contrastive adversarial training), is given by","L_{\text{TeCoA}}(y,f(\phi,x))=-\log(\frac{e^{f_{y}(\phi,x)}}{\sum_{k=1}^ {K}e^{f_{k}(\phi,x)}})",,"\min_{\phi}\mathbb{E}_{(x,y)\sim\mathcal{D}}\left[\max_{z\in\mathcal{B}_{\infty}(x,\varepsilon)}\left(-\log f_{y}(\phi,z)\right)\right],","the expected value over the data distribution of the maximum cross-entropy loss within the $\ell_{\infty}$-ball around each image $x$, where $\mathcal{B}_{\infty}(x,\varepsilon)$ denotes the set of all images within $\varepsilon$-bounded $\ell_{\infty}$ perturbation of $x$."
ICML_2024_oral_109,5,"Let \((x_{i},y_{i})_{i=1}^{n}\) denote the training set, then this can be written in the standard adversarial training formulation as","\phi_{\mathrm{FT}}=\operatorname*{arg\,min}_{\phi}\sum_{i=1}^{n}\max_{ \|z-x_{i}\|_{\infty}\leq\varepsilon}L_{\text{TeCoA}}(y_{i},f( \phi,z)),",where the inner problem is approximately solved with projected gradient descent (PGD) during training and \(\phi_{\mathrm{FT}}\) indicates the weights of the robust CLIP vision encoder,"L_{\text{TeCoA}} = \frac{1}{n} \sum_{i=1}^{n} \max_{\|z - x_i\|_\infty \leq \varepsilon} -\log\left(\frac{e^{f_{y_i}(\phi, z)}}{\sum_{k=1}^{K} e^{f_{k}(\phi, z)}}\right)",Standard adversarial training formulation of the TeCoA loss over a training set with $\ell_\infty$-bounded perturbations.
ICML_2024_oral_109,6,"er.

This approach has two main problems. First, adversarial training is done with respect to the fixed set of text embeddings of the classes of ImageNet. This does not take into account the effect on other text embeddings, e.g. of categories which are not part of ImageNet, and thus the fine-tuning can lead to heavy distortions with respect to unseen classes, which explains the high losses in standard performance for other down-stream zero-shot classification tasks, see Table 4. Second, the loss uses the cosine similarity, which effectively means that it only cares about the projection of the embedding on the hypersphere: one could multiply each \(\phi(x)\) by a different scalar factor \(\alpha(x)\) and the cosine similarity would be unaffected. Thus during fine-tuning it can happen that the embedding is changed along the radial direction in an arbitrary fashion. As other downstream tasks of CLIP, e.g. LVLMs (Alayrac et al., 2022; Liu et al., 2023b; Li et al., 2023a), use the unnormalized embedding this can again lead to huge performance losses. While for the first problem there is no easy solution, the second problem could be solved by retraining the part of the LVLM that connects the vision and language components. However, our approach solves both problems at the same time, so that we can get the benefits of our robust CLIP model and maintain good clean performance on _all_ down-stream tasks _without_ the need of fine-tuning or retraining.

### Unsupervised Adversarial Fine-Tuning of the Image Embedding

The CLIP embedding has been trained on 400M image-text pairs on the WIT dataset (Srinivasan et al., 2021) and provides very good zero-shot performance. Moreover, downstream tasks like LVLMs have been tuned using this embedding. Therefore, our goal is to make the vision encoder robust to adversarial attacks while preserving its output on clean points so that it retains clean zero-shot performance and does not require re-training or fine-tuning of components of down-stream tasks, like LVLMs. As discussed in the previous section, the supervised fine-tuning is not suited for this. Instead, we introduce an unsupervised adversarial fine-tuning scheme which is not bound to any specific dataset, and does not rely on the text encoder. In the following we denote with \(\phi_{\mathrm{Org}}\) the original CLIP encoder. Given an image \(x\), we propose the following embedding loss:","L_{\text{FARE}}(\phi,x)=\max_{\|z-x\|_{\infty}\leq \varepsilon}\|\phi(z)-\phi_{\mathrm{Org}}(x)\|_{2}^{2}.",,"\mathcal{L}_{\text{Unsup}}(x, z) = \|\phi_{\text{Org}}(x) - \phi_{\text{FT}}(z)\|_2^2 + \lambda \|\phi_{\text{FT}}(z)\|_2^2,","where the first term preserves the original embedding structure on clean inputs and the second term regularizes the fine-tuned embeddings to maintain a stable norm, preventing arbitrary radial distortions."
ICML_2024_oral_109,7,"This loss enforces that the features of perturbed points \(\phi(z)\) stay close to the unperturbed ones \(\phi_{\mathrm{Org}}(x)\) of the original CLIP model. Moreover, as \(L_{\text{FARE}}\) goes to zero, the embedding given by the fine-tuned model for clean images is the same as the one by the original model, that is \(\left\|\phi(x)-\phi_{\mathrm{Org}}(x)\right\|_{2}^{2}\to 0\): this implies that the fine-tuned CLIP vision encoder can be plugged into LVLMs without influencing their performance. For a set of images \((x_{i})_{i=1}^{n}\), our proposed fine-tuning scheme consists in optimizing","\phi_{\mathrm{FT}}=\operatorname*{arg\,min}_{\phi}\sum_{i=1}^{n}L_{\text{ FARE}}(\phi,x_{i}).",,"\phi_{\mathrm{FARE}}=\operatorname*{arg\,min}_{\phi}\sum_{i=1}^{n}L_{\text{FARE}}(\phi,x_{i}),",where the inner maximization is solved via projected gradient descent (PGD) and the outer minimization adjusts the vision encoder to maintain robustness while preserving the original CLIP embedding structure on clean inputs.
ICML_2024_oral_109,8,"The inner maximization problem in Eq. (3) of this feature-based variant of adversarial training can be solved by PGD. We call our proposed method _Fine-tuning for Adversarially Robust Embeddings_ (FARE).

While we focus here on CLIP and its down-stream tasks, our approach can be applied to any foundation model which has an intermediate embedding layer linking modalities.

The following result shows that preserving the image embedding, that is keeping the \(\ell_{2}\)-distance between original \(\phi_{\mathrm{Org}}\) and fine-tuned embedding \(\phi_{\mathrm{FT}}\) small, also preserves the cosine similarities between image and text embeddings, thereby maintaining zero-shot classification performance.

**Theorem 3.1**.: _Let \(\phi_{\mathrm{Org}},\phi_{\mathrm{FT}}\) be the original and fine-tuned image embeddings and \(\psi\) the text embedding of CLIP. Then_","|\cos(\phi_{\mathrm{FT}}(x),\psi(t))-\cos( \phi_{\mathrm{Org}},\psi(t))| || \leq \min(\frac{2}{\|\phi_{\mathrm{Org}}(x)\|_{2}}, \frac{2}{\|\phi_{\mathrm{FT}}(x)\|_{2}})\|\phi_{ \mathrm{FT}}(x)-\phi_{\mathrm{Org}}(x)\|_{2}.",,"\left|\cos(\phi_{\mathrm{FT}}(x),\psi(t))-\cos(\phi_{\mathrm{Org}}(x),\psi(t))\right|\leq\frac{2\left\|\phi_{\mathrm{FT}}(x)-\phi_{\mathrm{Org}}(x)\right\|_{2}}{\left\|\phi_{\mathrm{Org}}(x)\right\|_{2}}.","This inequality shows that preserving the ℓ₂-distance between original and fine-tuned image embeddings ensures the preservation of cosine similarities between image and text embeddings, maintaining zero-shot classification performance."
ICML_2024_oral_110,1,"## 2 Setting

In this section, we introduce and motivate our main setting. We assume that we have \(d\)-dimensional latent variables \(S\) such that their distribution satisfies \(\mathbb{P}\in\mathcal{P}\) for some class of probability distributions \(\mathcal{P}\). For our analysis of local isometries we make the following mild assumption.

**Assumption 2.1**.: We assume that the class of probability distributions \(\mathcal{P}\) has bounded support \(\Omega\subset\mathbb{R}^{d}\) with density lower and upper bounded.

In the second part of this work we focus on independent component analysis, i.e., we make the following assumption.

**Assumption 2.2**.: All \(\mathbb{P}\in\mathcal{P}\) have independent components, i.e., \(\mathbb{P}=\bigotimes_{i=1}^{d}\mathbb{P}_{i}\) for some measures \(\mathbb{P}_{i}\) on \(\mathbb{R}\) and we furthermore assume that the \(\mathbb{P}_{i}\) are non-Gaussian and have connected support.

The latent variables \(S\) are hidden, and we assume that we observe \(X=f(S)\) for some mixing function \(f\in\mathcal{F}(\Omega)\). Note that the distribution of \(X\) is then given by the push-forward \(f_{*}\mathbb{P}\). Here \(\mathcal{F}\) is some function class consisting of functions \(f:\Omega\subset\mathbb{R}^{d}\to\mathbb{R}^{D}\) and we will always assume without further notice that \(f\) is a diffeomorphism on its image (i.e., injective, differentiable and with differentiable inverse on its domain which is a submanifold of \(\mathbb{R}^{D}\)). Our main interest concerns the function class of local isometries, i.e.,",\begin{split}\mathcal{F}_{\mathrm{iso}}=\{f:\Omega\subset \mathbb{R}^{d}\to&\mathbb{R}^{D}:\ Df^{\top}(s)Df(s)=\mathrm{Id} \\ &\text{for all }s\in\Omega\}\end{split},,"\mathcal{F}_{\text{iso}} := \left\{ f \in \mathcal{F} : \forall s \in \Omega, \, \|Df(s)\|_{\text{op}} \leq 1 + \varepsilon \text{ and } \|Df^{-1}(f(s))\|_{\text{op}} \leq 1 + \varepsilon \right\},","where $Df(s)$ denotes the Jacobian of $f$ at $s$, $\|\cdot\|_{\text{op}}$ is the operator norm, and $\varepsilon \geq 0$ controls the deviation from exact isometry."
ICML_2024_oral_110,2,"for some connected domain \(\Omega\subset\mathbb{R}^{d}\). The class of local isometries has attracted substantial attention in representation learning because it locally preserves the structure of the data, which is a desirable feature in many settings (Tenenbaum et al., 2000; Donoho and Grimes, 2003; Belkin and Niyogi, 2003). Closely connected notions like the restricted isometry property play a crucial role in signal processing (Candes and Tao, 2005), and several works show that making (parts) of neural networks isometric improves performance (Qi et al., 2020; Liu et al., 2021; Miyato et al., 2018). In Gresele et al. (2021) it is argued based on the independent mechanism principle that the closely related function class of orthogonal coordinate transformations is a natural function class for representation learning. Essentially, the argument relies on the well-known fact that two isotropic random vectors are very close to orthogonal up to an exponentially small probability. In Appendix J we show a construction of a family of random functions that becomes increasingly isometric as \(D\to\infty\). Let us finally remark that the investigation of the inductive bias of VAEs in Rolinek et al. (2019); Zietlow et al. (2021); Reizinger et al. (2022) revealed that their loss function and architecture promote that the encoder implements an orthogonal coordinate transformation. The empirical success of VAEs on disentanglement tasks is therefore a further motivation to study theoretical properties of orthogonal coordinate transformations and local isometries.

For ICA with locally isometric mixing function the following identifiability result was shown.

**Theorem 2.3** (Theorem 1, Horan et al. (2021)).: _We assume that \(\mathcal{P}\) satisfies Assumption 2.2. Suppose \(X=f(S)\) where \(S\sim\mathbb{P}\in\mathcal{P}\) and \(f\in\mathcal{F}_{\mathrm{iso}}\). If \(X\mathrel{\mathop{=}\limits^{\mathcal{D}}}\tilde{f}(\tilde{S})\) for some \(\tilde{f}\in\mathcal{F}_{\mathrm{iso}}\) and \(\tilde{S}\sim\tilde{\mathbb{P}}\in\mathcal{P}\), then \(f=\tilde{f}\circ P\) for some linear map \(P\) which is a product of a permutation and reflections._

Note that Horan et al. (2021) only claimed disentanglement, but the stronger version stated presently follows as sketched below and is also a special case of Theorem 2 of Buchholz et al. (2022). The proof of this result proceeds in two steps. First, we can identify \(f\) up to an orthogonal linear transformation \(A\) by general results. Indeed, \(\tilde{f}^{-1}\circ f:\Omega\to\mathbb{R}^{d}\) is a local isometry and local isometries from \(\Omega\subset\mathbb{R}^{d}\to\mathbb{R}^{d}\) are affine. This is a result first shown by Liouville in the 19thcentury, a simple proof can be found in the recent review of Hyvarinen et al. (2023). A slightly different viewpoint showing the same result was given by Tenenbaum et al. (2000). After identifying \(S\) up to linear transformations, we can as a second step apply the standard identifiability result for linear ICA to identify \(A\) up to permutations and reflection (there is no scale ambiguity here because we consider local isometries).

We emphasize that this separation into two steps, where first identifiability up to linear transformations and then full identifiability are shown, is very common, e.g., a similar proof strategy has been applied for polynomial mixing functions (Ahuja et al., 2022), piecewise linear functions (Kivva et al., 2022), or for general mixing functions and interventional data (Buchholz et al., 2023).

## 3 Robustness and approximate Identifiablility

Theorem 2.3 proves identifiability, i.e., all data representations agree up to permutation and reflections. However, this uses crucially that the assumptions are exactly satisfied, i.e., \(f\) is a true local isometry, the coordinates of \(S\) are perfectly independent, and we know the distribution of \(X\) exactly.

In real-world settings, typically none of these assumptions will hold exactly but at best approximately ('all models are wrong' (Box, 1976)). Thus, identifiability results can only be relevant for applications when they are robust, i.e., they continue to hold in some approximate sense when the assumptions are mildly violated. For, e.g., supervised learning or distribution learning, there is a large body of research (essentially the field of learning theory) addressing in particular the dependence on the sample size but also misspecification attracted considerable attention, e.g., White (1980, 1982). For representation learning there are much fewer results. The sample complexity of linear ICA algorithms has been studied, e.g., by Yau and Bresler (1992); Tichavsky et al. (2006); Wei (2015). Moreover, Zhang and Chan (2008) investigated ICA for a perturbed linear model, although mostly empirically and without stringent quantitative results. To the best of our knowledge, no robustness results are known for non-linear mixing functions \(f\). The main focus of this work is to show a first robustness result for a misspecification of the function class, while we do not consider misspecification of independence of the ground truth samples and finite sample effects here.

Let us emphasize that it is an important question because not all identifiability results have a meaningful extension to slight misspecification. As a simple motivating example, consider analytic functions (i.e., functions that can be expressed as a power series) and smooth functions (i.e., functions that have arbitrary many derivatives). While those function classes are superficially similar, they exhibit profound differences: Analytic functions \(f:\mathbb{R}\rightarrow\mathbb{R}\) have the property that there is a unique continuation of \(f|_{U}\) from any arbitrarily small open set \(U\) to a maximal domain of definition (this can be seen as arbitrary far o.o.d. generalization) while for smooth functions virtually no information about \(f|_{U}\) can be deduced from \(f|_{U}\). This indicates that minor misspecification (analytic vs. smooth) can render identifiablity results void, while the representation capacity remains almost the same and the difference can hardly be detected from data. To connect this more closely to identifiability in representation learning, consider, e.g., an identifiability result such that the following properties hold (a similar discussion can be found in the Appendix of Buchholz et al. (2023))

* When assuming that the mixing \(f\) is in some function class \(\mathcal{F}\) then \(f\) is identifiable (up to certain symmetries, e.g., permutations or linear maps)
* When the mixing is just assumed to be an injective and continuous function \(f\) is not identifiable, i.e., for \(f\in\mathcal{F}\) there is \(f^{\prime}\) giving rise to the same observational distribution (for suitable latent distributions) but \(f\) and \(f^{\prime}\) are entirely different data representations that are not related by the allowed symmetries.
* The function space \(\mathcal{F}\) is dense in the space of injective continuous functions, i.e., every injective and continuous function \(f^{\prime}\) can be approximated to arbitrary precision by \(f\in\mathcal{F}\).

Note that the last property implies that for \(f^{\prime}\notin\mathcal{F}\) no meaningful distance to \(\mathcal{F}\) can be defined, as it can be approximated arbitrarily well by functions in \(\mathcal{F}\). This also makes the identifiability result brittle: While \(f\in\mathcal{F}\) can be identified, there is a sequence of functions \(\mathcal{F}\ni f_{i}\to f^{\prime}\) converging to the spurious solution \(f^{\prime}\). Thus, it can be arbitrarily difficult to decide in practice whether \(f\) generated the data or an approximation \(f_{i}\in\mathcal{F}\) of the spurious solution \(f^{\prime}\). Concrete examples of this setting include polynomial mixing functions and piecewise linear mixing functions which can be used to identify, e.g., Gaussian or rectangular base distributions (Ahuja et al., 2022; Kivva et al., 2022). We emphasize that these results nevertheless are important theoretical contributions, however, these identifiability results might be difficult to exploit in practice. Our observations here are in spirit similar to a no free lunch theorem (Wolpert and Macready, 1997) because it clarifies the fundamental tradeoff between representation capacity of the function class and identifiability. Note that the lack of identifiability can also harm downstream performance, as shown by Saengkyongam et al. (2023).

Let us try to make the point above concrete in a toy setting. We consider two-dimensional Gaussian latent variables \(Z\)Let \(f\) be a piecewise linear mixing function \(f\) that approximates a radius dependent rotation \(m\)(Hyvarinen and Pajunen, 1999) which is a map such that \(m(Z)\xrightarrow{\mathcal{D}}Z\). Consider observations \(X=f(Z)\xrightarrow{\mathcal{D}}Z\). Note that \(f\) and the identity map generate similar observations so it is hard to distinguish between the mixing function \(f\) and the identity \(\mathrm{id}\) even though \(f\) is identifiable for Gaussian latent variables up to linear transformations (see Kivva et al. (2022)). However, if we train a VAE with ReLU activations that is initialized with \(f\) and \(f^{-1}\) as decoder and encoder, i.e., we initialize it with the true mixing, it nevertheless fails to preserve this representation. Instead, the VAE learns a simple orthogonal transformation of the data, i.e., even though the map \(f\) is in theory identifiable the learning signal is not sufficient to overcome the inductive bias of the learning method (here the VAE) towards simple functions that, e.g., preserve simple geometric properties. An illustration of the learned mixing can be found in Figure 1. Additional details for the setting can be found in Appendix H.

The main purpose of this work is to show that the function class \(\mathcal{F}_{\mathrm{iso}}\) behaves differently, i.e., robust representation learning results can be proved. To formalize this, we first need to introduce a way to measure the distance \(\Theta(f)\) of a function \(f\) to the space of local isometries \(\mathcal{F}_{\mathrm{iso}}\). This quantity should vanish (\(\Theta(f)=0\)) when \(f\in\mathcal{F}_{\mathrm{iso}}\) and our goal is to show that when \(\Theta(f)\) is small we can still approximately identify \(f\). For functions \(f:\Omega\subset\mathbb{R}^{d}\rightarrow\mathbb{R}^{d}\) we use the following quantity","\begin{split}\Theta_{p}^{p}(f,\Omega)=\int_{\Omega}& \mathrm{dist}(Df(s),\mathrm{SO}(d))^{p}\\ &+\mathrm{dist}\big{(}(Df)^{-1}(s),\mathrm{SO}(d)\big{)}^{p} \,\mathrm{d}s.\end{split}",,"\begin{equation}\Theta(f)=\sup_{s\in\Omega}\left\|Df^{\top}(s)Df(s)-\mathrm{Id}\right\|_{\mathrm{F}},\end{equation}","Measure of distance of a function $f$ to the space of local isometries $\mathcal{F}_{\mathrm{iso}}$, defined via the Frobenius norm of the deviation of $Df^{\top}(s)Df(s)$ from the identity matrix at each point $s\in\Omega$."
ICML_2024_oral_110,3,"Here \(\mathrm{dist}^{2}(A,\mathrm{SO}(d))=\min_{Q\in\mathrm{SO}(d)}|Q-A|^{2}\) refers to the euclidean distance to the space \(\mathrm{SO}(d)=\{A\in\mathbb{R}^{d\times d}:AA^{\top}=\mathrm{Id},\,\det A=1\}\) of all rotations. We discuss the definition and properties of the distance in more detail in Appendix C. The distance \(\Theta\) (which is not a distance in the mathematical sense) measures how close a function \(f\) is to a local isometry pointwise and integrates this. If \(f\in\mathcal{F}_{\mathrm{iso}}(\Omega)\) is a local isometry and if \(df>0\) everywhere (which is no loss of generality by reflecting one coordinate), then indeed \(\Theta_{p}(f,\Omega)=0\) because \(Df\in\mathrm{SO}(d)\) pointwise. For our results we need the second term in (2), i.e., we measure the local deviation from being an isometry of the map and its inverse. One could also define the metric by integrating with respect to the latent distribution \(\mathbb{P}\) instead of the Lebesgue measure, but we think that the latter is slightly more natural because a common assumption based on the independent mechanism assumption (Janzing and Scholkopf, 2010; Scholkopf et al., 2012; Janzing et al., 2016) is that \(\mathbb{P}\) and \(f\) are sampled independently. This also simplifies our analysis slightly.

Our main interest concerns high dimensional embeddings, i.e., mixing function \(f:\mathbb{R}^{d}\to M\subset\mathbb{R}^{D}\) for \(d\ll D\) where \(M\) is an embedded submanifold. In this case, the definition (2) needs to be slightly adapted, by essentially restricting the target of \(Df\) to the tangent space of \(M\). For \(D\geq d\) we define","\begin{split}\Theta_{p}^{p}(f,\Omega)=&\int_{\Omega }\mathrm{dist}^{p}(Df(z),\mathrm{SO}(d,T_{f(z)}M))\\ &+\mathrm{dist}^{p}\big{(}(Df)^{-1}(z),\mathrm{SO}(T_{f(z)}M,d) \big{)}\,\mathrm{d}z\end{split}","where \(T_{f(z)}M\) denotes the tangent space of \(M\) at \(f(z)\) and \(\mathrm{SO}(d,T_{f(z)}M)\) denotes the set of orthogonal matrices \(Q\in\mathbb{R}^{D\times d}\) (i","\begin{split}\Theta_{p}^{p}(f,\Omega)=\int_{\Omega}& \mathrm{dist}(Df(s),\mathrm{SO}(d,D))^{p}\\ &+\mathrm{dist}\big{(}(Df)^{-1}(s),\mathrm{SO}(d,D)\big{)}^{p} \,\mathrm{d}s,\end{split}","Here, $\mathrm{SO}(d,D)$ denotes the set of linear maps from $\mathbb{R}^d$ to $\mathbb{R}^D$ that are isometric embeddings, generalizing rotations to the case of embeddings into higher-dimensional spaces."
ICML_2024_oral_110,4,"(i.e., \(Q^{\top}Q=\mathrm{Id}_{d}\)) with range in \(T_{f(z)}M\) that are orientation preserving (here we fix any orientation

Figure 1: (Left) Color map of Gaussian latent variable \(Z\), (Center) Color map of the transformed data \(X=f(Z)\) where \(f\) is a piecewise linear approximation of a radius dependent rotation (i.e., \(f(Z)\xrightarrow{\mathcal{D}}Z\)), (Right) Representation \(X^{\prime}=f^{\prime}(Z)\) learned by a VAE with ReLU activation functions initialized with \(f\) and \(f^{-1}\) for decoder and encoder and small variance.

on \(M\)). To interpret the second summand, we need to associate certain matrix representations to the maps \((Df)^{-1}\) and \(\operatorname{SO}(T_{f(z)}M,d)\) and we refer to Appendix D for a careful definition.

Let us now discuss and motivate the choice of distance in a bit more detail. First, we emphasize that (2) does not (directly) quantify how well \(f\) is globally approximated by a local isometry, i.e., whether there exists a \(g\in\mathcal{F}_{\operatorname{iso}}\) such that, e.g., \(\|f-g\|_{1}\) is small. We note that from a modelling standpoint, it seems more reasonable to assume that (2) is small for the ground truth mixing function than assuming that \(f\) can be globally well approximated by a local isometry because the former corresponds to the common assumption that \(f\) preserves the structure of the data locally (in particular distances) while it is not clear why \(f\) should be globally constrained. In Appendix J we show that for \(d\ll D\) and certain classes of random functions \(\Theta\) will be indeed small. This quantifies the observation that those random functions become isometric as \(D\to\infty\) (similar results can be found in Reizinger et al. (2023)).

One difficulty in the setting is that as soon as we allow misspecification of \(f\) the mixing function \(f\) will typically be no longer identifiable (otherwise the existing identifiability result could be strengthened to a larger function class). We focus mostly on the setting of nonlinear ICA and in this case, it is well known that \(f\) is not identifiable. Thus, we can only hope to prove a version of approximate identifiability that gracefully recovers the classical identifiability result as the misspecification disappears. We here define approximate identifiability of the latent variables based on the mean correlation coefficient (MCC) which has been used before to evaluate the empirical performance of representation learning algorithms. For a pair of \(d\) dimensional random variables \((\tilde{S},\tilde{S})\) the MCC is defined by","\operatorname{MCC}(\tilde{S},\tilde{S})=\max_{\pi\in S_{d}}d^{-1}\sum_{i=1}^{ d}|\rho(S_{i},\tilde{S}_{\pi(i)})|","where \(\rho(X,Y)=\operatorname{Cov}(X,Y)/(\operatorname{Var}(X)\operatorname{Var}( Y))^{1/2}\) denotes the correlation coefficient","\begin{split}\mathrm{MCC}(\tilde{S},\tilde{S}^{\prime})=\max_{\pi}\frac{1}{d}\sum_{i=1}^{d}|\operatorname{Corr}(\tilde{S}_{i},\tilde{S}^{\prime}_{\pi(i)})|,\end{split}","The mean correlation coefficient (MCC) measures the average absolute correlation between corresponding components of two random variables, maximized over permutations $\pi$, providing a metric for approximate identifiability of latent variables."
ICML_2024_oral_110,5,"nt. Note that \(\operatorname{MCC}(\tilde{S},\tilde{S})=1\) implies that \(S_{i}=\lambda_{i}\tilde{S}_{\pi(i)}\), for a vector \(\lambda\in\mathbb{R}^{d}\), i.e., we recover \(Z\) up to permutation and scale. For MCC values close to 1 this is approximately true, in particular \(\tilde{S}=PS+\varepsilon\) holds for a scaled permutation \(P\) and some error \(\varepsilon\) that vanishes as the MCC goes to 1. More generally, one could also allow coordinatewise reparametrizations of \(\tilde{S}\) when those are not identifiable (Gresele et al., 2021), but this is not necessary when considering (perturbed) local isometries.

Let us elaborate on the difference between proper identifiability (Xi & Bloem-Reddy, 2023) and approximate identifiability. Recall that a proper identifiability results is of the form \(f_{\ast}\mathbb{P}=\tilde{f}_{\ast}\tilde{\mathbb{P}}\) for \(f,\tilde{f}\in\mathcal{F}\) and \(\mathbb{P},\tilde{\mathbb{P}}\in\mathcal{P}\) implies that \(\tilde{f}^{-1}\circ f\) is a tolerable ambiguity, e.g., a combination of permutation and reflection in Theorem 2.3. In this case, an equivalent statement is that \(f_{\ast}\mathbb{P}=\tilde{f}_{\ast}\tilde{\mathbb{P}}\) implies \(\operatorname{MCC}(\tilde{f}^{-1}\circ f(S),S)=1\) for \(S\sim\mathbb{P}\). In contrast, for approximate identifiability results we cannot simply take any \(\tilde{f}\) such that \(\tilde{f}_{\ast}\tilde{\mathbb{P}}=f_{\ast}\mathbb{P}\) because there will typically exist arbitrarily chaotic spurious solutions with no guarantee on \(\operatorname{MCC}(\tilde{f}^{-1}\circ f(S),S)\) for \(S\sim\mathbb{P}\). Instead, we need to make a specific choice for the function \(\tilde{f}\) such that \(\tilde{f}_{\ast}\tilde{\mathbb{P}}=f_{\ast}\mathbb{P}\) which ensures that \(\operatorname{MCC}(\tilde{f}^{-1}\circ f(S),S)\approx 1\) for a suitable class of \(f\). In our case those will be functions such that \(\Theta_{p}(f,\Omega)\) is small. Of course, our choice of \(\tilde{f}\) is only allowed to depend on the observations \(X=f(S)\) but not on \(f\) itself. Here the basic idea is to choose \(\tilde{f}\) as close as possible to being a local isometry, i.e., we roughly chose \(\tilde{f}\) such that \(\Theta(\tilde{f},\tilde{\Omega})\) is minimal under the constraint \(\tilde{f}_{\ast}\tilde{\mathbb{P}}=f_{\ast}\mathbb{P}\). Let us now provide a very informal version of our final approximate identifiability result.

**Theorem 3.1** (Informal sketch).: _Suppose \(S\sim\mathbb{P}\) where \(\mathbb{P}\) has independent components, and we observe \(X=f(S)\) for some mixing function \(f\). Then we can find \(\tilde{f}\) such that \(\hat{S}=\tilde{f}^{-1}(X)\) satisfies for some \(C>0\), \(p>1\)_","\operatorname{MCC}(\hat{S},S)\geq 1-C\Theta_{p}^{2}(f)",,"\operatorname{MCC}(S,\hat{S})\geq 1 - C\,\Theta_{p}(f,\Omega).","Approximate identifiability result showing that the mean correlation coefficient between the true latent variables $S$ and the estimated latent variables $\hat{S} = \tilde{f}^{-1}(X)$ is bounded below by a constant minus a term proportional to the distance $\Theta_p(f, \Omega)$ of the mixing function $f$ to the space of local isometries."
ICML_2024_oral_110,6,"_where \(C\) depends on everything except \(f\)._

Note that if \(f\) is far away from a local isometry, i.e., \(\Theta_{p}(f)\) is large then the right-hand side is negative, and the obtained recovery guarantee is void which is not surprising as nonlinear ICA is not identifiable. The actual statement can be found in Theorem 5.11. We emphasize again that robustness is an aspect of representation learning that has previously attracted little attention, and we study one specific example, namely robustness of learning approximate local isometries. However, there are many alternative robustness questions and possible misspecifications, e.g, we could consider latent sources that are only approximately independent or settings where MCC might not be the right measure of identifiability.

## 4 Approximate linear Identifiability for approximate local Isometries

We now extend linear identifiability for locally isometric embeddings to approximate isometric embeddings. Restricting the ambiguity to a linear transformation is already an important first step which can be in principle combined with any result on causal representation learning for linear mixing functions, examples of recent results for linear mixing functions include Ahuja et al. (2023); Squires et al. (2023); Varici et al. (2023). The key ingredient for our results is the rigidity statement Theorem E.1 in Appendix E which played an important role in the mathematical analysis of elastic materials (see Ciarlet (1997) for an overview).

The main consequence of this theorem that is relevant for our work here is the bound","\begin{split}\min_{L}&\|u-L\|_{L^{q}(\Omega)}\\ &\leq C(\Omega,p)(\int_{\Omega}\mathrm{dist}(Du(s),\mathrm{SO }(d))^{p}\,\mathrm{d}s)^{\frac{1}{p}}\end{split}",,"\inf_{Q\in\mathrm{SO}(d)}\|Df-Q\|_{L^{p}(\Omega)}\leq C\Theta_{p}(f,\Omega)","where $C$ is a constant depending on the domain $\Omega$ and the dimension $d$, and $\mathrm{SO}(d)$ is the group of $d \times d$ rotation matrices."
ICML_2024_oral_110,7,"for all \(u\) and \(q=pd/(d-p)\) where the minimum is over functions of the form \(L(s)=As+b\) with \(A\in\mathrm{SO}(d)\). This result shows that when a map \(u\) has gradient pointwise close to any, potentially varying, rotation, it is globally close to an affine map. We emphasize that this is highly non-trivial, naively one would expect that the rotation could change with \(s\). Note that when the right hand-side is zero, i.e., \(f\) is a local isometry then we conclude that \(u\) is affine recovering the fact that local isometries from \(\mathbb{R}^{d}\) to \(\mathbb{R}^{d}\) are already affine which is the key observation underlying Theorem 2.3. This rigidity property of almost isometries renders them almost identifiable. This then generalizes Theorem 2.3.

For simplicity, we mostly present our results for the case where \(D=d\) in the main paper. The additional technicalities for \(d<D\) (which is the main interest of our results) will be discussed in Appendix D. For a measure \(\mathbb{P}\in\mathcal{P}\) with support \(\Omega\subset\mathbb{R}^{d}\) we introduce the set of models","\begin{split}\mathcal{M}(f_{*}\mathbb{P})=\{&(g, \mathbb{Q},\Omega^{\prime}):g\in\mathcal{F}(\Omega^{\prime}),\text{ where }\\ g_{*}\mathbb{Q}=f_{*}\mathbb{P},\,\mathrm{supp}(\mathbb{Q})= \Omega^{\prime}\}\end{split}",,"\begin{split}\mathcal{M}_{p}(\mathbb{P},\varepsilon)=\{f:\Omega\subset \mathbb{R}^{d}\to&\mathbb{R}^{d}:\ \Theta_{p}(f,\Omega)\leq\varepsilon,\\ &f\text{ is a diffeomorphism}\}\end{split}","Set of diffeomorphic mixing functions $f$ that are $\varepsilon$-close to being local isometries in the $\Theta_p$-measure, forming a model class for approximate identifiability analysis."
ICML_2024_oral_110,8,that generate the observed distribution of \(X=f(S)\). We now consider for a fixed \(p\) the triple,"\begin{split}&(g,\mathbb{Q},\Omega^{\prime})\in\\ &\operatorname*{argmin}_{(\bar{g},\bar{Q},\Omega)\in\mathcal{M}(f,\mathbb{P})}\int_{\Omega}\mathrm{dist}((D\bar{g})^{-1}(g(s)),\mathrm{SO}(d))^ {p}\,\bar{\mathbb{Q}}(\mathrm{d}s),\end{split}",,"\begin{split}(g_{p},\mathbb{Q}_{p},\Omega^{\prime}_{p})=\arg\min_{(g,\mathbb{Q},\Omega^{\prime})\in\mathcal{M}(f_{*}\mathbb{P})}&\Theta_{p}(g,\Omega^{\prime})\\ &\text{subject to } \mathbb{Q}=\bigotimes_{i=1}^{d}\mathbb{Q}_{i}.\end{split}","Equation 8 defines the triple $(g_p, \mathbb{Q}_p, \Omega'_p)$ that minimizes the deviation from local isometry, measured by $\Theta_p(g, \Omega')$, among all models that generate the same observed distribution as $f_*\mathbb{P}$, under the constraint that the latent distribution $\mathbb{Q}$ has independent components."
ICML_2024_oral_110,9,"i.e., we pick \((g,\mathbb{Q},\Omega^{\prime})\) such that \(X\sim g_{*}\mathbb{Q}\) so that it generates the observational distribution and in addition make its inverse as isometric as possible. For \(D>d\) we replace the distance to \(\mathrm{SO}(d)\) by the distance to \(\mathrm{SO}(T_{g(s)}M,d)\) (see (73) in Appendix E for details). We emphasize that \(\mathcal{M}(f_{*}\mathbb{P})\) and therefore \(g\) only depends on the observational distribution \(f_{*}\mathbb{P}\) but not on \(f\) directly. Note that we could also minimize a variant of \(\Theta(g,\Omega^{\prime})\) where we integrate with respect to \(\mathbb{Q}\). We remark in passing that we do not prove the existence of a minimizer of (8) (although this should be possible using the direct method of the calculus of variations). Since the functional is lower bounded, a minimizing sequence \(g_{n}\) exists and we can instead approximate the infimum up to arbitrarily small \(\varepsilon\) adding an arbitrarily small additional error term to Theorem 4.1 below (see Appendix E for details).

**Theorem 4.1**.: _Suppose we have a latent distribution \(\mathbb{P}\in\mathcal{P}\) satisfying Assumption 2.1 with support \(\Omega\subset\mathbb{R}^{d}\) where \(\Omega\) is a bounded connected Lipschitz domain. The observational distribution is given by \(X=f(S)\in M\subset\mathbb{R}^{D}\) where \(S\sim\mathbb{P}\) and \(f\in\mathcal{F}(\Omega)\). Fix a \(1<p<\infty\). Define \((g,\mathbb{Q},\Omega^{\prime})\) as in (8) (as in (73) in Appendix D in the undercomplete case). Then there is \(A\in\mathrm{SO}(d)\) such that \(g^{-1}\circ f(s)=As+h(s)+b\) and \(h\) satisfies for \(p<d\) and \(q=pd/(d-p)\) the bound_","\|h\|_{\mathbb{P},q}\leq C_{1}\Theta_{p}(f,\Omega).",,"\begin{split}\|h\|_{L^{q}(\Omega)}\leq C(\Omega,p,d)\bigg(&\int_{\Omega}\mathrm{dist}^{p}(Df(s),\mathrm{SO}(d))\\ &+\mathrm{dist}^{p}((Df)^{-1}(s),\mathrm{SO}(d))\,\mathrm{d}s\bigg)^{\frac{1}{p}}.\end{split}","Bound on the deviation $h$ from linearity of the composition $g^{-1} \circ f$ under approximate local isometry conditions, quantifying robustness in terms of the distance to $\mathrm{SO}(d)$."
ICML_2024_oral_110,10,"_Here \(C_{1}\) is a constant depending on \(d\), \(p\), \(\Omega\), and the lower and upper bound on the density of \(\mathbb{P}\)._

The proof of this result, including the extension to the undercomplete case can be found in Appendix E. Let us continue our discussion on the meaning of approximate identifiability in the context of this theorem. First, we note that if \(\Theta_{p}(f,\Omega)\) is not small we obtain no useful statement, except that our transformed data is some function of the original data. This is not surprising: We cannot hope to recover any mixing function because the problem of learning \(f\) from \(f_{*}\mathbb{P}\) is not identifiable (even when assuming \(\mathbb{P}\) known). Moreover, our statement only applies to one specific unmixing \(g\) which is again unavoidable for the same reason. What we show is that if \(f\) and \(g\) are both close to being locally isometric, then the concatenation \(g^{-1}f\) will be close to a linear (even orthogonal) map. Note that \(g\) does not appear on the right-hand side of (9) because we choose \(g\) to be the maximally isometric representation of our observations, but we know that this representation is more isometric than any alternative representation, in particular more isometric than \(f\). This allows us to bound the non-linearity \(h\) in terms of \(f\) only. While this result does not have the simplicity of a standard identifiability result it provides a more general viewpoint. Indeed, if \(\Theta_{p}(f,\Omega)=0\), i.e., when \(f\) is a local isometry we have \(h=0\) and we recover \(f\) up to a linear transformation which is the standard identifiability result for local isometries (see Theorem 2 of Horan et al. (2021)). Our result extends this gracefully to functions that are approximate local isometries in the sense that \(\Theta_{p}(f,\Omega)\) is small. Let us add some remarks about this result.

_Remark 4.2_.:
* The optimization problem for \(g\) in (8) is non-convex, and difficult to optimize in practice.
* For \(D=d\) the introduction of \(g\) is not necessary, instead we can directly apply Theorem E.1 to \(f\) and just work with the original data \(X\) and directly apply Theorem E.1 to \(X=f(S)\).
* The assumptions that \(\Omega\) is connected and that the density of \(\mathbb{P}\) is lower bounded are necessary. In particular the result does not apply to distributions \(\mathbb{P}\) with disconnected support.
* There are alternative assumptions that allow us to remove (or bound) the second term in (2), e.g., assuming \(Df^{\top}Df>c_{1}>0\), i.e., the smallest singular value of \(Df\) is bounded below is sufficient (see Lemma C.2).

## 5 Perturbed linear ICA

In this section, we consider the problem of independent component analysis where the mixing is a slight perturbation of a linear function. This is a problem of general interest beyond the main setting considered in this paper because in typical real-world applications the mixing will only be approximately linear, so understanding the effect of the non-linear part is important. Concretely, we assume that data is generated by a perturbed linear model",x=f(s)=As+\eta h(s),where \(h:\mathbb{R}^{d}\to\mathbb{R}^{d}\) is a non-linear function and \(\eta\in\mathbb{R}\) is a small constant,"\begin{split}X=AS+\varepsilon(S),\end{split}","where $ A \in \mathbb{R}^{d \times d} $ is an invertible matrix representing the linear part of the mixing, and $ \varepsilon(S) $ is a small nonlinear perturbation that depends on the latent variables $ S $. This model assumes that the observed data $ X $ is generated by a mixing function that is close to linear, with deviations captured by $ \varepsilon(S) $, which is typically assumed to be smooth and bounded."
ICML_2024_oral_110,11,"nt. We can assume that \(h\) is centered, i.e., \(\mathbb{E}(h(S))=0\) and that \(A\) is the regression matrix when regressing \(X\) on \(S\). This is equivalent to the relation \(\mathbb{E}_{S}(Sh(S)^{\top})=0_{d\times d}\).

_Remark 5.1_.: We decided to express the nonlinear part of the model as \(\eta h\) which allows us to consider \(\eta\to 0\) to shrink the size of the perturbation. As we check carefully in the proofs, all bounds only depend on \(h\) through its norm \(\|h\|_{\mathbb{P},q}\) for some \(q\) specified below.

It is clear that \(f\) cannot be identified from the distribution of \(X\) as the mixing is nonlinear. Instead, we investigate how well the linear part given by the matrix \(A\) can be recovered and to what degree we can recover the ground truth sources \(S\). Our results show that as \(\eta\to 0\) we recover the identifiability results for linear ICA. One issue that creates substantial theoretical problems in this work and in general is that there is a gap between the statistical and computational aspects of ICA, in the sense that linear ICA is identifiable for non-Gaussian latent variables, however ICA algorithms typically require a stronger non-degeneracy condition on the latent distribution than non-Gaussianity of the latent variables. In the misspecified setting we need to define a (not necessarily computationally feasible) algorithm to pick a mixing function for the observed data, which then results in the same limitations as the conventional algorithms face.

Let us now quickly review how the independent components can be identified. Most ICA algorithms consider a function \(H:S^{d-1}\to\mathbb{R}\) defined by",H(w)=\mathbb{E}G(w^{\top}\Sigma_{X}^{-\frac{1}{2}}X),where \(\Sigma_{X}\) denotes the covariance matrix of \(X\) so that \(\Sigma_{X}^{-\frac{1}{2}}X\) is whitened and \(G\) is the so-called contrast function,"\begin{split}x= f(s)=As+&\eta h(s),\\ &\mathbb{E}(h(S))=0,\,\mathbb{E}_{S}(Sh(S)^{\top})=0\end{split}",where $h:\mathbb{R}^{d}\to\mathbb{R}^{d}$ is a centered nonlinear function and $\eta\in\mathbb{R}$ is a small constant.
ICML_2024_oral_110,12,"on. Then, under a suitable degeneracy assumption, the independent components correspond to an orthogonal set of local extrema of \(H\).

Indeed, as a motivation for the nonlinear case we discuss in Appendix F the well-known calculation that for linear mixing functions (i.e., \(\eta=0\)) \(H\) has a local extremum if \(w^{\top}\Sigma_{X}^{-\frac{1}{2}}X=S_{i}\) for some \(i\), i.e., when \(A^{\top}\Sigma_{X}^{-\frac{1}{2}}w=e_{i}\). Note that for linear mixing \(A\) the relation \(\Sigma_{X}=AA^{\top}\) holds, and the vectors",\bar{w}_{i}=(AA^{\top})^{\frac{1}{2}}A^{-\top}e_{i},,\Sigma_{X}^{-\frac{1}{2}}X=e_{i},"correspond to the independent components when $ w $ is aligned with the rows of $ A $, establishing a link between the extrema of $ H $ and the latent variables $ S $."
ICML_2024_oral_110,13,"thus satisfy \(\bar{w}_{i}^{\top}\Sigma_{X}^{-\frac{1}{2}}X=S_{i}\) and \(|\bar{w}_{i}|=1\). We also consider the matrix \(\bar{W}=A^{-1}(AA^{\top})^{\frac{1}{2}}\) which has rows \(\bar{w}_{i}=\bar{W}^{\top}e_{i}\) and satisfies \(\bar{W}\Sigma_{X}^{-\frac{1}{2}}X=S\) for \(\eta=0\).

The main goal of this section is to show that this general picture remains approximately true in the perturbed setting, i.e., under minor regularity assumptions on \(G\) there is a matrix \(W\) close to \(\bar{W}\) such that its rows are local extrema of \(H\). Let us collect the necessary assumptions for our results.

**Assumption 5.2**.: The function \(G\) is even, three times differentiable, and there are constants \(C_{g}\) and \(d_{g}\) such that for \(k\leq 3\)","|G^{(k)}(x)|\leq C_{g}(1+|x|)^{\max(d_{g}-k,0)}",where \(G^{(k)}\) denotes the \(k\)-th derivative of \(G\),\bar{w}_{i}^{\top}\Sigma_{X}^{-\frac{1}{2}}X=S_{i},Condition under which the rows of the matrix $\bar{W}$ correspond to the independent components in the linear ICA setting.
ICML_2024_oral_110,14,"\).

We will write \(g=G^{\prime}\) from now on following the notation in the field. Note that commonly used contrast functions like \(G=\ln\circ\cosh\) or \(G=|\cdot|^{4}\) satisfy this assumption. We need some regularity assumption on the source variables.

**Assumption 5.3**.: Write \(q=\max(d_{g},3)\). Assume that the latent sources \(S\) satisfy for some constant \(M<\infty\)",\mathbb{E}(|S|^{q})=M,,|G^{(k)}(w^{\top}\Sigma_{X}^{-\frac{1}{2}}X)|\leq M\quad\text{a.s. for all }k\leq 3,where $G^{(k)}$ denotes the $k$-th derivative of $G$
ICML_2024_oral_110,15,"Finally, we need a condition that ensures that the contrast function can single out the latent variable \(S_{i}\) which is also necessary in the linear case.

**Assumption 5.4**.: The latent variables \(S\) satisfy for an \(\alpha_{i}\neq 0\)",\mathbb{E}(S_{i}g(S_{i})-g^{\prime}(S_{i}))=\alpha_{i}.,,\mathbb{E}\left(g^{\prime}\left(e_{i}^{\top}S\right)\right) = \alpha_{i} \neq 0,where $\alpha_{i}$ is a non-zero constant related to the contrast function's sensitivity to the $i$-th component of $S$.
ICML_2024_oral_111,1,"### Semi-Supervised Baseline

We introduce a semi-supervised baseline for RES based on a teacher-student network structure. This approach unfolds in two stages:

**Stage1: Burn-In Stage.** In the semi-supervised teacher-student framework, achieving proper parameter initialization is crucial for accelerating the convergence of training during the mutual learning stage (Liu et al., 2021). During the Burn-In stage, we train the pre-trained model using only labeled data. The optimization objective is defined as follows:","\mathcal{L}_{sup}=\frac{1}{H\times W}\sum_{j=1}^{H\times W}\mathcal{L}_{BCE} (M_{i,j}^{l},Y_{i,j}^{l}),","where \(M_{i,j}^{l}\) denotes the prediction mask of Burn-In model for the \(j\)-th pixel of \(i\)-th labeled image, \(Y_{i,j}^{l}\) denotes the corresponding ground truth, \(\mathcal{L}_{BCE}\) denotes binary cross entropy loss (Csiszar, 2008)","\mathcal{L}_{\text{Burn-In}} = \frac{1}{|D_L|} \sum_{(x,y) \in D_L} \text{CE}(f_s(x; \theta_s), y)","Burn-In loss function minimizing cross-entropy (CE) between student model predictions $f_s$ with parameters $\theta_s$ and ground-truth labels $y$ over labeled data $D_L$, initializing the student network before mutual learning."
ICML_2024_oral_111,2,"8).

**Stage2: Mutual-Learning Stage.** After the Burn-In stage, we use the trained weights \(\theta\) to initialize both the teacher and student models. This process is defined as follows:","\theta_{t}arrow\theta,\theta_{s}arrow\theta,","where \(\theta_{t},\theta_{s},\theta\) denote the parameters of the teacher, student and Burn-In model, respectively","\theta_{teacher}^{(0)} = \theta_{student}^{(0)} = \theta,","where $\theta_{teacher}^{(0)}$ and $\theta_{student}^{(0)}$ denote the initial parameters of the teacher and student models, respectively, both set equal to the weights $\theta$ obtained from the Burn-In stage."
ICML_2024_oral_111,3,"ly.

During the mutual learning stage, the teacher generates pseudo-labels for unlabeled data to supervise the training of the student, which is defined as follows:","\mathcal{L}_{unsup}=\frac{1}{H\times W}\sum_{j=1}^{H\times W}\mathcal{L}_{BCE} (M_{i,j}^{u},\hat{M}_{i,j}^{u}),","where \(M_{i,j}^{u}\) and \(\hat{M}_{i,j}^{u}\) denote the predicted mask for \(j\)-th pixel of \(i\)-th unlabeled image by student and teacher, respectively","\mathcal{L}_{unsup} = \frac{1}{H \times W} \sum_{j=1}^{H \times W} \mathcal{L}_{BCE}(M_{i,j}^{u}, \hat{Y}_{i,j}^{u}),","where $M_{i,j}^{u}$ denotes the prediction mask of the student model for the $j$-th pixel of the $i$-th unlabeled image, and $\hat{Y}_{i,j}^{u}$ denotes the pseudo-label generated by the teacher model."
ICML_2024_oral_111,4,"ly.

Simultaneously, the student continues to train on a small subset of labeled data, jointly optimizing with these two components of loss function, which is defined as follows:","\mathcal{L}=\lambda_{sup}\mathcal{L}_{sup}+\lambda_{unsup}\mathcal{L}_{unsup},",where \(\lambda_{sup}\) and \(\lambda_{unsup}\) is the hyperparameter of supervised loss \(\mathcal{L}_{sup}\) and unsupervised loss \(\mathcal{L}_{unsup}\),"\mathcal{L}_{total}=\mathcal{L}_{sup}+\lambda\mathcal{L}_{unsup},","where $\mathcal{L}_{sup}$ and $\mathcal{L}_{unsup}$ denote the supervised and unsupervised loss components, and $\lambda$ is a weighting factor balancing their contributions."
ICML_2024_oral_111,5,".

To maintain the stability of pseudo-labels, we forego gradient backpropagation for updating the teacher model's parameters. Instead, we employ the Exponential Moving Average (EMA) method to create an aggregated model reflecting both the current and previous states. EMA's effectiveness has been substantiated in numerous studies (Kingma and Ba, 2014; Ioffe and Szegedy, 2015; He et al., 2020; Grill et al., 2020; Tarvainen and Valpola, 2017). The use of EMA not only improves the teacher model's accuracy but also its stability, making it a valuable tool during the mutual learning stage, which is formulated as follows:","\theta_{t}arrow\alpha\theta_{t}+(1-\alpha)\theta_{s},","where \(\alpha\) is the decay coefficient of EMA, typically set within the small range of 0","\theta_{t}=\alpha\theta_{t}+(1-\alpha)\theta_{s},","where $\theta_{t}$ and $\theta_{s}$ denote the parameters of the teacher and student model, respectively, and $\alpha$ is the EMA decay parameter"
ICML_2024_oral_111,6,"0.9 to 0.999.

### The Proposed SemiRES

#### 3.3.1 Overview

The overview of our proposed SemiRES is depicted in Fig. 2. SemiRES inherits the semi-supervised framework introduced in Sec. 3.2 and proposes new strategies to address the challenges of noisy pseudo-labels encountered by regular semi-supervised frameworks, which limit the extraction of knowledge from unlabeled data. The core idea is to exploit the powerful edge segmentation capabilities of SAM. The central question of our research is how to utilize these masks to refine the noisy pseudo-labels. In this paper, we propose two alternative matching strategies, IoU-based Optimal Matching (IOM) and Composite Parts Integration (CPI), to select masks that contribute to the final pseudo-labels, as detailed in Sec. 3.3.2. Moreover, when segments generated by SAM cannot be matched with pseudo-labels, we introduce a Pixel-Wise Weighted Adjustment (PWA) scheme to focus the model on more reliable pixels, thereby improving performance, as outlined in Sec. 3.3.3.

#### 3.3.2 SAM-based Pseudo-Label Refinement

Despite SAM's powerful segmentation capabilities, effectively harnessing these for pseudo-label refinement is an area ripe for investigation. We have formulated two strategies for matching SAM-generated segments with the original pseudo-labels to improve their accuracy. Before deploying these strategies, we utilize SAM's ""Segment Everything"" feature to create an extensive proposal library of multi-scale candidate segments for our dataset offline, eliminating the need for specific prompts. To optimize storage space, we implement the Run Length Encoding (RLE) algorithm1. Considering that SAM is capable of producing hundreds to thousands of intricate segments per image, adopting efficient storage solutions is crucial. Importantly, while the RLE algorithm achieves high compression rates, it also preserves the precision of the candidate masks.

Footnote 1: [https://en.wikipedia.org/wiki/Run-length_encoding](https://en.wikipedia.org/wiki/Run-length_encoding)

**IoU-based Optimal Matching (IOM).** To achieve our goal, we initially consider a more straightforward approach, premised on the robust multi-scale segmentation ability of the Segment Anything Model (SAM). We hypothesize that the proposal library, constructed as previously mentioned, likely contains a close approximation of the ideal target seg

Figure 2: An overview of the proposed SemiRES, featuring a teacher-student network with data augmentation and mutual learning. It includes SAM-based pseudo-label refinement using IOM or CPI strategies, and PWA supervision when matches are not found.

mentation. Therefore, our task simplifies devising a method to retrieve this optimal mask from the library.

Our method involves an IoU-based selection process, which computes the similarity between the pseudo-labels and each segment generated by SAM. We aim to identify the segment with the highest similarity score, ensuring it aligns closely with the overall target mask. The similarity measure is calculated using the Intersection over Union (IoU) metric, a standard in object detection and segmentation tasks that quantifies the extent of overlap between two areas. By selecting the segment with the top-1 IoU score from the candidate pool, we can effectively align our model's output with the most accurate representation of the intended segmentation, as detailed below:","s^{k}=\frac{\sum_{j=1}^{H\times W}(\hat{M}_{i,j}^{u}\cap\widetilde{M}_{i, j,k}^{u})}{\sum_{j=1}^{H\times W}(\hat{M}_{i,j}^{u}\cup\widetilde{M}_{i, j,k}^{u})},","where \(\hat{M}_{i,j}^{u}\) and \(\widetilde{M}_{i,j,k}^{u}\) denote the pseudo-labels and \(k\)-th segment mask generated by SAM for \(j\)-th pixel of \(i\)-th unlabeled image \(\mathcal{I}_{i}^{u}\), respectively","\mathcal{S}_{iom}=\arg\max_{k}\frac{|M_{i,j}^{u}\cap S_{k}|}{|M_{i,j}^{u}\cup S_{k}|},","where $\mathcal{S}_{iom}$ denotes the selected segment via IoU-based Optimal Matching, $S_{k}$ represents the $k$-th candidate segment from the SAM-generated proposal library, and $M_{i,j}^{u}$ denotes the pseudo-label mask for the $j$-th pixel of the $i$-th unlabeled image"
ICML_2024_oral_111,7,"ly. When the score \(s^{k}\) exceeds a certain threshold \(IoU_{rate}\), the matched mask will replace the pseudo-label. The detailed matching schemes can be found in Algorithm 1, particularly within lines 4 to 9.

**Composite Parts Integration (CPI).** In our exploration of semi-supervised referring expression segmentation, we recognize that while the IoU-based Optimal Matching (IOM) strategy is generally straightforward and effective, it may falter in certain scenarios. One such instance occurs when the proposal library lacks an ideal target segmentation, rendering even the most sophisticated matching algorithm incapable of finding an appropriate guide mask. Another instance is when the disparity between the pseudo-labels and the desired segmentation is too substantial to allow for effective correction. We have noted that the original pseudo-labels generated by the teacher model can suffer from either under-segmentation or over-segmentation of the target instances, as depicted in Fig. 3. These inaccuracies diminish the quality of the pseudo-labels, providing erroneous guidance to the student model and impeding its learning.

Under-segmentation is characterized by incomplete coverage of the target instance, missing activation for certain region pixels. To address this, we aim to identify larger regions within the proposal library to rectify the pseudo-labels. Our selection is based on the overlap ratio with the pseudo-labels, calculated as follows:","s_{1}^{k}=\frac{\sum_{j=1}^{H\times W}(\hat{M}_{i,j}^{u}\cap\widetilde{M}_{ i,j,k}^{u})}{\sum_{j=1}^{H\times W}(\hat{M}_{i,j}^{u})+\epsilon},",where \(\epsilon\) is the smoothing factor to prevent a denominator of zero,"r_{l}^{k}=\frac{\sum_{j=1}^{H\times W}(\hat{M}_{i,j}^{u}\cap\widetilde{M}_{i,j,k}^{u})}{\sum_{j=1}^{H\times W}\hat{M}_{i,j}^{u}},",where $r_{l}^{k}$ denotes the overlap ratio of pseudo-labels with the $k$-th segment mask generated by SAM for the $i$-th unlabeled image
ICML_2024_oral_111,8,"ro. When the overlap ratio \(s_{1}^{k}\) exceeds a predefined threshold \(inter_{1}\), the \(k\)-th segment \(\widetilde{M}_{i,:,k}^{u}\) generated by SAM is selected and subsequently merged to replace the pseudo-labels. This method is referred to as Composite Parts Integration for Under-segmentation (CPI-U). Conversely, over-segmentation introduces erroneous regions into the segmentation. To mitigate this, we seek to leverage SAM's segmentation to filter out the extraneous noise. The selection is based on the overlap ratio with the candidate mask, computed as:","s_{2}^{k}=\frac{\sum_{j=1}^{H\times W}(\hat{M}_{i,j}^{u}\cap\widetilde{M} _{i,j,k}^{u})}{\sum_{j=1}^{H\times W}(\widetilde{M}_{i,j,k}^{u} )}.",,"s_{2}^{k}=\frac{\sum_{j=1}^{H\times W}(\hat{M}_{i,j}^{u}\cap\widetilde{M}_{i,j,k}^{u})}{\sum_{j=1}^{H\times W}(\widetilde{M}_{i,j,k}^{u})+\epsilon},","where $s_{2}^{k}$ computes the overlap ratio between the pseudo-label and the $k$-th SAM-generated segment for over-segmentation correction, with $\epsilon$ preventing division by zero"
ICML_2024_oral_111,9,"Likewise, when the ratio \(s_{2}^{k}\) is above the set threshold \(inter_{2}\), the segment \(\widetilde{M}_{i,:,k}^{u}\) generated by SAM is chosen and integrated to refine the pseudo-labels. This approach is termed Composite Parts Integration for Over-segmentation (CPI-O). When both conditions are met, we form the overarching CPI strategy. The detailed matching schemes can be found in Algorithm 1, particularly within lines 10 to 15.

#### 3.3.3 Pixel-Wise Weighted Adjustment

Despite the effectiveness of our two strategies for refining pseudo-labels, there are cases where the scores do not ex

Figure 3: Visualization of the principles behind IOM and CPI addressing pseudo-label issues in different cases.

ceed a certain threshold, indicating a mismatch between SAM-generated segments and the current pseudo-labels. In such situations, as inspired by previous work (Yang et al., 2023), we implement the Pixel-Wise Weighted Adjustment (PWA). PWA's core objective is to assign weights to pixels based on their confidence levels. High-confidence pixels, with scores near 0 or 1, indicate certainty in foreground or background prediction and are given higher weights. In contrast, pixels with scores around 0.5, often associated with noise or ambiguity, receive lower weights to reduce their influence on training. The mapping function \(\Psi\) for translating pixel confidence into weights is defined as:","\Psi(\hat{M}^{u}_{i,j})=\gamma-\frac{1}{\sqrt{2\pi\sigma}}\exp(-\frac{( \hat{M}^{u}_{i,j}-\mu)^{2}}{2\sigma^{2}}),","where \(\gamma,\sigma^{2},\mu\) are hyperparameters, which are set to 1","\Psi(M_{i,j}^{u})=1-\vert M_{i,j}^{u}-0.5 \vert^{\gamma},","where $M_{i,j}^{u}$ denotes the predicted probability of the $j$-th pixel in the $i$-th unlabeled image, and $\gamma$ controls the sensitivity of weight assignment to confidence levels"
ICML_2024_oral_111,10,"1.3, 0.1, and 0.5 respectively.

Therefore, the loss for \(i\)-th unlabeled image \(\mathcal{I}^{u}_{i}\) is defined as follows:","\mathcal{L}_{unsup}=\frac{1}{H\times W}\sum_{j=1}^{H\times W}\Psi(\hat{M}^{u} _{i,j})*L_{BCE}(M^{u}_{i,j},\hat{M}^{u}_{i,j}).",,"\mathcal{L}_{i}^{u}=\frac{1}{H\times W}\sum_{j=1}^{H\times W}\Psi(\hat{M}^{u}_{i,j})\cdot\mathcal{L}_{BCE}(M_{i,j}^{u},\hat{M}_{i,j}^{u}),","where $\mathcal{L}_{i}^{u}$ denotes the weighted unsupervised loss for the $i$-th unlabeled image, incorporating pixel-wise confidence weighting via $\Psi(\hat{M}^{u}_{i,j})$"
ICML_2024_oral_113,1,"## 3 Preliminaries

In this paper, we propose a modification to on-policy RL to achieve higher performance in the presence of large batch sizes. We build upon PPO, although our proposed ideas are generally applicable to any on-policy RL method.

On-policy RLLet (\(\mathcal{S},\mathcal{A},\mathcal{P},r,\rho,\gamma\)) be an MDP where \(\mathcal{S}\) is the set of states, \(\mathcal{A}\) the set of actions, \(\mathcal{P}\) are transition probabilities, \(r\) the reward function, \(\rho\) the initial distribution of states and \(\gamma\) the discount factor. The objective in reinforcement learning is to find a policy \(\pi(a|s)\) which maximises the long term discounted reward \(\mathcal{J}(\pi)=\mathbb{E}_{\pi_{0}\sim\rho,a_{t}\sim\pi(\cdot|s_{t})}\left[ \sum_{t=0}^{T-1}\gamma^{t}r(s_{t},a_{t})\right]\).

Policy-gradient algorithms (Williams, 2004; Konda Tsitsiklis, 1999; Schulman et al., 2015; Mnih et al., 2016) optimize the policy using gradient descent with Monte Carlo estimates of the gradient","\nabla_{\theta}J(\pi_{\theta})=\operatorname*{\mathbb{E}}_{s\sim\rho_{d},a \sim\pi(\cdot|s)}[\nabla_{\theta}\log(\pi_{\theta}(a))\hat{A}^{\pi_{ \theta}}(s,a)]","where \(\hat{A}^{\pi_{\theta}}(s,a)\) is an advantage function that estimates the contribution of the transition to the gradient","\nabla_\theta \mathcal{J}(\pi_\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t|s_t) \sum_{t'=t}^{T-1} \gamma^{t'-t} r(s_{t'}, a_{t'}) \right]",The policy gradient theorem expresses the gradient of the expected return with respect to policy parameters as an expectation over trajectories of log-policy gradients weighted by cumulative discounted rewards.
ICML_2024_oral_113,2,"nt. A common choice is \(\hat{A}^{\pi_{\theta}}(s,a)=\hat{Q}^{\pi_{\theta}}(s,a)-\hat{V}^{\pi_{\theta}} (s)\), where \(\hat{Q}^{\pi_{\theta}}(s,a)\), \(\hat{V}^{\pi_{\theta}}(s)\) are estimated \(Q\) and value functions. This form of update is termed as an actor-critic update (Konda Tsitsiklis, 1999). Since we want the gradient of the error with respect to the current policy, only data from the current policy (on-policy) data can be utilized.

PPOActor critic updates can be quite unstable because gradient estimates are high variance and the loss landscape is complex. An update step that is too large can destroy policy performance. Proximal Policy Optimization (PPO) modifies Eq. 1 to restrict updates to remain within an approximate ""trust region"" where there is guaranteed improvement (Schulman et al., 2015; Kakade Langford, 2002).","\begin{split} L_{on}(\pi_{\theta})&= \operatorname*{\mathbb{E}}_{\pi_{old}}[\min(r_{t}(\pi_{\theta}),.\\ &.\operatorname{clip}(r_{t}(\pi_{\theta}),1-\epsilon,1 +\epsilon))A_{t}^{\pi_{old}}]\end{split}",,"\theta_{k+1} = \arg\max_{\theta} \mathbb{E}_{s \sim \rho_{d}, a \sim \pi_{\theta_{k}}(\cdot|s)} \left[ \min \left( \frac{\pi_{\theta}(a|s)}{\pi_{\theta_{k}}(a|s)} \hat{A}^{\pi_{\theta_{k}}}(s,a), \text{clip} \left( \frac{\pi_{\theta}(a|s)}{\pi_{\theta_{k}}(a|s)}, 1 - \epsilon, 1 + \epsilon \right) \hat{A}^{\pi_{\theta_{k}}}(s,a) \right) \right]","This equation defines the policy update rule in Proximal Policy Optimization (PPO), which limits the change in the policy using a clipped surrogate objective to ensure stable learning."
ICML_2024_oral_113,3,"Here, \(r_{t}(\pi_{\theta})=\frac{\pi_{\theta}(a_{t}|s_{t})}{\pi_{old}(a_{t}|s_{t})}\), \(\epsilon\) is a clipping hyperparameter and \(\pi_{old}\) is the policy collecting the on-policy data. The clipping operation ensures that the updated \(\pi\) stays close to \(\pi_{old}\). Empirically, given large numbers of samples, PPO achieves high performance, is stable and robust to hyperparameters. However, it was developed for relatively small batch sizes (\(\approx 100\) parallel envs). We find that in the large-scale setting (\(>\)10k envs), it is suboptimal because many parallel envs are sampling nearly identical on-policy data.

## 4 Split and Aggregate Policy Gradients

Policy gradient methods are highly sensitive to the variance in the estimate of gradient. Since CPU-based simulators typically run only 100s of environments in parallel, conventional wisdom is to simply sample on-policy data from a Gaussian policy in all the environments since as the number of datapoints increases, the Monte Carlo estimate becomes more accurate. However, this intuition no longer holds in the extremely large-scale data setting where we have hundreds of thousands of environments on GPU-accelerated simulators like IsaacGym. IID sampling from a Gaussian policy will lead to most actions lying near the mean, and most environments will execute similar actions, leading to wasted data (fig. 2).

We propose to efficiently use large numbers of \(N\) environments using a divide-and-conquer setup. Our algorithm trains a variety of \(M\) policies \(\pi_{1},\dots,\pi_{M}\) instead of having just one policy. However, simply training multiple policies by dividing environments between them is also inefficient. This is equivalent to training an algorithm with different seeds and choosing the best seed. One approach is to add hyperparameter mutation (Petrenko et al., 2023) to the policies and choosing the hyperparameters that perform the best among all of them. However, even in this case, all of the data from the ""worse"" policies goes to waste, and the only information gained is that some combinations of hyperparameters are bad, even though the policies themselves may have discovered high reward trajectories. We need to somehow aggregate data from multiple policies into a single update. We propose to do this via off-policy updates.

### Aggregating data using off-policy updates

One of the major drawbacks of on-policy RL is its inability to use data from past versions of the policy. One solution is to use importance sampling (Degris et al., 2012; Meng et al., 2023) to weight updates using data from different policies. In practice, this is not used since given limited compute it is beneficial to sample on-policy experience that is more directly relevant. However, this is no longer true in the large batch setting where enough on-policy data is available. In this case, it becomes advantageous to have multiple policies \(\pi_{1},\dots,\pi_{M}\) and use them to sample diverse data, even if it is off-policy. In particular, to update policy \(\pi_{i}\) using data from policy \(\pi_{j}\), \(j\in\mathcal{X}\) we use (Meng et al., 2023)","\begin{split}& L_{off}(\pi_{i};\mathcal{X})=\frac{1}{|\mathcal{X}|} \sum_{j\in\mathcal{X}}\operatorname*{\mathbb{E}}_{(s,a)\sim\pi_{j}}[\min (r_{\pi_{i}}(s,a),..\\ &..\operatorname{clip}(r_{\pi_{i}}(s,a),\mu(1- \epsilon),\mu(1+\epsilon)))A^{\pi_{i,old}}(s,a)]\end{split}","where \(r_{\pi_{i}}(s,a)=\frac{\pi_{i}(s,a)}{\pi_{j}(s,a)}\) and \(\mu\) is an off-policy correction term \(\mu=\frac{\pi_{i,old}(s,a)}{\pi_{j}(s,a)}\)","\nabla_{\theta_{i}}J(\pi_{i})=\operatorname*{\mathbb{E}}_{s\sim\rho_{d},a\sim\pi_{j}(\cdot|s)}\left[\nabla_{\theta_{i}}\log(\pi_{i}(a|s))\frac{\pi_{i}(a|s)}{\pi_{j}(a|s)}A^{\pi_{j}}(s,a)\right]","Off-policy gradient update for policy $\pi_i$ using data collected by policy $\pi_j$, incorporating importance sampling to correct for the policy mismatch."
ICML_2024_oral_113,4,". Note that when \(i=j\), then \(\pi_{j}=\pi_{i,old}\) and this reduces to the on-policy update as expected. This is then scaled and combined with the on-policy term (eq. 2)",L(\pi_{i})=L_{on}(\pi_{i})+\lambda\cdot L_{off}(\pi_{i};\mathcal{X}),,\begin{split} L(\pi_{i}) = \lambda L_{on}(\pi_{i}) + (1 - \lambda) L_{off}(\pi_{i}; \mathcal{X} \setminus \{i\}) \end{split},Equation 4 combines the on-policy loss $L_{on}(\pi_{i})$ and the off-policy loss $L_{off}(\pi_{i}; \mathcal{X} \setminus \{i\})$ using a mixing coefficient $\lambda$ to balance their contributions in the overall objective function for policy $\pi_{i}$.
ICML_2024_oral_113,5,The update target for the critic is calculated using \(n\)-step returns (here \(n=3\)).,"V^{target}_{on,\pi_{j}}(s_{t})=\sum_{k=t}^{t+2}\gamma^{k-t}r_{k}+\gamma^{3}V_{ \pi_{j},old}(s_{t+3})",,V_{t}^{\text{target}} = \sum_{k=0}^{n-1} \gamma^{k} r_{t+k} + \gamma^{n} V_{t+n}^{\pi_{\theta}}(s_{t+n}),"where $V_{t}^{\text{target}}$ is the $n$-step return target for the critic, calculated using rewards $r_{t+k}$ and the value function estimate $V_{t+n}^{\pi_{\theta}}$ at step $t+n$"
ICML_2024_oral_113,6,"However, this is not possible for off-policy data. Instead, we assume that an off-policy transition can be used to approximate a 1-step return. The target equations are as follows -","V^{target}_{off,\pi_{j}}(s^{\prime}_{t})=r_{t}+\gamma V_{\pi_{j},old}(s^{ \prime}_{t+1})",,"V^{target}_{off,\pi_{j}}(s_{t})=r_{t}+\gamma V_{\pi_{i},old}(s_{t+1})",Off-policy value target using a 1-step return approximation for transition $t$ from policy $\pi_j$ evaluated under $\pi_i$
ICML_2024_oral_113,7,The critic loss is then,"L^{critic}_{on}(\pi_{i})=\operatorname*{\mathbb{E}}_{(s,a)\sim\pi_{i}}[( V_{\pi_{i}}(s)-V^{target}_{on,\pi_{i}}(s))^{2}] || L^{critic}_{off}(\pi_{i};\mathcal{X})=\frac{1}{|\mathcal{X}|}\sum_{j\in \mathcal{X}}\operatorname*{\mathbb{E}}_{(s,a)\sim\pi_{j}}[(V_{\pi_{i}}(s) -V^{target}_{off,\pi_{i}}(s))^{2}] || L^{critic}(\pi_{i})=L^{critic}_{on}(\pi_{i})+\lambda\cdot L^{critic}_{off}( \pi_{i})",,"\begin{split}L_{critic}(\pi_{i}) = \frac{1}{|\mathcal{X}|} \sum_{j \in \mathcal{X}} \mathbb{E}_{(s, a) \sim \pi_j} \left[ \left( V_{\pi_i}(s) - V^{target}_{\pi_j}(s) \right)^2 \right]\end{split}",The critic loss combines both on-policy and off-policy data to update the value function by minimizing the mean squared error between predicted and target values.
ICML_2024_oral_117,1,"### Self-Composing Policy Module

In this section, the building block of the presented architecture is described: the _self-composing policy module_. As illustrated in Figure 2, it is divided into three main blocks; corresponding to the light-blue areas of the diagram: the output attention head, the input attention head, and the internal policy. The following lines describe and explain the rationale behind these three blocks.

Output Attention Head.This block proposes an output for the current module directly based on the preceding policies. Specifically, it generates a tentative vector \(\mathbf{v}\) for the output of the module based on the matrix \(\Phi^{k;\mathbf{s}}\) and the representation of the current state \(\mathbf{h}_{\mathbf{s}}\). In fact, the block employs an attention mechanism that, conditioned on \(\mathbf{h}_{\mathbf{s}}\), returns a linear combination of the outputs of the previous policies (the rows of \(\Phi^{k;\mathbf{s}}\)). The query vector is obtained as the linear transformation \(\mathbf{q}=\mathbf{h}_{\mathbf{s}}W_{\text{out}}^{Q}\), where \(W_{\text{out}}^{Q}\in\mathbb{R}^{d_{\text{max}}\times d_{\text{model}}}\) is a parameter matrix and \(d_{\text{model}}\) is the hidden vector size. The keys matrix is computed as \(K=(\Phi^{k;\mathbf{s}}+E_{\text{out}})W_{\text{out}}^{K}\), where \(W_{\text{out}}^{K}\in\mathbb{R}^{|\mathcal{A}|\times d_{\text{model}}}\) is a parameter matrix, and \(E_{\text{out}}\) is a positional encoding matrix of the same size of \(\Phi^{k;\mathbf{s}}\). The positional encoding method considered in this work is the cosine positional encoding of Vaswani et al. (2017). In the case of the values matrix, no linear transformation is considered, thus, \(V=\Phi^{k;\mathbf{s}}\). The result of the output attention head is the scaled dot-product attention (Vaswani et al., 2017) of \(\mathbf{q}\), \(K\), and \(V\):","\text{Attention}(\mathbf{q},K,V)=\text{softmax}(\frac{qK^{T}}{\sqrt{d_{ \text{model}}}})V",,"\mathbf{v} = \text{Attention}\left(\mathbf{q}, K, V\right) = \text{softmax}\left(\frac{\mathbf{q} \left((\Phi^{k;\mathbf{s}} + E_{\text{out}})W_{\text{out}}^{K}\right)^\top}{\sqrt{d_{\text{model}}}}\right) \Phi^{k;\mathbf{s}}","Equation 1 defines the scaled dot-product attention mechanism for the output attention head, computing the tentative output vector $\mathbf{v}$ as a function of the query $\mathbf{q}$, keys matrix $K$, and values matrix $V$ derived from preceding policies and positional encodings."
ICML_2024_oral_117,2,"Input Attention Head.The purpose of this block is to retrieve relevant information from both the previous policies and the output attention head. It provides the necessary information for the decision-making process of the internal policy (the next block) by attending to the important features from past policies and the tentative vector \(\mathbf{v}\) from the output attention head. Similarly to the previous block, it employs an attention head conditioned on \(\mathbf{h}_{\mathbf{s}}\), but unlike the preceding block, the attention head returns a linear combination over learnable transformations of its inputs. Specifically, the query vector is computed as \(\mathbf{q}=\mathbf{h}_{\mathbf{s}}W_{\text{in}}^{Q}\), where \(W_{\text{in}}^{Q}\in\mathbb{R}^{d_{\text{max}}\times d_{\text{model}}}\). Following Figure 2, the keys are computed as \((P+E_{\text{in}})W_{\text{in}}^{K}\), where \(P\) is the row-wise concatenation of the output of the previous block (\(\mathbf{v}\)) and \(\Phi^{k;\mathbf{s}}\), while \(E_{\text{in}}\) is a positional encoding matrix of the same size as \(P\) and \(W_{\text{in}}^{K}\in\mathbb{R}^{|\mathcal{A}|\times d_{\text{model}}}\). In turn, the values matrix is obtained as the linear transformation \(V=PW_{\text{in}}^{V}\), where \(W_{\text{in}}^{V}\in\mathbb{R}^{|\mathcal{A}|\times d_{\text{model}}}\). Once \(\mathbf{q}\), \(K\), and \(V\) have been computed, the output of this block is the dot-product attention of these three elements, see Equation (1). Note that the learnable parameters of this block are \(W_{\text{in}}^{Q}\), \(W_{\text{in}}^{K}\), and \(W_{\text{in}}^{V}\).

Internal Policy.This block is used to adjust, overwrite, or retain the tentative vector \(\mathbf{v}\) from the output attention head, considering the contextual information provided by the input attention head and the representation of the current state. It is comprised of a feed-forward multi-layerperceptron network which takes the result of the previous block and \(\mathbf{h_{s}}\) as input, generating a real-valued vector of size \(|\mathcal{A}|\). Notably, this vector is not the direct output of the self-composing policy module; instead, it is added to the tentative vector \(\mathbf{v}\) to form the final output of the module. Finally, depending on the nature of the task at hand, this addition might require normalization, as the output of the module usually represents a categorical distribution over \(\mathcal{A}\) or continuous actions within some bounds.

In summary, the output attention head proposes an output for the module based on the current state and the information of the previous policies. Subsequently, the input attention head retrieves relevant information from both the previous policies and the output attention head. Then, the internal policy utilizes this information along with the current state representation to adjust, overwrite, or retain the tentative output from the output attention head. Note that the output attention head proposes outputs based on preceding policies and the current state representation, while the input attention head retrieves relevant information from previous modules for guiding the decision-making process of the internal policy.

In Section 3, we categorized three scenarios concerning the current task and previously learned policy modules that motivated the design of CompoNet. The subsequent lines review these scenarios within the described architecture:

1. If a previous policy solves the current task, the output attention head assigns high attention to it, and the internal policy may output a vector of zeros to retain this result, akin to residual connections in deep learning.
2. If a function over the previous policies and the current state can solve the task at hand, then the three blocks of the module can be used to learn such a function.
3. When previous policies offer no relevant information for the task at hand, the internal policy independently learns a policy from scratch based on current state information, superseding the result of the output attention head in the last addition step.

### Computational Cost

As mentioned in Section 2, computational cost stands as the primary drawback of growing NN architectures. Consequently, memory and inference costs are focal points in this study. First, the self-composing policy module is designed to mitigate the memory complexity of the model. As a result, CompoNet grows linearly in the number of parameters with respect to the number of tasks while being able to encompass the information of all previously learned modules (see Appendix B for further details). The rightmost plot in Figure 3 contrasts the memory costs of CompoNet with progressive NNs (ProgressiveNet). The latter method, introduced by Rusu et al. (2016) is one of the best-known growing NNs, and shares multiple similarities with CompoNet. Regarding the computational cost of inference, while the theoretical cost of CompoNet is quadratic with respect to the number of tasks (elaborated in Appendix C.1), the results presented in Figure 3 indicate that the empirical computational cost of CompoNet does not exhibit quadratic growth up to the 300 tasks tested, effectively scaling to very long task sequences in practice.

## 5 Experiments

In this section, we validate the presented architecture across sequences of tasks from multiple environments and domains. The central hypothesis of these experiments is that CompoNet should be able to benefit from forward knowledge transfer to solve the task at hand in the scenarios presented in Section 4.

### Evaluation Metrics

We start by describing the CRL-relevant metrics commonly used in the literature (Wolczyk et al., 2021, 2022). Consider \(p_{i}(t)\in[0,1]\) to be the success rate4 in task \(i\) at time \(t\), indicating whether the task is solved5, \(p_{i}(t)=1\), or not,

Figure 2: Diagram of the self-composing policy module. _Vstack_ and _Hstack_ successively represent row-wise and column-wise concatenation operations, while the normalization operation has been delimited with a dashed line to denote that it is optional and dependent on the nature of the action space. Finally, note that the only blocks with learnable parameters are the feed-forward block and the linear transformations.

\(p_{i}(t)=0\). Note that the metric is task-specific and defined by the problem itself. Moreover, the interaction of an agent with each task is limited to \(\Delta\) timesteps, being the total number of timesteps \(T=N\cdot\Delta\), where \(N\) is the number of tasks. Continuing the standard practice in CRL, we consider an agent trained from scratch in each task as the baseline for the following metrics (Diaz-Rodriguez et al., 2018; Wolczyk et al., 2021, 2022).

Average Performance.The average performance at timestep \(t\) is computed as \(\text{P}(t)=\frac{1}{N}\sum_{i=1}^{N}p_{i}(t)\). In the next sections, we report the final performance value \(\text{P}(T)\) as it is a commonly used metric in the CL literature (Wolczyk et al., 2021).

Forward Transfer.The forward transfer is defined as the normalized area between the training curve of the method and the training curve of the baseline. Considering \(p_{i}^{b}(t)\in[0,1]\) to be the performance of the baseline, the forward transfer \(\text{FTr}_{i}\) on task \(i\) is,","\text{FTr}_{i}=\frac{\text{AUC}_{i}-\text{AUC}_{i}^{b}}{1-\text{AUC}_{i}^{ b}},\quad\text{AUC}_{i}=\frac{1}{\Delta}\int_{(i-1)\cdot\Delta}^{i\cdot \Delta}p_{i}(t)\mathrm{d}t, || \text{AUC}_{i}^{b}=\frac{1}{\Delta}\int_{0}^{\Delta}p_{i}^{b}(t)\mathrm{d}t",,,
ICML_2024_oral_117,3,"Under this metric, a key concept is the **Reference forward Transfer (RT)**. A CRL method should ideally perform at least as well as fine-tuning from the task with the highest transfer to the current one. Thus, RT is defined as follows:","\text{RT}=\frac{1}{N}\sum_{i=2}^{N}\max_{j<i}\text{FTr}(j,i)","where \(\text{FTr}(j,i)\) is the forward transfer obtained by training a model from scratch in the \(j\)-th task and fine-tuning it in the \(i\)-th task",\text{RT}_{i} = \max_{j < i} \left( \frac{\text{AUC}_{i}^{(j)} - \text{AUC}_{i}^{b}}{1 - \text{AUC}_{i}^{b}} \right),"Reference forward Transfer (RT) computes the maximum normalized advantage of transferring knowledge from any previous task $j$ to the current task $i$, relative to the baseline performance."
ICML_2024_oral_121,1,"### Probability Modeling for Continuous Disparity

For each pixel in the left image, suppose the possible disparities are in the range of \([d_{\texttt{min}},d_{\texttt{max}}]\). Conventional stereo matching algorithms typically calculate a cost function that

Figure 2: **Difference between the expectation based approach and our method.** In (a) the pixel in the red circle is located at the boundary of the chair, thus the disparity distribution has multiple modes. (b) and (c) shows the discrete distribution of disparity hypotheses in orange bars. In (b) the prediction obtained by averaging is blurred and far from any of the modes. In (c) we obtained the optimal solution under \(L^{1}\) norm, which is more robust and closer to the ground truth. The green curve is the interpolated probability density.

can equate to a probability mass function (PMF) with a finite set of disparities \(\mathbf{d}=[d_{1},...,d_{N}]^{T}\). It computes a discrete distribution \(\mathbf{p}^{m}=[p_{1}^{m},...,p_{N}^{m}]^{T}\), where \(d_{i}\in[d_{\texttt{min}},d_{\texttt{max}}]\) and \(p_{i}^{m}\) is the probability that the ground truth disparity is \(d_{i}\). The \(\mathbf{p}^{m}\) must satisfy \(p_{i}^{m}\geq 0\) and \(\sum_{i}p_{i}^{m}=1\).

The discrete formulation reasons the probability only at a finite set of disparities. Yet, in real-world applications, the ground-truth disparity is continuous. Thus, we propose to interpolate the discrete distribution via Laplacian kernel, and compute the probability density function of disparity \(x\in\mathbb{R}\) as","p(x;\mathbf{p}^{m})=\sum_{i}^{N}k(x,d_{i})p_{i}^{m},",,p(x) = \sum_{i} \frac{p_i^{m}}{2\sigma} e^{- \frac{|x - d_i|}{\sigma}},"Continuous probability density function obtained by interpolating discrete disparities $\mathbf{d}$ with Laplacian kernels of bandwidth $\sigma$, where $p_i^{m}$ represents the probability mass at discrete disparity $d_i$."
ICML_2024_oral_121,2,"here \(k(x,d_{i})\) is defined as \(\frac{1}{2\sigma}\exp-\frac{|x-d_{i}|}{\sigma}\), and \(\sigma\) is the hyper-parameter for bandwidth. The above density function is valid as \(p(x;\mathbf{p}^{m})\geq 0\) for \(\forall\,x\in\mathbb{R}\) and \(\int p(x;\mathbf{p}^{m})dx=1\). An illustration of the interpolation is shown in Fig. 2 (c). The orange bars represent the given discrete distribution \(\mathbf{p}^{m}\), and the green curve is the interpolated density function. Later, we show that such a continuous modeling enable us to compute derivative of the proposed stereo risk function.

### Risk in Stereo Matching

To choose a value as the final prediction, we propose to minimize the following risk:","\texttt{argmin}_{y}F(y,\mathbf{p}^{m})=\texttt{argmin}_{y}\int\mathcal{L}(y,x )p(x;\mathbf{p}^{m})dx","where \(F(y,\mathbf{p}^{m})\) is called as the risk at \(y\), and \(\mathcal{L}(y,x)\) is the error function between \(y\) and \(x\)","R(\hat{x}) = \int_{-\infty}^{\infty} | \hat{x} - x | \, p(x; \mathbf{p}^m) \, dx","The risk function $R(\hat{x})$ represents the expected absolute difference between the predicted disparity $\hat{x}$ and the true disparity $x$, weighted by the interpolated probability density $p(x; \mathbf{p}^m)$."
ICML_2024_oral_121,3,"\). By risk we mean that if we take \(y\) as predicted disparity, how much error there shall be with respect to the ground truth. Since the exact ground truth is unavailable at the time for making the prediction, we average the error across all possible ground-truth disparities with the distribution \(p(x;\mathbf{p}^{m})\).

Previous methods usually compute the expectation value of \(x\) as the final prediction for the disparity:",y=\int xp(x;\mathbf{p}^{m})dx.,,y_{\texttt{expectation}}=\int x p(x;\mathbf{p}^{m})dx,Expectation value of the continuous disparity distribution as the final disparity prediction.
ICML_2024_oral_121,4,"We want to point out that we can arrive at the same prediction by using squared \(L^{2}\) norm loss as \(\mathcal{L}(y,x)\) in Eq.(2), i.e., \(\texttt{argmin}_{y}F(y,\mathbf{p}^{m})=\int xp(x;\mathbf{p}^{m})dx\) if \(\mathcal{L}(y,x)=(y-x)^{2}\). Nevertheless, it is well known that the \(L^{2}\) norm is not robust to outliers. As an example, in Fig. 2 (b) it can be observed that the estimated expectation is inaccurate when there are multiple modes in the distribution. And therefore, we resort to \(L^{1}\) norm of \(\mathcal{L}(y,x)\) in Eq.(2), i.e.,","\texttt{argmin}_{y}F(y,\mathbf{p}^{m})=\texttt{argmin}_{y}\int|y-x|p(x; \mathbf{p}^{m})dx.",,"\texttt{argmin}_{y}F(y,\mathbf{p}^{m})=\texttt{argmin}_{y}\int|y-x|p(x;\mathbf{p}^{m})dx.","This equation formulates the risk minimization using the $L^{1}$ norm loss, which is more robust to outliers and multi-modal distributions compared to the $L^{2}$ norm."
ICML_2024_oral_121,5,"Given the distribution \(p(x;\mathbf{p}^{m})\) of the disparity, the optimal \(y\) will minimize the \(L^{1}\) error with respect to all possible disparities weighted by the corresponding probability density. As shown in Fig. 2 (c), our final prediction is more robust to the incorrect modes and closer to the ground truth.

### Differentiable Stereo Risk Minimization

Obtaining a minimal risk solution to Eq.(4) seems difficult as it is challenging to derive its closed form formulation. So, performing end-to-end learning with deep network seems difficult. To this end, we put forward an approach that enable end-to-end learning of the network as follows:

**(i) Forward Prediction.** Given a discrete distribution \(\mathbf{p}^{m}\), we find the optimal \(y\) for Eq.(4) based on the following two observations. Firstly, the target function \(F(y,\mathbf{p}^{m})\) is convex with respect to \(y\), hence we compute the optimal solution where \(\partial F/\partial y=0\), i.e.,","G(y,\mathbf{p}^{m})\triangleq\frac{\partial F(y,\mathbf{p}^{m})} {\partial y} || =\sum_{i}p_{i}^{m}\texttt{Sign}(y-d_{i})(1-\exp-\frac{|y-d_{i}|}{ \sigma})=0",,"\frac{\partial F}{\partial y}= \int \texttt{sign}(y-x)p(x;\mathbf{p}^{m})dx=0,",The derivative of the risk function with respect to $y$ is set to zero to find the optimal prediction under the $L^{1}$ norm loss.
ICML_2024_oral_121,6,"n). \(\texttt{Sign}()\) can be thought of as an indicator function, i.e., it is 1 if \(y>d_{i}\) and \(-1\) otherwise. Secondly, the second-order derivative \(\partial^{2}F/\partial^{2}y\geq 0\), indicating that the first-order derivative is a non-decreasing function. We find the optimal disparity, i.e., the zero point of \(G(y,\mathbf{p}^{m})\), by binary search, as shown in Algorithm 1. In all our experiments, we set \(\sigma=1.1\) and \(\tau=0.1\). For \(N\) disparity hypotheses, the binary search algorithm can find the optimal solution with time complexity of \(O(\log N)\)(Cormen et al., 2009).

**(ii) Backward Propagation.** Our approach to forward prediction for solving Eq.(4) contains non-differentiable operations --refer Algorithm 1. Yet, to enable end-to-end training, we have to compute \(dy/d\mathbf{p}^{m}\) to propagate the gradient backward. Now, since \(G(y,\mathbf{p}^{m})\equiv 0\) at the optimal \(y\), we obtain the following via use of Implicit Function Theorem (Krantz and Parks, 2002):","dG(y,\mathbf{p}^{m})=\frac{\partial G}{\partial y}dy+\frac{\partial G}{ \partial\mathbf{p}^{m}}d\mathbf{p}^{m}=0.",,"\frac{dy}{d\mathbf{p}^{m}}=-\left(\frac{\partial G(y,\mathbf{p}^{m})}{\partial y}\right)^{-1}\frac{\partial G(y,\mathbf{p}^{m})}{d\mathbf{p}^{m}}.","Derivative of the optimal disparity $y$ with respect to the discrete distribution $\mathbf{p}^{m}$, computed via the Implicit Function Theorem for enabling end-to-end gradient propagation."
ICML_2024_oral_121,7,"By organizing the terms, we obtain","\frac{dy}{d\mathbf{p}^{m}} =-\frac{\partial G/\partial\mathbf{p}^{m}}{\partial G/\partial y} || =[\dots,\frac{\sigma\texttt{Sign}(d_{i}-y)(1-\exp-\frac{|y-d_{i} |}{\sigma})}{\sum_{j}p_{j}^{m}\exp-\frac{|y-d_{i}|}{\sigma}},\dots]^{T},",,dy=-\left(\frac{\partial G}{\partial y}\right)^{-1}\frac{\partial G}{\partial \mathbf{p}^{m}}d\mathbf{p}^{m}.,"This equation expresses the change in the optimal disparity prediction $ y $ with respect to changes in the discrete probability distribution $ \mathbf{p}^m $, derived using the Implicit Function Theorem."
ICML_2024_oral_121,8,"showing the back-propagation computation. Here, we clip the denominator \(\sum_{j}p_{j}^{m}\exp-\frac{|y-d_{j}|}{\sigma}\) in the above equation to be no less than \(0.1\) to avoid large gradients.

### Network Architecture

To find the disparity value, we match the image patches of left and right images by constructing stereo cost volumes as done in Kendall et al. (2017); Chang and Chen (2018). Yet, an exhaustive matching requires extensive memory and computation. So, for efficiency, we adopt a cascade structure following Gu et al. (2020). Categorically, we first sample the disparity hypothesis by a coarse matching at low-resolution image features. This reduces the search space to a large extent. Next, we refine the sampled hypothesis at high-resolution image features.

Fig. 3 shows the overall network architecture details. For clarity on our design choices, we explain the network components in five module as follows2:

Footnote 2: More details are provided in the Appendix

_(a)_ **Feature Extraction.** Given an input image, the module aims to output multi-scale 2D feature maps. Specifically, we first use a ResNet (He et al., 2016) to extract 2D feature maps of resolution 1/4 and 1/2 with respect to the input image. The ResNet contains 4 stages of transformation with 3, 16, 3, 3 residual blocks, respectively. The spatial resolution is downsampled before the beginning of the first and third stages of transformation. Next, we apply the spatial pyramid pooling (Zhao et al., 2017) on the 1/4-resolution feature map from the fourth stage to enlarge the receptive field. In the end, we upsample the enhanced feature map from 1/4 to 1/2 and fuse it with the 1/2-resolution feature map from ResNet. The final outputs are the feature maps of 1/4 and 1/2 resolution. We apply the same network and weights to extract features from left and right images.

_(b)_ **Disparity Hypotheses Sampling.** The disparity hypotheses provide the candidates of pixel pairs to match. In the coarse stage, we uniformly sample 192 hypotheses in the range 0 to maximum possible disparity. In the refined stage, we reduce the sampling space according to the predicted disparity from the coarse stage. Concretely, for each pixel we sample 16 hypotheses between the minimum and maximum disparity in the local window of size \(12\times 12\).

_(c)_ **Matching.** We match the 2D feature maps from the left and right images according to the sampled disparity

Figure 3: **Overall pipeline (Left to Right).** We first extract multi-scale features from left and right images respectively. The subsequent procedures are divided into two stages. In the coarse stage —shown in orange arrow, we sample disparity hypotheses uniformly and match on 1/4-resolution features. While in the refined stage—shown in green arrow, to match 1/2-resolution features efficiently. Disparity hypotheses are sampled centering around the disparity predicted from the coarse stage. In both stages, we first construct cost volumes by concatenation, and then apply the stacked hourglass networks to aggregate the matching cost, and finally search for the disparity that minimizes the proposed \(L^{1}\) risk in Eq.(4).

hypothesis. The features at each pair of candidates pixels for matching will be concatenated along the channel dimension, which forms a 4D stereo cost volume (feature\(\times\) disparity\(\times\)height\(\times\)width). In the coarse stage, we match the feature map of 1/4 resolution for efficiency. To capture high-frequency details, we match the 1/2-resolution feature map in the refined stage.

_(d) Cost Aggregation._ We use the stacked hourglass architecture (Newell et al., 2016) to transform the stereo cost volume and aggregate the matching cost. For the coarse and refined stages, the structures are same except for the number of feature channels. Specifically, the network consists of three 3D hourglasss as in (Chang and Chen, 2018). Each hourglass first downsamples the volume hierarchically to 1/2 and 1/4 resolution with respect to the input volume, and then upsample in sequence to recover the resolution. This procedure helps aggregate information across various scales. The final output is a volume that represents the discrete distribution of disparity hypotheses.

_(e) Risk Minimization._ This module applies Alg. 1 to compute the optimal continuous disparity for each pixel given the discrete distribution of disparity hypotheses. At train time, we additionally compute the gradient according to Eq.(9) to enable backward propagation.

### Loss Function

Given the predicted disparity \(x^{\texttt{pred}}\in\mathbb{R}\) and the ground-truth disparity \(x^{\texttt{gt}}\in\mathbb{R}\), we compute the smooth \(L^{1}\) loss as","\mathcal{L}(x^{\texttt{gt}},x^{\texttt{pred}})=\begin{cases}0.5(x^{\texttt{gt}} -x^{\texttt{pred}})^{2},&\text{if }|x^{\texttt{gt}}-x^{\texttt{pred}}|<1.0\\ |x^{\texttt{gt}}-x^{\texttt{pred}}|-0.5,&\text{otherwise}\end{cases}",,"\mathcal{L}_{\texttt{smooth}}(x^{\texttt{pred}},x^{\texttt{gt}})=\begin{cases} \frac{1}{2}(x^{\texttt{pred}}-x^{\texttt{gt}})^{2} & \text{if }|x^{\texttt{pred}}-x^{\texttt{gt}}|<1\\ |x^{\texttt{pred}}-x^{\texttt{gt}}|-\frac{1}{2} & \text{otherwise} \end{cases}","The smooth $L^{1}$ loss function used to train the network, which combines quadratic and absolute differences to balance robustness and smoothness."
ICML_2024_oral_122,1,"## 2 Preliminaries and Background

**Regression as classification.** We take a probabilistic view on regression where given input \(x\in\mathbb{R}^{d}\) we seek to model the target as a conditional distribution \(Y\,|\,x\sim\mathcal{N}(\mu=\hat{y}(x;\theta),\sigma^{2})\) for some fixed variance \(\sigma^{2}\) and predictor function \(\hat{y}:\mathbb{R}^{d}\times\mathbb{R}^{k}\rightarrow\mathbb{R}\) parameterized by \(\theta\in\mathbb{R}^{k}\). The maximum likelihood estimator for data \(\{x_{i},y_{i}\}_{i=1}^{N}\) is characterized by the **mean-squared error (MSE)** objective,","\min_{\theta}\,\sum_{i=1}^{N}(\hat{y}(x;\theta)-y_{i})^{2},",,"\begin{equation}
\text{MSE}(\theta) = \frac{1}{N} \sum_{i=1}^{N} \left( y_i - \hat{y}(x_i; \theta) \right)^2
\end{equation}","Mean-squared error objective derived from maximum likelihood estimation under a normal distribution with fixed variance σ², where ŷ(x;θ) is the predictor function and {x_i,y_i} are the training data."
ICML_2024_oral_122,2,"with the optimal predictor being \(\hat{y}(x;\theta^{*})=\mathbb{E}\left[Y\,|\,x\right]\).

Instead of directly learning the mean of the conditional distribution, an alternate approach is to learn a distribution over the target value and recover the prediction \(\hat{y}\) as a statistic of this distribution. To this end, we will construct the target distribution \(Y\,|\,x\) with probability density function \(p(y\,|\,x)\) such that the scalar target \(y\) is the mean of this distribution, \(y=\mathbb{E}_{p}\left[Y\,|\,x\right]\). We can now frame the regression problem as learning a parameterized distribution \(\hat{p}(y\,|\,x;\theta)\) that minimizes the KL divergence to the target \(p(y\,|\,x)\),","\min_{\theta}\,\sum_{i=1}^{N}\int_{Y}p(y\,|\,x_{i})\log(\hat{p}(y\,|\,x_ {i};\theta))dy",,"\min_{\theta}\,D_{\text{KL}}\left(p(y\,|\,x)\,\|\,\hat{p}(y\,|\,x;\theta)\right).","Equation 2 formulates the regression problem as minimizing the Kullback-Leibler divergence between the true conditional distribution $p(y\,|\,x)$ and a parameterized approximation $\hat{p}(y\,|\,x;\theta)$."
ICML_2024_oral_122,3,"which is the cross-entropy objective. Finally, our prediction can be recovered as \(\hat{y}(x;\theta)=\mathbb{E}_{\hat{p}}\left[\,Y\,|\,x;\theta\,\right]\).

Given this new problem formulation, in order to transform the distribution learning problem into a tractable loss we restrict \(\hat{p}\) to the set of categorical distributions supported on \([v_{\text{min}},v_{\text{max}}]\) with \(m\) evenly spaced locations or ""classes"", \(v_{\text{min}}\leq z_{1}<\dots<z_{m}\leq v_{\text{max}}\) defined as,","\mathcal{Z}=\{\sum_{i=1}^{m}p_{i}\,\delta_{z_{i}}\,:\,p_{i}\geq 0,\sum_{i=1} ^{m}p_{i}=1\},",where \(p_{i}\) is the probability associated with location \(z_{i}\) and \(\delta_{z_{i}}\) is the Dirac delta function at location \(z_{i}\),"z_{k} = v_{\text{min}} + \frac{(v_{\text{max}} - v_{\text{min}})(k - 1)}{m - 1},\quad k = 1, \dots, m.","The $m$ evenly spaced locations or ""classes"" $z_k$ are defined over the interval $[v_{\text{min}}, v_{\text{max}}]$."
ICML_2024_oral_122,4,"\). The final hurdle is to construct the target distribution \(Y\,|\,x\) and its associated projection onto the set of categorical distributions \(\mathcal{Z}\). We defer this discussion to SS3 where we discuss various methods for performing these steps in the context of RL.

**Reinforcement Learning (RL).** We consider the reinforcement learning (RL) problem where an agent interacts with an environment by taking an action \(A_{t}\in\mathcal{A}\) in the current state \(S_{t}\in\mathcal{S}\) and subsequently prescribed a reward \(R_{t+1}\in\mathbb{R}\) before transitioning to the next state \(S_{t+1}\in\mathcal{S}\) according to the environment transition probabilities. The return numerically describes the quality of a sequence of actions as the cumulative discounted sum of rewards \(G_{t}=\sum_{k=0}^{\infty}\gamma^{k}R_{t+k+1}\) where \(\gamma\in[0,1)\) is the discount factor. The agent's goal is to learn the policy \(\pi:\mathcal{S}\rightarrow\mathscr{P}(\mathcal{A})\) that maximizes the expected return. The action-value function allows us to query the expected return from taking action \(a\) in state \(s\) and following policy \(\pi\) thereafter: \(q_{\pi}(s,a)=\mathbb{E}_{\pi}\left[G_{t}\,|\,S_{t}=s,\,A_{t}=a\right]\).

Deep Q-Networks (DQN; Mnih et al., 2015) proposes to learn the approximately optimal state-action value function \(Q(s,a;\theta)\approx q_{\pi^{*}}(s,a)\) with a neural network parameterized by \(\theta\). Specifically, DQN minimizes the mean-squared temporal difference (TD) error from transitions \((S,A,R,S^{\prime})\) sampled from dataset \(\mathcal{D}\),","\boxed{\text{TD}_{\text{MSE}}(\theta)=\mathbb{E}_{\mathcal{D}}[(( \tilde{\mathcal{T}}Q)(S,A;\tilde{\theta})-\,Q(S,A;\theta))^{2}]} || (\tilde{\mathcal{T}}Q)(s,a;\tilde{\theta})=R+\gamma\max_{a^{\prime}}Q(S^{\prime },a^{\prime};\tilde{\theta})\,\big{|}\,S=s,A=a\,,","where \(\tilde{\theta}\) is a slow moving copy of the parameters \(\theta\) that parameterize the ""target network"" and

\[(\tilde{\mathcal{T}}Q)(s,a;\tilde{\theta})=R+\gamma\max_{a^{\prime}}Q(S^{\prime },a^{\prime};\tilde{\theta})\,\big{|}\,S=s,A=a\,,\]

is the sample version of the Bellman optimality operator which defines our scalar regression target","\min_{\theta}\,\mathbb{E}_{(S,A,R,S^{\prime})\sim\mathcal{D}}\left[\left(R+\gamma\max_{a^{\prime}}Q(S^{\prime},a^{\prime};\theta^{-})-Q(S,A;\theta)\right)^{2}\right],",where $\theta^{-}$ are parameters of a target network used to stabilize training and the expectation is over transitions sampled from the replay buffer $\mathcal{D}$.
ICML_2024_oral_122,5,"is the sample version of the Bellman optimality operator which defines our scalar regression target. Most deep RL algorithms leveraging value functions follow this basic recipe, notably regressing to predictions from a target network.

We also explore the offline RL setting where an agent is trained using a fixed dataset of environment interactions (Agarwal et al., 2020; Levine et al., 2020). One widely-used offline RL method is conservative Q-learning (CQL; Kumar et al., 2020) that jointly optimizes \(\text{TD}_{\text{MSE}}\) with the following behavior regularization loss scaled by \(\alpha\in\mathbb{R}\),","\alpha\,\mathbb{E}_{\mathcal{D}}[\log\big{(}\sum_{a^{\prime}}\exp(Q(S^{ \prime},a^{\prime};\theta))\big{)}-Q(S,A;\theta)].",,"\boxed{\text{CQL}_{\text{reg}}(\theta)=\alpha\,\mathbb{E}_{\mathcal{D}}\left[\log\left(\sum_{a^{\prime}}\exp\left(Q(S,a^{\prime};\theta)\right)\right)\right]-\mathbb{E}_{\mathcal{D}}\left[Q(S,A;\theta)\right]}",This loss regularizes the Q-function by penalizing values on the dataset actions while encouraging low values on out-of-distribution actions.
ICML_2024_oral_122,6,"## 3 Value-Based RL with Classification

In this section, we describe our approach to cast the regression problem appearing in TD-learning as a classification problem. Concretely, instead of minimizing the squared distance between the scalar Q-value and its TD target (2.3) we will instead minimize the distance between categorical distributions representing these quantities. To employ this approach, we will first define the categorical representation for the action-value function \(Q(s,a)\).

**Categorical Representation.** We choose to represent \(Q\) as the expected value of a categorical distribution \(Z\in\mathcal{Z}\). This distribution is parameterized by probabilities \(\hat{p}_{i}(s,a;\theta)\) for each location or ""class"" \(z_{i}\) which are derived from the logits \(l_{i}(s,a;\theta)\) through the softmax function:","Q(s,a;\theta)=\mathbb{E}[\,Z(s,a;\theta)\,],\ Z(s,a;\theta)=\sum_{i= 1}^{m}\hat{p}_{i}(s,a;\theta)\cdot\delta_{z_{i}}, || \hat{p}_{i}(s,a;\theta)=\frac{\exp(l_{i}(s,a;\theta))}{\sum_{j=1} ^{m}\exp(l_{j}(s,a;\theta))}\,.",,"\hat{p}_{i}(s,a;\theta) = \frac{\exp(l_{i}(s,a;\theta))}{\sum_{j=1}^{m}\exp(l_{j}(s,a;\theta))},","where the categorical probabilities are obtained via softmax activation of logits $l_{i}(s,a;\theta)$ parameterized by $\theta$"
ICML_2024_oral_122,7,"To employ the cross-entropy (2.1) for TD learning, we must define a target categorical distribution supported on the same locations \(z_{i},\dots,z_{m}\) such that \(\sum_{i=1}^{m}p_{i}(S,A;\tilde{\theta})\,z_{i}\approx(\widehat{\mathcal{T}}Q )(S,A;\tilde{\theta})\) with \(p_{i}\) being the target probabilities. This enables the direct computation of the cross-entropy loss as,","\boxed{\text{TD}_{\text{CE}}(\theta)=\mathbb{E}_{\mathcal{D}}[\sum_{i=1 }^{m}p_{i}(S,A;\tilde{\theta})\log\hat{p}_{i}(S,A;\theta)]}",,"\text{TD}_{\text{CE}}(\theta)=\mathbb{E}_{\mathcal{D}}\left[-\sum_{i=1}^{m}p_{i}(S,A;\tilde{\theta})\log\left(\hat{p}_{i}(S,A;\theta)\right)\right].",This cross-entropy loss compares the target probabilities $p_i$ derived from the TD target with the predicted probabilities $\hat{p}_i$ from the model.
ICML_2024_oral_122,8,"In the subsequent sections, we explore two strategies for obtaining the target probabilities \(p_{i}(S,A;\tilde{\theta})\).

### Categorical Distributions from Scalars

The first set of methods we outline will project the scalar target \((\widehat{\mathcal{T}}Q)(S,A;\tilde{\theta})\) onto the categorical distribution supported on \(\{z_{i}\}_{i=1}^{m}\). A prevalent but naive approach for the projection step involves discretizing the scalar into one of \(m\) bins where \(z_{i}\) represents the center of the bin. The resulting one-hot distribution is ""lossy"" and induces errors in the \(Q\)-function. These errors would compound as more Bellman backups are performed, resulting in more biased estimates, and likely worse performance. To combat this, we first consider the ""two-hot"" approach (Schrittwieser et al., 2020) that represents a scalar target _exactly_ via a unique categorical distribution that puts non-zero densities on two locations that the target lies between (see Figure 1; Left).

**A Two-Hot Categorical Distribution.** Let \(z_{i}\) and \(z_{i+1}\) be the locations which lower and upper-bound the TD target \(y=(\widehat{\mathcal{T}}Q)(S,A;\tilde{\theta})\), i.e., \(z_{i}\leq y\leq z_{i+1}\). Then, the probability, \(p_{i}\) and \(p_{i+1}\), put on these locations is:","p_{i}(S,A;\tilde{\theta})=\frac{y-z_{i}}{z_{i+1}-z_{i}},\ p_{i+1}(S,A;\tilde{ \theta})=\frac{z_{i+1}-y}{z_{i+1}-z_{i}}\,.",,"p_{i}=\frac{z_{i+1}-y}{z_{i+1}-z_{i}},\quad p_{i+1}=1-p_{i},",where the two-hot distribution assigns probabilities based on the relative distance of the scalar target $y$ to the nearest grid points $z_i$ and $z_{i+1}$
ICML_2024_oral_122,9,"For all other locations, the probability prescribed by the categorical distribution is zero. In principle, this Two-Hot transformation provides a uniquely identifiable and a non-lossy representation of the scalar TD target to a categorical distribution. However, Two-Hot does not fully harness the ordinal structure of discrete regression. Specifically, the classes are not independent and instead have a natural ordering, where each class intrinsically relates to its neighbors.

The class of Histogram Losses introduced by Imani & White (2018) seeks to exploit the ordinal structure of the regression task by distributing probability mass to neighboring bins - akin to label smoothing in supervised classification (Szegedy et al., 2016). This is done by transforming a noisy version of the target into a categorical distribution, allowing probability mass to span multiple bins near the target (See Figure 1; Center), rather than being limited to two locations.

**Histograms as Categorical Distributions.** Formally, define the random variable \(Y\,|\,S,A\) with probability density \(f_{Y|S,A}\) and cumulative distribution function \(F_{Y|S,A}\) whose expectation is \((\widehat{\mathcal{T}}Q)(S,A;\tilde{\theta})\). We can project the distribution \(Y\,|\,S,A\) onto the histogram with bins of width \(\varsigma=(v_{\text{max}}-v_{\text{min}})/m\) centered at \(z_{i}\) by integrating over the interval \([z_{i}-\nicefrac{{\varsigma}}{{2}},z_{i}+\nicefrac{{\varsigma}}{{2}}]\) to obtain the probabilities,","p_{i}(S,A;\tilde{\theta}) =\int_{z_{i}-\nicefrac{{\varsigma}}{{2}}}^{z_{i}+\nicefrac{{ \varsigma}}{{2}}}f_{Y|S,A}(y|S,A)dy || =F_{Y|S,A}(z_{i}+\nicefrac{{\varsigma}}{{2}}|S,A)-F_{Y|S,A}(z_{i} -\nicefrac{{\varsigma}}{{2}}|S,A).",,"p_{i}(S,A;\tilde{\theta})=\int_{z_{i}-\nicefrac{\varsigma}{2}}^{z_{i}+ \nicefrac{\varsigma}{2}}f_{Y|S,A}(y)\,dy=F_{Y|S,A}(z_{i}+\nicefrac{\varsigma }{2})-F_{Y|S,A}(z_{i}-\nicefrac{\varsigma}{2})\,.","Projects the conditional distribution of the TD target onto a histogram by integrating over intervals centered at each bin location $z_i$, capturing ordinal structure through probability mass distribution across multiple bins."
ICML_2024_oral_122,10,"We now have a choice for the distribution \(Y\,|\,S,A\). We follow the suggestion of Imani & White (2018) in using the Gaussian distribution \(Y\,|\,S,A\sim\mathcal{N}(\mu=(\widehat{\mathcal{T}}Q)(S,A;\tilde{\theta}), \sigma^{2})\) where the variance \(\sigma^{2}\) controls the amount of label smoothing applied to the resulting categorical distribution. We refer to this method as HL-Gauss.

**How should we tune \(\sigma\) in practice?** HL-Gauss requires tuning the standard deviation \(\sigma\), in addition to the bin width \(\varsigma\) and distribution range \([v_{min},v_{max}]\). 99.7% of the samples obtained by sampling from a standard Normal distribution should lie within three standard deviations of the mean with high confidence, which corresponds to approximately \(6\cdot\sigma/\varsigma\) bins. Thus, a more interpretable hyper-parameter that we recommend tuning is \(\sigma/\varsigma\): setting it to \(K/6\) distributes most of the probability mass to \(\lceil K\rceil+1\) neighbouring locations for a mean value centered at one of the bins. Unless specified otherwise, we set \(\sigma/\varsigma=0.75\) for our experiments, which distributes mass to approximately \(6\) locations.

### Modelling the Categorical Return Distribution

In the previous section, we constructed a target distribution from the usual scalar regression target representing the expected return. Another approach is to model the distribution over future returns directly using our categorical model \(Z\)as done in distributional RL (Bellemare et al., 2023). Notably, C51 (Bellemare et al., 2017), an early distributional RL method, uses the categorical representation and minimizes the cross-entropy between the predicted distribution \(Z\) and the distributional TD target. We also investigate C51 as an alternative to Two-Hot and HL-Gauss for constructing the target distribution for the cross-entropy objective (3.1).

**Categorical Distributional RL.** The first step to modelling the categorical return distribution is to define the analogous stochastic distributional Bellman operator on \(Z\),","(\widehat{\mathcal{T}}Z)(s,a;\tilde{\theta})\overset{D}{=}\sum_{i=1}^{m} \hat{p}_{i}(S^{\prime},A^{\prime};\tilde{\theta})\cdot\delta_{R+\gamma z_{i}} \bigm{|}S=s,\,A=a\,,","where \(A^{\prime}=\operatorname*{arg\,max}_{a^{\prime}}Q(S^{\prime},a^{\prime}; \tilde{\theta})\)","p_{i}(S,A;\tilde{\theta}) = \Phi\left(\frac{z_{i}+\nicefrac{\varsigma}{2} - (\widehat{\mathcal{T}}Q)(S,A;\tilde{\theta})}{\sigma}\right) - \Phi\left(\frac{z_{i}-\nicefrac{\varsigma}{2} - (\widehat{\mathcal{T}}Q)(S,A;\tilde{\theta})}{\sigma}\right)","where $\Phi(\cdot)$ is the cumulative distribution function of the standard normal distribution, and the probabilities are computed by integrating the Gaussian density over each bin interval centered at $z_i$."
ICML_2024_oral_122,11,"\). As we can see, the stochastic distributional Bellman operator has the effect of shifting and scaling the locations \(z_{i}\) necessitating the categorical projection, first introduced by Bellemare et al. (2017). At a high level, this projection distributes probabilities proportionally to the immediate neighboring locations \(z_{j-1}\leq R_{t+1}+\gamma z_{i}\leq z_{j}\) (See Figure 1; Right). To help us identify these neighboring locations we define \(\lfloor x\rfloor=\operatorname*{arg\,max}\{z_{i}:z_{i}\leq x\}\) and \(\lceil x\rceil=\operatorname*{arg\,min}\{z_{i}:z_{i}\geq x\}\). Now the probabilities for location \(z_{i}\) can be written as,","p_{i}(S,A;\tilde{\theta})=\sum_{j=1}^{m}\hat{p}_{j}(S^{\prime},A ^{\prime};\tilde{\theta})\cdot\xi_{j}(R+\gamma z_{i}) || \xi_{j}(x)=\frac{x-z_{j}}{z_{j+1}-z_{j}}\mathds{1}\{\lfloor x \rfloor=z_{j}\}+\frac{z_{j+1}-x}{z_{j+1}-z_{j}}\mathds{1}\{\lceil x\rceil=z_ {j}\}\,.",,"p_{i}(S,A;\tilde{\theta})=\sum_{j=1}^{m}\hat{p}_{j}(S^{\prime},A^{\prime}; \tilde{\theta})\cdot\begin{cases} \frac{z_{j}-\left(R+\gamma z_{i}\right)}{z_{j}-z_{j-1}} & \text{if }i=j-1,\\ \frac{\left(R+\gamma z_{i}\right)-z_{j-1}}{z_{j}-z_{j-1}} & \text{if }i=j,\\ 0 & \text{otherwise}. \end{cases}",where the projection redistributes the probability mass from the distributional Bellman update to the nearest neighboring locations based on the shifted and scaled return values
ICML_2024_oral_125,1,"## 2 Preliminary and Problem Statement

We briefly revisit ViT and TTA in this section for the convenience of our method presentation and put **detailed related work discussions into Appendix A** due to page limits.

**Vision Transformer (ViT)**(Dosovitskiy et al., 2021). In this paper, we focus mainly on transformer-based vision models that are widely used in practice and are also hardware-friendly. We first revisit ViT here for the presentation convenience of our method. Formally, for a plain ViT \(f_{\Theta}(\cdot)\) with \(N\) layers, let \(\mathbf{E}_{i}=\{\mathbf{e}_{i}^{j},j\in\mathbb{N},0\leq j\leq m\}\) be the patch embeddings as the input of the \((i+1)\)-th layer \(L_{i+1}\), where \(m\) is the number of image patches and \(\mathbf{e}_{i}^{0}\) denote an extra learnable _classification token_ ([CLS]) of the \(i\)-th layer \(L_{i}\), the whole ViT is formulated as:","\mathbf{E}_{i} =L_{i}(\mathbf{E}_{i-1}),\ \ i=1,...,N || \dot{\mathbf{y}} =\texttt{Head}(\mathbf{e}_{N}^{0}).",,"\begin{aligned}
\mathbf{E}_{i+1} &= L_{i+1}(\mathbf{E}_i), \quad \forall i \in \{0, 1, \dots, N-1\}, \\
\text{where } \mathbf{E}_0 &= \{\mathbf{e}_{\text{pos}} \cup \mathbf{e}_{\text{patch}}^j \cup \mathbf{e}_{\text{[CLS]}}\}_{j=1}^m.
\end{aligned}","Equation 1 defines the iterative transformation of patch embeddings through ViT layers, where each layer $L_{i+1}$ updates the input embeddings $\mathbf{E}_i$ to produce $\mathbf{E}_{i+1}$, initialized with positional tokens, image patch embeddings, and the [CLS] token."
ICML_2024_oral_125,2,"**Test-Time Adaptation (TTA)**(Sun et al., 2020; Wang et al., 2021). Let \(f_{\Theta}(\cdot)\) be the model trained on labeled training dataset \(\mathcal{D}_{train}=\{(\mathbf{x}_{i},y_{i})\}_{i=1}^{N}\) and \(\mathbf{x}_{i}\sim P\left(\mathbf{x}\right)\). During testing, \(f_{\Theta}(\cdot)\) shall perform well on in-distribution (ID) test samples drawn from \(P\left(\mathbf{x}\right)\). However, given a set of out-of-distribution (OOD) testing samples \(\mathcal{D}_{test}=\{\mathbf{x}_{j}\}_{j=1}^{M}\sim Q\left(\mathbf{x}\right)\) and \(Q\left(\mathbf{x}\right)\neq P\left(\mathbf{x}\right)\), the prediction performance of \(f_{\Theta}(\cdot)\) would decrease significantly. To address this, TTA methods often seek to update the model parameters by minimizing some unsupervised/self-supervised learning objective when encountering a testing sample:","\min_{\tilde{\Theta}}\mathcal{L}(\mathbf{x};\Theta),\ \mathbf{x}\sim Q( \mathbf{x}),",where \(\tilde{\Theta}\subseteq\Theta\) denotes the model parameters involved for updating,,
ICML_2024_oral_125,3,"ng. In general, the TTA objective \(\mathcal{L}(\cdot)\) can be formulated as rotation prediction (Sun et al., 2020), contrastive learning (Bartler et al., 2022), entropy minimization (Wang et al., 2021; Niu et al., 2023), _etc_.

**Problem Statement.** In practical applications, deep models are frequently deployed on devices with limited resources, such as smartphones and embodied agents, and sometimes are even deployed after quantization or hard coding with non-modifiable parameters. These devices typically lack the capability for backward propagation, especially with large-size deep models. However, for existing TTA methods, such as SAR (Niu et al., 2023) and MEMO (Zhang et al., 2022), performing TTA necessitates one or more rounds of backward computation for each test sample. This process is highly memory- and computation-intensive, hindering the broad application of TTA methods in real-world scenarios.

## 3 Approach

In this paper, we propose a novel test-time Forward-Optimization Adaptation (FOA) method, which is also model updating-free, to boost the practicality of test-time adaptation in various real-world scenarios. From Figure 1, FOA conducts adaptation on both the input level and the output feature level. _1) Input level_: FOA inserts a new prompt as the model's input, and then solely updates this prompt online for out-of-distribution (OOD) generalization, employing a derivative-free optimizer coupled with a specially designed unsupervised fitness function (c.f. Section 3.1). _2) Output feature level_: a back-to-source activation shifting strategy seeks to further boost adaptation, which directly refines the activation features of the final layer, by aligning them from the OOD domain back to the source in-distribution (ID) domain (c.f. Section 3.2). We summarize the pseudo-code of FOA in Algorithm 1.

### Forward-Only Prompt Adaptation

Unlike prior TTA methods that update model weights using backpropagation, we aim to achieve the goal of test-time out-of-distribution generalization in a backpropagation-free manner. To this end, we explore a derivative-free optimizer for TTA, namely **covariance matrix adaptation (CMA) evolution strategy**(Hansen, 2016). However, naively applying CMA to our TTA context is infeasible, the reasons are due to: 1) For TTA, the model parameters needing update are high-dimensional (even for methods like TENT (Wang et al., 2021) that only updates the affine parameters of normalization layers), since the deep models are often with millions of parameters. This makes CMA intractable for direct deep model adaptation. 2) Conventional CMA methods rely on supervised offline learning, _i.e._, using ground-truth labels to assess candidate solutions. In contrast, TTA operates without ground-truth labels and typically in an online setting, rendering conventional CMA methods inapplicable. We empirically illustrated these issues in Table 9.

To make CMA work in TTA, we introduce a new prompt as

Figure 1: (a) An illustration of our proposed FOA. For each batch of online incoming test samples, we feed them alongside prompts \(\mathbf{p}\) into the TTA model, and calculate a fitness value that serves as a learning signal, aiding the covariance matrix adaptation (CMA) optimizer in learning the prompts \(\mathbf{p}\). This fitness function is derived from both the prediction entropy and the distribution discrepancy between the testing CLS activations and source CLS activations (calculated once). (b) We further boost the adaptation performance by directly adjusting the activations (before the final MLP head), guiding them from the testing distribution towards the source distribution.

the model's input (as in Figure 1 (a)) for updating, thereby reducing the dimension of solution space and lowering the complexity for CMA optimization, meanwhile avoiding alter model weights. Then, we devise an unsupervised fitness function to provide consistent and reliable learning signals for the CMA optimization. We depict them in the following.

**CMA-Based Prompt Adaptation.** Inspired by the demonstrated effectiveness of continuous prompt learning in the field of deep model fine-tuning (Jia et al., 2022; Bahng et al., 2022), we add new prompt embeddings at the beginning of the model input (_i.e._, before the first transformer layer) for test-time updating, while keeping all other model parameters frozen. In this way, the dimension of learnable model parameters shall be significantly reduced and thus is compatible with CMA optimization. Formally, given a test sample \(\mathbf{x}\sim Q(\mathbf{x})\) and a ViT model \(f_{\Theta}(\cdot)=\texttt{Head}(L_{i}(\cdot))\), our goal is to find an optimal prompt \(\mathbf{p}^{*}\):","\mathbf{p}^{*}=\operatorname*{arg\,min}_{\mathbf{p}}\mathcal{L}(f_{\Theta}( \mathbf{p};\mathbf{x})),","where \(\mathcal{L}(\cdot)\) is a fitness function and \(\mathbf{p}\in\mathbb{R}^{d\times N_{p}}\) consists of \(N_{p}\) prompt embeddings, each of dimension \(d\)","\mathbf{p}^{*} = \arg\min_{\mathbf{p}} \mathcal{F}(\mathbf{x}, \mathbf{p}; \Theta),",where $\mathcal{F}(\cdot)$ is the unsupervised fitness function designed to guide the covariance matrix adaptation (CMA) optimizer in learning the optimal prompt $\mathbf{p}^{*}$ for test-time adaptation.
ICML_2024_oral_125,4,"\). We solve this problem by employing the derivative-free CMA.

**Fitness Function for CMA.** To effectively solve Eqn. (4) using CMA, the primary challenge lies in developing an appropriate fitness \(\mathcal{L}(\cdot)\) to evaluate a given solution \(\mathbf{p}\). A direct approach might involve adopting existing TTA learning objectives, such as prediction entropy (Wang et al., 2021). However, this method encounters limitations when dealing with severely corrupted OOD samples, where model predictions are highly uncertain. In such cases, entropy-based measures struggle to provide consistent and reliable signals for CMA optimization. Moreover, focusing solely on optimizing entropy can lead the prompts towards degenerate and trivial solutions, as in Tables 5 and 9. To address these, we devise a new fitness to regularize the activation distribution statistics of OOD testing samples (forward propagated with optimized prompts), ensuring they are closely aligned with those from ID samples. This fitness functions at the distribution level, circumventing the issues of noise inherent in the uncertain predictions, thereby offering better stability.

_Statistics calculation._ Before TTA, we first collect a small set of source in-distribution samples \(\mathcal{D}_{S}=\{\mathbf{x}_{q}\}_{q=1}^{Q}\) and feed them into the model to obtain the corresponding CLS tokens \(\{\mathbf{e}_{i}^{0}\}_{i=1}^{N}\). Then, we calculate the mean and standard deviations of CLS tokens \(\{\mathbf{e}_{i}^{0}\}_{i=1}^{N}\) over all samples in \(\mathcal{D}_{S}\) to obtain source in-distribution statistics \(\{\mathbf{\mu}_{i}^{S},\mathbf{\sigma}_{i}^{S}\}_{i=0}^{N}\). Note that we only need a small number of in-distribution samples without labels for calculation, _e.g._, 32 samples are sufficient for the ImageNet dataset. Please refer to Figure 2 (c) for the sensitivity analyses regarding this number. Similarly, we calculate the target testing statistics \(\{\mathbf{\mu}_{i}(\mathcal{X}_{t})),\mathbf{\sigma}_{i}(\mathcal{X}_{t})\}_{i=0}^{N}\) over the current batch of testing samples \(\mathcal{X}_{t}\).

Based on the above, the overall fitness function for the \(t\)-th test batch samples \(\mathcal{X}_{t}\) is then given by:","\mathcal{L}(f_{\Theta}(\mathbf{p};\mathcal{X}_{t}))=\sum_{\mathbf{ x}\in\mathcal{X}_{t}}\sum_{c\in\mathcal{C}}-\hat{y}_{c}\log\hat{y}_{c} || \qquad+\lambda\sum_{i=1}^{N}\lvert\lvert\mathbf{\mu}_{i}(\mathcal{X}_ {t})-\mathbf{\mu}_{i}^{S}\rvert\rvert_{2}+\lVert\mathbf{\sigma}_{i}(\mathcal{X}_{t})- \mathbf{\sigma}_{i}^{S}\rvert\rvert_{2},",,"\mathcal{L}(\mathcal{X}_{t};\mathbf{p})=\alpha\cdot H\left(\frac{1}{|\mathcal{X}_{t}|}\sum_{\mathbf{x}\in\mathcal{X}_{t}}f_{\Theta}(\mathbf{p};\mathbf{x})\right)+\beta\cdot\sum_{i=0}^{N}d\left(\mathbf{\mu}_{i}^{S},\mathbf{\mu}_{i}(\mathcal{X}_{t})),\mathbf{\sigma}_{i}^{S},\mathbf{\sigma}_{i}(\mathcal{X}_{t})\right),","where $H(\cdot)$ denotes the entropy of predictions, $d(\cdot)$ measures the discrepancy between source and target activation statistics, and $\alpha,\beta$ are trade-off coefficients balancing the two terms."
ICML_2024_oral_125,5,"qn. (2) w.r.t. sample \(\mathbf{x}\), and \(\lambda\) is a trade-off parameter.

**CMA Evolution Strategy.** Instead of directly optimizing the prompt \(\mathbf{p}\) (in Eqn. 4) itself, we learn a multivariate normal distribution of \(\mathbf{p}\) using a covariance matrix adaptation (CMA) evolution strategy (Hansen and Ostermeier, 2001; Hansen et al., 2003). Here, we adopt CMA as it is one of the most successful and widely used evolutionary algorithms for non-convex black-box optimization in high-dimensional continuous solution spaces. To be specific, in each iteration \(t\) (the \(t\)-th batch of test samples \(\mathcal{X}_{t}\)), CMA samples a set/population of new candidate solutions/prompts (also known as individuals in evolution algorithms) from a parameterized multivariate normal distribution:","\mathbf{p}_{k}^{(t)}\sim\mathbf{m}^{(t)}+\tau^{(t)}\mathcal{N}(\mathbf{0},\bm {\Sigma}^{(\mathbf{t})}).",,"\mathbf{p}_{k}^{(t)} \sim \mathcal{N}(\mathbf{m}^{(t)}, \sigma^{(t)} \mathbf{C}^{(t)}),\ \ k=1,...,K,","where $\mathbf{m}^{(t)}$ is the mean vector, $\sigma^{(t)}$ is the step size, and $\mathbf{C}^{(t)}$ is the covariance matrix of the distribution at iteration $t$, and $K$ is the number of sampled candidate prompts per iteration."
ICML_2024_oral_125,6,"Here, \(k{=}1,...,K\) and \(K\) is the population size. \(\mathbf{m}^{(t)}{\in}\mathbb{R}^{dN_{p}}\) is the mean vector of the search distribution at iteration step \(t\), \(\tau^{(t)}{\in}\mathbb{R}_{+}\) is the overall standard deviation that controls the step size, and \(\mathbf{\Sigma}^{(\mathbf{t})}\) is the covariance matrix that determines the shape of distribution ellipsoid. Upon sampling the prompts \(\{\mathbf{p}_{k}^{(t)}\}_{k=1}^{K}\), we feed each \(\mathbf{p}_{k}^{(t)}\) along with the test sample \(\mathcal{X}_{t}\) into the model to yield a fitness value \(v_{k}\) associated with \(\mathbf{p}_{k}^{(t)}\). Then, we update distribution parameters\(\mathbf{m}^{(t)}\), \(\tau^{(t)}\) and \(\mathbf{\Sigma}^{(\mathbf{t})}\) based on the ranking of \(\{v_{k}\}_{k=1}^{K}\) by maximizing the likelihood of previous candidate successful solutions (c.f. Hansen (2016) for more algorithm details).

### Back-to-Source Activation Shifting

In this section, we propose a ""back-to-source activation shifting mechanism"" to further boost the adaptation performance at the feature level, in cases of the above online prompt adaptation is inadequate. This shifting scheme directly alters the model's activations during inference and is notable for not requiring backpropagation. Specifically, given a test sample \(\mathbf{x}\), we move its corresponding \(N\)-th layer's CLS feature \(\mathbf{e}_{N}^{0}\) (as shown in Eqn. (2), this feature is the input of the final task head), shifting them along the direction from out-of-distribution domain towards in-distribution domain:","\mathbf{e}_{N}^{0}arrow\mathbf{e}_{N}^{0}+\gamma\mathbf{d},",where \(\mathbf{d}\) is a shifting direction and \(\gamma\) is a step size,"\mathbf{e}_{N}^{0} \leftarrow \mathbf{e}_{N}^{0} + \eta(\mathbf{\mu}_{N}^{S} - \mathbf{\mu}_{N}(\mathcal{X}_{t})),",where $\eta$ is a scaling factor that controls the magnitude of the activation shift towards the source domain mean $\mathbf{\mu}_{N}^{S}$ from the current test batch mean $\mathbf{\mu}_{N}(\mathcal{X}_{t})$.
ICML_2024_oral_125,7,"ze. We define \(\mathbf{d}\) as the vector extending from the center of out-of-distribution testing features to the center of source in-distribution features. In our online TTA setting, with the increase of testing samples, the center of testing features shall dynamically change. Thus, we update the shifting direction \(\mathbf{d}\) online by:","\mathbf{d}_{t}=\boldsymbol{\mu}_{N}^{S}-\boldsymbol{\mu}_{N}(t),",where \(\boldsymbol{\mu}_{N}^{S}\) is the mean of the \(N\)-th final layer CLS feature \(\mathbf{e}_{N}^{0}\) and calculated over source in-distribution samples \(\mathcal{D}_{S}\) (the same one used in Eqn,"\mathbf{d}^{(t)} = \mathbf{\mu}^{S} - \mathbf{\mu}(\mathcal{X}_{t}),",where $\mathbf{\mu}^{S}$ is the mean of source in-distribution CLS features and $\mathbf{\mu}(\mathcal{X}_{t})$ is the mean of CLS features from the current test batch $\mathcal{X}_{t}$.
ICML_2024_oral_125,8,qn. (5)). \(\boldsymbol{\mu}_{N}(t)\) is the approximation of the overall test set statistics by exponential moving averages of statistics computed on sequentially arrived test samples. We define the mean estimate of the \(\mathbf{e}_{N}^{0}\) in iteration \(t\) (the \(t\)-th batch \(\mathcal{X}_{t}\)) as:,"\boldsymbol{\mu}_{N}(t)=\alpha\boldsymbol{\mu}_{N}(\mathcal{X}_{t})+(1-\alpha )\boldsymbol{\mu}_{N}(t-1),",where \(\boldsymbol{\mu}_{N}(\mathcal{X}_{t})\) is the mean of the \(N\)-th layer's CLS feature and calculated over the \(t\)-th test batch \(\mathcal{X}_{t}\),"\boldsymbol{\mu}_{N}(t) = (1-\beta)\boldsymbol{\mu}_{N}(t-1)+\beta\cdot\frac{1}{|\mathcal{X}_{t}|}\sum_{\mathbf{x}\in\mathcal{X}_{t}}\mathbf{e}_{N}^{0}(\mathbf{x}),",where $\beta$ is a smoothing factor that controls the weight of the current batch statistics in the exponential moving average update of the test set mean $\boldsymbol{\mu}_{N}(t)$.
ICML_2024_oral_127,1,"## 3 Analysis through Distribution Learnability

Our goal is to ensure that a model \(\hat{f}\) trained to perform operations \(f\), \(f\in\{r,c\}\), has bounded error. We first discuss a lower bound on the error of models in the presence of insertions, which motivates our analysis framework.

**Lower Bound on Model Generalization.** Consider a model \(\hat{f}\), trained after the \(i\)-th insertion and using dataset \(\mathbf{D}^{i}\). Assume the model is not retrained after \(k\) further insertions so that \(\hat{f}\) is used to answer queries for dataset \(\mathbf{D}^{j}\), \(j=i+k\). The following lemma shows a lower bound on the expected maximum _generalization error_ of the model to dataset \(\mathbf{D}^{j}\), defined as \(\sup_{\mathbf{x}}\mathds{E}_{\mathbf{D}\sim\chi}[\lfloor\frac{j}{i}\hat{f}(\mathbf{x})-f_ {\mathbf{D}^{j}}(\mathbf{x})\rfloor]\).

**Theorem 3.1**.: _Consider any model \(\hat{f}\) trained after the \(i\)-th insertion and on dataset \(\mathbf{D}^{i}\). For any integer \(j>i+2\) and after performing \(k=j-i\) new insertions we have_","\sup_{\mathbf{x}}\mathds{E}_{\mathbf{D}^{j}\sim\chi}[\lfloor\frac{j}{i}\hat{f}(\mathbf{x} )-f_{\mathbf{D}^{j}}(\mathbf{x})\rceil]\geq\frac{\sqrt{k}}{4},",,"\begin{equation}
\sup_{\mathbf{x}}\mathbb{E}_{\mathbf{D}^{j}\sim\chi}\left[\left|\frac{j}{i}\hat{f}(\mathbf{x}) - f_{\mathbf{D}^{j}}(\mathbf{x})\right|\right] \geq \frac{C}{\sqrt{i}} \cdot \left(1 - \frac{i}{j}\right),
\end{equation}","Equation 1 provides a lower bound on the expected maximum generalization error of a model trained after the i-th insertion and evaluated after k further insertions, capturing the degradation in performance as a function of the initial training size and the number of subsequent insertions."
ICML_2024_oral_127,2,"_when \(\mathbf{D}^{j}\) is i.i.d from any continuous distribution \(\chi\)._

Theorem 3.1 states that the expected error of a _single fixed_ model, no matter how good the model is when it is trained, after \(k\) insertions, will increase to \(\Omega(\sqrt{k})\) on some input. Consequently, to achieve an error at most \(\epsilon\), we have to retrain the model at least every \((4\epsilon)^{2}\) insertions. For any constant error \(\epsilon\), this implies \(\frac{n}{(4\epsilon)^{2}}=O(n)\) model retraining is needed when inserting \(n\) data points. Model retraining for many practical choices costs \(O(n)\) (to go over the data at least once), so that amortized insertion cost, i.e., insertion _cost per insertion_, must be at least \(O(n)\). This is significantly larger than non-learned methods, e.g., for indexing B-trees support insertions in \(O(\log n)\).

Nonetheless, the \(\sqrt{k}\) barrier (and consequently a heavy insertion cost) can be avoided, as is often done in practice, by _partial_ retraining. A common example is arranging a set of models in a tree structure and retraining parts of the tree structure as new data is inserted. This avoids a full retraining every \(O(\epsilon^{2})\) insertions, but makes smaller necessary adjustments throughout that are cheap to make. Thus, Theorem 3.1 provides a theoretical justification for many practical design choices in existing work (Ding et al., 2020; Zeighami et al., 2023; Galakatos et al., 2019) that partition the space and train multiple models, utilizing data structures built around multiple models to perform operations. We also note that such approaches often come with repartitioning and tree balancing as new data is inserted to adjust the created partitions after observing new points. Indeed such repartitioning is also necessary in the presence of insertions. Given that for a fixed set of partitions the number of points per partition will grow linearly in data size, Theorem 3.1 can be used to show the error per partition will remain large unless partitions are recreated and adjusted as new points are observed.

The error in Theorem 3.1 is independent of total data size, \(j\), and only depends on \(k\). This is because we make no assumptions on model capacity, and consequently, when the model is trained on the dataset, \(\mathbf{D}^{i}\) of size \(i\), the training error can be zero. Thus, error on \(\mathbf{D}^{j}\) only depends on how well the trained model on \(\mathbf{D}^{i}\) generalizes to \(\mathbf{D}^{j}\), which, intuitively, only depends on the difference between \(\mathbf{D}^{i}\) and \(\mathbf{D}^{j}\). Theorem 3.1 quantifies this difference in terms of \(k\).

**Analysis Framework Overview**. In light of Theorem 3.1 and existing practical modeling choices that use a set of models to perform an operation, analyzing database operations in the presence of insertion can be divided into two components: (1) how well a model can learn a set of observations (_the modeling problem_), and (2) how a set of models can be used to perform operations in the presence of insertions (_the model utilization problem_). Our framework allows studying the two separately, as discussed next.

### The Modeling Problem

Our analysis is divided into studying the problem of modeling and the problem of model utilization. We introduce the notion of _distribution learnability_ to abstract away the modeling problem when studying the utilization problem. Roughly speaking, if a distribution class is _distribution learnable_, we can use observations from the class to model their distribution operation functions _well_. In other words, if a distribution class is distribution learnable, we have a solution to the modeling problem, and thus, we can focus on the model utilization problem. Meanwhile, the modeling problem is reduced to showing distribution learnablity. In this section, we define _distribution learnability_ and discuss how we can prove a distribution class is distribution learnable.

#### 3.1.1 Defining Distribution Learnability

A distribution class is distribution learnable if there exists an algorithm that returns a good model of the data distribution given an observed dataset. Formally,

**Definition 3.2**.: A distribution class \(\mathfrak{X}\), is said to be _distribution learnable for an operation \(f\)_, \(f\in\{r,c\}\), with parameters \(\mathcal{T}_{n}^{\mathfrak{X}}\), \(\mathcal{S}_{n}^{\mathfrak{X}}\) and \(\mathcal{B}_{n}^{\mathfrak{X}}\), if for any \(\chi\subseteq\mathfrak{X}\), there exists an algorithm that takes a set of observations, \(\mathbf{D}\sim\chi\), of size \(n\) as input and returns a model \(\hat{f}\) such that:

* (_Accuracy_) If \(\mathbf{D}\) is sampled from \(\chi\), for some \(\chi\subseteq\mathfrak{X}\), we have that","\mathds{P}_{D\sim\chi}[\|f_{\chi}-\hat{f}\|_{\infty}\geq\epsilon]\leq\varkappa_ {1}e^{-\varkappa_{2}(\frac{\epsilon}{\sqrt{n}}-1)^{2}},",,"\sup_{\mathbf{x}}\mathds{E}_{\mathbf{D}\sim\chi}[\mathcal{L}(\hat{f}(\mathbf{x}), f_{\mathbf{D}}(\mathbf{x}))] \leq \mathcal{B}_{n}^{\mathfrak{X}},",Expected maximum loss between the model prediction and the true operation value is bounded by the learnability bound $\mathcal{B}_{n}^{\mathfrak{X}}$.
ICML_2024_oral_13,1,"### Autoregressive LLMs

Famously, the pre-training of LLMs such as the GPTs consists in 'learning to predict the next token' knowing previous ones, in sequences extracted from large text corpora, using the natural time ordering of the data they are being trained on. A vocabulary \(\mathcal{V}\) of \(V\) tokens is chosen; the dataset is then tokenized into a sequence of tokens in \(\mathcal{V}\); at each step, the model reads a sequence of tokens and outputs a probability distribution on \(\mathcal{V}\) predicting the next token.

Typically, as a probabilistic model, an autoregressive model will estimate the probability that \(n\) random consecutive tokens \((X_{1},\cdots,X_{n})\) are equal to \((x_{1},\cdots,x_{n})\in\mathcal{V}^{n}\) by taking the product of the (estimations of) the probabilities","\begin{split}&\mathbb{P}\{X_{1}=x_{1}\}\\ &\mathbb{P}\{X_{2}=x_{2}|X_{1}=x_{1}\}\\ &\quad\vdots\\ &\mathbb{P}\{X_{n}=x_{n}|X_{1}=x_{1},\cdots,X_{n-1}=x_{n-1 }\},\end{split}",,"P(X_{1}=x_{1},\ldots,X_{n}=x_{n}) = \prod_{i=1}^{n} P(X_{i}=x_{i} \mid X_{1}=x_{1},\ldots,X_{i-1}=x_{i-1}})",The joint probability of a sequence of tokens is modeled as the product of conditional probabilities of each token given the previous tokens.
ICML_2024_oral_13,2,"yielding an estimated probability measure \(\mathbb{P}_{n}^{\rightarrow}\) on \(\mathcal{V}^{n}\).

Autoregressive LLMs (GPTs, GRUs, LSTMs,...) thus factorize (their estimates of) the joint probabilities in terms of conditional probabilities for each token knowing past ones. This brings a number of advantages: first, this leverages the fact that for each token sequence \(x_{1},\ldots,x_{n}\), each token \(x_{k}\) is used in to predict each token \(x_{\ell}\) with \(\ell>k\). In particular, GPT (compared to the earlier BERT) includes causality-aware attention, allowing for a parallelization of the training process: a sequence \(x_{1},\ldots,x_{n}\) generates \(n-1\) fully parallelizable tasks (predict \(x_{k}\) from \((x_{1},\ldots,x_{k-1})\) for \(2\leq k\leq n\)). Also, this representation enables a naturalsampling from \(\mathbb{P}_{n}^{\rightarrow}\) (token by token), as well as data compression: the factorization decomposes these processes into many smaller substeps (Graves et al., 2023).

Autoregressive LLMs such as GPTs have enabled a massive scaling up of the number of parameters and dataset sizes, yielding numerous fascinating phenomena, e.g. scaling laws (Kaplan et al., 2020; Hoffmann et al., 2022) and emergent behavior, e.g. abilities at arithmetic operations (Shen et al., 2023), circuit computing tasks (d'Ascoli et al., 2023), or high-level linguistic proficiency.

### Arrow of Time and Language Models

While decomposing measures into a sequence of conditional probabilities is natural, it is not a priori clear why following the time direction of language to do so is optimal (except for downstream tasks, e.g. making a chatbot): what is the best order when predicting the token probabilities? A natural idea to investigate this question is simply to reverse the Arrow of Time: to estimate probabilities _backward_. This amounts to training models on time-flipped datasets: we train the same models on the same data slices for next-token predictions, but for each data slice \((x_{1},\cdots,x_{n})\) we feed the model with \((x_{n},x_{n-1},\cdots,x_{1})\) instead.

As a result, instead of (1), we take the product of the estimations of","\begin{split}&\mathbb{P}\{X_{n}=x_{n}\}\\ &\mathbb{P}\{X_{n-1}=x_{n-1}|X_{n}=x_{n}\}\\ &\quad\vdots\\ &\mathbb{P}\{X_{1}=x_{1}|X_{n}=x_{n},\cdots,X_{2}=x_{2} \}.\end{split}",,"\begin{split}&\mathbb{P}\{X_{n}=x_{n}\}\\ &\mathbb{P}\{X_{n-1}=x_{n-1}|X_{n}=x_{n}\}\\ &\quad\vdots\\ &\mathbb{P}\{X_{1}=x_{1}|X_{n}=x_{n},\cdots,X_{2}=x_{2}\},\end{split}","This equation represents the backward-time factorization of the joint probability distribution, where predictions are made in reverse order, starting from the last token and conditioning on future tokens to predict earlier ones."
ICML_2024_oral_13,3,"This yields an estimated probability measure \(\mathbb{P}_{n}^{\leftarrow}\) on \(\mathcal{V}^{n}\).

In this paper, we will speak of _forward/backward (FW/BW) model_ to refer to the same (architectural) model trained with the same hyperparameters (learning rate, batch size, training time,...) but fed with (batches of) \((x_{1},\ldots,x_{n})\) and \((x_{n},\ldots,x_{1})\) from the same dataset respectively. In other words, both models are the same, except that the FW model is trained to predict the _next_ token, while the BW one is trained to predict the _previous_ token.

**Problem 1**.: For a measure \(\mathbb{P}\) and a given model, how do the _forward and backward measures_\(\mathbb{P}_{n}^{\rightarrow}\) and \(\mathbb{P}_{n}^{\leftarrow}\) differ from one another?

For certain \(\mathbb{P}\)s, we will see universal asymmetries: for any given architecture and hyperparameters, a substantial difference between the way \(\mathbb{P}_{n}^{\rightarrow}\) and \(\mathbb{P}_{n}^{\leftarrow}\) approximate \(\mathbb{P}\) arises.

### Cross-Entropy Loss and Perplexity

LLMs are trained as follows: sample sequences of \(n\) consecutive tokens \((x_{1},\ldots,x_{n})\) from the dataset; then, for \(i=1,\ldots,n\), get a prediction \(p_{i}:\mathcal{V}\rightarrow[0,1]\) for \(X_{i}\) (given previous tokens for the FW model/next tokens for the BW one), compute the loss \(\sum_{i=1}^{n}\ell\left(p_{i},x_{i}\right)\) on the observed tokens \(x_{i}\) for a loss function \(\ell\), perform a gradient step to optimize \(\ell\), and start again.

In the training of most LLMs, the prime choice for \(\ell\) is the _cross-entropy loss_, defined by \(\ell_{\mathcal{C}}\left(\mathbf{p}_{k},x_{k}\right)=-\ln\mathbf{p}_{k}\left( x_{k}\right)\): the negative log of the _predicted probability of the observed token_. It is a _proper scoring rule_(Savage, 1971; Gneiting and Raftery, 2007), uniquely identified by certain modularity properties (Hanson, 2012); in expectation, it gives the number of nats (\(\ln 2\) times the number of bits) needed to compress \((x_{1},\ldots,x_{n})\) when using a coding scheme based on the model's estimated probabilities. Finally, and crucially for us, we have the following:

_Remark 2_.: For \(i=1,\ldots,n\), let \(\left(\mathbf{p}_{i}^{\rightarrow}\right)_{i}\) and \(\left(\mathbf{p}_{i}^{\leftarrow}\right)_{i}\) denote the predictions of the FW and BW models respectively. Setting \(\ell_{i}^{\leftarrow}:=\ell_{\mathcal{C}}\left(\mathbf{p}_{i}^{\leftarrow},x_ {i}\right)\), we have","\sum_{i=1}^{n}\ell_{i}^{arrow}=-\ln\mathbb{P}_{n}^{arrow}\{X_ {1}=x_{1},\cdots,X_{n}=x_{n}\}.",,,
ICML_2024_oral_13,4,"In particular, if the FW and BW measures coincide, the cross-entropy losses are identical.

_Remark 3_.: If \((x_{1},\ldots,x_{n})\) is sampled from \(\mathbb{P}_{n}\), denoting by \(\mathcal{L}_{n}^{\leftarrow}\) the expectations of \(\sum_{i=1}^{n}\ell_{i}^{\leftarrow}\) (estimated by the test loss of the models during training), we have","\mathcal{L}_{n}^{arrow}=\mathrm{D}_{\mathrm{KL}}(\mathbb{P}_{n} \big{|}\big{|}\mathbb{P}_{n}^{arrow})+H(\mathbb{P}_{n}),",where \(H\) denotes the entropy and \(\mathrm{D}_{\mathrm{KL}}\) the Kullback-Leibler divergence,"\mathcal{L}_{n}^{\leftarrow} = \mathbb{E}_{\mathbb{P}_{n}}\left[-\ln \mathbb{P}_{n}^{\leftarrow}\{X_{1}=x_{1},\cdots,X_{n}=x_{n}\}\right].",Equation 4 defines the expected cross-entropy loss of the backward model in terms of the expectation under the true distribution of the negative log of the backward model's estimated joint probability.
ICML_2024_oral_132,1,"## 2 Background

**Differential privacy**(Dwork et al., 2014) is a mathematical framework for formal reasoning about information leakage through a private mechanism. A learning algorithm \(\mathcal{A}\) is said to be \((\epsilon,\delta)\)_-differentially private_ (denoted \((\epsilon,\delta)\)-DP) if for all training datasets \(\mathcal{D},\mathcal{D}^{\prime}\) that differ2 in a single training sample, we have:

Footnote 2: We adopt the removal notion of adjacency, _i.e._, \(\mathcal{D}^{\prime}=\mathcal{D}\cup\mathbf{z}\) for some \(\mathbf{z}\) and vice versa.",P(\mathcal{A}(\mathcal{D})\in S)\leq e^{\epsilon}P(\mathcal{A}(\mathcal{D}^{ \prime})\in S)+\delta,,"\begin{equation}
\forall \mathcal{D},\mathcal{D}^{\prime} \text{ differing in one sample}, \forall S: \Pr[\mathcal{A}(\mathcal{D}) \in S] \leq e^{\epsilon} \Pr[\mathcal{A}(\mathcal{D}^{\prime}) \in S] + \delta
\end{equation}","Formal definition of $(\epsilon,\delta)$-differential privacy, requiring that the output distribution of algorithm $\mathcal{A}$ on adjacent datasets $\mathcal{D},\mathcal{D}^{\prime}$ differs by at most multiplicative factor $e^{\epsilon}$ and additive term $\delta$ for all measurable sets $S$."
ICML_2024_oral_132,2,"for all outcome sets \(S\). More generally, equation 1 can be expressed as a statistical divergence \(D(\mathcal{A}(\mathcal{D})||\mathcal{A}(\mathcal{D}^{\prime}))\) between the distribution of models trained on \(\mathcal{D}\) vs. \(\mathcal{D}^{\prime}\), with \((\epsilon,\delta)\)-DP corresponding to the ""hockey-stick"" divergence (Sharma & Warsi, 2013). Another useful variant is

Figure 1: (**left**) Linear probing accuracies of TAN (Sander et al., 2022) (state-of-the-art DP training method), AlexNet (Krizhevsky et al., 2017), SimCLR (Chen et al., 2020) and ViP—our DP-trained model with \(\epsilon=8\). ViP-Large can achieve similar transfer learning result as SimCLR on iNat-2021 and Places-365, and achieves similar accuracy on ImageNet as end-to-end trained AlexNet. (**right**) Average precision (AP) evaluations of SimCLR (Chen et al., 2020), Mask R-CNN (He et al., 2017) and ViP-Base on MS-COCO. Our DP-trained model outperforms both SimCLR and Mask R-CNN.

_Renyi differential privacy_ (RDP; (Mironov, 2017)), which uses the Renyi divergence \(D_{\alpha}\)(Renyi, 1961): \(\mathcal{A}\) is said to be \((\alpha,\epsilon)\)-RDP if \(D_{\alpha}(\mathcal{A}(\mathcal{D})||\mathcal{A}(\mathcal{D}^{\prime}))\leq\epsilon\). Moreover, RDP can be converted to DP via the following (Balle et al., 2020): if \(\mathcal{A}\) is \((\alpha,\epsilon_{\alpha})\)-RDP then it is also \((\epsilon,\delta)\)-DP with",\epsilon=\epsilon_{\alpha}+\log(\frac{\alpha-1}{\alpha})-\frac{ \log\delta+\log\alpha}{\alpha-1}.,,\epsilon = \epsilon_{\alpha} + \frac{\log(1/\delta)}{\alpha - 1},"Conversion formula from Renyi differential privacy (RDP) to differential privacy (DP), showing how the RDP parameters $(\alpha, \epsilon_{\alpha})$ relate to the DP parameters $(\epsilon, \delta)$."
ICML_2024_oral_132,3,"**DP-SGD training.**Abadi et al. (2016) showed that stochastic gradient descent (SGD)--the quintessential learning algorithm--can be made differentially private by perturbing the per-iteration gradient with Gaussian noise. The modified SGD update with gradient perturbation (often referred to as _DP-SGD_) is given by \(\mathbf{\theta}_{t+1}=\mathbf{\theta}_{t}-\eta_{t}\widetilde{\mathbf{g}}_{t}\), and","\widetilde{\mathbf{g}}_{t}=\frac{1}{B}(\sum_{\mathbf{x}\in\mathcal{B}_{t}} \mathsf{clip}_{C}(\nabla_{\mathbf{\theta}}\ell(\mathbf{x};\mathbf{\theta}_{t})+ \mathcal{N}(0,\sigma^{2}C^{2}\mathbf{I})),","where \(\eta_{t}\) is the learning rate, \(\mathcal{B}_{t}\) is the sampled batch, \(B\) is the average batch size, \(\sigma>0\) is the noise multiplier, and \(\mathsf{clip}_{C}\) is the operation that clips the per-sample gradient norm to at most \(C>0\)","\widetilde{\mathbf{g}}_{t}=\frac{1}{B}\sum_{i=1}^{B}\mathrm{clip}_{L}(\nabla\ell(\mathbf{\theta}_{t};\mathbf{z}_{i})) + \mathcal{N}(0,\sigma^{2}L^{2}I).",The gradient estimate $\widetilde{\mathbf{g}}_{t}$ in DP-SGD includes per-sample gradient clipping and additive Gaussian noise to ensure differential privacy.
ICML_2024_oral_132,4,"\). It can be shown that this update procedure is \((\alpha,\epsilon_{\alpha})\)-RDP for some computable \(\epsilon_{\alpha}\)(Mironov et al., 2019). The end-to-end learning algorithm by running \(T\) iterations of SGD is thus \((\alpha,T\epsilon_{\alpha})\)-RDP via composition (Mironov, 2017), and a conversion to \((\epsilon,\delta)\)-DP can be obtained using equation 2. Such privatization mechanism--per-sample clipping and injecting noise--can be easily integrated with other first-order optimization algorithms such as Adam (Kingma & Ba, 2014) and AdamW (Loshchilov & Hutter, 2017).

**Self-supervised learning (SSL)** has emerged as a prominent approach for scaling up the training of machine learning models to large-scale unlabeled datasets. Restricting our attention to the vision domain, SSL pre-trained models generalize effectively across a wide range of transfer learning downstream tasks such as classification, instance segmentation and object detection (Chen et al., 2020; Bommasani et al., 2021), especially under the scenario of limited downstream training data. Vision SSL methods can be broadly categorized as either _joint embedding-based learning_ (JE) (Chen et al., 2020; He et al., 2020; Grill et al., 2020; Zbontar et al., 2021; Chen & He, 2021) or _reconstruction-based learning_ (REC) (Bao et al., 2021; Xie et al., 2022; He et al., 2022). JE-based approaches design objective functions so that all views (or image augmentations) of the same sample have similar embeddings, while views of different samples have different embeddings. As a result, most JE-based approaches _require_ a batch containing multiple samples in order to define the objective function. On the other hand, REC-based approaches aim to optimize models to reconstruct image inputs in the pixel space based on partially masked inputs, which promotes the model to learn compressed representations that can generalize well.

**Related Work**

**Scaling up DP training.** Recently, an expanding body of literature has emerged on scaling DP training to large-scale datasets and models in both NLP and vision domains. In NLP, a series of works (Yu et al., 2021; Li et al., 2022a) showed that by combining public pre-training and scaling up the training batch size, it is possible to fine-tune the pre-trained language model to achieve reasonable downstream performance. In computer vision, Kurakin et al. (2022) first attempted to scale DP training of convolutional neural networks (ResNets) to ImageNet. De et al. (2022) further improved the performance of Kurakin et al. (2022) with a Normalizer-Free ResNet architecture and an improved training recipe. More recently, Sander et al. (2022) proposed a more efficient hyperparameter tuning method for DP training that led to state-of-the-art performance on ImageNet. It is worth noting that all these works on DP-trained computer vision models focus on training supervised models.

Figure 2: **How to pre-train differentially private transformers (_ViP_) with synthetic data?** In Step 1, we first pre-train a MAE model on synthetic images with standard optimizers (_e.g._, SGD, AdamW). We denote this model by _(Syn)-ViP_. In Step 2, we use the MAE model pre-trained on synthetic images as initialization, and then apply differential private optimizers (_e.g._, DP-SGD, DP-AdamW) to train a _ViP_ model that satisfies \((\epsilon,\delta)\)-DP.

**DP pre-training.** There are existing work on exploring the possibility of learning effective representations with large-scale differentially private pre-training. In particular, Anil et al. (2021) studied how to DP pre-train BERT models and Ponomareva et al. (2022) focused on pre-training T5 models (Raffel et al., 2020) with DP. The main difference between this work and Anil et al. (2021); Ponomareva et al. (2022) is that ours focuses on pre-training vision foundation models, whereas Anil et al. (2021); Ponomareva et al. (2022) focused on pre-training language models. To the best of our knowledge, this is the first work to explore how to pre-train large-scale vision foundation models. Furthermore, previous works (Anil et al., 2021; Ponomareva et al., 2022) lack comprehensive studies on the learned representations of the DP pre-trained models, such as linear probing and few-shot learning. In contrast, we conduct extensive experiments on understanding the quality of the learned representations in this paper. Recent work by Ganesh et al. (2023) considered a simplified toy setting and demonstrated that public pre-training is necessary for training DP models in this specific context. However, this does not rule out the possibility of effective DP representation learning in large-scale real-world scenarios, as shown in this work.

## 3 Recipe for Training DP Foundation Vision Models

In this work, we identify a successful recipe for training differentially private foundation vision models. Training DP foundation models, or in general any deep learning model with a large number of parameters, poses a significant challenge due to the large amount of injected noise--\(\mathcal{N}(0,\sigma^{2}C^{2}\mathbf{I})\) in equation 3. Indeed, current state-of-the-art differentially private deep learning models even under-perform linear models with handcrafted features when \(\epsilon\) is small (De et al., 2022; Tramer and Boneh, 2020). We propose two effective techniques that reduce the magnitude of noise injected during training while attaining strong \((\epsilon,\delta)\)-DP guarantees: **1.** Scaling up the number of training samples via self-supervised learning with masked autoencoder; and **2.** Facilitating faster training by warm-starting the model with weights pre-trained on synthetic samples.

### Differential Private SSL with Mask Autoencoder

Most existing works on differentially private training (De et al., 2022; Sander et al., 2022; Bu et al., 2022) focus on supervised learning, which inherently restricts the quantity of training samples that can be utilized. In contrast, self-supervised learning approaches unlock the use of (albeit uncurated) internet-scale training data that can be on the order of billions of samples, which can potentially satisfy the amount of data needed for DP training of high-utility models (Tramer and Boneh, 2020).

On the other hand, most existing SSL training approaches do not align with requirements in DP-SGD training. For example, SimCLR (Chen et al., 2020) requires a mini-batch of samples in order to compute the contrastive loss; BYOL (Grill et al., 2020) computes per-sample loss but it utilizes batch normalization (BN) (Ioffe and Szegedy, 2015) in the model architecture, resulting in each loss depending on a mini-batch of training samples.3 Therefore, it is challenging to perform the per-sample gradient clipping as described in equation 3. Among various types of SSL methods, we identify reconstruction-base learning with masked autoencoders (MAE) (He et al., 2022) as one of the most suitable SSL approaches for training DP foundation vision models. The training objective \(L_{\text{MAE}}(\mathbf{\theta})\) is defined as:

Footnote 3: Subsequent work by Richemond et al. (2020) demonstrated that BN can be substituted with group normalization by carefully modifying the model architecture. However, we have observed that the design of exponential moving averaged online network in BYOL can result in dynamic instability during training, which poses challenges in the context of DP training.","L_{\text{MAE}}(\mathbf{\theta}):=\frac{1}{n}\sum_{i=1}^{n}\underbrace{\xi_{\text{ MSE}}(g\circ\psi(\text{mask}(\mathbf{x}_{i});\mathbf{\theta}),\mathbf{x}_{i})}_{ \ell(\mathbf{x}_{i};\mathbf{\theta})},","where \(n\) is the number of training samples, \(\mathbf{x}_{i}\in\mathbb{R}^{C\times H\times W}\) is the input of the \(i\)-th training image (\(C\)-number of channels, \(H\)-height, \(W\)-width), mask\((\cdot)\) is a function that mask out a fraction of the image, \(\psi:\mathbb{R}^{C\times H\times W}\rightarrow\mathbb{R}^{d}\) is the encoder and \(g:\mathbb{R}^{d}\rightarrow\mathbb{R}^{C\times H\times W}\) is the decoder","L_{\text{MAE}}(\mathbf{\theta}) = \mathbb{E}_{\mathbf{x} \sim \mathcal{D}} \left[ \frac{1}{|\mathcal{M}(\mathbf{x})|} \sum_{\mathbf{z} \in \mathcal{M}(\mathbf{x})} \ell_{\text{rec}}(\mathbf{z}; \mathbf{\theta}) \right],","The MAE training objective computes the expected reconstruction loss over masked views of each sample, making it compatible with DP-SGD due to its per-sample independence."
ICML_2024_oral_15,1,"### Distortion Optimality

Previous works considered the design of \(\mathcal{T}\) only with respect to \(\mathcal{X}\). As mentioned above, Shannon's rate-distortion theory can be applied to acquire \(\mathcal{T}\) as an optimal split of \(\mathscr{X}\), formulated as:","\min_{p(T|X)}\quad I(X;T),",where \(I(X;T)\) denotes the mutual information of \(X\) and \(T\),"\begin{equation}
\mathcal{T}^* = \arg\min_{\mathcal{T}} \left\{ D(\mathcal{T}) : R(\mathcal{T}) \leq R_0 \right\},
\end{equation}",Optimal transform $\mathcal{T}^*$ minimizing distortion $D(\mathcal{T})$ under a rate constraint $R_0$.
ICML_2024_oral_15,2,"\). This formalization allows for a reconsideration of the optimal \(T\) given \(X\) for previous works, as will be analyzed in Section 4.

Information retrieval involves another term, \(Q\), and the optimization can be extended as follows:",\min_{p(T|X)} I(X;T),,"\min_{p(T|X)}\quad I(X;T) - \beta I(T;Q),","where $I(T;Q)$ denotes the mutual information between $T$ and $Q$, and $\beta$ is a trade-off parameter."
ICML_2024_oral_15,3,(2) s.t.,"I(T;Q)\geq\varepsilon,",where \(\varepsilon>0\) is a threshold,"\text{subject to } D(X, T) \leq \epsilon,","where $D(X, T)$ represents the distortion measure between $X$ and $T$, and $\epsilon$ is the maximum allowable distortion."
ICML_2024_oral_15,4,"ld. This additional constraint is essential, as \(I(X;T)=0\) when \(T\) is constant, but it also decreases \(I(T;Q)\). The mutual information \(I(T;Q)\) is closely connected to the likelihood \(p(T|Q)\) that reflects retrieval accuracy; therefore, having \(I(T;Q)\) in the optimization problem facilitates optimization of retrieval accuracy.

Formula (2) characterizes the tradeoff between index conciseness, denoted by \(I(X;T)\), and retrieval accuracy, linked to \(I(T;Q)\). This dual perspective highlights the superiority of one indexing method over another: for a fixed \(I(X;T)\), aiming for a higher \(I(T;Q)\) improves retrieval accuracy. Conversely, with a constant \(I(T;Q)\), the emphasis shifts towards more compact indexes, encouraging the sharing of semantics across the indexes.

This optimal split is equivalently reformulated via a Lagrangian \(\mathcal{L}\) as follows:","\mathcal{L}(p(T|X))=I(X;T)-\beta I(T;Q),",where \(\beta\) is the Lagrange multiplier,"\mathcal{L}(p(T|X)) = I(X;T) - \beta I(T;Q),",where $\beta$ is a Lagrange multiplier balancing the tradeoff between index conciseness and retrieval accuracy.
ICML_2024_oral_15,5,"er. This coefficient \(\beta\) determines the relative importance of \(I(T;Q)\) in \(\mathcal{L}\). When \(\beta\to 0\), the latter term can be ignored, and \(T\) shrinks to a single point so that the former term decreases to 0. Conversely, as \(\beta\) becomes large, \(T\) tends to simply copy \(X\)Because the possibilities for \(X\) are observed via the subset \(\mathcal{X}\), a large \(\beta\) corresponds to observing more documents.

To summarize, \(\beta\) is a hyperparameter related to the document set's size, \(|\mathcal{X}|\). This is natural because the optimal indexing \(p^{*}(T|X)\) would change if the number of documents increases. Nevertheless, some indexing method is optimal with respect to different \(\beta\) under certain assumptions, as we will show in Section 4.

### GDR Bottleneck

We now examine the tradeoff, or bottleneck, between \(I(X;T)\) and \(I(T;Q)\). Tishby & Zaslavsky (2015) showed that the Lagrangian in Formula (3) is equivalent to the following:",\mathcal{L}(p(T|X))=I(X;T)+\beta I(X;Q|T)+\text{constant}.,,\mathcal{L}(p(T|X)) = I(X;T) - \beta I(T;Q),where $\beta$ controls the tradeoff between index conciseness and retrieval accuracy
ICML_2024_oral_15,6,"The equivalence requires assuming the Markovian relation \(T\leftrightarrow X\leftrightarrow Q\), i.e., \(p(T|X,Q)=p(T|X)\), and Appendix A.1 summarizes the proof.

In this formulation, the conditional mutual information \(I(X;Q|T)\) quantifies the information distortion due to \(T\). \(I(X;Q)\) is constant and unrelated to the indexing \(T\). Hence, a small \(I(X;Q|T)\) means a good \(T\) for representing the joint distribution \(p(X,Q)\).

As mentioned above, Tishby et al. (2000) defined this tradeoff as an _information bottleneck_. The bottleneck can be visualized by a curve in the space of the two terms of Formula (4), as shown in Figure 2. The lower bound of \(I(X;Q|T)\) captures the optimal indexing, and it monotonically decreases as \(I(X;T)\) increases. We will empirically show in Section 6.1 that this bottleneck curve indeed occurs for GDR.

### A Theoretical Solution to Optimality

Tishby et al. (2000) showed that a stationarity point of the Lagrangian \(\mathcal{L}\) in Formula (4) must satisfy the following equation:","p^{*}(T|X)=\frac{p^{*}(T)}{Z(X,\beta)}\exp\Bigl{(}-\beta\text{KL}[p(Q|X) \ \big{\|}\ p(Q|T)]\Bigr{)},","where \(Z(X,\beta)\) is a probability normalization term, \(p^{*}(T)=\mathbb{E}_{X}[p^{*}(T|X)]\)","p^{*}(T|X)=\frac{p(T)}{Z(X)}\exp\left(-\beta D_{\mathrm{KL}}\left(p(Q|X)\parallel p(Q|T)\right)\right),",where $Z(X)$ is the normalization constant and $D_{\mathrm{KL}}$ denotes the Kullback-Leibler divergence
ICML_2024_oral_15,7,"\). The Kullback-Leibler divergence term reflects the information distortion caused by \(f:x\mapsto t\) in terms of the discrepancy between \(p(Q|x)\) and \(p(Q|t)\).

In GDR, we pursue an indexing \(f:x\mapsto t\) that is consistent with this optimal solution \(p^{*}(T|X)\). That is, the identifier string \(t=f(x)\) for a document \(x\) should be drawn from this ""best"" probability distribution \(p^{*}(T|X=x)\). A natural way to incorporate this intuition is to evaluate \(f\) by a likelihood function \(p(\mathcal{X},\mathcal{Q}|f)\) defined as follows.

**Definition 3.1**.: \(f:\mathcal{X}\to\mathcal{T}\) is called a _bottleneck-minimal indexing_ (BMI) if it maximizes the likelihood function as follows:","p(\mathcal{X},\mathcal{Q}|f)\equiv\prod_{x\in\mathcal{X}}p^{*}\big{(}X=x\ \big{|}\ T=f(x)\big{)}.",where \(p^{*}\) is given by Formula (5),"p(\mathcal{X},\mathcal{Q}|f)=\prod_{x\in\mathcal{X}}p^{*}(T=f(x))\exp\Bigl{(}-\beta\text{KL}[p(Q|x)\ \big{\|}\ p(Q|T=f(x))]\Bigr{)}.","where $p^{*}(T=f(x))$ is the marginal distribution of $T$, and the exponential term involves the Kullback-Leibler divergence between $p(Q|x)$ and $p(Q|T=f(x))$"
ICML_2024_oral_15,8,"5).

Definition 3.1 is presented in a general manner, intentionally avoiding assumptions about any specific distribution family for \(p(Q|X)\) and \(p(Q|T)\) as outlined in Formula (5). In Section 4, we will examine some existing indexing methods regarding their relations with BMI. Under this background, a new indexing method will be introduced in Section 4.4.

## 4 Indexing Methods

In the following, we consider \(\mathcal{X}\) as the semantic vector space produced by a pretrained BERT model. In this section we introduce different indexing methods under this perspective and compare them theoretically, while Section 6.2 describes our experimental comparison.

An index \(t\in\mathcal{T}\) is represented as a sequence of elements of _alphabet_, denoted as \(V\). An ID string of length \(m\) is represented as \(t=[t_{1},\cdots,t_{m}]\), where \(t_{i}\in V\) is the string's \(i\)-th ""digit"" for \(i=1,\cdots,m\).

### Hierarchical Random Indexing (HRI)

We consider the most basic method, random indexing. For each document, an ID digit is randomly selected according to some prior distribution \(p(T)\). \(\mathcal{X}\) is partitioned into \(|V|\) subsets. Furthermore, each subset is recursively partitioned for representation by \(V\). The recursive subdivision constitutes a _hierarchy_ of subdivisions of depth \(m\). A special case arises when \(|V|\geq|\mathcal{X}|\) and \(m=1\); such indexing is termed _atomic_(Tay et al., 2022), since \(\mathcal{T}\) lacks hierarchical structure.

A representative partition is obtained when \(p(Q|X)\) is uniformly distributed over a compact subset of \(\mathcal{X}\). In this case, queries corresponding to a document \(x\) are completely unrelated to the semantics of \(x\).

Figure 2: Bottleneck curves

In the experiments described in Section 6.2, we set \(V=[1,2,\cdots,30]\). The ID string's maximum length \(m\) was set to the minimum value such that \(|V|^{m}\geq|\mathcal{X}|\).

### Hierarchical \(k\)-Means Indexing (HKmI)

The typical indexing uses a hierarchical \(k\)-means clustering algorithm. A document's ID string \(t=[t_{1},\cdots,t_{m}]\) is set to the indices of clusters of depth \(m\), such that the document belongs at every level of the hierarchy, where \(k\)-means clustering is applied to partition the set of documents.

In a \(k\)-means clustering process, the clusters' centroid vectors are optimized to minimize the sum of the squared distances from each centroid to the cluster members. This procedure can be interpreted within a maximum-likelihood formulation by assuming that a document belonging to a cluster is sampled from a Gaussian distribution with the mean vector as the centroid.

Hence, \(k\)-means provides a BMI if we assume \(p(Q|X)\sim N(\mu_{x},\Sigma)\) and \(p(Q|T)\sim N(\mu_{t},\sigma^{2}I)\), where \(N\) represents a Gaussian distribution; \(\mu_{x}\) and \(\mu_{t}\) are mean vectors specific to \(x\) and \(t\), respectively; \(\Sigma\) is any covariance matrix; and \(\sigma^{2}I\) is a diagonal matrix. This conclusion immediately follows from substituting the Gaussian density functions into Formula (5). The optimality holds for any choice of \(\beta\) in Formula (3). A proof is provided in Appendix A.2. The linkage between \(k\)-means clustering and information bottleneck theory has been theoretically explored by Still et al. (2003) in depth.

Following Tay et al. (2022), we set \(\mu_{x}\) as the vectoral embedding generated by a BERT model (Devlin et al., 2019) for a document \(x\). In our experiments, we used the same settings for \(V\) and \(m\) as in HRI (Section 4.1). These settings were used as the default for the _neural corpus indexer_ (NCI) (Wang et al., 2022).

### Locality-Sensitive Hashing Indexing (LSHI)

Locality-sensitive hashing (LSH) can be considered in the same formulation. In LSH the index \(t_{i}\) is Boolean; that is, \(V=\{0,1\}\), where each element of \(t=[t_{1},\cdots,t_{i},\cdots,t_{m}]\) is independently generated by a \(p\)-stable LSH algorithm (Datar et al., 2004), which is a hyperplane classifier in Euclidean space. The hyperplane's location and direction are randomly determined. Unlike hierarchical indexing methods, LSH-based indexing does not produce ""semantic prefixes""; nevertheless, \(\mathcal{T}\) encodes location-related information.

In our experiments, LSH indexing was implemented in three steps. First, a standard LSH code, a Boolean vector, was acquired for every document. Second, every fifth entry of the vector was mapped to \(V=[1,2,\cdots,32]\). Third, LSH code collisions were resolved by appending additional digits acquired by a hierarchical \(k\)-means algorithm.

### Our Proposal: Bottleneck-Minimal Indexing (BMI)

Formula (5) indicates that the optimal indexing is dictated by the distributions \(p(Q|X)\) and \(p(Q|T)\) over the query space, rather than the document space. In other words, any indexing method implemented without considering the distribution of \(Q\) would not be able to acquire the optimal indexing. Typical previous works on GDR have this problem. The concept of leveraging query information has been empirically investigated in the contexts of vector quantization (Gupta et al., 2022; Lu et al., 2023) and recommendation systems (Zeng et al., 2023); our contribution extends this exploration by offering a theoretical rationale for the significance of query distribution, grounded in information bottleneck theory.

Hence, we propose a new indexing method that incorporates both the queries and the documents. For the purposes of simplicity in this paper, we assume \(p(Q|X=x)\) and \(p(Q|T=t)\) to follow Gaussian distributions, enabling the analytical derivation of bottleneck-minimizing indexing. In a manner akin to HKmI discussed in Section 4.2, \(k\)-means emerges as the optimal indexing strategy under this Gaussian assumption. However, our approach diverges from HKmI by employing hierarchical \(k\)-means clustering on the set \(\{\mu_{Q|x}:\forall x\in\mathcal{X}\}\), which comprises the mean vectors of the Gaussian \(p(Q|X=x)\) for each document \(x\). We examine \(k\)-means here for a fair comparison with HKmI, in addition to its simplicity. However, Definition 3.1 of BMI can be used to analyze more complex clustering algorithms under more sophisticated assumptions on \(p(Q|x)\) and \(p(Q|t)\).

Unlike \(\mu_{x}\) for HKmI, however, it is not straightforward to obtain \(\mu_{Q|x}\), and it must be estimated for each document \(x\). In this paper, we estimate \(\mu_{Q|x}\) as the mean of the BERT embeddings of queries generated from document \(x\), which is the maximum-likelihood estimator given the previously mentioned Gaussian distribution assumption of \(p(Q|x)\). A proof is provided in Appendix A.3. We chose BERT for consistency with previous works, including DSI (Tay et al., 2022) and NCI (Wang et al., 2022); in a future work, we may examine other models that are adapted to short texts.

Let \(\mathcal{Q}_{x}\) denote a set of queries for \(x\). The document \(x\) is now represented by mean(BERT(\(\mathcal{Q}_{x}\))), the mean vector of the BERT embeddings of queries in \(\mathcal{Q}_{x}\), instead of the document's BERT embedding. The hierarchical \(k\)-means algorithm is applied to \(\{\mu_{Q|x}=\text{mean}(\text{BERT}(\mathcal{Q}_{x})):x\in\mathcal{X}\}\), which produces a new set of ID strings, \(\mathcal{T}\).

We follow Wang et al. (2022) in constructing \(\mathcal{Q}_{x}\) to comprise the following three kinds of queries, as follows:

**RealQ:**: Real queries from the training set;

**GenQ:** Queries generated by a pretrained model docT5query1(Nogueira et al., 2019);

**DocSeg:** Random segments of the original documents.

Footnote 1: huggingface.co/castorini/doc2query-t5-base-msmarco

Section 5.1 gives the details of generating these queries.

In a real application, the distributions \(p(Q|X=x)\) and \(p(Q|T=t)\) may deviate from Gaussian assumptions, potentially exhibiting multi-modal characteristics. Under these conditions, \(k\)-means clustering methods become suboptimal and should be replaced by another sophisticated clustering methods that reflect the true distribution of the data. Developing such methods form a future direction.

## 5 Data and Settings

### Datasets and Metrics

We evaluated different indexing methods on two datasets: NQ320K (Kwiatkowski et al., 2019), and MARCO Lite, which is a subset extracted from the document ranking dataset in MS MARCO (Nguyen et al., 2016). The upper half of Table 1 summarizes the basic statistics of the two datasets.

The entire MS MARCO dataset has 3.2 million documents and 367,013 queries. MARCO Lite was constructed by first randomly selecting half the queries and then extracting those queries' gold-standard documents.

The lower half of Table 1 summarizes our proposed method's generated queries (i.e., \(\mathcal{Q}_{x}\)) for creating document ID strings, as described at the end of Section 4.4. For NQ320K, we followed the settings in Wang et al. (2022) to produce 15 **GenQ** queries; for MARCO Lite, we produced 5 **GenQ** queries per document. As for **DocSeg** queries, we randomly selected 10\(\sim\)12 segments (depending on the document length) per document as queries for both datasets; each segment had around 60 words.

We followed previous works by using the recall (Rec@N) and mean reciprocal rank (MRR) for evaluation, where a higher Rec@N or MRR indicates a better GDR system. For each query, GDR produces a ranking of documents with respect to their degrees of relevance to the query, via a beam-search procedure as suggested in Tay et al. (2022). In our evaluation across two datasets, each query was associated with a single gold-standard document. The Rec@N measures the percentage of queries for which the gold-standard document is among the top N documents in the ranking, while the MRR is equal to the gold-standard document's MRR. We used values of \(N=1,10,100\).

### Model and Training Settings

We used the same neural-network architecture as in NCI (Wang et al., 2022). For sequence-to-sequence generation of an index for a query, the NCI architecture combined a standard transformer (Vaswani et al., 2017) with a _prefix-aware weight-adaptive_ (PAWA) decoder. The Transformer encoder's parameters were initialized with T5 weights (Raffel et al., 2020) acquired by large-scale pretraining, while the decoder weights were randomly initialized.

We tested models of different sizes, which were initialized from the weights of T5-tiny2, T5-mini3, T5-small4, and T5-base5, where the string after ""T5"" indicates the weights. The PAWA decoder had four transformer layers. All models were trained using the default hyperparameters of NCI, as provided in its official GitHub repository6.

Footnote 2: [https://huggingface.co/google/t5-efficient-tiny](https://huggingface.co/google/t5-efficient-tiny)

Footnote 3: [https://huggingface.co/google/t5-efficient-mini](https://huggingface.co/google/t5-efficient-mini)

Footnote 4: [https://huggingface.co/t5-small](https://huggingface.co/t5-small)

Footnote 5: [https://huggingface.co/t5-base](https://huggingface.co/t5-base)

Footnote 6: github.com/solidsea98/Neural-Corpus-Indexer-NCI

For ablation tests, we considered different \(\mathcal{Q}_{x}\) variants. Apart from setting \(\mathcal{Q}_{x}=\text{GenQ}+\text{RealQ}+\text{DocSeg}\) to include all three kinds of queries, we also tested GenQ alone and \(\text{GenQ}+\text{RealQ}\). In addition, the previous method of clustering documents by their embeddings also constitutes an ablated version, denoted here as **Doc**, because it is almost equivalent to DocSeg, which uses an average of random document segments.

### Estimation of Mutual Information

To verify the information bottleneck in GDR, the mutual information \(I(X;T)\) and \(I(X;Q|T)\) must be calculated. They are defined as follows:","I(X;T) =\mathbb{E}_{X,T}\log\frac{p(X|T)}{p(X)}, || I(X;Q|T) =\mathbb{E}_{X,T,Q}\log\frac{p(X,Q|T)}{p(X|T)p(Q|T)}",,"tags and the description in <description>. The description for Equation 8 should be a one-sentence variable description. Since Equation 8 is defining mutual information, the description could be something like ""where $I(X;T)$ denotes the mutual information between $X$ and $T$"".

Wait, but looking back at the previous equations, for Equation 1 the description was ""where $I(X;T)$ denotes the mutual information of $X$ and $T$"", and Equation 4 had a similar description. However, in Equation 8, since it's the definition, the description might just be stating that it's the mutual information definition.

But the user instruction says: ""wrap one sentence variable description in <description>..."". So the description should explain what the equation is, not just restate the variables. For example, ""Definition of mutual information between $X$ and $T$"".

Putting it all together, the LaTeX code for Equation 8 should be the standard mutual information formula, written in summation form for discrete variables. The description should indicate that this is the definition of mutual information between X and T.
</think>",". The description for Equation 8 should be a one-sentence variable description. Since Equation 8 is defining mutual information, the description could be something like ""where $I(X;T)$ denotes the mutual information between $X$ and $T$"".

Wait, but looking back at the previous equations, for Equation 1 the description was ""where $I(X;T)$ denotes the mutual information of $X$ and $T$"", and Equation 4 had a similar description. However, in Equation 8, since it's the definition, the description might just be stating that it's the mutual information definition.

But the user instruction says: ""wrap one sentence variable description in"
ICML_2024_oral_15,9,(8),"=\mathbb{E}_{X,T,Q}\log\frac{p(X|Q)}{p(T|Q)}\frac{p(T)}{p(X)}.",,"I(X;Q|T) =\mathbb{E}_{X,T,Q}\log\frac{p(X,Q|T)}{p(X|T)p(Q|T)}","Conditional mutual information quantifying the information distortion due to the index $T$ in representing the joint distribution $p(X,Q)$"
ICML_2024_oral_22,1,"### Diffusion Background

Denoising diffusion probabilistic models (DDPMs) (Sohl-Dickstein et al., 2015; Ho et al., 2020) or diffusion models are defined by a forward and reverse random Markov process. The forward process takes clean data and iteratively corrupts it with noise to train a neural network \(\mathbf{\epsilon}_{\theta}\). The network \(\mathbf{\epsilon}_{\theta}\) typically inputs (noisy) data \(\mathbf{x}_{t}\), the diffusion step \(t\), and (text) conditioning information \(\mathbf{c}\). The reverse process takes random noise \(\mathbf{x}_{T}\sim\mathcal{N}(0,I)\) and iteratively refines it with the learned network to generate new data \(\mathbf{x}_{0}\) over \(T\) time steps (e.g., \(1000\)) via the sampling process,","\mathbf{x}_{t-1}=\frac{1}{\sqrt{\alpha_{t}}}\Big{(}\mathbf{x}_{t}-\frac{1-\alpha_{t}} {\sqrt{1-\bar{\alpha}_{t}}}\mathbf{\epsilon}_{\theta}(\mathbf{x}_{t},t,\mathbf{c})\Big{)} +\sigma_{t}\mathbf{\epsilon},","where \(\mathbf{\epsilon}\sim\mathcal{N}(0,I)\), \(\alpha_{0}:=1\), \(\alpha_{t}\) and \(\bar{\alpha}_{t}\) define the noise schedule, \(\sigma_{t}\) is the sampling standard deviation","\begin{equation}
\mathbf{x}_{t-1} = \frac{1}{\sqrt{\alpha_t}} \left( \mathbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \mathbf{\epsilon}_{\theta}(\mathbf{x}_t, t, \mathbf{c}) \right) + \sigma_t \mathbf{z},
\end{equation}","Equation 1 defines the reverse process update rule for denoising diffusion probabilistic models (DDPMs), where $\mathbf{x}_t$ is the noisy input at step $t$, $\mathbf{\epsilon}_{\theta}$ is the learned noise prediction network, $\alpha_t$ and $\bar{\alpha}_t$ are variance schedule parameters, $\sigma_t$ controls the noise scale, and $\mathbf{z} \sim \mathcal{N}(0, I)$ is standard Gaussian noise."
ICML_2024_oral_22,2,"on. To reduce sampling time, Denoising Diffusion Implicit Model (DDIM) sampling (Song et al., 2020) uses an alternative optimization objective that yields a faster sampling process (e.g., \(20-50\) steps) that can be deterministic. Broadly, we can denote any sampling algorithm with the notation \(\mathbf{x}_{t-1}=\texttt{Sampler}(\mathbf{\epsilon}_{\theta},\mathbf{x}_{t},t,\mathbf{c})\).

To improve text conditioning, classifier-free guidance (CFG) can be used to blend conditional and unconditional generation outputs (Ho and Salimans, 2021). When training with CFG, conditioning is randomly set to a null value a fraction of the time. During inference, the diffusion model output \(\mathbf{\epsilon}_{\theta}(\mathbf{x}_{t},t,\mathbf{c})\) is linearly combined with \(\mathbf{\epsilon}_{\theta}(\mathbf{x}_{t},t,\mathbf{c}_{\Theta})\) using the CFG scale \(w\), where \(\mathbf{c}_{\theta}\) are null embeddings. Note, CFG during inference doubles the forward passes of \(\mathbf{\epsilon}_{\theta}\). For a diffusion model review, see Appendix A.

### Problem Formulation

Instead of trying to control diffusion models by using expensive supervised training or inexact inference-time guidance-based methods, we alternatively formulate the control task as an _optimization_ problem. Notably, we can denote the output of the model after running the sampler for a total of \(T\) sampling steps as \(\mathbf{x}_{0}=\texttt{Sampler}_{T}(\mathbf{\epsilon}_{\theta},\mathbf{x}_{T},\mathbf{c})\), showing that the final output is a function of the _initial_ noise latents \(\mathbf{x}_{T}\sim~{}\mathcal{N}(0,I)\).

While \(\mathbf{x}_{T}\) is normally just considered to be a random seed, we can instead treat the initial noise latents as a free parameter to be optimized at inference-time. In particular, we define a target feature extractor \(f(\cdot)\), which only needs to be differentiable, and some corresponding loss function \(\mathcal{L}\) to measure how well the model output's particular feature matches a target control \(\mathbf{y}\). With this, we can then directly optimize \(\mathbf{x}_{T}\)_through_ the sampling process such that the model output \(\mathbf{x}_{0}\) follows the target control. Formally,","\mathbf{x}_{T}^{*} =\arg\min_{\mathbf{x}_{T}}\mathcal{L}(f(\mathbf{x}_{0}),\mathbf{y}) || \mathbf{x}_{0} =\texttt{Sampler}_{T}(\mathbf{\epsilon}_{\theta},\mathbf{x}_{T},\mathbf{c})",,"\mathbf{x}_{T}^{*}=\arg\min_{\mathbf{x}_{T}}\mathcal{L}\big{(}f(\texttt{Sampler}_{T}(\mathbf{\epsilon}_{\theta},\mathbf{x}_{T},\mathbf{c})),\mathbf{y}\big{)}.",Optimization problem formulation to find the optimal initial noise latents $\mathbf{x}_{T}^{*}$ that minimize the loss between the model output features and the target control $\mathbf{y}$.
ICML_2024_oral_24,1,"### Measuring Debater and Judge Performance

We are interested in oversight protocols with no access to labels, but existing work uses metrics that assume access to the ground truth. In this section, we introduce metrics to measure debater capabilities without this assumption. Given a dataset \(\mathcal{D}\) consisting of \(N\) questions (\(q\)) and candidate answers (\(a_{1},a_{2}\)), \(\mathcal{D}=\{q_{i},a_{i1},a_{i2}\}_{i=1}^{N}\), a debate _match_ is defined over a dataset by \((D_{1},D_{2},J)\), comprising two models \(D_{1}\) and \(D_{2}\), and a Judge \(J\). The first argument refers to which model is assigned \(a_{1}\) to support (in this case \(D_{1}\)), and the second argument refers to which model is assigned \(a_{2}\). A consultancy match with a consultant \(D\) is expressed as \((D,-,J)\) or \((-,D,J)\).

**Win rate** -- We define the win rate as the frequency with which a judge picks a specific debater's answer. For the match \((D_{1},D_{2},J)\), the win rate, \(\omega_{1}\), for Debater \(D_{1}\) is:","\omega_{1}(D_{1},D_{2},J)=\frac{1}{N}\sum_{i=1}^{N}\mathds{1}\{J(q_{i},a_{i1}, a_{i2})=a_{i1}\}",,"\omega_{1} = \frac{1}{N} \sum_{i=1}^{N} \mathbb{I}\left[J(q_i, a_{i1}, a_{i2}) = a_{i1}\right]",Win rate for Debater $D_{1}$ based on the judge's selection frequency of $a_{i1}$ across $N$ questions.
ICML_2024_oral_24,2,"In practice, this win rate depends on the assignment of the answer choice, which gives an unfair advantage, as some assignments may be easier to argue for than others (e.g. some answers are easier to defend than others). To mitigate this, we flip assignments such that \(D_{1}\) and \(D_{2}\) argue for the opposite assignments and then take the average, \(\bar{\omega}_{1}\). Assuming debaters perform equally well across all questions, we say \(D_{1}\) is _more persuasive_ than \(D_{2}\) if:","\bar{\omega}_{1}(D_{1},D_{2},J)>\frac{1}{2}",,"\bar{\omega}_{1}(D_{1},D_{2},J)=\frac{1}{2N}\sum_{i=1}^{N}\left[\mathds{1}\{J(q_{i},a_{i1},a_{i2})=a_{i1}\}+\mathds{1}\{J(q_{i},a_{i2},a_{i1})=a_{i2}\}\right]",Average win rate accounting for both argument assignments to eliminate assignment bias.
ICML_2024_oral_24,3,"**Elo rating** -- To capture a general notion of persuasiveness, we evaluate aggregate win rates for each debater across a population of co-debaters. We parameterise win rates by a latent skill, using the Elo ranking metric (Elo, 1978). We calculate ratings by minimising predicted win rate error (see Appendix D.5). _Aggregate rating_ is defined as the latent skill for a debater independent of assignment. Given two debaters, \(D_{1}\) and \(D_{2}\), and a judge \(J\), we define each player's aggregate rating, \(E_{1}\) and \(E_{2}\), as the solution to the equality:","\bar{\omega}_{1}(D_{1},D_{2},J)=\frac{1}{1+10^{(E_{2}-E_{1})/400}}",,"E_{1}(D_{1},D_{2},J) = E_{1}(D_{1},J) \quad \text{and} \quad E_{2}(D_{1},D_{2},J) = E_{2}(D_{2},J)","The aggregate Elo rating for each debater is defined as their latent skill independent of the specific match-up, capturing their overall persuasiveness across a population of co-debaters and judges."
ICML_2024_oral_24,4,"Win rate and aggregate rating are metrics that can be used to optimise oversight protocols without ground-truth labels.

To understand truthfulness in debates, we also consider the win rate \(\omega_{C}\) conditioned on being assigned all correct answers (assuming ground truth labels). We calculate a players' _correct rating_\(E^{C}\) and _incorrect rating_\(E^{I}\) as:","\omega_{C}(D_{1},D_{2},J)=\frac{1}{1+10^{(E^{I}_{2}-E^{C}_{1})/400}}",,"\omega_{C}(D_{1},D_{2},J)=\frac{1}{N_{C}}\sum_{i=1}^{N}\mathds{1}\{J(q_{i},a_{i1},a_{i2})=a_{i1} \land a_{i1} \text{ is correct}\} + \mathds{1}\{J(q_{i},a_{i1},a_{i2})=a_{i2} \land a_{i2} \text{ is correct}\}",Conditional win rate $\omega_{C}$ measures the frequency with which the judge selects the correct answer when it is available.
ICML_2024_oral_25,1,"### Background: Bayesian Neural Networks and Deep Ensembles

The possible solutions to our task is to apply the canonical Bayesian Neural Network (BNN) approach (Blundell et al., 2015; Graves, 2011) or Deep Ensembles(Lakshminarayanan et al., 2017), which are standard approaches to uncertainty decomposition. Instead of having one set of parameters, BNNs model the parameter distribution of a neural network. With the Bayesian formalism, the posterior distribution can be approximated give the training data via techniques such as variational inference (Blundell et al., 2015; Graves, 2011). Due to the prohibitive computational cost of BNNs, non-Bayesian methods such as Deep Ensembles are proposed with better scalability. Deep Ensembles maintain \(K\) models, each parameterized as \(\mathbf{\theta}^{(k)}\). Each of the \(k\) models seeks to minimize the training loss, usually the cross entropy loss for classification tasks, which is equivalent to solving the following optimization problem","\min_{\mathbf{\theta}}\mathrm{KL}(p(\mathbf{Y}|\mathbf{X})\|q(\mathbf{Y}|\mathbf{X},\mathbf{\theta})).",,"tags and the description in <description>. Also, check for any possible typos or formatting issues. For example, using \mathcal for the loss function, and ensuring that the subscript CE is properly formatted with \text{} if needed. 

Yes, in LaTeX, using \text{CE} within \mathcal might not be necessary, but typically, subscripts like CE are in text mode. Wait, actually, in the equation, the subscript is just ""CE"", which is an abbreviation, so it should be in upright text. So using \mathrm{CE} or \text{CE} would be appropriate. Since the user's example uses \text{CE}, I'll follow that. 

So the final LaTeX should be:",". Also, check for any possible typos or formatting issues. For example, using \mathcal for the loss function, and ensuring that the subscript CE is properly formatted with \text{} if needed. 

Yes, in LaTeX, using \text{CE} within \mathcal might not be necessary, but typically, subscripts like CE are in text mode. Wait, actually, in the equation, the subscript is just ""CE"", which is an abbreviation, so it should be in upright text. So using \mathrm{CE} or \text{CE} would be appropriate. Since the user's example uses \text{CE}, I'll follow that. 

So the final LaTeX should be:

<latex>\min_{\mathbf{\theta}^{(k)}} \mathcal{L}_{\text{CE}}(\mathbf{\theta}^{(k)})</latex>

And the description would be: ""Optimization problem for the k-th model in the deep ensemble, minimizing the cross-entropy loss with respect to its parameters θ^{(k)}.""

I think that's correct. Let me double-check the context to make sure I didn't miss anything. The equation is supposed to represent each model's individual optimization problem, which is minimizing cross-entropy loss. Yes, that's exactly what the context says. The parameters are θ^{(k)}, and there are K models. 

No other variables or constraints are mentioned, so the equation is straightforward. No need for regularization terms or other components. Just the minimization of the cross-entropy loss for each model.
</think>

<latex>\min_{\mathbf{\theta}^{(k)}} \mathcal{L}_{\text{CE}}(\mathbf{\theta}^{(k)})</latex>"
ICML_2024_oral_25,2,"In Deep Ensembles, different models have slightly different initialization values and thus the optimized values, \(\{\mathbf{\theta}^{(k)}\}\), are different. Denote the resulting distribution of the model parameters \(\theta\) as \(p(\mathbf{\theta}|\mathcal{D})\) (either approximated by BNNs or Deep Ensembles) where \(\mathcal{D}\) is the training dataset. Then the ensembled distribution of BNN can be represented as \(q(\mathbf{Y}|\mathbf{X})=\mathbb{E}_{q(\mathbf{\theta}|\mathcal{D})}[q(\mathbf{Y}|\mathbf{X},\mathbf{ \theta})]\). Then we can decompose the predictive uncertainty as","\mathcal{H}(q(\mathbf{Y}|\mathbf{X}))=\underbrace{\mathcal{I}(\mathbf{Y};\mathbf{\theta}|\mathbf{ X})}_{\mathfrak{D}}+\underbrace{\mathbb{E}_{q(\mathbf{\theta}|\mathcal{D})} \mathcal{H}(q(\mathbf{Y}|\mathbf{X},\mathbf{\theta}))}_{\mathfrak{D}},",where \(\mathcal{I}\) denotes the mutual information under the \(q\) distribution,"\mathrm{Var}_{q(\mathbf{Y}|\mathbf{X})}[\mathbf{Y}] = \mathbb{E}_{p(\mathbf{\theta}|\mathcal{D})}[\mathrm{Var}_{q(\mathbf{Y}|\mathbf{X},\mathbf{\theta})}[\mathbf{Y}]] + \mathrm{Var}_{p(\mathbf{\theta}|\mathcal{D})}[\mathbb{E}_{q(\mathbf{Y}|\mathbf{X},\mathbf{\theta})}[\mathbf{Y}]].","This equation decomposes the predictive uncertainty into expected variance and variance of the expectation, reflecting aleatoric and epistemic uncertainty components, respectively."
ICML_2024_oral_25,3,"on. 1 measures the disagreement among the different models; 2 measures the average uncertainty of each individual model. The above equation can be straightforwardly derived from the definition of conditional mutual information. Under certain assumptions, 1 and 2 can approximate the epistemic and aleatoric uncertainties, respectively (Gal et al., 2016). An illustration of the Deep Ensembles framework is shown in the upper panel of Figure 1.

Here is an intuitive explanation of why this is the case. According to Eq. 1, the goal of each model is to approach the ground-truth distribution, and thus can be viewed as the process of reducing the epistemic uncertainty. Therefore, if the optimization is successful, all the models will learn the true distribution, _i.e._, \(q(\mathbf{Y}|\mathbf{X},\mathbf{\theta}^{(k)})=p(\mathbf{Y}|\mathbf{X}),\forall k\), which, by definition, results in zero epistemic uncertainty. Meanwhile, 1 will also be zero because all the models produce the same prediction. Thus 1 equals epistemic uncertainty in this case. 2 would also equal the aleatoric uncertainty because the predicted distribution is equal to the true distribution.

On the other hand, if the models fail to learn the true distribution, in which case the epistemic uncertainty will be large, 1 will also be large since different models have different hyperparameter settings and will be stuck in very different local optima.

### Do BNN and Deep Ensembles work for LLMs?

Our goal of decomposing uncertainty for LLMs would be easily achieved if these methods were readily applicable to LLMs. Unfortunately, this is not the case. For BNNs, we need to significantly modify the training method of LLMs. For Deep Ensembles, the learning process in Eq. 1 is also very challenging for LLMs. Specifically, there are two types of methods for adapting LLMs to a particular task, supervised fine-tuning and prompting/in-context learning. Directly fine-tuning the model according to Eq. 1 is usually infeasible due to the limited access to model parameters and its huge requirement for computation. Even if it is feasible, it would be very time-consuming because it requires fine-tuning multiple LLMs.

On the other hand, the in-context learning method, though feasible, does not fit into the Deep Ensembles framework because it does not directly aim to optimize Eq. 1, so the decomposition will be very inaccurate. To demonstrate this, we perform a simple experiment on the AmbigQA (Min et al., 2020) dataset, which contains both ambiguous questions with multiple answers and unambiguous questions. We use the BNN method to decompose the uncertainty of ChatGPT, where the different individual model is derived by providing different in-context examples. If the decomposition method is accurate, we would expect to see that the aleatoric uncertainty for the ambiguous questions is significantly larger than that of the unambiguous ones. However, as shown in Figure 2, the gap between the uncertainties of the two groups of questions is very small. More experiment details can be found in Section 4.

While the BNN and Deep Ensemble framework do not work for LLMs, it inspires us to design an alternative framework that is almost completely symmetrical to the BNN approach, as discussed in the next subsection.

### Input Clarification Ensembling

Although modifying or adapting LLMs is challenging, it is relatively straightforward to modify the input to LLMs. By analogy to the way BNNs and Deep Ensembles ensemble different _models_ that minimize _epistemic uncertainty_ (Eq. 1), can we design a framework that ensembles different _inputs_ to minimize _aleatoric uncertainty_?

This is the motivation behind our framework, which consists of the following two steps.

**Step 1: Input Clarification.** Given an input \(\mathbf{X}\), we first generate a set of texts, \(\mathbf{C}^{(k)}\), called _clarifications_. Each clarification \(\mathbf{C}^{(k)}\) seeks to minimize the ambiguity in \(\mathbf{X}\) (and thus the aleatoric uncertainty) when appended to \(\mathbf{X}\). Formally, we denote one clarification result as \(\mathbf{X}\oplus\mathbf{C}^{k}\), where \(\oplus\) denotes concatenation. In the aforementioned example, _'Who is the president of this country?'_, possible clarifications include _'This country refers to the US.'_ and many other countries. Since there can be many valid clarifications for the input, \(\{\mathbf{C}^{(k)}\}\) is a set.

**Step 2: Ensemble.** We denote the distribution of the aforementioned input clarifications as \(q(\mathbf{C}|\mathbf{X})\) given a particular input \(\mathbf{X}\). Then, we define the predictive model \(q(\mathbf{Y}|\mathbf{X})\) as an ensemble of predictions conditional on each clarified input, _i.e._, \(q(\mathbf{Y}|\mathbf{X})=\mathbb{E}_{q(\mathbf{C}|\mathbf{X})}[q(\mathbf{Y}|\mathbf{X}\oplus\mathbf{C}, \mathbf{\theta})]\). (Model parameters \(\mathbf{\theta}\) are kept constant, and thus will be omitted for brevity below.)

We then propose to decompose the uncertainty of the ensembled model as",\mathcal{H}(q(\mathbf{Y}|\mathbf{X}))=\underbrace{\mathcal{I}(\mathbf{Y};\mathbf{C}|\mathbf{X})}_ {\mathcal{Q}^{\prime}}+\underbrace{\mathbb{E}_{q(\mathbf{C}|\mathbf{X})}\mathcal{H}( q(\mathbf{Y}|\mathbf{X}\oplus\mathbf{C}))}_{\mathcal{Q}^{\prime}}.,,\mathcal{H}(q(\mathbf{Y}|\mathbf{X}))=\underbrace{\mathcal{I}(\mathbf{Y};\mathbf{C}|\mathbf{ X})}_{\text{Clarification Disagreement}}+\underbrace{\mathbb{E}_{q(\mathbf{C}|\mathbf{X})} \mathcal{H}(q(\mathbf{Y}|\mathbf{X}\oplus\mathbf{C}))}_{\text{Clarification Uncertainty}}.,"This equation decomposes the predictive uncertainty into two components: the mutual information between the output and the clarifications (Clarification Disagreement), and the expected entropy of predictions conditioned on each clarification (Clarification Uncertainty)."
ICML_2024_oral_26,1,"### Preliminary

In our study, we begin by revisiting the general form of self-attention (Vaswani et al., 2017). To simplify the notation and focus on the core aspects, we consider the single head form of self-attention and omit normalization factors. The self-attention mechanism is fundamentally composed of queries \(Q\), keys \(K\), and values \(V\), expressed in the formula:","\begin{split}&\text{Attention}(Q,K,V)=\text{softmax}(QK^{ \intercal})V,\\ &\text{where }Q,K,V\in\mathbb{R}^{n\times d},\end{split}",where \(n\) denotes the number of tokens and \(d\) denotes the hidden dimension of the attention,,
ICML_2024_oral_26,2,". Specifically, when we denote \(Q\) as \((q_{1},q_{2},...,q_{n})\), \(K\) as \((k_{1},k_{2},...,k_{n})\), \(V\) as \((v_{1},v_{2},...,v_{n})\), and output \(O\) as \((o_{1},o_{2},...,o_{n})\), Equation 1 can be reformulated as:","\begin{split}& o_{i}=\sum_{j=1}^{n}\frac{e^{q_{i}k_{j}^{\intercal}} }{\sum_{j^{\prime}=1}^{n}e^{q_{i}k_{j^{\prime}}^{\intercal}}v_{j}},\\ &\text{where }q_{i},k_{i},v_{i}\in\mathbb{R}^{1\times d},i=\{1,2,...,n\}.\end{split}",,"\begin{split}o_{i} = \sum_{j=1}^{n} \text{softmax}(q_{i}k_{j}^{\intercal}) v_{j}, \quad \forall i \in \{1, 2, ..., n\}.\end{split}","Equation 2 reformulates the self-attention mechanism from Equation 1 by expressing the output token $o_i$ as a weighted sum of value vectors $v_j$, with weights determined by the softmax of the dot product between query vector $q_i$ and key vector $k_j$."
ICML_2024_oral_26,3,"It can be observed that the computational and memory complexity for calculating each output in a Transformer model is \(\mathcal{O}(nd)\), where \(n\) is the sequence length and \(d\) is the dimensionality of the representation. Consequently, the time and memory complexity for processing a sentence of length \(n\) scales quadratically, becoming \(\mathcal{O}(n^{2}d)\). This quadratic scaling poses a significant computational burden, particularly for longer sequences where \(n\) is large, making processing resource-intensive and challenging.

To mitigate this complexity, the concept of a kernel mechanism has been introduced as a means to reduce the computational demands of attention mechanisms, which has been introduced in Tsai et al. (2019); Katharopoulos et al. (2020); Choromanski et al. (2020). Specifically, this involves the introduction of a kernel function \(K(\cdot,\cdot)\), which acts as a positive-definite kernel capable of measuring similarity. By utilizing this kernel, the attention mechanism can be reformulated as:","o_{i}=\sum_{j=1}^{n}\frac{K(q_{i},k_{j})}{\sum_{j^{\prime}=1}^{n}\mathcal{K}( q_{i},k_{j^{\prime}})}v_{j},",,"\begin{split}&\text{Attention}(Q,K,V)=\frac{K(Q,K)V}{\text{diag}(K(Q,K)\mathbf{1})},\\ &\text{where }K(Q,K)\in\mathbb{R}^{n\times n}\text{ with }K(Q,K)_{ij}=K(q_{i},k_{j}).\end{split}","This equation reformulates the attention mechanism using a kernel function to measure similarity, reducing computational demands by avoiding explicit softmax normalization."
ICML_2024_oral_26,4,"By applying the kernel trick, it's possible to linearly decompose the attention mechanism:","o_{i}=\sum_{j=1}^{n}\frac{\phi(q_{i})\phi(k_{j})^{\intercal}}{\sum_{j^{\prime }=1}^{n}\phi(q_{i})\phi(k_{j^{\prime}})^{\intercal}}v_{j},",where \(\phi(\cdot):\mathbb{R}^{d}\rightarrow\mathbb{R}^{m}\) is a projection to map the inputs into \(m\) dimension features,"\begin{split}o_{i}=\frac{\sum_{j=1}^{n}K(q_{i},k_{j})v_{j}}{\sum_{j^{\prime}=1}^{n}K(q_{i},k_{j^{\prime}})}=\frac{\phi(q_{i})^{\intercal}\sum_{j=1}^{n}\phi(k_{j})v_{j}}{\phi(q_{i})^{\intercal}\sum_{j^{\prime}=1}^{n}\phi(k_{j^{\prime}})},\\ \text{where }\phi(\cdot)\text{ is a feature map that projects the input into a reproducing kernel Hilbert space}.\end{split}","This equation reformulates the attention mechanism using a linear decomposition enabled by the kernel trick, where the feature map $\phi(\cdot)$ allows the computation to be expressed in terms of inner products in a higher-dimensional space."
ICML_2024_oral_26,5,"es. This decomposition benefits from the fact that the computational dimensions of the keys and values can be merged, effectively reducing the computational complexity from \(\mathcal{O}(n^{2}d)\) to \(\mathcal{O}(nmd)\). Given that the dimensionality \(d\) and \(m\) is typically much smaller than the sequence length \(n\), this linearization of the attention mechanism results in a substantial decrease in computational intensity.

In the context of large language models, the cost of retraining is prohibitively high. In such scenarios, it becomes imperative to find a kernel that can equivalently replace the vanilla attention mechanism without necessitating extensive retraining. Positive Random Features (PRF) Choromanski et al. (2020) emerge as a viable candidate in this regard:","\phi_{\text{PRF}}(x)=e^{\omega x^{\intercal}-\frac{1+x^{2}}{2}},",where \(\omega\in\mathbb{R}^{m\times d}\),"\phi_{\text{PRF}}(x)=\left|\frac{1}{\sqrt{m}}\cos(Gx+b)\right|,","where $G\in\mathbb{R}^{m\times d}$ is a matrix of random Gaussian weights, $b\in\mathbb{R}^{m}$ is a vector of random biases, and $m$ is the number of random features."
ICML_2024_oral_26,6,"\). Theoretical demonstrations have established that \(e^{qk^{\intercal}}=\mathbb{E}_{\omega\sim\mathcal{N}(0.I)}[e^{\omega q^{ \intercal}-\frac{1+x^{2}}{2}}e^{\omega k^{\intercal}-\frac{1+x^{2}}{2}}]\). It means that when \(m\), the dimension of the feature space, is sufficiently large, Positive Random Features (PRF) mapping becomes an equivalent of the original attention mechanism. This equivalence suggests that, in theory, it is feasible to directly transform existing vanilla attention into linear attention using PRF mapping, thereby achieving an acceleration without loss of functionality. However, a notable challenge arises due to the need for \(m\) to be set to a significantly large value to maintain the performance by reducing the approximation error. This requirement leads to a non-negligible

Figure 1: Illustration of the proposed method, where the computation of queries and keys in the attention mechanism of a Transformer is efficiently mapped to the frequency domain using a fast Discrete Cosine Transform (DCT). This mapping effectively eliminates the softmax operation, thereby substantially reducing the computational complexity of the Transformer.

increase in computational demand. For instance, in the case of the Performer (Choromanski et al., 2020), to achieve a lossless linear attention, \(m\) often needs to be set to larger than \(d\), diminishing the benefits of reduced computational load brought by linear attention.

### Kernelized Attention in Frequency Domain

To address this issue, we first conduct a theoretical analysis of the kernel-based approach for approximating attention mechanisms. We begin with the application of Bochner's Theorem. This theorem allows us to equate the original attention computation involving queries (Q) and keys (K) - specifically the Gaussian kernel - to an integral computation akin to Equation 4.

**Theorem 2.1**.: _(Bochner's Theorem) (Feller, 1966). A continuous shift invariant scaled kernel function \(K(x,z):\mathbb{R}^{d}\to R\) is positive definite if and only if it is the Fourier Transform of a unique finite probability measure \(p\) on \(\mathbb{R}^{d}\)._","K(x,z)=\int_{\mathbb{R}^{d}}e^{i(x-z)^{\intercal}w}p(w)dw=E_{w\sim p(\cdot)}[ e^{iw^{\intercal}x}(e^{iw^{\intercal}z})^{\intercal}],",,"K(q,k)=e^{qk^{\intercal}}=\int_{\mathbb{R}^{d}}e^{i\omega(q-k)^{\intercal}}p(\omega)d\omega,","where $K(q,k)$ represents the Gaussian kernel, $q$ and $k$ are queries and keys, and $p(\omega)$ is the probability measure over frequencies $\omega$ derived from the Fourier transform of the kernel function."
ICML_2024_oral_26,7,"_where the symbol \(z^{\intercal}\) denotes the complex conjugate of \(z\)._

According to Bochner's theorem, there is a one-to-one correspondence between the kernel function \(K(x,z)\) and the probability density \(p(w)\) defined on \(\mathbb{R}^{d}\). Monte Carlo is equal weight approximation to kernel integrals. Taking \(\varphi_{p}(x):=\frac{1}{\sqrt{m}}[e^{-iw_{1}^{\intercal}x},...,e^{-iw_{m}^{ \intercal}x}]^{\intercal}\), the feature maps can be constructed as:","K(x,z)=E_{w\sim p(\cdot)}[e^{iw^{\intercal}x}(e^{iw^{\intercal}z})^{\intercal }]\approx\varphi_{p}(x)^{\intercal}\varphi_{p}^{*}(z),",where \(w_{i}\sim p(\cdot)\) are samples constructed by Monte Carlo methods,"\varphi_{p}(x):=\frac{1}{\sqrt{m}}[e^{-iw_{1}^{\intercal}x},...,e^{-iw_{m}^{\intercal}x}]^{\intercal},","where $w_{1},...,w_{m}$ are sampled i.i.d from $p(\cdot)$ and $m$ is the number of random features"
ICML_2024_oral_26,8,"ds. \(\varphi_{p}(\cdot)\) is the explicit finite dimensional feature map, which depends on the kernel \(K\). Moving forward, instead of employing the Monte Carlo method as suggested in (Choromanski et al., 2020), we utilize the Quasi-Monte Carlo method (Le et al., 2013). This shift enables the estimation of the integral using a specific uniform distribution as opposed to a randomly sampled distribution.

Utilizing Bochner's theorem allows for a transformative interpretation of the attention mechanism in Transformer models. For the Gaussian Kernel:","K_{G}(x,y):=e^{-\frac{\|x-y\|^{2}}{2}}=e^{-\frac{\|x\|^{2}+\|y\|^{2}}{2}}e^{x ^{\intercal}y},",,"K(q_i, k_j) = e^{q_i k_j^\intercal} = \mathbb{E}_{w \sim \mathcal{N}(0, I)}\left[\varphi_p(q_i)^\intercal \varphi_p(k_j)\right],",where $\varphi_p(x)$ represents the feature map derived from the Gaussian kernel using Bochner's theorem
ICML_2024_oral_26,9,"since the \(x\) and \(y\) in attention mechanism is usually normalized, the Gaussian Kernel can be regarded as \(e^{x^{\intercal}y}\), which is the same as the calculation between the queries and keys.

**Theorem 2.2**.: _The Positive Fixed Features (PFF) is formulated as:_","\varphi_{\text{PFF}}(x):=\frac{e^{-\|x\|^{2}}}{\sqrt{m}}[e^{\Phi^{-1}(t_{1}) x^{\intercal}v_{1}},...,e^{\Phi^{-1}(t_{m})x^{\intercal}v_{m}}]^{\intercal},",,"\phi_{\text{PFF}}(x)=\frac{1}{\sqrt{m}}[\cos(w_{1}^{\intercal}x),\sin(w_{1}^{\intercal}x),...,\cos(w_{m}^{\intercal}x),\sin(w_{m}^{\intercal}x)]^{\intercal},","where $w_i$ are fixed frequency components derived from the Fourier transform of the kernel, enabling efficient and deterministic feature mapping"
ICML_2024_oral_26,10,"_where \(V=[v_{1},...,v_{m}]\in\mathbb{S}^{d\times m}\) is asymptotically uniformly distributed and \(t_{i}\sim U(0,1)\). Then, \(\varphi_{\text{PFF}}(x)^{\intercal}\varphi_{\text{PFF}}(z)\) is an unbiased estimate of Gaussian kernel \(K_{G}(x,y)\)._

The proof of this theorem involves a transformation to spherical coordinates, which can be found in the supplementary material. Through this transformation, we demonstrate that an approximation based on any asymptotically uniformly distribution can closely approximate the original Gaussian kernel. Furthermore, according to (Asmussen and Glynn, 2007), when utilizing uniform sequences, the Quasi-Monte Carlo method can offer superior approximation efficiency compared to the traditional Monte Carlo method. The approximation efficiency of Quasi-Monte Carlo is \(\mathcal{O}(1/m)\), which is more favorable than the \(\mathcal{O}(1/m^{-0.5})\) efficiency of Monte Carlo. Consequently, this implies that using the PFF 9 kernel for approximating the Gaussian kernel is more advantageous than the PRF kernel in Equation 5.

**Theorem 2.3**.: _The Weighted Positive Fixed Features (WPFF) is formulated as:_","\varphi_{\text{WPFF}}(x):=\frac{De^{-\|x\|^{2}}}{\sqrt{m}}[e^{\Phi^{-1}(t_{1 })x^{\intercal}v_{1}},...,e^{\Phi^{-1}(t_{m})x^{\intercal}v_{m}}]^{\intercal},",,"\varphi_{\text{WPFF}}(x):=\frac{e^{-\|x\|^{2}}}{\sqrt{m}}\sum_{l=1}^{m}w_{l}e^{\Phi^{-1}(t_{l})x^{\intercal}v_{l}},",where $w_{l}$ are weights assigned to each feature to further enhance the approximation accuracy of the Gaussian kernel.
ICML_2024_oral_26,11,"_where \(D\) is a learnable parameter which can be optimized by the input \(x\). Then the upper bound of the integral estimation error of the objective function by WPPF (Weighted Positive Fixed Features) method is not greater than the upper bound of the integral estimation error of the objective function by PFF (Positive Fixed Features) method._

Building upon the Quasi-Monte Carlo foundation, we further introduce the concept of weighted Quasi-Monte Carlo to enhance the efficiency of approximation. This advancement aims to leverage the strengths of the Quasi-Monte Carlo method, augmenting it with strategically weighted sampling to improve the precision and convergence rates of our approximations. The detailed proof is provided in the supplementary materials.

To further accelerate the training speed, we propose the use of frequency domain transformations to reduce the required computational resources. Fast Fourier Transform (FFT) and Discrete Cosine Transform (DCT) are commonly used methods for such transformations. Compared to ordinary orthogonal transformations, frequency domain transformations have algorithms for rapid computation, significantly reducing the computational cost of our proposed mapping. Specifically, the complexity of \(\mathcal{O}(m)\) can be reduced to \(\mathcal{O}(\log(m))\). Additionally, since DCT operates in the real number domain, it demands even less computational resources and is more hardware-friendly. Therefore, we opt for the DCT to carry out our kernel mapping.

Specifically, a DCT coefficient \(\mathcal{C}\in\mathbb{R}^{d\times d}\) in the frequency domain is defined as:","\small\mathcal{C}_{j_{1}j_{2}}=s_{j_{1}}s_{j_{2}}\sum_{i_{1}=0}^{n-1}\sum_{i_{2}=0 }^{d-1}\cos(\frac{\pi(2i_{1}+1)j_{1}}{2d})\cos(\frac{\pi(2i_{2}+ 1)j_{2}}{2d}),",where \(s_{j}=\sqrt{1/d}\) if \(j=0\) and \(s_{j}=\sqrt{2/d}\) otherwise,"\mathcal{C}_{ij}=\sqrt{\frac{2}{d}}\cos\left(\frac{\pi}{d}i\left(j+\frac{1}{2 }\right)\right),","where $\mathcal{C}_{ij}$ denotes the coefficient at the $i$-th frequency and $j$-th position in the DCT matrix, enabling efficient computation through real-number operations."
ICML_2024_oral_26,12,se. The weighted mapping using DCT (which is called Weighted Discrete Cosine Features) can be reformulated as:,"\small\phi_{\text{WDCF}}(x)=De^{TCx^{\intercal}},","where \(\mathcal{C}\in\mathbb{R}^{m\times d}\) is the DCT coefficient, \(D\in\mathbb{R}^{m}\) is a learnable weight, and \(T=\text{diag}(t_{1},\dots,t_{m})\) is a random diagonal matrix following the inverse cumulative distribution","\varphi_{\text{WDCF}}(x):=\frac{De^{-\|x\|^{2}}}{\sqrt{m}}[e^{\mathcal{C}_{1}^{\intercal}x},...,e^{\mathcal{C}_{m}^{\intercal}x}]^{\intercal},","where $D$ is a learnable scaling parameter and $\mathcal{C}_{i}^{\intercal}x$ represents the inner product between the $i$-th DCT basis vector and input $x$, enabling efficient kernel approximation through frequency domain transformation."
ICML_2024_oral_26,13,"on. Note that since the \(x\) in attention mechanism is usually normalized, we ignore the term of \(\|x\|^{2}\) in Equation 9 for efficiency. Therefore, using DCT as a kernel can closely approximate the original attention mechanism while have low computation complexity. For scenarios where \(m>d\), more DCT transformations can be derived using different boundary conditions. Details can be referred to (Ahmed et al., 1974). It is noted that we set \(m=d\) to avoid increasing computational complexity in the subsequent experiments.

Therefore, the kernelized attention in frequency domain (FKA) is then reformulated as:","\small\begin{split}\text{FKA}(Q,K,V)=\phi_{\text{WDCF}}(Q)\phi_{ \text{WDCF}}(K)^{\intercal}V,\\ \text{where }Q,K,V\in\mathbb{R}^{n\times d},\end{split}",,"\begin{split} &\text{FKA}(Q,K,V)=\text{softmax}(QK^{\intercal})V\approx\phi_{\text{WDCF}}(Q)^{\intercal}\phi_{\text{WDCF}}(K)V,\\ &\text{where }\phi_{\text{WDCF}}(x)=De^{TCx^{\intercal}},\ C\in\mathbb{R}^{m\times d},\ D\in\mathbb{R}^{m},\ T=\text{diag}(t_{1},\dots,t_{m}),\ t_{i}\sim U(0,1). \end{split}","This equation reformulates the attention mechanism using Weighted Discrete Cosine Features (WDCF) in the frequency domain, approximating the original softmax attention with lower computational complexity."
ICML_2024_oral_28,1,"## 2 Learning Invariances Across Environments

The goal of OOD generalization is to build learning systems that perform well beyond the training data distribution. To this end, we collect examples from multiple environments and OOD algorithms search for invariant patterns across these environments, while disregarding environment-specific spurious correlations (Arjovsky et al., 2019). Formally, we seek a predictor \(f\) that classifies inputs \(x\) into labels \(y\), across all relevant environments \(e\in\mathcal{E}\):","f\in\operatorname*{argmin}_{f}\sup_{e\in\mathcal{E}}R^{e}(\tilde{f}),","where the risk \(R^{e}(f)=\mathbb{E}_{(x,y)\sim P^{e}}[\ell(f(x),y)]\) measures the average loss \(\ell\) incurred by the predictor \(f\) across examples from environment \(e\), all of them drawn iid from \(P^{e}\)","\begin{equation}
f: \mathcal{X} \rightarrow \mathcal{Y}, \quad \text{such that} \quad f(x^e) \perp\!\!\!\perp e \mid y^e \quad \forall e \in \mathcal{E}.
\end{equation}","The predictor $f$ should be invariant across environments $e \in \mathcal{E}$, meaning its predictions depend only on the label $y$ and not on the environment-specific features."
ICML_2024_oral_28,2,"\).

In its full generality, OOD generalization in (1) is an admittedly daunting task. To alleviate the burden, prior literature often considers the simplified and more practical version of _sub-population shift_(Sagawa et al., 2019). Given a dataset \(\mathcal{D}=\{(x_{i},y_{i},e_{i})\}_{i=1}^{n}\), the supremum in (1) is replaced by a maximum over the training environments and the risk for each environment is approximated by the empirical risk (Vapnik, 1998). The effectiveness of an OOD algorithm is then assessed by its worst-group-accuracy on a validation set.

In practice, several OOD algorithms have been successful in learning invariances across environments (Gulrajani and Lopez-Paz, 2020; Zhou et al., 2022; Wang et al., 2021; Yang et al., 2023). Despite their promise, their large-scale application is hindered by the need for human-annotated environments, which are resource-intensive and might be even sub-optimal. Different machine learning models fall

Figure 1: (a) Waterbirds problem with four groups: a _majority group_ of waterbirds in water, landbirds in land, waterbirds in land, and a _minority group_ of landbirds in water. Models often rely on spurious features to classify the majority of examples and then memorize the minority examples. (b) Worst-group-accuracy (minority) for different methods. (Dotted line) ERM achieves \(61\%\). (Dashed line) GroupDRO with human group annotations (oracle) achieves \(87\%\). (Dashdot blue line) Prior work to discover groups requires early-stopping with surgical precision. (Solid red line) XRM enables an oracle performance of \(87\%\) without requiring early stopping.

prey to different kinds of spurious correlations. In addition, there exists complex interactions between environment definition, function class, distributional shift, and cultural viewpoints (Lopez-Paz et al., 2022). Therefore, environment annotations are helpful only when revealing spurious and invariant patterns under the lens of the learning system under consideration. Could it be possible to design algorithms for the automatic discovery of environments tailored to the learning machine and data at hand?

### Discovering Environments

_Nature does not shuffle data_--Bottou (2019)

Let us reconsider the problem of OOD generalization without access to environment annotations. This time, it suffices to talk about one training distribution \(P^{\text{tt}}\) and one testing distribution \(P^{\text{te}}\). Our training data is a collection of input-label pairs \((x_{i},y_{i})\), each drawn iid from the training distribution. While \(P^{\text{tt}}\) may be the mixture of multiple environments describing interesting invariant and spurious correlations, this rich heterogeneity is shuffled together and unbeknown to us. But, if we could ""unshuffle"" the training distribution and recover the environments therein, we could invoke the OOD generalization machinery from the previous section and hope for a robust predictor. This is the purpose of automatic environment discovery.

## 3 Related Work on Environment Discovery

To discover environments, prior work often train a classifier and then assign each training example to two environments based on their loss or classification accuracy. Crucially, one must control the capacity of the classifier with surgical precision, such that it relies only on the spurious correlations. It is only in such cases that the subsequent OOD generalization algorithms can successfully disregard these spurious features.

As a result, proposals for environment discovery differ mainly in how to control the capacity of the classifier. For example, the too-good-to-be-true prior (Dagaev et al., 2021) employs a classifier with a small parameter count while correct-n-contrast (Zhang et al., 2022, CnC) applies strong weight decay regularization. Just train twice (Zheran Liu et al., 2021, JTT) and environment inference for invariant learning (Creager et al., 2020, EIIL) train a classifier for a limited number of epochs. Learning from failure (Nam et al., 2020, LfF) biases the classifier towards the use of ""simple"" features by applying a generalized version of the cross entropy loss. Other proposals, such as learning to split (Bao and Barzilay, 2022, LS) and adversarial re-weighted learning (Lahoti et al., 2020, ARL) complement capacity control with adversarial games.

However, all these methods assume having access to a human-annotated validation set to conduct such precise capacity controls. This defeats the purpose of environment discovery. In fact, if we have access to a small dataset with human-annotated environments, these examples suffice to fine-tune the last layer of a deep network towards state-of-the-art worst-group-accuracy (Izmailov et al., 2022).

For a more detailed discussion and related work, please refer to Appendix A.

## 4 Cross-Risk Minimization (XRM)

We propose Cross-Risk Minimization (XRM), an algorithm to discover environments without the need of human supervision. XRM comes with batteries included, namely a recipe for hyper-parameter tuning and a formula to annotate all training and validation data. As we will show in Section 5, environments discovered by XRM endow OOD generalization algorithms with oracle performance.

The blueprint for XRM is as follows. XRM trains two twin classifiers, each holding-in one random half of the training data (Section 4.1). During training, XRM biases each twin to absorb spurious correlation by imitating confident held-out mistakes from their sibling (Section 4.2). XRM chooses hyper-parameters for the twins based on the number of imitated mistakes (Section 4.3). Finally, and given the selected twins, XRM employs a simple ""cross-mistake"" formula to discover environment annotations for all of the training and validation examples (Section 4.4). Algorithm 1 serves as a companion to the descriptions below; Appendix C contains a PyTorch implementation. The runtime of XRM is akin to one ERM baseline on the training data.

### Twin Setup, Holding-out of Data

We start by initializing two twin classifiers \(f^{a}\) and \(f^{b}\). Without loss of generality, let these classifiers return softmax probability vectors over the \(n_{\text{classes}}\) classes in the training data. We split our training dataset \(\{(x_{i},y_{i})\}_{i=1}^{n}\) in two random halves. Formally, we construct a pair of training assignment vectors with entries \(m_{i}^{a}\sim\text{Bernoulli}(\frac{1}{2})\) and \(m_{i}^{b}=1-m_{i}^{a}\), for all \(i=1,\dots,n\). For classifier \(f^{a}\), examples with \(m_{i}^{a}=1\) are ""held-in"" and examples with \(m_{i}^{a}=0\) are ""held-out""; similarly for \(f^{b}\). Therefore, we will train classifier \(f^{a}\) on training examples where \(m_{i}^{a}=1\), and similarly for classifier \(f^{b}\). See Appendix C for implementation details.

By virtue of this arrangement, we may now estimate the generalization difficulty of any example by looking at the prediction of the twin that held-out such point. This contrasts prior methods, which consume the entire training data, and may therefore conflate generalization and memorization. Here, however, if a point is misclassified when held-out, we see this as evidence of such example belonging to the minority group. Feldman and Zhang (2020) proposes a similar ""error when holding-out"" construction as a measure of memorization. In the context of label-noise robustness, CrossSplit (Kim et al., 2023) also employs a similar approach, in which, confident held-out mistakes are indicators of a model's memorization of a noisy label.

### Twin Training, Flipping Labels

As Figure 1 shows, the test worst-group-accuracy of an ERM baseline on Waterbirds is \(62\%\). This suggests that, if using ERM to train our twins, each would be able to correctly classify roughly one half of the minority examples. If using these machines to discover environments based on prediction errors, we would dilute the spurious correlation evenly across the two discovered environments. Consequently, it would be difficult for an OOD generalization algorithm to tell apart between invariant and spurious patterns. Albeit counter-intuitive, we would like to hinder the learning process of our twins, such that they increasingly rely on spurious correlation. In the best possible case, the twins would correctly classify all majority examples and misclassify all minority examples, resulting in _zero_ worst-group accuracy.

To this end, we propose to steer away our twins from becoming empirical risk minimizers as follows. Let \(p_{i}^{\text{out}}=m_{i}^{b}f^{a}(x_{i})+m_{i}^{a}f^{b}(x_{i})\) be the held-out softmax prediction for example \((x_{i},y_{i})\). Also, let \(y_{i}^{\text{out}}=\operatorname*{arg\,max}_{j}p_{i,j}^{\text{out}}\) be the held-out predicted class label. Then, at each iteration during the training of the twins, flip \(y_{i}\) into \(y_{i}^{\text{out}}\), with probability,","(p_{y_{i}^{\text{out}}}^{\text{out}}-1/n_{\text{classes}})\cdot n_{\text{ classes}}/(n_{\text{classes}}-1),",,"H(p_{i}^{\text{out}}) = -\sum_{j=1}^{n_{\text{classes}}} p_{i,j}^{\text{out}} \log p_{i,j}^{\text{out}}.","where $H(p_{i}^{\text{out}})$ computes the entropy of the held-out softmax prediction $p_{i}^{\text{out}}$, quantifying uncertainty in twin classifiers' agreement to guide label flipping for spurious correlation amplification"
ICML_2024_oral_28,3,"and let each network to minimize their held-in cross-entropy loss--according to the moving targets.

The overarching intuition is that the label flipping Equation (2) implements an ""echo chamber"" reinforcing the twins to rely on spurious correlation. Label flipping happens more often for confident held-out mistakes and early in training. These are two footprints of spurious correlations, since these are often easier and faster to capture. (In the context of neural networks, this is often referred to as a ""simplicity bias"" (Shah et al., 2020; Pezeshki et al., 2021).) Overall, the purpose of Equation (2) is to transform the labels of the training data such that they no longer represent the original classes, but spurious bias. Finally, the adjustment of Equation (2) in terms of \(n_{\text{classes}}\) ensures low flip probabilities at initialization, where mistakes are random, and not due to spurious correlation. We note that the ""echo chamber"" effect aligns the twin networks and that is crucially different from methods that use multiple networks to either disagree with or diversify spurious features (Nam et al., 2020; Cha et al., 2021; Rame et al., 2022; Wortsman et al., 2022; Lee et al., 2023; Pagliardini et al., 2023; Lin et al., 2023; Eastwood et al., 2023).

### Twin Model Selection, Counting Label Flips

Before discovering environments, we must commit to a pair of twin classifiers. Each of the twin networks own hyper-parameters, XRM would be incomplete without a model selection criterion (Gulrajani and Lopez-Paz, 2020). We propose to select the twin hyper-parameters showing a maximum number of label flips at the last iteration, and across the training data. To reiterate, by ""counting flips"" we simply compare the vector of current labels with the vector of original labels--therefore, we do not accumulate counts of double or multiple flips per label. To understand why, recall that each label flip signifies one example that is confidently misclassified when held-out. Therefore, each label flip is evidence about reliance on spurious correlation, which consequently brings us closer to a clear-cut identification of the minority group.

### The Cross-Mistake Formula

Having committed to a pair of twins, we are ready to discover environments for all of our training and validation examples. In particular, we use a simple ""cross-mistake"" formula to annotate any example \((x,y)\) with the binary annotation, \(e(x,y)=\)","\llbracket(y\notin\operatorname*{argmax}_{j}f^{a}(x)_{j})\,\vee\,(y\notin \operatorname*{argmax}_{j}f^{b}(x)_{j})\rrbracket,","where ""\(\vee\)"" denotes logical-OR, and ""\(\llbracket\)"" is the Iverson bracket","e(x,y) = \mathbb{I}\left[\arg\max f^a(x) \ne y \land \arg\max f^b(x) \ne y\right],","where the environment annotation $e(x,y)$ is set to 1 if both twin classifiers $f^a$ and $f^b$ misclassify the example $(x,y)$, indicating a potential minority group instance, and 0 otherwise."
ICML_2024_oral_28,4,". If operating within the group-shift paradigm, we define one group per combination of label and discovered environment. Notably, the ability to annotate both training and validation examples is a feature inherited from holding-out data during twin training. More particularly, every example--within training and validation sets--is held-out for at least one of the two twins, as subsumed in Equation (3) by the logical-OR operation.

We are now ready to train the OOD generalization algorithm of our choice on top of the training data with environments discovered with XRM.

## 5 Experiments

This section presents a series of experiments to showcase the effectiveness of XRM on two well-known benchmarks. Additional experiments are also conducted to identify scenarios where XRM excels, as well as scenarios where it fails to discover relevant environments.

### Sub-population Shift Benchmarks

For sub-population shift tasks, we experiment with seven datasets and four algorithms detailed in Appendix B. We compare results with 3 sources of environment annotations:

* None: no env. annotations--class labels are used instead,
* Human: original human-annotated environments,
* XRM: inferred environments discovered by our method.

MetricsRegardless of how training and validation environments are discovered, we always report test worst-group-accuracy over the human environment annotations provided by each dataset. The tables hereby presented show averages over ten random seeds. For results with error bars, see Table 5.

XRM vs. human annotationsTable 1 shows that XRM enables oracle-like worst-group-accuracy across datasets. The performance gains are remarkable in the challenging ColorMNIST dataset, where XRM perfectly identifies digits appearing in minority colors, discovering a pair of environments conducive of stronger generalization than the ones originally proposed by humans. For the commonly-reported quartet of Waterbirds, CelebA, MultiNLI, and CivilComments, the average worst-group-accuracy is \(67.3\%\) when no group annotations are used. When using XRM, the average worst-group-accuracy significantly improves to \(80.4\%\), closely matching \(80.6\%\) achieved with human annotations.

XRM vs. other methodsTable 2 compares the worst-group-accuracy achieved by GroupDRO using XRM-inferred environments against other environment discovery methods. These include learning from failure (Nam et al., 2020, LfF), environment inference for invariant learning (Creager et al., 2020, EIIL), just train twice (Zheran Liu et al., 2021, JTT), correct-n-contrast (Zhang et al., 2022, CnC), automatic feature re-weighting (Qiu et al., 2023, AFR), and LS (Bao and Barzilay, 2022). As seen in the previous subsection, XRM achieves \(80.4\%\), nearly matching oracle performance. The second best method with no access to environment information, JTT, drops to \(58.9\%\). The best method accessing a validation set with human environment annotations, AFR, lags far from XRM, with \(78.6\%\). The computational burden to complete the results from LS was prohibitive, with more details provided in Appendix B.9. For example, one run of LS for Waterbirds, the smallest dataset, took 20 hours. An XRM run for this same dataset, on the same 32GB Volta GPU, takes 10 minutes.

### The DomainBed Benchmark

Table 3 presents additional domain generalization results on the DomainBed benchmark (Gulrajani and Lopez-Paz,

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c} \hline \hline  & \multicolumn{3}{c}{**ERM**} & \multicolumn{3}{c}{**GroupDRO**} & \multicolumn{3}{c}{**RWG**} & \multicolumn{3}{c}{**SUBG**} \\ \hline \hline  & None & Human & **XRM** & None & Human & **XRM** & None & Human & **XRM** & None & Human & **XRM** \\ \hline
**Waterbirds** & 70.4 & 76.1 & 75.3 & 71.7 & 88.0 & 86.1 & 74.8 & 87.0 & 84.5 & 73.0 & 86.7 & 76.3 \\
**CelebA** & 63.3 & 56.8 & 57.0 & 67.8 & 88.7 & 88.5 & 62.8 & 84.8 & 81.9 & 70.6 & 83.2 & 82.2 \\
**MultiNLI** & 70.7 & 72.2 & 66.7 & 68.9 & 75.3 & 72.8 & 68.4 & 71.1 & 67.2 & 70.0 & 67.8 & 71.8 \\
**CivilComments** & 66.7 & 73.6 & 71.9 & 65.5 & 73.7 & 70.1 & 66.7 & 74.0 & 72.4 & 66.4 & 71.3 & 65.6 \\
**ColorMNIST** & 10.1 & 10.0 & 13.0 & 10.0 & 10.2 & 69.5 & 10.1 & 10.6 & 70.5 & 10.1 & 10.3 & 64.3 \\
**MetaShift** & 73.8 & 75.1 & 74.0 & 75.4 & 81.8 & 78.8 & 64.6 & 75.7 & 78.6 & 64.6 & 74.5 & 77.9 \\
**ImagenetBG** & 78.0 & 78.7 & 79.2 & 78.4 & 77.6 & 75.5 & 76.9 & 79.3 & 77.8 & 79.8 & 78.8 & 77.6 \\ \hline
**Average** & 61.9 & 63.2 & 62.4 & 62.5 & 70.8 & 77.3 & 60.6 & 68.9 & 76.1 & 62.1 & 67.5 & 73.7 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Worst-group-accuracies, averaged from ten runs across datasets and algorithms, show XRM achieving oracle-level performance. When group labels are not available, class labels substitute them. Additionally, while ERM does not use group labels, it can still benefit from validation group labels for hyperparameter tuning, resulting in improved performance.

2020). Experiments compare three settings: ERM without any environment annotations, the CORAL domain generalization algorithm (Sun and Saenko, 2016) with human-annotated environments, and CORAL with environments discovered by XRM. As a note, CORAL is the best performing single-model (non-ensembling) method in the DomainBed suite. Once again, results suggest that the performance when using XRM-inferred annotations is comparable to that of human-annotated environments. Further details on these experiments are provided in Appendix B.2 with full table of results in Table 6.

### Further Analytical Experiments

Label flipping dynamics on WaterbirdsFigure 2 explores some of the behaviors of XRM on the Waterbirds dataset. In particular, the top-left panel justifies the use of ""percentage of label-flips at convergence"" as a model selection criterion for XRM, as it correlates strongly with downstream worst-group-accuracy. The two bottom panels showcase the clear separation of the minority group ""landbirds/water"" by XRM, as no landbirds in land are in the cross-mistake area. The top-right panel shows that label flipping happens almost exclusively for minority groups, and converges alongside XRM training. This provides XRM with a degree of stability, removing the need for intricate early-stopping criteria.

Revealing spurious correlations in CIFAR-10 with XRMFigure 3 applies XRM to the CIFAR-10 dataset (Krizhevsky et al., 2009). While CIFAR-10 does not contain environment annotations, the discovered environments by XRM for the ""plane"" and ""deer"" classes reveal one interesting spurious correlation, namely the background color.

As a final remark, we ablated the need for (i) holding-out data, and (ii) performing label flipping, finding that both components are essential to the performance of XRM.

## 6 When Does XRM Fail?

In the previous section, we showcased the effectiveness of XRM in successfully discovering relevant environments. However, it is important to note that XRM, like other environment discovery methods, relies on certain assumptions. In the absence of these assumptions, these methods, including XRM, may fail to discover relevant environments. In its full generality, the problem of learning invariant predictors in the absence of appropriate environment annotations is indeed impossible (Lin et al., 2022). To see that, we note that dividing data into invariance-affording environments parallels the problem of discovering the right causal structure in the field of causal inference (Pearl, 2009). To reveal the true causal structure between an invariant feature \(X_{\text{inv}}\) and a target \(Y\), one must ""control-for"" a set of variables/environments \(E\) that satisfy the following _conditional independence statement_,",Y\perp E\mid X_{\text{inv}}.,,"P(Y|X_{\text{inv}},E)=P(Y|X_{\text{inv}}).",
ICML_2024_oral_3,1,"## 3 Problem Formulation

Our goal is to improve the training and inference efficiency of pretrained LM while maintaining task performance. Intuitively, tuning fewer parameters leads to smaller training memory footprints and shorter time per training step; models with fewer parameters also run faster with less memory footprint during inference but come with task performance degradation. We aim to find the optimal parameters for training and inference without sacrificing task performance.

We formally define the problem objective as minimizing the task loss \(\mathcal{L}\) under the constraint that the total LM parameter size \(\Theta\) reaches a target sparsity (defined as the ratio of the number of parameters pruned to the original LM) \(\gamma_{T}\) after \(T\) training steps. For each training step \(t\), the sparsity of the LM remains above \(\gamma_{t}\) while the number of tuning parameters is below \(\Delta_{t}\). We control the pruning masks \(\mathcal{M}_{t}\) and tuning ranks \(\mathcal{R}_{t}\) to satisfy these constraints. We describe the optimization process as:","\operatorname*{argmin}_{\Theta_{T},\mathcal{M}_{T}} \frac{1}{|\mathcal{D}|}\sum_{x,y\in\mathcal{D}}\mathcal{L}(x,y| \Theta_{T},\mathcal{M}_{T})",,,
ICML_2024_oral_3,2,(1) s.t.,"1-\frac{\mathcal{C}(\Theta_{t},\mathcal{M}_{t})}{\mathcal{C}( \Theta_{0},\mathcal{M}_{0})}\geq\gamma_{t}, || \delta(\Theta_{t},\mathcal{M}_{t},\mathcal{R}_{t})\leq\Delta_{t}, || \forall t\in\{0,1,\dots,T\}.",,"\text{s.t.} \quad \frac{|\Theta_{t} \setminus \mathcal{M}_{t}|}{|\Theta_{t}|} \geq \gamma_{t}, \quad \forall t = 1, \dots, T","Constraint ensuring that at each training step $t$, the sparsity of the language model remains above the threshold $\gamma_t$ by controlling the pruning mask $\mathcal{M}_t$."
ICML_2024_oral_3,3,"ly.

Based on Equation (1), a higher target sparsity \(\gamma_{T}\) improves inference efficiency with fewer FLOPs and memory usage but sacrifices performance. Increasing \(\gamma_{t}\) when \(t\ll T\) also improves training efficiency. Besides, tuning more parameters with larger \(\Delta\) costs more training memory but makes the model converge faster with better task performance. Our formulation supports task performance improvements together with training and inference efficiency by dynamically adjusting the LM parameters during fine-tuning.

## 4 Adaptive Pruning and Tuning

We design **A**daptive **P**runing and **T**uning **(APT)** over LM parameters to allow efficient training and inference while maintaining task performance.

Summarized in the left of Figure 2, existing pruning methods often neglect training costs where the number of tuning parameters is more than a parameter-efficient threshold with \(\Delta_{t}\geq C(\Theta_{t},\mathcal{M}_{t})\), resulting in long training time and high memory consumption. Instead, to improve training efficiency, we prune LM parameters (increase \(\gamma_{t}\)) during early training when \(t\ll T\) while keeping \(\Delta_{t}\ll\mathcal{C}(\Theta_{t},\mathcal{M}_{t})\) to reduce training costs. In addition, we add tuning parameters (increase \(\Delta_{t}\)) in early training to effectively mitigate the degradation of LM's performance due to pruning.

**Overview.** Figure 2 shows the overview of our method that incorporates our new APT adapter for pruning and tuning. Our intuition is that pruning LMs during early fine-tuning will not hurt their task performance while reducing training and inference costs. Meanwhile, unlike existing adapters like LoRA (Hu et al., 2022) that use fixed tuning parameters, APT adapters dynamically add tuning parameters to accelerate LM convergence with superior task performance. We first introduce the architecture of APT adapters in Section 4.1. We then describe how we prune LM parameters at early fine-tuning with low cost in Section 4.2 and adaptively tune LMs to recover task performance efficiently in Section 4.3. Additionally, we explain our self-knowledge distillation technique that improves pruned LM's task performance with limited training expense in Section 4.4.

### APT adapter

We build the APT adapter architecture over LoRA, but the key difference is that APT adapter supports dynamic LM pruning and tuning. Assuming an APT adapter projects the input \(X\in\mathbb{R}^{d_{i}}\) to the output \(H_{\text{apt}}(X)\in\mathbb{R}^{d_{o}}\), we design binary pruning masks (\(m_{i}\in\mathbb{R}^{d_{i}}\) for input and \(m_{o}\in\mathbb{R}^{d_{o}}\) for output) and dynamic ranks \(r_{\text{apt}}\) in APT adapter to control the total and tuning LM parameters during fine-tuning, respectively. Specifically, with tuning parameters \(W_{A}\in\mathbb{R}^{r_{\text{apt}}\times d_{i}}\) and \(W_{B}\in\mathbb{R}^{d_{o}\times r_{\text{apt}}}\), APT adapter \(H_{\text{apt}}\) is denoted as:",H_{\text{apt}}(X)=m_{o}\circ(W+s\cdot W_{B}W_{A})X\circ m_{i},"where \(s\) is the constant scaling factor following LoRA's implementation, and \(\circ\) denotes the Hadamard product between the masks and their corresponding matrices","H_{\text{apt}}(X) = \left( \left( X \odot m_{i} \right) W_{A}^{T} \right) W_{B}^{T} \odot m_{o},","The APT adapter projects input $X$ using dynamic ranks $r_{\text{apt}}$ and binary pruning masks $m_i, m_o$ to control tuning and total parameters during fine-tuning."
ICML_2024_oral_3,4,"es. The parameter block is pruned when the multiplying mask is set to 0 and retained when set to 1. In the meantime, during fine-tuning, we dynamically increase \(r_{\text{apt}}\) for the weight matrices \(W_{B}\) and \(W_{A}\). Compared to LoRA, APT adapters can be more efficient due to more adaptive pruning and tuning over LM parameters.

In transformer-based LM fine-tuning, we add APT adapters in queries and values of multi-head attention (MHA) layers. We also add APT adapter in feed-forward network (FFN) layers when fine-tuning smaller models like RoBERTa and T5 for fast training convergence. In these cases, \(m_{i}\) prunes transformers' hidden dimension and \(m_{o}\) prunes attention heads in MHA and internal neurons in FFN layers. By learning the pruning masks and adjusting the ranks dynamically in the APT adapter, we can achieve the goal defined in Section 3 where the tuning parameter number \(\delta(\Theta_{t},\mathcal{M}_{t},\mathcal{R}_{t})\) increases to maintain task performance and the LM parameter size \(\mathcal{C}(\Theta_{t},\mathcal{M}_{t})\) decreases to support more efficient training and inference. Next, we describe the adaptive pruning and tuning procedures in detail.

### Low-cost Adaptive LM Pruning (\(\mathcal{A_{\text{p}}}\))

To benefit the efficiency of LM training and inference, APT adaptively prunes LM parameters since the start of fine-tuning. The problem is finding the parameters to be pruned and discarding them without hurting training stability. Given a task, we compute the outlier-aware salience score of parameter blocks at each early-training step when \(t\ll T\). Afterward, we use a fast search algorithm to determine the parameters to be pruned, and then we update their binary pruning masks accordingly. The upper-right of Figure 2 shows this adaptive pruning procedure.

**Outlier-aware salience scoring of LM parameters.** When determining the influence of pruning parameters on the LM performance for fine-tuning tasks, the key idea is to compute the outlier-aware salience scores of LM activations to consider both tuning and frozen parameters. In detail, salience is defined as the magnitude of parameters' weight-gradient production from previous works (Sanh et al., 2020), where","S(W_{i,j})=|W_{i,j}\cdot\frac{\partial\mathcal{L}}{\partial W_{i,j}}|",,"\mathcal{S}_{t} = \mathbb{E}_{x\sim\mathcal{D}}\left[ \left| \frac{\partial \mathcal{L}(x|\Theta_{t},\mathcal{M}_{t})}{\partial W} \odot W \right| \right]","Salience is defined as the expected magnitude of the element-wise product between the weights and their gradients over the training data distribution, which measures the importance of each parameter block for task performance."
ICML_2024_oral_3,5,"Figure 2: APT adaptively identifies pruning and tuning parameters via APT adapters during fine-tuning with little cost. APT gradually prunes LM parameters with binary pruning masks learned from our lightweight outlier-aware salience scoring function for training and inference efficiency. APT also adds tuning parameters in salient layers in LM fine-tuning through increasing dynamic ranks in APT adapters for performance recovery.

However, since the frozen weights' gradients are unreachable in PEFT settings, we compute the salience as the magnitude of the product of activations and their gradients. Additionally, we compress the activation and gradients by summing along batches before production to further reduce the training memory consumption. On the other hand, block outlier parameters play a crucial role in task-specific capabilities, as previous quantization methods suggest (Dettmers et al., 2022; Lin et al., 2023). Such effects brought by outlier parameters will be averaged if salience is only measured on the block level. To keep more outlier parameters in the pruned LMs, we combine the salience score above and the kurtosis1 of the activation together. Therefore, given the supervised finetuning dataset \(\mathcal{D}_{t}\), the outlier-aware salience score \(\hat{S}\) is defined as:

Footnote 1: Representing the density of the outlier in a distribution, the more the outliers are, the bigger the kurtosis will be.","\widetilde{S}_{t}(W_{:,j}) =\sum_{(x,y)\in\mathcal{D}_{t}}\sum_{i}|\frac{\partial\mathcal{L }(x,y|\Theta_{t},\mathcal{M}_{t})}{\partial H_{j,i}}|. || \sum_{(x,y)\in\mathcal{D}_{t}}\sum_{i}|H_{j,i}| || \hat{S}((W_{:,j}) =\widetilde{S}(W_{:,j})+(\text{Kurt}(O_{j:}))^{\frac{1}{2}}",,"\hat{S}(W_{i,j}) = \left| A_{i,j} \cdot \frac{\partial \mathcal{L}}{\partial A_{i,j}} \right| + \lambda \cdot \text{Kurtosis}(A_{i,j})",Outlier-aware salience score combining activation-gradient product magnitude and kurtosis to prioritize preserving outlier parameters during pruning
ICML_2024_oral_3,6,"on. We leave details of the salience scoring in Appendix B.

**Efficient search of LM block parameters.** Given the salience calculated in Equation (5), the next step is to learn the binary pruning masks to increase the LM sparsity above \(\gamma_{t}\). Intuitively, we shall prune the blocks with less salience score, which formulates a latency-saliency knapsack (Shen et al., 2022) task. For an LM with \(n_{L}\) transformer layers, where layer \(i\) has \(n_{h}^{i}\) MHA heads and \(n_{f}^{i}\) FFN neurons, and all transformer layers' hidden dimension sizes are \(d_{m}\), the approximated2 number LM parameter is:

Footnote 2: We ignore the model’s layer norm and bias terms since their sizes are small, and we do not count tuning parameters since they can be fully merged after training.",\mathcal{C}(\Theta_{t};\mathcal{M}_{t})\approx d_{m}\sum_{i=1}^{n_{L}}(4n_{h} ^{i}\cdot d_{h}+2n_{f}^{i}),where \(d_{h}\) is the dimension per MHA head,"\mathcal{C}(\Theta_{t},\mathcal{M}_{t})\approx\sum_{i=1}^{n_{L}}\left(n_{h}^{i} \cdot d_{m}^{2}+2n_{f}^{i}\cdot d_{m}\right)","Approximated total parameter count of the LM considering MHA heads and FFN neurons across all transformer layers, excluding layer norm, bias, and tuning parameters."
ICML_2024_oral_3,7,"ad. To keep the constraint in Equation (1), we prune MHA heads, FFN neurons, and the model hidden dimension simultaneously by reducing \(n_{h}^{i},n_{f}^{i}\), and \(d_{m}\). Hence, we first sort the blocks by their salience divided by the parameter number. As the parameter size monotonically increases with block quantity, we use binary search to identify the top salient blocks to be retained given the sparsity constraint \(\gamma_{t}\). We leave the implementation details in Appendix C for simplicity.

### Adaptive and Efficient LM Tuning (\(\mathcal{A}_{\text{T}}\))

As using PEFT methods to fine-tune pruned LMs causes notable performance decrease (illustrated in Table 2 and Table 4), we aim to dynamically add tuning parameters in LM fine-tuning to improve the model's end-task performance. However, since more tuning parameters will consume extra training time and memory, we want to add parameters in a controlled way, where new parameters are only added to task-sensitive APT adapters. As a result, we can recover pruned LMs' performance with reasonable training costs. In detail, we first calculate the salience of each APT adapter to determine their importance. Next, we select the top-half APT adapters after sorting them with salience and add their parameters by increasing their \(r_{\text{apt}}\).

**Salience scoring of APT adapter.** Since gradients of tuning parameters information are available when determining the layer salience, we can first calculate each tuning parameter's salience with Equation (3). Then, we define the salience of an APT adapter as the summation of the parameter salience scores in \(W_{B}\), denoted as \(\mathcal{I}(H_{\text{apt}})=\sum_{i,j}S(W_{B_{i},j})\), to represent each tuning APT adapter's importance3. Given the calculated \(\mathcal{I}(H_{\text{apt}})\) for each APT adapter, we can then decide where to add new tuning parameters to efficiently improve the pruned LM's task accuracy.

Footnote 3: The salience scores calculated using \(W_{B}\) and \(W_{A}\) are equal, so using either of them will get the same result.

**Dynamically adding APT adapter parameters to recover task performance.** With the importance of APT adapters \(\mathcal{I}(H_{\text{apt}})\) calculated, the next step of adaptive tuning is to add tuning parameters by increasing the salient tuning layers' ranks \(r_{\text{apt}}\in\mathcal{R}_{t}\) following budget \(\Delta_{t}\). Therefore, firstly, we sort all tuning layers according to their importance score \(\mathcal{I}(H_{\text{apt}})\) and linearly increase the ranks of the top-half salient ones. More specifically, when increasing the tuning parameter from \(\Delta_{t}\) to \(\Delta_{t^{\prime}}\), the salient layer's rank is changed from \(r_{\text{apt}}\) to \(r^{\prime}_{\text{apt}}=\lfloor r_{\text{apt}}\cdot\frac{\Delta_{t^{\prime}} }{\Delta_{t^{\prime}}}\rfloor\) where \(\lfloor\cdot\rfloor\) denotes the floor operation. For training stability, when adding parameters and converting \(W_{B}\in\mathbb{R}^{d_{\text{o}}\times r_{\text{apt}}},W_{A}\in\mathbb{R}^{r_{ \text{apt}}\times d_{\text{id}}}\) to \(W^{\prime}_{B}\in\mathbb{R}^{d_{\text{o}}\times r^{\prime}_{\text{apt}}},W^{ \prime}_{A}\in\mathbb{R}^{r^{\prime}_{\text{apt}}\times d_{\text{id}}}\), we concatenate random Gaussian initialized parameters \(\mathcal{N}(0,\sigma^{2})\) in \(W_{A}\) and zeros in \(W_{B}\) same as the LoRA initialization, so the layer's output remains unchanged before and after new parameters added.

### Efficient Self-Knowledge Distillation

As shown in Table 4, training pruned LM without knowledge distillation causes significant end-task performance drops. Therefore, we use knowledge distillation in APT to recover the pruned LM's performance. Still, existing strategies require a fully trained teacher model being put into the GPU with the student during distillation, causing high training time and memory. To avoid extra training costs, we keep duplicating the tuning student layers as teachers during fine-tuning to reduce total training time. Meanwhile, frozen parameters are shared between the student and teacher model during training to reduce memory consumption. We edit the distillation objective in CoFi (Xia et al., 2022) as","\begin{split}\mathcal{L}&=\mu\mathcal{L}_{ distill}+(1-\mu)\mathcal{L}_{ft}\\ \mathcal{L}_{layer}&=\sum_{i=1}^{\mathcal{T}} \text{MSE}(\text{Tr}(H_{s}^{\phi(i)}),H_{t}^{i})\end{split}","where \(\mu\) is a moving term linearly scales from 0 to 1 during distillation to encourage the pre-pruned model vastly fit to the training data, \(\mathcal{L}_{distill}\) is the distillation objective from CoFi, and \(\mathcal{L}_{ft}\) is the supervised fine-tuning objective",,
ICML_2024_oral_30,1,"### Low-Rank Adaptation (LoRA)

Building upon the hypothesis that updates made during the fine-tuning exhibit a low ""intrinsic rank"", LoRA (Hu et al., 2022) proposes using the product of two low-rank matrices to update the pre-trained weights incrementally. For a pre-trained weight matrix \(W_{0}\in\mathbb{R}^{d\times k}\), LoRA models the weight update \(\Delta W\in\mathbb{R}^{d\times k}\) utilizing a low-rank decomposition, expressed as \(BA\), where \(B\in\mathbb{R}^{d\times r}\) and \(A\in\mathbb{R}^{r\times k}\) represent two low-rank matrices, with \(r\ll min(d,k)\). Consequently, the fine-tuned weight \(W^{\prime}\) can be represented as:",W^{\prime}=W_{0}+\Delta W=W_{0}+\underline{BA},"where \(W_{0}\) remains static during the fine-tuning process, and the underlined parameters are being trained",W^{\prime} = W_{0} + \Delta W = W_{0} + B A,The fine-tuned weight matrix $W^{\prime}$ is obtained by adding a low-rank update $BA$ to the pre-trained weight matrix $W_{0}$.
ICML_2024_oral_30,2,"ed. The matrix \(A\) is initialized with uniform Kaiming distribution (He et al., 2015), while \(B\) is initially set to zero, resulting in \(\Delta W=BA\) being zero at the start of training. Notably, this decomposition of \(\Delta W\) can be substituted with other LoRA variants, such as VeRA (Kopiczko et al., 2024). Additionally, based on Eq. (1), we can merge the learned \(\Delta W\) with the pre-trained weight \(W_{0}\) and obtain \(W^{\prime}\) in advance of deployment, and given that both \(W^{\prime}\) and \(W_{0}\) both fall within the dimensionality of \(\mathbb{R}^{d\times k}\), LoRA and its related variants do not introduce any extra latency during the inference compared to the original model.

### Weight Decomposition Analysis

The study presented in LoRA (Hu et al., 2022) suggests that LoRA can be considered a general approximation of full fine-tuning. By gradually increasing the rank \(r\) of LoRA to align with the rank of pre-trained weights, LoRA can attain a level of expressiveness akin to that of FT. Consequently, many previous studies have attributed the discrepancy in accuracy between LoRA and FT primarily to the limited number of trainable parameters, often without further analysis (Hu et al., 2022; Kopiczko et al., 2024). Drawing inspiration from Weight Normalization (Salimans and Kingma, 2016), which reparameterizes the weight matrix into magnitude and direction for accelerating optimization, we introduce an innovative weight decomposition analysis. Our analysis restructures the weight matrix into two separate components, _magnitude_ and _direction_, to reveal the inherent differences in LoRA and FT learning patterns.

**Analysis Method:** This analysis examines the updates in both magnitude and direction of the LoRA and FT weights relative to the pre-trained weights to reveal the fundamental differences in the learning behaviors of both. The weight decomposition of \(W\in\mathbb{R}^{d\times k}\) can be formulated as:",W=m\frac{V}{||V||_{c}}=||W||_{c}\frac{W}{||W||_{c}},"where \(m\in\mathbb{R}^{1\times k}\) is the magnitude vector, \(V\in\mathbb{R}^{d\times k}\) is the directional matrix, with \(||\cdot||_{c}\) being the vector-wise norm of a matrix across each column","W = \alpha \cdot \frac{W}{\|W\|_F}, \quad \text{where } \alpha = \|W\|_F","where $\alpha$ represents the Frobenius norm (magnitude) of $W$, and $\frac{W}{\|W\|_F}$ represents the normalized direction of $W$"
ICML_2024_oral_30,3,"mn. This decomposition ensures that each column of \(V/||V||_{c}\) remains a unit vector, and the corresponding scalar in \(m\) defines the magnitude of each vector.

For our weight decomposition analysis, we select the VL-BART model fine-tuned on four image-text tasks as outlined in (Sung et al., 2022) for a case study. Following (Sung et al., 2022), which applies LoRA only to the query/value weight matrix in the self-attention module. We decompose the pre-trained weight \(W_{0}\), the full fine-tuned weight \(W_{\text{FT}}\), and the merged LoRA weight \(W_{\text{LoRA}}\) of query/value weight matrix using Eq. (2). The magnitude and directional variations between \(W_{0}\) and \(W_{\text{FT}}\) can be defined as follows:","\Delta M_{\text{FT}}^{t}=\frac{\sum_{n=1}^{k}|m_{\text{FT}}^{n,t}-m_{0}^{n}|}{ k} || \Delta D_{\text{FT}}^{t}=\frac{\sum_{n=1}^{k}(1-\text{\bf cos}(V_{\text{FT}}^{ n,t},W_{0}^{n}))}{k}",,"\Delta m_{\text{FT}} = ||W_{\text{FT}}||_{c} - ||W_{0}||_{c}, \quad \Delta V_{\text{FT}} = \frac{W_{\text{FT}}}{||W_{\text{FT}}||_{c}} - \frac{W_{0}}{||W_{0}||_{c}}",where $\Delta m_{\text{FT}}$ captures the change in magnitude and $\Delta V_{\text{FT}}$ captures the change in direction between the pre-trained weight $W_{0}$ and the fully fine-tuned weight $W_{\text{FT}}$
ICML_2024_oral_30,4,"Here, \(\Delta M_{\text{FT}}^{t}\) and and \(\Delta D_{\text{FT}}^{t}\) represent the magnitude difference and directional difference between \(W_{0}\) and \(W_{\text{FT}}\) at \(t\) training step respectively, with \(\text{cos}(\cdot,\cdot)\) being the cosine similarity function. \(M_{\text{FT}}^{n,t}\) and \(M_{0}^{n}\) are the \(n^{th}\) scalars in their respective magnitude vectors, while \(V_{\text{FT}}^{n,t}\) and \(W_{0}^{n}\) are the \(n^{th}\) columns in \(V_{\text{FT}}^{t}\) and \(W_{0}\). The magnitude and directional differences between \(W_{\text{LoRA}}\) and \(W_{0}\) are calculated similarly, as per Eq. (3) and Eq. (4). We select checkpoints from four different training steps for analysis, comprising three intermediate steps and the final checkpoint from both FT and LoRA, and we perform weight decomposition analysis on each of these checkpoints to determine the \(\Delta M\) and \(\Delta D\) throughout different layers.

**Analysis Results:** Figure 2 (a) and (b) illustrate the alterations in the query weight matrix of FT and LoRA, with each point representing a (\(\Delta D^{t}\), \(\Delta M^{t}\)) pair from query weight matrices across different layers and training steps. Similarly, Figure 6 in the appendix displays the value weight matrix modifications. It is noticeable that LoRA exhibits a consistent positive slope trend across all the intermediate steps, signifying a proportional relationship between the changes in direction and magnitude. In contrast, the FT displays a more varied learning pattern with a relatively negative slope. This distinction between FT and LoRA likely mirrors their respective learning capability. While LoRA tends to either increase or decrease the magnitude and direction updates proportionally, it lacks the nuanced capability for more subtle adjustments. Specifically, LoRA does not show proficiency in executing slight directional changes alongside more significant magnitude alterations, or vice versa, a feature more characteristic of the FT method. We suspect that such limitation of LoRA might stem from the challenge of concurrent learning both magnitude and directional adaptation, which could be overly complex for LoRA. Consequently, in this work, we aim to propose a variant of LoRA that exhibits a learning pattern more closely resembling that of FT, and can improve the learning capacity over LoRA.

## 4 Method

### Weight-Decomposed Low-Rank Adaptation

Drawing from the insights of our weight decomposition analysis, we introduce Weight-**D**ecomposed **L**ow-**R**ank **A**daptation (**DoRA**). DoRA initially decomposes the pre-trained weight into its magnitude and directional components and finetunes both of them. Because the directional component is large in terms of parameter numbers, we further decompose it with LoRA for efficient finetuning.

Our intuitions are two-fold. Firstly, we believe that limiting LoRA to concentrate exclusively on directional adaptation while also allowing the magnitude component to be tunable simplifies the task compared to the original approach, where LoRA is required to learn adjustments in both mag nitude and direction. Secondly, the process of optimizing directional updates is made more stable through weight decomposition, which we delve into more thoroughly in Section.4.2. It is important to highlight that the main distinction between DoRA and weight normalization (Salimans and Kingma, 2016) lies in their training approaches. Weight normalization trains both components from scratch, making the method sensitive to different initializations. Conversely, DoRA avoids such initialization concerns since both components begin with pre-trained weights. We initialize DoRA with pre-trained weight \(W_{0}\) as outlined in Eq. (2), where \(m=||W_{0}||_{c}\) and \(V=W_{0}\) after initialization. We then keep \(V\) frozen and \(m\) a trainable vector. The directional component is then updated through LoRA. DoRA can be formulated similar to Eq. (1) as:",W^{\prime}=\underline{m}\frac{V+\Delta V}{||V+\Delta V||_{c}}=\underline{m} \frac{W_{0}+\underline{BA}}{||W_{0}+\underline{BA}||_{c}},"where \(\Delta V\) is the incremental directional update learned by multiplying two low-rank matrices \(B\) and \(A\), and the underlined parameters denote the trainable parameters",W^{\prime}=m^{\prime}\frac{V+\Delta V}{||V||_{c}}=m^{\prime}\frac{V+\underline{BA}}{||V||_{c}},"where $m^{\prime}$ is the updated magnitude vector and $\Delta V=BA$ represents the directional update via LoRA, with $B$ and $A$ being low-rank matrices"
ICML_2024_oral_30,5,"rs. The matrices \(B\in\mathbb{R}^{d\times r}\) and \(A\in\mathbb{R}^{r\times k}\) are initialized in line with LoRA's strategy to ensure that \(W^{\prime}\) equals \(W_{0}\) before the finetuning. Furthermore, DoRA can be merged with the pre-trained weight before inference, thereby not introducing any additional latency.

We visualize the magnitude and directional differences of the query weight matrix between the merged DoRA weight and \(W_{0}\) in the same setting as for FT and LoRA in Figure 2 (c) and leave the visualization of the value weight matrix in the appendix. From the regression line for \((\Delta D,\Delta M)\) of both DoRA and FT, we reveal that in contrast to LoRA's pattern, DoRA, and FT are characterized by a distinct negative slope. We reason that FT tends towards a negative slope because pre-trained weights already possess substantial knowledge suitable for various downstream tasks. Therefore, when provided with adequate learning capacity, having a larger magnitude or direction alteration alone is sufficient enough for downstream adaptation. We additionally compute the correlation between \(\Delta D\) and \(\Delta M\) for FT, LoRA, and DoRA, and we found that both FT and DoRA exhibit negative correlation values of -0.62 and -0.31, respectively. In contrast, LoRA shows a positive correlation with a value of 0.83. In conclusion, the fact that DoRA demonstrates the ability to make only substantial directional adjustments with relatively minimal changes in magnitude or the reverse while showing learning patterns closer to FT's signifies its superior learning capacity over LoRA.

### Gradient Analysis of DoRA

In this section, we first derive the gradient of DoRA and illustrate how our proposed decomposition benefits the optimization of \(\Delta V\). Subsequently, we analyze from the gradient's perspective to explicate the learning pattern of DoRA, which tends to have a negative slope.

From Eq. (5), we can obtain the gradient of Loss \(\mathcal{L}\) with respect to \(m\) and \(V^{\prime}=V+\Delta V\) as:",\nabla_{V^{\prime}}\mathcal{L} =\frac{m}{||V^{\prime}||_{c}}(I-\frac{V^{\prime}V^{\prime \mathbf{T}}}{||V^{\prime}||_{c}^{2}})\nabla_{W^{\prime}}\mathcal{L} || \nabla_{m}\mathcal{L} =\frac{\nabla_{W^{\prime}}\mathcal{L}\cdot V^{\prime}}{||V^{ \prime}||_{c}},,"\nabla_{m}\mathcal{L}=\frac{V^{\prime}}{||V^{\prime}||_{c}}\cdot\nabla_{W^{\prime}}\mathcal{L},\quad\nabla_{V^{\prime}}\mathcal{L}=\frac{m}{||V^{\prime}||_{c}}\left(I-\frac{V^{\prime}(V^{\prime})^{\top}}{||V^{\prime}||_{c}^{2}}\right)\cdot\nabla_{W^{\prime}}\mathcal{L}","where $\nabla_{m}\mathcal{L}$ and $\nabla_{V^{\prime}}\mathcal{L}$ denote the gradients of the loss with respect to the magnitude vector $m$ and the updated directional component $V^{\prime}$, respectively"
ICML_2024_oral_30,6,"Eq. (6) reveals that the weight gradient \(\nabla_{W^{\prime}}\mathcal{L}\) is scaled by \(m/||V^{\prime}||_{c}\) and is projected away from the current weight matrix. These two effects contribute to aligning the gradient's covariance matrix more closely with the identity matrix, which is advantageous for optimization (Salimans and Kingma, 2016). Additionally, given that \(V^{\prime}=V+\Delta V\), the gradient \(\nabla_{V^{\prime}}L\) is equivalent to \(\nabla_{\Delta V}L\). Therefore, the optimization benefits derived from this decomposition are fully transferred to \(\Delta V\), enhancing the learning stability of LoRA.

We can gain further insight into the learning pattern of DoRA by referring to Eq. (7). In the subsequent discussion, we represent vectors using lower-case letters instead of the previous matrix form notation. Consider \(w^{\prime\prime}=w^{\prime}+\Delta w\) as the parameter update for a weight vector, where \(\Delta w\propto\nabla_{w^{\prime}}\mathcal{L}\). In two hypothetical update

Figure 2: Magnitude and direction updates of (a) FT, (b) LoRA, and (c) DoRA of the query matrices across different layers and intermediate steps. Different markers represent matrices of different training steps and different colors represent the matrices of each layer.

scenarios, \(S1\) and \(S2\), \(S1\) involves a smaller directional update (\(\Delta D_{S1}\)), while \(S2\) involves a larger one (\(\Delta D_{S2}\)). Assuming \(||\Delta w_{S1}||=||\Delta w_{S2}||\), and at time 0, we have \(\Delta v=0\) and \(v^{\prime}=v\). From \(\Delta D_{S1}<\Delta D_{S2}\), it follows that \(|\textbf{cos}(\Delta w_{S1},w^{\prime})|>|\textbf{cos}(\Delta w_{S2},w^{\prime })|\). Since \(\Delta w\propto\nabla_{w^{\prime}}\mathcal{L}\), it implies \(|\textbf{cos}(\nabla_{w^{\prime}}^{S1}\mathcal{L},w^{\prime})|>|\textbf{cos}( \nabla_{w^{\prime}}^{S2}\mathcal{L},w^{\prime})|\). From Sec 4.1, with \(v\) initialized as \(v_{0}\) and \(w^{\prime}=w_{0}\) at time 0, we get \(|\textbf{cos}(\nabla_{w^{\prime}}\mathcal{L},w^{\prime})|=|\textbf{cos}( \nabla_{w^{\prime}}\mathcal{L},v^{\prime})|=|\textbf{cos}(\nabla_{w^{\prime}} \mathcal{L},v)|\). Using the cosine similarity equation with \(\Delta v=0\):","cos(\nabla_{w^{\prime}}\mathcal{L},v^{\prime})=cos(\nabla_{w^{\prime}} \mathcal{L},v)=\frac{\nabla_{w^{\prime}}\mathcal{L}\cdot v}{||\nabla_{w^{ \prime}}\mathcal{L}||||v||}",,"\textbf{cos}(\nabla_{w^{\prime}}\mathcal{L},v)=\frac{\nabla_{w^{\prime}}\mathcal{L}^{T}v}{||\nabla_{w^{\prime}}\mathcal{L}||\cdot||v||}",cosine similarity between the gradient of the loss with respect to the updated weight and the initial directional component
ICML_2024_oral_30,7,denote \(m_{*}\) as the magnitude scalar of vector \(w^{\prime}\) then Eq. (7) w.r.t \(m_{*}\) can be rewritten to:,"\nabla_{m_{*}}\mathcal{L}=\frac{\nabla_{w^{\prime}}\mathcal{L}\cdot v^{\prime }}{||v^{\prime}||}=||\nabla_{w^{\prime}}\mathcal{L}||\cdot cos(\nabla_{w^{ \prime}}\mathcal{L},v)",,"\nabla_{m_{*}}\mathcal{L}=\frac{\nabla_{w^{\prime}}\mathcal{L}\cdot v}{||v||}=||\nabla_{w^{\prime}}\mathcal{L}||\text{\bf cos}(\nabla_{w^{\prime}}\mathcal{L},v)","where $\nabla_{m_{*}}\mathcal{L}$ represents the gradient of the loss with respect to the magnitude scalar $m_{*}$, and the inner product term is expressed via cosine similarity"
ICML_2024_oral_30,8,"Given that \(||\Delta w_{S1}||=||\Delta w_{S2}||\) for \(S1\) and \(S2\), and \(||\nabla_{w^{\prime}}^{S1}\mathcal{L}||=||\nabla_{w^{\prime}}^{S2}\mathcal{L} ||\). Therefore, with:","||\nabla_{w^{\prime}}^{S1}\mathcal{L}||\cdot|cos(\nabla_{w^{\prime}}^{S1} \mathcal{L},v)|>||\nabla_{w^{\prime}}^{S2}\mathcal{L}||\cdot|cos(\nabla_{w^{ \prime}}^{S2}\mathcal{L},v)|",,"\Delta D_{S1}<\Delta D_{S2}\Rightarrow|\text{\bf cos}(\nabla_{w^{\prime}}^{S1}\mathcal{L},v)|>|\text{\bf cos}(\nabla_{w^{\prime}}^{S2}\mathcal{L},v)|\Rightarrow|\nabla_{m_{*}}^{S1}\mathcal{L}|>|\nabla_{m_{*}}^{S2}\mathcal{L}|","Equation 8 establishes the relationship between directional updates and magnitude gradients, showing that smaller directional updates correspond to larger magnitude gradients under equal update norms."
ICML_2024_oral_30,9,"it can be inferred that \(|\nabla_{m_{*}}^{S1}\mathcal{L}|>|\nabla_{m_{*}}^{S2}\mathcal{L}|\) which indicate that \(S1\) has larger magnitude updates over \(S2\) while having smaller directional alteration than that of \(S2\). Our conclusion generally holds in practice, as evidenced by Figure 2 (c). Consequently, we have effectively shown how DoRA can be utilized to adjust the learning pattern, diverging from that of LoRA and aligning more closely with the pattern of FT.

### Reduction of Training Overhead

In Eq. (1), the gradients of \(W^{\prime}\) and \(\Delta W\) are the same. However, with DoRA, which redirects the low-rank adaptation towards the directional component, the gradient of the low-rank updates differs from that of \(W^{\prime}\), as illustrated in Eq. (6). This divergence necessitates extra memory during backpropagation. To address this, we suggest treating \(||V+\Delta V||_{c}\) in Eq. (5) as a constant, thereby detaching it from the gradient graph. This means that while \(||V+\Delta V||_{c}\) dynamically reflects the updates of \(\Delta V\), it won't receive any gradient during backpropagation. With this modification, the gradient w.r.t \(m\) remains unchanged, and \(\nabla_{V^{\prime}}\mathcal{L}\) is redefined as:",\nabla_{V^{\prime}}\mathcal{L}=\frac{m}{C}\nabla_{W^{\prime}}\mathcal{L}\text { where }C=||V^{\prime}||_{c},,\nabla_{V^{\prime}}\mathcal{L} =\frac{m}{||V^{\prime}||_{c}}\nabla_{W^{\prime}}\mathcal{L},
ICML_2024_oral_37,1,"## 3 Our Method

A checkpoint \(\mathcal{P}_{t}\) of a neural network during the \(t^{th}\) training iteration generally contains the model weights \(\mathcal{W}_{t}\) and parameters \(\mathcal{O}_{t}\) of the optimizer momentum.","\mathcal{P}_{t}=\{\mathcal{W}_{t},\mathcal{O}_{t}\}.",,"\mathcal{P}_t = \{\mathcal{W}_t, \mathcal{O}_t\}",Equation 1 defines the neural network checkpoint $\mathcal{P}_t$ at iteration $t$ as the combination of model weights $\mathcal{W}_t$ and optimizer parameters $\mathcal{O}_t$.
ICML_2024_oral_37,2,Saving checkpoints for \(T\) times during training leads to a series of checkpoints \(\mathcal{P}\).,"\mathcal{P}=\{\mathcal{P}_{1},\mathcal{P}_{2},\cdots,\mathcal{P}_{t}\,\cdots, \mathcal{P}_{T}\}.",,"\mathcal{P} = \{\mathcal{P}_1, \mathcal{P}_2, \dots, \mathcal{P}_T\}.","Equation 2 defines the sequence of checkpoints $\mathcal{P}$ saved during $T$ training iterations, where each checkpoint $\mathcal{P}_t$ contains the model weights and optimizer parameters at iteration $t$."
ICML_2024_oral_37,3,"For the widely used Adam optimizer, the parameters with most significant storage cost are the first-order and second-order moments \(v_{t}\), \(m_{t}\), _i.e_.,","\mathcal{O}_{t}=\{v_{t},m_{t}\}.",,"\mathcal{O}_t = \{m_t, v_t\}",where $\mathcal{O}_t$ consists of the first-order moment $m_t$ and second-order moment $v_t$ in the Adam optimizer.
ICML_2024_oral_37,4,"Note that some variables such as learning rate and weight decay _etc_. are also stored in the optimizer checkpoint, but can be simply neglected when compared with moments.

In the traditional pruning-related work, researchers only concern about the weights of models, since the main purpose of pruning is reducing the overhead of calculation and storage during the inference stage. However, when we turn to the checkpoint compression during the training process, the pruning of momentum is also important to reduce the total size of training checkpoints. Take the most general optimizer Adam used in LLM training as an example, it

\begin{table}
\begin{tabular}{c|c|c} \hline \hline Model & Param. & Storage \\ \hline GPT3 (Brown et al., 2020) & 175B & 2.3TB \\ PaLM (Chowdhery et al., 2023) & 540B & \(\sim\)7TB \\ LLaMA-70B (Touvron et al., 2023) & 75B & 1.0TB \\ PanGu-\(\pi\)(Wang et al.) & 7B & 99GB \\ \hline \hline \end{tabular}
\end{table}
Table 1: The parameter and checkpoint size of part LLMs. High-capacity storage devices are essential for checkpoints for LLM training process.

saves the first-order and second-order moment of gradients which require double storage of weights. Therefore, we have to take both model weights and optimizer momentum states into consideration for extreme compression of model training checkpoints.

### Residual Checkpoint

During the \(t^{th}\) iteration of training, since we have already stored \(t-1\) checkpoints1 during previous training period, it is important to jointly utilize the temporal information of checkpoints to obtain more compact storage. The model weights will be updated upon previous ones according to the gradient during training, thus the difference between adjacent model weights is mostly to be sparse, which is more suitable for compression. In contrast, the momentum states stored in the optimizer checkpoints are the moving average of the first-order and second-order moments, which are only weakly related to the parameters in the previous checkpoint after updating for hundreds to thousands of steps, especially for the first-order moment whose general \(\beta_{1}=0.9\). So we do not apply residual calculation on optimizer momentum. The residual checkpoint \(\Delta\mathcal{P}_{t}\) is defined as

Footnote 1: We give a detailed description about how to deal with previous checkpoints saved in residual format in Section 3.4","\Delta\mathcal{P}_{t}=\{\Delta\mathcal{W}_{t},\mathcal{O}_{t}\}=\{\mathcal{W}_ {t}-\mathcal{W}_{t-1},\mathcal{O}_{t}\}.",,"\Delta\mathcal{P}_{t} = \{\mathcal{W}_{t} - \mathcal{W}_{t-1}, \mathcal{O}_{t}\}.","The residual checkpoint $\Delta\mathcal{P}_{t}$ consists of the difference between the current and previous model weights, and the current optimizer states, enabling more efficient compression by exploiting temporal redundancy in model weights."
ICML_2024_oral_37,5,"There is a comparison between the pruning on residual checkpoint and pruning on original checkpoint in Figure 3. We plot the histogram of the original weights, weights after direct pruning and weights after pruning on residual checkpoints. We find that pruning the residual checkpoint has almost no impact on the parameter distribution. This helps us to further prune the parameters.

### Joint Weight-Momentum Pruning

Weight pruning is a common way to discard unimportant values while maintaining the performance to the maximum extent. For the checkpoint compression, we need to obtain the corresponding pruning masks for model weights and momentum states, which are denoted as \(\mathcal{M}_{w}\) and \(\mathcal{M}_{o}\), respectively. Intuitive way for pruning model weights and momentum states is to discard values with some pre-defined metric. However, this separate strategy leads to sub-optimal solution since there are strong relations between model weights and momentum states. Therefore, in this paper we propose a novel joint weight-momentum pruning method that obtains better performance for checkpoint compression.

**Weight Pruning.** For weights pruning, using the magnitude or the gradients of weights as an indicator is the common practice. There is a little difference between our weight pruning task and the general one. As we introduced in Section 3.1, we need to prune the residual values of weights of two adjacent checkpoints instead of the original value of weights. Thus we recommend to use the second-order moment of gradients of weights as an indicator, since they can represent the statistical average of the weight change during training process. We use the indicator to calculate the pruning threshold of each layer and the formula is shown below,","r_{w}=\frac{\alpha}{\sqrt{m_{t}}}\times\text{median}(\mathcal{W}),\mathcal{M }_{w}(i)=\mathds{1}_{w_{t}(i)>r_{w}}.",,"\mathcal{M}_{w}^{(l)} = \left\{ \begin{array}{ll} 1 & \text{if } |w_{i,j}^{(l)}| > \tau_{w}^{(l)} \\ 0 & \text{otherwise} \end{array} \right.,","where $\mathcal{M}_{w}^{(l)}$ denotes the pruning mask for weights in layer $l$, $w_{i,j}^{(l)}$ represents the weight element, and $\tau_{w}^{(l)}$ is the layer-specific pruning threshold determined by the second-order moment of gradients."
ICML_2024_oral_37,6,"In which \(\mathcal{W}\) and \(m_{t}\) represents the weights and the second-order moment, respectively. \(\alpha\) is a hyperparameter. After determining the pruning threshold of each layer, we prune the residual of weights to zero by magnitude for each layer.

**Momentum Pruning.** For momentum pruning, we use the first-order moment as an indicator to determine whether to prune this momentum states or not. We give a brief proof in the next section to explain why we choose it. Besides, if a specific location of weights is pruned, intuitively it is not important to preserve the corresponding momentum states. We prune the parameters of momentum following the formula below, in which \(\beta\) is a hyper-parameter.","r_{o}=\beta\times\text{mean}(v_{t}),\mathcal{M}_{o}(i)=\mathds{1}_{v_{t}(i)> r_{o}\text{ and }\mathcal{M}_{w}(i)=1}.",,"r_{o}=\frac{\beta}{m_{t}}\times\text{median}(v_{t}),\mathcal{M}_{o}(i)=\mathds{1}_{m_{t}(i)>r_{o}}.","The momentum pruning threshold $r_{o}$ is determined using the first-order moment $m_{t}$ and the median of the second-order moment $v_{t}$, with $\beta$ as a hyperparameter controlling the sparsity level."
ICML_2024_oral_37,7,"Figure 2: Framework of our proposed compression process. We calculate the residual \(\Delta\mathcal{W}_{t}\) and apply joint-pruning on \(\Delta\mathcal{W}_{t}\) and \(\mathcal{O}_{t}\). Then we quantize them separately and save the cluster center \(\mathcal{C}_{t}\) and cluster index \(\mathcal{I}_{t}\).

**Convergence Analysis.** Since we prune both the model weights and momentum states during training, it is important to analyze whether the whole training with checkpoint compression still have convergence guarantee.

**Theorem 3.1**.: _According the convergence analysis in Adam (Kingma & Ba, 2014), assume that the function \(f_{t}\) has bounded gradients, \(\left\|\nabla f_{t}(\theta)\right\|_{2}\leq G,\left\|\nabla f_{t}(\theta) \right\|_{\infty}\leq G_{\infty}\) for all \(\theta\in R^{d}\) and distance between any \(\theta_{t}\) generated by Adam is bounded, \(\left\|\theta_{n}-\theta_{m}\right\|_{2}\leq D\), \(\left\|\theta_{m}-\theta_{n}\right\|_{\infty}\leq D_{\infty}\) for any \(m,n\in\{1,\dots,T\}\), and \(\beta_{1},\beta_{2}\in[0,1)\) satisfy \(\frac{\beta_{t}^{2}}{\sqrt{\beta_{2}}}<1\). Let \(\alpha_{t}=\frac{\alpha}{\sqrt{t}}\) and \(\beta_{1,t}=\beta_{1}\lambda^{t-1},\lambda\in(0,1)\). If we prune the moments with a mask \(\mathcal{M}_{o}\in\{0,1\}\) at iteration \(\tau\), Adam could also achieves the following guarantee, for all \(T\geq 1\)._","\tilde{R}(T)\leq \frac{D^{2}}{2\alpha(1-\beta_{1})}\sum_{i=1}^{d}\sqrt {T\widehat{v}_{T,i}} || +\frac{\alpha(1+\beta_{1})G_{\infty}}{(1-\beta_{ 1})\sqrt{1-\beta_{2}}(1-\gamma)^{2}}\sum_{i=1}^{d}\|g_{1,\tau,i} \|_{2} || +\frac{D_{\infty}^{2}G_{\infty}\sqrt{1-\beta_{2}}}{2\alpha}\sum_ {i=1}^{d}\sum_{t=1}^{t}\frac{\beta_{1,t}}{(1-\beta_{1,t})}\sqrt{t} || +\frac{D^{2}}{2\alpha(1-\beta_{1})}\sum_{i=1}^{d}( \sqrt{T\widehat{v}_{\tau,i}}-\sqrt{T\widehat{v}_{\tau,i}\mathcal{M}_{o}})",,\mathbb{E}\left[\frac{1}{\sqrt{T}}\sum_{t=1}^{T}\left\|\nabla f_{t}(\theta_{t})\right\|_{2}\right]\leq\frac{1}{\sqrt{T}}\left(\frac{D^{2}}{\alpha}+\frac{\alpha}{2}\left(1+\frac{1}{1-\beta_{1}}\right)\right)+\frac{\beta_{1}}{1-\beta_{1}}G_{\infty}\cdot\left\|\mathcal{M}_{o}-1\right\|_{1}.,"The convergence bound of Adam with joint weight-momentum pruning, showing the trade-off between gradient magnitude and pruning mask sparsity."
ICML_2024_oral_37,8,"Compared with the original convergence analysis of Adam (Kingma & Ba, 2014), the regret bound for our checkpoint compression method has an additional term:","\begin{split}\Delta\tilde{R}(T)&=\frac{D^{2}}{2 \alpha(1-\beta_{1})}\sum_{i=1}^{d}(\sqrt{T\widehat{v}_{\tau,i}}- \sqrt{T\widehat{v}_{\tau,i}\mathcal{M}_{o}})\\ &=\frac{D^{2}}{2\alpha(1-\beta_{1})}\sum_{i=1}^{d}( \sqrt{T\widehat{v}_{\tau,i}(1-\mathcal{M}_{o})}).\end{split}",,"\frac{D^{2}}{2\alpha(1-\beta_{1})}\sum_{i=1}^{d}( \sqrt{T\widehat{v}_{\tau,i}}-\sqrt{T\widehat{v}_{\tau,i}\mathcal{M}_{o}}).",This term quantifies the impact of momentum pruning on the convergence guarantee of Adam optimizer during checkpoint compression.
ICML_2024_oral_37,9,"Since we only prune the values that \(v\) is relatively small, thus the regret bound is quite close to that of original training process.

Similar to the original optimization process of Adam, the average regret of our method also converges. Denote the regret bound of original Adam as \(R(T)\), thus we have",\lim_{Tarrow\infty}\frac{\tilde{R}(T)}{T}\leq\lim_{Tarrow\infty} \frac{R(T)+\Delta\tilde{R}(T)}{T}=0,,\tilde{R}(T) \leq R(T) + \Delta\tilde{R}(T).,The regret bound of the proposed method is bounded by the original Adam regret plus an additional term due to pruning.
ICML_2024_oral_37,10,"Therefore, our pruning method for momentum also achieves the following guarantee for all \(T\geq 1\):",\frac{R(T)}{T}=O(\frac{1}{\sqrt{T}}),,"\tilde{R}(T)\leq \frac{D^{2}}{2\alpha(1-\beta_{1})}\sum_{i=1}^{d}\sqrt{T\widehat{v}_{T,i}}+\frac{\alpha(1+\beta_{1})G_{\infty}}{(1-\beta_{1})\sqrt{1-\beta_{2}}(1-\gamma)^{2}}\sum_{i=1}^{d}\|g_{1,\tau,i}\|_{2}+\frac{D_{\infty}^{2}G_{\infty}\sqrt{1-\beta_{2}}}{2\alpha}\sum_{i=1}^{d}\sum_{t=1}^{T}\frac{\beta_{1,t}}{(1-\beta_{1,t})}\sqrt{t}.","This equation presents the regret bound guarantee for the proposed momentum pruning method, showing that it maintains convergence properties similar to the original Adam optimizer."
ICML_2024_oral_4,1,"## 1 Introduction

Submodular optimization is a classical problem in computer science and machine learning with applications spanning various domains such as data summarization, active learning, network inference, video analysis, and facility location (see Krause, 2013) for a survey).

The submodular cover problem, initially introduced by (Wolsey, 1982), is a well-studied classical variant of the problem where the objective is to minimize the sum of the weight of selected elements chosen from a set subject to a submodular function constraint. Specifically, given a set of elements \(V\), a monotone submodular function \(f:2^{V}\rightarrow\mathbb{R}^{\geq 0}\), and a weight function \(w:V\rightarrow\mathbb{R}^{\geq 0}\), we seek to pick a set \(S\) minimizing \(\sum_{v\in S}w(v)\) that satisfies \(f(S)=f(V)\).

This problem generalizes various noteworthy problems such as the set cover problem, 2-set cover, dominating set, and others. It can also be seen as a dual of the submodular maximization problem, in which the goal is to maximize \(f(S)\) subject to the constraint \(|S|\leq k\) for some parameter \(k\).

While the submodular cover problem has been extensively studied (see (Bar-Ilan et al., 2001) for a survey), the majority of the algorithms in the literature predominantly depend on having access to the entire ground set throughout their execution, which is not a valid assumption in numerous real-world applications dealing with ever-changing data and makes them impractical.

Given the mentioned limitation, there has recently been a surge of interest in reexamining classical problems under a variety of massive data models such as streaming, distributed, dynamic, and online settings. For submodular maximization, the problem has been considered in the streaming (Badanidiyuru et al., 2014; Chakrabarti & Kale, 2015; Mirzasoleiman et al., 2018; Kazemi et al., 2019), distributed (Mirrokni & Zadimoghaddam, 2015; Liu & Vondrak, 2018), and dynamic (Monemizadeh, 2020; Lattanzi et al., 2020; Chen & Peng, 2022; Duetting et al., 2023; Banihashem et al., 2023;b, 2024) settings. Similarly for submodular cover, recent works have studied the problem in the distributed, streaming, and scalable settings (Mirzasoleiman et al., 2015; Norouzi-Fard et al., 2016; Chen & Crawford, 2023; Crawford, 2023).

Motivated by these advances, we consider the submodular cover in a dynamic setting where the elements of the ground set are inserted and deleted, and the goal is to alwaysmaintain an approximately optimal solution. While this can easily be done by re-running an offline algorithm after each update, the goal is to do this with small update time per query. We formally define the dynamic submodular cover problem as follows.

**Definition 1.1** (Dynamic Submodular Cover problem).: We assume that \(f:2^{\mathcal{V}}\rightarrow\mathbb{R}^{+}\) is a monotone, non-negative submodular function on the ground set \(\mathcal{V}=\{v_{1},v_{2},\ldots,v_{n}\}\), and each element \(v\) in the \(\mathcal{V}\) has a weight denoted by \(w(v)\). For any subset \(S\subseteq\mathcal{V}\), \(\textsc{Cost}(S)\) is defined to be \(\sum_{v\in S}w(v)\). At each time \(t\), the objective of the problem is to choose a subset \(S\subseteq V_{t}\) of minimum cost whose submodular value is equal to \(f(V_{t})\), i.e.,","S_{opt_{t}}=\arg\min_{S\subseteq V_{t}}\{\textsc{Cost}(S):f(S)=f(V_{t})\}\enspace,",where \(V_{t}\) denotes the set of the currently present elements after the first \(t\) updates,"\begin{equation}
\textsc{Cost}(S) = \sum_{v \in S} w(v) \quad \text{subject to} \quad f(S) = f(V_t).
\end{equation}","Equation 1 formally defines the dynamic submodular cover problem, where the goal at each time $t$ is to select a subset $S \subseteq V_t$ of minimum cost such that the submodular function value $f(S)$ equals $f(V_t)$."
ICML_2024_oral_4,2,"es. \(\textsc{OPT}_{\textsc{cost}_{t}}\) is defined to be \(\textsc{Cost}(S_{opt_{t}})\), and \(f(S_{opt_{t}})=f(V_{t})\).

Note that, throughout the paper, as we consider a fixed point of time, we drop the subscript \(t\) for simplicity.

We note that while (Gupta & Levin, 2020) also considered the submodular cover problem in a dynamic setting, their model is different as it assumes that the submodular function \(f\) is changing dynamically, whereas we assume that the ground set undergoes updates. To illustrate the difference, consider the special case of set cover where the elements of \(V\) correspond to sets in a set system and we define \(f(\mathcal{S}):=|\cup_{S\in\mathcal{S}}S|\) for any \(\mathcal{S}\subseteq V\). In this case, the model in (Gupta & Levin, 2020) assumes that the elements of the set system are inserted and deleted, while our model assumes that sets of the set system are inserted and deleted. Our model is consistent with the models considered for the streaming version of the problem (Norouzi-Fard et al., 2016) where the elements are inserted one by one (but the elements are never deleted in their setting) and dynamic setting considered for the submodular maximization problem(Monemizadeh, 2020; Lattanzi et al., 2020; Chen & Peng, 2022; Duetting et al., 2023; Banihashem et al., 2023a;b, 2024).

In this paper, we assume that the updates are specified by an _oblivious adversary_, that is an adversary who knows are algorithm but does not have access to the random bits we use. This is equivalent to assuming that all of the updates are specified before the algorithm is run and as such are not adapted to the algorithm's output.

### Our Contribution

In this paper, we design an algorithm for the dynamic submodular cover problem that maintains an approximately optimal solution using polylogarithmic update time. As is standard for the submodular cover problem (Norouzi-Fard et al., 2016; Chen & Crawford, 2023), our approximation guarantees are bicriteria given the two objectives of the problem. A set \(S\) is called a \((1-\epsilon,c)\)-bicriteria approximate solution if it satisfies","f(S)\geq(1-\epsilon)f(V),\quad\text{and}\quad\textsc{Cost}(S)\leq c\textsc{ Cost}(S_{opt}),",where \(S_{opt}\) denotes the optimal solution,"f(S)\geq(1-\epsilon)f(V)\quad\text{and}\quad\textsc{Cost}(S)\leq c\cdot\textsc{OPT}_{\textsc{cost}}\enspace,","A $(1-\epsilon,c)$-bicriteria approximate solution guarantees coverage of at least a $(1-\epsilon)$ fraction of the optimal submodular value while its cost is within a factor $c$ of the optimal cost."
ICML_2024_oral_4,3,"on. We say a (random) set \(S\) is _expected_\((1-\epsilon,c)\)-bicriteria, if the first guarantee holds in expectation, i.e., \(\mathbb{E}\left[\,f(S)\,\right]\geq(1-\epsilon)f(V)\). Our main result is stated in the following theorem.

**Theorem 1.2**.: _Define the weight ratio of \(\mathcal{V}\) as \(\rho:=\frac{\max_{v\in\mathcal{V}}w(v)}{\min_{v\in\mathcal{V}}w(v)}\), and set \(n:=|\mathcal{V}|\). For any \(\epsilon>0\), there is an algorithm for the dynamic submodular cover problem that maintains an expected \((1-O(\epsilon),O(\epsilon^{-1}))\)-bicriteria approximate solution with expected amortized \(\text{poly}(\log(n),\log(\rho),\varepsilon^{-1})\) update time query complexity._

In terms of techniques, we build on and generalize the recent advances for dynamic submodular maximization (Monemizadeh, 2020; Lattanzi et al., 2020; Banihashem et al., 2023a), in particular the multi-level construction proposed by (Banihashem et al., 2023a), but require important changes given the ""two-dimensional"" nature of the problem involving both submodular value \(f(.)\) and the weights \(w(.)\). Indeed, the underlying ""static"" algorithm in our approach can be seen as a generalization of the algorithm in (Norouzi-Fard et al., 2016) that can support arbitrary weights (as opposed to the uniform weight setting of (Norouzi-Fard et al., 2016)). Our bucketing structure is two-dimensional in order to handle the effect of deletions, unlike (Banihashem et al., 2023a). We handle our parallel runs, and solution retrieval differently. Additionally, to simplify the analysis of the approximation guarantee, we check the marginal density of each element in the solution at the time of forming our solution sets, as opposed to (Banihashem et al., 2023a) who add their elements in bulk.1 As such, we do not need the complicated potential function analysis used in (Banihashem et al., 2023a), which is crucial for simplifying the analysis given our more involved two-dimensional setting.

## 2 Related Work

Submodular coverThe offline version of submodular cover has been extensively studied and it is well-known that the greedy algorithm by (Wolsey, 1982) obtains a logarithmic approximation ratio for the problem (see (Bar-Ilan et al., 2001) for a survey of developments and applications). Recently, given the emergence of Big Data algorithms, there has been an interest in considering the problem in the streaming setting. In particular, (Norouzi-Fard et al., 2016) obtain a \((1-\epsilon,O(\epsilon^{-1}))\)-bicriteria approximation algorithm for the problem in the special case where the weights are uniform, i.e., each element has weight \(1\). Here, the elements of the ground set arrive in a stream and the goal is to build an approximately optimal solution with low-memory. As mentioned earlier, our underlying static algorithm can be seen as a generalization of this approach for handling arbitrary weights. Subsequent works have considered the non-monotone objective functions and designed scalable algorithms for the problem (Crawford, 2023; Chen & Crawford, 2023)

For the dynamic setting, (Gupta & Levin, 2020) consider a different variant of the problem in which the submodular function \(f\) changes overtime and obtain a fully dynamic algorithm with bounded recourse. In contrast, our approach assumes that the underlying function is fixed and the ground set changes. This is aligned with the models considered in the streaming setting (Norouzi-Fard et al., 2016; Crawford, 2023) as well the models considered for streaming and dynamic settings for the submodular maximization problem (Monemizadeh, 2020; Lattanzi et al., 2020).

Submodular maximizationA closely related problem to submodular cover is submodular maximization. In the classical version of this problem, we are given a ground set of elements \(V\), a submodular function \(f:2^{V}\rightarrow\mathbb{R}^{\geq 0}\) and a parameter \(k\), and the goal is to maximize the function \(f\) over all sets \(S\) of size at most \(k\). For the offline version of the problem, it is well-known that a standard greedy algorithm that iteratively chooses a remaining element with maximum marginal gain obtains an approximation ratio of \(1-1/e\)(Nemhauser et al., 1978). The approximation ratio cannot be improved efficiently under complexity assumptions as shown by (Feige, 1998) via a reduction from set cover.

In the streaming setting, submodular maximization was first studied by (Badanidiyuru et al., 2014) who obtained a \(1/2-\epsilon\)-approximation algorithm. The \(1/2\) bound was later shown to by (Norouzi-Fard et al., 2018). The study of the dynamic version of the problem was first initiated independently by (Monemizadeh, 2020) and (Lattanzi et al., 2020) who obtain \((1/2-\epsilon)\)-approximation algorithms with \(O(k^{2}\log^{2}(n)\epsilon^{-3})\) and \(O(\log^{8}(n)\epsilon^{-6})\) update times respectively. 2 The \(1/2\) approximation is essentially tight as shown by (Chen & Peng, 2022) using a lower bound construction based on the streaming version of the problem. Recent works have generalized the dynamic results for non-monotone objectives (Banihashem et al., 2023b), as well as matroid constraints (Duetting et al., 2023; Banihashem et al., 2024).

Footnote 2: The original version of (Lattanzi et al., 2020) had correctness issues in the proof pointed by out (Banihashem et al., 2023a) who provided an alternative algorithm with polylogarithmic update time. The issues were subsequently fixed by (Lattanzi et al., 2020).

Deletion robust algorithmsA closely related but distinct area to dynamic submodular optimization is the robust submodular optimization (Mirzasoleiman et al., 2017; Kazemi et al., 2018; Duetting et al., 2022) in which the goal is to obtain a set that is robust to deletions performed by the adversary. The number of deletions is known upfront, is bounded. In contrast, the dynamic model assumes that insertions and deletions are performed arbitrarily and the goal is to always maintain a good solution.

## 3 Preliminaries

Notation:For a natural number, the set \(\{1,2,\ldots,x\}\) is denoted as \([x]\). Bold letters represent random variables, while their non-bold counterparts denote specific values. For instance, a random variable is denoted as **X** and its value as \(X\). Probability and expectation of a random variable **X** are represented by \(\text{Pr}\left[\,\textbf{X}\,\right]\) and \(\mathbb{E}\left[\,\textbf{X}\,\right]\) respectively. The notation \(\text{Pr}\left[\,A|B\,\right]\) denotes the conditional probability of event \(A\) given event \(B\). For an event \(A\) with nonzero probability and a discrete random variable **X**, the conditional expectation of **X** given \(A\) is denoted as \(\mathbb{E}\left[\,\textbf{X}|A\,\right]=\sum_{x}x\cdot\text{Pr}\left[\,\textbf {X}=x|A\,\right]\). Likewise, for discrete random variables **X** and **Y**, the conditional expectation of **X** given **Y** is denoted as \(\mathbb{E}\left[\,\textbf{X}|\textbf{Y}=y\,\right]\). The _indicator function_ of an event \(E\) is denoted by \(\mathds{1}\left\{E\right\}\), where \(\mathds{1}\left\{E\right\}\) is assigned one if \(E\) occurs and zero otherwise.

Submodular functions:Consider a _non-negative_ utility function \(f:2^{\mathcal{V}}\rightarrow\mathbb{R}^{+}\) defined on the given ground set \(\mathcal{V}=\{v_{1},v_{2},\ldots,v_{n}\}\). For any element \(v\in\mathcal{V}\) and a set \(A\subseteq\mathcal{V}\), \(\Delta(v|A)\) is called marginal gain of element \(v\) with respect to \(S\) and it is defined as \(\Delta(v|A):=f(A\cup\{v\})-f(A)\). Similarly, for any sets \(A,B\subseteq\mathcal{V}\), \(\Delta(B|A)\) is defined as \(f(A\cup B)-f(A)\). Function \(f\) is called _submodular_ whenfor any \(A,B\subseteq\mathcal{V}\), we have \(f(A)+f(B)\geq f(A\cup B)+f(A\cap B)\) or equivalently when for any \(A\subseteq B\subseteq\mathcal{V}\) and element \(v\), we have \(\Delta(v|A)\geq\Delta(v|B)\). This function \(f\) is called _monotone_ when for any \(A\subseteq B\subseteq\mathcal{V}\), we have \(f(A)\leq f(B)\), and it is called _normalized_ when \(f(\emptyset)=0\).

Density:Given a submodular function \(f:2^{\mathcal{V}}\rightarrow\mathbb{R}^{+}\) defined on the given ground set \(\mathcal{V}=\{v_{1},v_{2},\ldots,v_{n}\}\), and a weight function \(w:\mathcal{V}\rightarrow[1,\rho]\), we define density of an element \(v\in\mathcal{V}\) as \(d(v):=\frac{f(v)}{w(v)}\). Similarly, for any set \(A\subseteq\mathcal{V}\) we define marginal density of element \(v\) with respect to \(A\) as \(d(v|A):=\frac{\Delta(v|A)}{w(v)}\).

Update time:We assume access to the monotone submodular function \(f:2^{\mathcal{V}}\rightarrow\mathbb{R}^{+}\) through an _oracle_. This oracle supports _set queries_, allowing one to inquire about the value \(f(A)\) for any subset \(A\subseteq\mathcal{V}\). In this paper, we measure running time based on the total number of oracle calls, a common practice in submodular optimization, as the processing time of oracle calls typically dominates the running time of other parts of the algorithm (Duetting et al., 2023; Banihashem et al., 2024). We refer to the amortized number of query calls as query complexity and update time.

## 4 Dynamic Algorithm

### Setting

In the dynamic version of the problem, a sequence of updates, comprising insertions and deletions of elements from the ground set \(\mathcal{V}\), alters the set of the current elements denoted by \(V\). Each element may undergo multiple insertions and deletions. At each time frame, the set \(V\subseteq\mathcal{V}\) encompasses all inserted elements that have not been deleted since their last insertion. The algorithm's objective is to maintain a solution after each update, with its performance being assessed by its query complexity for each update. It is assumed that \(\epsilon\) and \(\epsilon_{\text{del}}\) are small enough parameters satisfying \(\epsilon<1/10\) and \(\epsilon_{\text{del}}<\epsilon/16\). And lastly, it is also assumed that the weight of each element in \(\mathcal{V}\) is between \(1\) and the parameter \(\rho\).

### Overview of the Algorithms

Our algorithm operates through multiple runs, each assigned a specific threshold parameter, denoted as \(\tau\), given to them as their input parameter. This threshold is a critical input used to assess the usefulness of the elements and is employed as a measure to distinguish between valuable and insignificant elements that are no longer relevant. In each time frame, the run with threshold \(\tau\) that meets some specific criteria will have the appropriate solution for that time frame.

A pivotal aspect of our algorithm is the use of a data structure in each run for keeping the elements, from which we can easily retrieve its solution, and it can efficiently be updated.

The fundamental idea behind this data structure is its hierarchical structure. This structure comprises different levels, where each level \(\ell\) includes the sets \(L_{\ell}\) and \(G_{\ell}\). The family of sets \(G_{\ell}\) is used to retrieve the solution. Each of them stores the elements that have been selected by the algorithm up to that level, and these sets form a cumulative hierarchy. The set \(L_{\ell}\) encompasses the elements with a marginal density of at least \(\tau\) with respect to \(G_{\ell-1}\).

Notably, the reconstruction of the entire data structure is a significant operation with potential query complexity implications and one of the key design features of a leveled data structure is its partial reconstruction capability. To further explain, throughout the execution of the algorithm, we can partially reconstruct the data structure starting from the level of our choosing without affecting the previous levels. This feature enables us to handle the insertion or deletion of an element with minimal changes to most levels of our data structure.

Note that even partial reconstructions are heavy operations, and it is in our best interest to avoid them as long as possible, which is why we utilize partial reconstruction only after reaching a level that is heavily affected by the updates up to that point in time and its reconstruction is necessary. To achieve this, in addition to the sets \(L_{\ell}\) and \(G_{\ell}\), we also maintain a set \(D\) and extended sets \(\overline{L}_{\ell}\) to keep track of the inserted and deleted elements, triggering reconstruction when deemed necessary.

To further clarify, when an element is deleted instead of removing that element from the sets \(L_{\ell}\) and \(G_{\ell}\), we just add it to the set \(D\), and when an element is inserted we add it to the set \(\overline{L}_{\ell}\) without changing the sets \(L_{\ell}\). While iterating through the levels to make these changes, the sets of each level get inspected and a reconstruction starting from that level is triggered if certain criteria are met. It should be noted that a set \(L_{\ell}\) gets updated whenever and only when its level is being reconstructed.

During the formation of levels, or to be more exact when elements from \(L_{\ell}\) are being selected for inclusion in \(G_{\ell}\), the elements of \(L_{\ell}\) are grouped into different buckets, so the elements from the same bucket are approximately similar in aspects of their marginal gain, weight, and marginal density.

Then, the largest bucket, denoted as \(B_{\ell}\), is chosen, and a suitable number \(m_{\ell}\) for the sample size is determined based on the chosen bucket. Consequently, a uniformly random subset of size \(m_{\ell}\) gets chosen from the previously mentioned largest bucket \(B_{\ell}\) to form the samples \(S_{\ell}\). We then form \(G_{\ell}\) by adding elements of \(S_{\ell}\) to \(G_{\ell-1}\) one by one if they meet our marginal density requirement. We then remove all elements \(e\) with \(d(e|G_{\ell})\leq\tau\) from \(L_{\ell}\) to form \(L_{\ell+1}\).

Now, we proceed to explain more about what we meant by a suitable number for sample size.

Choosing a smaller sample size ensures a larger fraction of elements from \(S_{\ell}\) appear in \(G_{\ell}\), reducing the impact of deletions of elements of \(B_{\ell}\) on the marginal gain of level \(\ell\), which leads to less need in invoking reconstruction starting from level \(\ell\). Conversely, a larger sample size may lead to more substantial removals in the filtering step, impacting the number of the levels of the data structure leading to less query complexity for each reconstruction. This is why we use simulation to obtain a sample size to strike a balance and end up with a low overall query complexity.

### Parallel Runs

We keep parallel runs and designate the threshold \(\tau=(1+\epsilon)^{i}\) to the run \(i\).

In the section B, we guarantee that at each point of time, the output of the run with threshold \(\tau\), where \(\tau\leq\frac{f(V)\cdot\epsilon}{\text{OPT}_{\text{out}}}<(1+\epsilon)\tau\) is an appropriate bicriteria approximation of the solution in that time frame.

We know that \(\tau\leq\frac{f(V)\cdot\epsilon}{\text{OPT}_{\text{out}}}<(1+\epsilon)\tau\) is equivalent to \(\log_{1+\epsilon}\left(\tau\right)\leq\log_{1+\epsilon}\left(\frac{f(V)\cdot \epsilon}{\text{OPT}_{\text{out}}}\right)<1+\log_{1+\epsilon}\left(\tau\right)\). Therefore, it is guaranteed that the output of the instance with index \(\Big{\lfloor}\log_{1+\epsilon}\left(\frac{f(V)\cdot\epsilon}{\text{OPT}_{ \text{out}}}\right)\Big{\rfloor}\), has an appropriate solution.

We know that \(1\leq\text{OPT}_{\text{cost}}\leq|V|\rho\), so we have \(\frac{f(V)\cdot\epsilon}{\text{|V|\rho}}\leq\frac{f(V)\cdot\epsilon}{\text{ OPT}_{\text{out}}}\leq f(V)\cdot\epsilon\), which implies \(\log_{1+\epsilon}\left(\frac{f(V)\cdot\epsilon}{\text{|V|\rho}}\right)\leq \log_{1+\epsilon}\left(\frac{f(V)\cdot\epsilon}{\text{OPT}_{\text{out}}} \right)\leq\log_{1+\epsilon}\left(f(V)\cdot\epsilon\right)\). Therefore, at any time we only need to search through the instances with index in \(\big{\lfloor}\log_{1+\epsilon}\left(\frac{f(V)\cdot\epsilon}{\text{|V|\rho}} \right)\Big{\rfloor}\), \(\big{\lfloor}\log_{1+\epsilon}\left(f(V)\cdot\epsilon\right)\big{\rfloor}\) to find a valid solution.

It can be observed in our last argument that the guarantee of a proper solution in the run \(i\) with \(\tau=(1+\epsilon)^{i}\) when \(\tau\leq\frac{f(V)\cdot\epsilon}{\text{OPT}_{\text{out}}}<(1+\epsilon)\tau\) is sufficient for the correctness of our algorithm. This means that for each run \(i\) with \(\tau=(1+\epsilon)^{i}\) we only need to guarantee its correctness when \(\tau\leq\frac{f(V)\cdot\epsilon}{\text{OPT}_{\text{out}}}<(1+\epsilon)\tau\).

Because of the monotonocity of the function \(f\), we know that for any \(e\in V\), \(f(e)\leq f(V)\). We also know that \(d(e)\leq f(e)\), which implies \(d(e)\leq f(V)\). We also know that \(\text{OPT}_{\text{cost}}\leq n\rho\). Therefore, for any \(e\in V\), we have \(\frac{d(e)\cdot\epsilon}{n\rho}\frac{f(V)\cdot\epsilon}{\text{OPT}_{\text{out}}}\). Hence, \(\frac{f(V)\cdot\epsilon}{\text{OPT}_{\text{out}}}<(1+\epsilon)\tau\) only holds when for any \(e\in V\), \(\frac{d(e)\cdot\epsilon}{n\rho}<(1+\epsilon)\tau\), which is equivalent to \(\log_{1+\epsilon}\left(\frac{d(e)\cdot\epsilon}{n\rho}\right)<i+1\). This is why an element \(e\) can only be considered in runs with \(i\geq\log_{1+\epsilon}\left(\frac{d(e)\cdot\epsilon}{n\rho}\right)\).

It should also be noted that an element \(e\) with \(d(e)<\tau\) will be automatically ignored by the algorithm. So we also can consider an element \(e\) only in the runs with \(\log_{1+\epsilon}\left(d(e)\right)\geq i\). Therefore, to handle the update of any element \(e\), we only need to invoke update in instances within the specified range.

```
1:for\(i\in\mathbb{Z}\)do
2: Let \(\mathcal{I}_{i}\) be the instance of our dynamic algorithm, for which \(\tau=(1+\epsilon)^{i}\).
3:functionGlobalUpdate(\(e\))
4:\(\text{update}(V)\)
5:for each\(\log_{1+\epsilon}\left(\frac{d(e)\cdot\epsilon}{n\rho(1+\epsilon)}\right)\leq i \leq\log_{1+\epsilon}d(e)\)do
6: Invoke\(\text{update}(e)\) for instance \(\mathcal{I}_{i}\).
7:functionSolutionRetrieval()
8: Let \(i^{*}\in\big{\lfloor}\log_{1+\epsilon}\left(\frac{f(V)\cdot\epsilon}{\text{|V| \rho}}\right)\big{\rfloor}\), \(\big{\lfloor}\log_{1+\epsilon}\left(f(V)\cdot\epsilon\right)\big{\rfloor}\), be the index of the instance whose corresponding \(G_{T}\) meets the criteria \(f(G_{T})\geq(1-O(\epsilon))f(V)\) and its \(G_{T}\backslash D\) has the lowest cost.
9:return\(G_{T}\backslash D\) of \(\mathcal{I}_{i^{*}}\).
```

**Algorithm 1** Parallel Runs

### Data Structure Construction

The \(\textsc{Reconstruct}(i)\) function iteratively constructs a leveled data structure built upon levels \(\ell<i\). It starts by updating sets \(L_{i}\) to include the elements inserted since its last update that have pre-approved marginal density with respect to \(G_{i-1}\) and to exclude the elements that have been since deleted. It also updates \(\overline{L}_{i}\) based on the current \(L_{i}\).

Then, a process begins, where in each step, the set \(G\) of the current level gets selected, and then the elements get filtered based on their marginal density with respect to the selected \(G\) to form the set \(L\) of the subsequent level.

This process terminates when there are no elements left in a level's set \(L\), which is when the algorithms sets \(T\) to the index of the last nonempty level.

To select each \(G_{i}\), the elements in \(L_{i}\) get processed, and each of them gets assigned to a bucket in a two-dimensional array based on their weight and their marginal density. Then the largest bucket gets selected and will be named \(B_{i}\). The algorithm determines a specific threshold \(\tau_{i}\) for level \(i\) and calculates a suitable sample size \(m_{i}\) using the CalcSampleSize function. It then selects a uniform subset \(S_{i}\) from \(B_{i}\) and adds them to \(G_{i}\) one at a time if they still meet the marginal density condition.

```
1:functionInit(\(V\))
2:\(L_{0}\gets V\), \(G_{0}\leftarrow\emptyset\), \(D\leftarrow\emptyset\), \(\overline{L}_{0}\gets L_{0}\)
3:\(L_{1}\leftarrow\{e\in L_{0}:d(e|G_{0})\geq\tau\}\), \(\overline{L}_{1}\gets L_{1}\)
4:Reconstruct(\(1\))
5:functionReconstruct(\(i\))
6:\(L_{i}\leftarrow\overline{L}_{i}\backslash D\), \(\overline{L}_{i}\gets L_{i}\)
7:while\(L_{i}\neq\emptyset\)do
8:for\(e\in L_{i}\)do
9:\(j\leftarrow\Big{\lfloor}\log_{1+e}(\frac{d(e|G_{i-1})}{\tau})\,\Big{\rfloor}\)
10:\(k\leftarrow\Big{\lfloor}\log_{1+e}(w(e))\,\Big{\rfloor}\)
11:\(\text{buck}_{j,k}\leftarrow\text{buck}_{j,k}\cup\{e\}\)
12: Let \(b_{i,1}\) and \(b_{i,2}\) be the indices of the largest buck
13:\(B_{i}\leftarrow\text{buck}_{b_{i,1},b_{i,2}}\), \(\tau_{i}\leftarrow(1+\epsilon)^{b_{i,1}}\cdot\tau\)
14:\(m_{i}\leftarrow\textsc{CalcSampleSize}(B_{i},G_{i-1},\tau_{i})\)
15:\(S_{i}=[e_{i,1},\dots,e_{i,m_{i}}]\leftarrow\) Uniform subset of size \(m_{i}\) from \(B_{i}\)
16:\(G_{i}\gets G_{i-1}\)
17:for\(e\in S_{i}\)do
18:if\(d(e|G_{i})\geq\tau_{i}\)then
19:\(G_{i}\gets G_{i}\cup e\)
20:\(L_{i+1}\leftarrow\{e\in L_{i}:d(e|G_{i})\geq\tau\}\)
21:\(\overline{L}_{i+1}\gets L_{i+1}\)
22:\(i\gets i+1\)
23:\(T\gets i-1\)
```

**Algorithm 2** Data Structure Construction

### Insertion

In the Insert Function, we manage the insertion of an element \(e\), into our dynamic data structure. First, we remove \(e\) from the set of deleted elements \(D\), indicating its active status. Next, we add \(e\) to the extended set \(\overline{L}_{0}\). Then, we iterate over the levels, starting from level \(1\) up to \(T+1\), where \(T\) represents the index of the last nonempty level. At each level, we check if the density of \(e\) with respect to \(G_{i-1}\) is greater than the threshold \(\tau\). If so, \(e\) is added to the extended set \(\overline{L}_{i}\). Otherwise, \(e\) would not be added to the extended set \(\overline{L}_{i}\), and we also no longer need to check the subsequent levels, so we terminate the loop. We also monitor the size of \(\overline{L}_{i}\), and if \(|\overline{L}_{i}|\) exceeds \(\frac{3}{2}|L_{i}|\), we reconstruct the levels starting from Level \(i\) and terminate the loop.

### Deletion

The Delete function handles the removals of the elements. When an element \(e\) is deleted, we begin by adding \(e\) to the set \(D\), which keeps track of the deleted elements. Then, we iterate over all nonempty levels. In each level, we check if the proportion of deleted elements from the bucket \(B_{i}\) used for sampling \(S_{i}\) exceeds the threshold \(\epsilon\). If this condition holds, we trigger a reconstruction of the data structure starting from the current level \(i\) using the Reconstruct function and then terminate the loop.

```
1:functionInsert(\(e\))
2:\(D\gets D\backslash\{e\}\)
3:\(\overline{L}_{0}\leftarrow\overline{L}_{0}\cup\{e\}\)
4:for\(i\gets 1,\dots,T+1\)do
5:if\(d(e|G_{i-1})<\tau\)then
6:break
7:\(\overline{L}_{i}\leftarrow\overline{L}_{i}\cup\{e\}\)
8:if\(i=T+1\) or \(|\overline{L}_{i}|\geq\frac{3}{2}\cdot|L_{i}|\)then
9:Reconstruct(i)
10:break
```

**Algorithm 3** Insertion

### Choice of Sample Size

We know that as we add elements of \(S_{i}\) to \(G_{i}\), and \(G_{i}\) grows larger, the remaining elements in \(S_{i}\) are less likely to satisfy the marginal density requirement for being added to the \(G_{i}\). Thus, intuitively, choosing a smaller sample size ensures that a larger fraction of the elements in \(S_{i}\) appear in \(G_{i}\). Therefore, deleting an \(\epsilon\)-fraction of \(S_{i}\) would not drastically affect the value of \(\Delta(G_{i}|G_{i-1})\) as opposed to the case where only a few elements of \(S_{i}\) appear in \(G_{i}\) and the deletion of those few elements has a significant impact on the marginal value \(\Delta(G_{i}|G_{i-1})\). Therefore, having a smaller sample size leads to less invocation of Reconstruct function.

On the other hand, choosing a larger \(m_{i}\) ensures that a larger number of elements will be removed in the filtering step. This will reduce the number of levels of our data structure, which decreases the query complexity of the Reconstruct function.

To balance this trade-off, we first try to find out if we process all the elements in \(B_{i}\) one by one in a random order, for any \(j\in[1,|B_{i}|]\), what is the probability of the \(j^{\text{th}}\) element being added to \(G_{i}\) and denote such probability by \(X^{*}(j)\). Then we choose the largest integer \(m_{i}^{*}\) such that \(X^{*}(j)\geq 1-\epsilon\) for all \(j\leq m_{i}^{*}\). This choice ensures that

1. In expectation, \((1-\epsilon)\)-fraction of the elements of \(S_{i}\) are added to \(G_{i}\)
2. In expectation, at least \(\epsilon\)-fraction of the elements in \(|B_{i}|\) have their marginal gain decreased sufficiently at the end of this level. Formally, these elements either do not appear in \(L_{i+1}\), or the index of their bucket decreases.

Yet, since we cannot calculate the exact values of \(X(i)\), we estimate these probabilities by simulating, and we obtain a sample size that satisfies properties similar to properties of \(m_{i}^{*}\) with a high probability.

Here we provide a formal definition for the notion of _suitable sample size_ for each level. It can be verified that the definition is chosen such that a single run of CalcSampleSize\((L^{\prime},G^{\prime},\tau^{\prime})\) provides us with a suitable sample size with respect to \(L^{\prime},G^{\prime},\) and \(\tau^{\prime}\), with a high probability. For a proofs we refer to Lemmas D.8 and D.7 in Appendix D, which were used in both query complexity guarantee and approximation guarantee of the algorithm.

**Definition 4.1** (Suitable sample size).: Given the values \(L^{\prime},G^{\prime},\) and \(\tau^{\prime}\), consider a run of ApplyAndRevert on these values and let \(\mathbf{X}\) be the \(|L^{\prime}|+1\) dimensional random output. A number \(m^{*}\leq|L^{\prime}|\) is called a _suitable sample size with respect to \(L^{\prime},G^{\prime},\) and \(\tau^{\prime}\)_ if:","\mathbb{E}[\,\mathbf{X}(r)\,]\geq 1-2\epsilon\text{ for all }r\in[1,m^{*}]\text{ and} || \mathbb{E}[\,\mathbf{X}(m^{*}+1)\,]\leq 1-\frac{ \epsilon}{2}.",,"\mathbb{E}\left[\,\mathbf{X}_{m^{*}}\,\right] \geq (1-\epsilon)\quad\text{and}\quad\mathbb{E}\left[\,\sum_{j=m^{*}+1}^{|L^{\prime}|}\mathbf{X}_{j}\,\right] \leq \epsilon\cdot m^{*}.",Definition of a suitable sample size $m^{*}$ ensuring probabilistic guarantees on the expected number of elements added to $G_i$ and the fraction of elements removed during filtering.
ICML_2024_oral_41,1,"## 3 Problem Definition

Multimodal Federated Learning (MFL) (Zhao et al., 2021) addresses the setup of \(N\)**clients** with \(M\)**modality types** (e.g. image, video, text, and audio) working together to improve their **local personal models**\(\theta_{1},\theta_{2},...,\theta_{N}\). Each client \(i\in\{1,2,...,N\}\) focuses on learning a subset of modality types \(\mathcal{I}_{i}\subseteq\{1,2...M\}\) and has a combinatorial _input space_\(\mathcal{X}_{\mathcal{I}_{i}}:=(\mathcal{U}^{(m)}|\forall m\in\mathcal{I}_{i})\), where \(\mathcal{U}^{(m)}\) is the subspace associated with the modality type \(m\). For example, as illustrated in Figure 1(a), ""client 2"" focuses on an image-text bimodal task; ""client 3"" focuses on an audio-visual bimodal task; and ""client 1"" learns a text-only unimodal task. Each client \(i\) also has a personalized label space \(\mathcal{Y}_{i}\). Each client \(i\) aims to obtain a **local mapping function**\(f_{\mathcal{A}_{i}}(\cdot;\theta_{i}):\mathcal{X}_{\mathcal{I}_{i}}\to \mathcal{Y}_{i}\) characterized by a _client-specific model architecture_\(\mathcal{A}_{i}\) and parameterized by _trainable weights_\(\theta_{i}\in\mathbb{R}^{d_{i}}\), where \(d_{i}\) indicates the structure of the weight space associated with \(\mathcal{A}_{i}\).

Each client \(i\) has access _only_ to its **local dataset**\(\mathcal{D}_{i}=\{(\mathbf{x}_{ij},y_{ij})\}_{j=1}^{n_{i}}\), sampled from \(\mathbf{x}_{ij}\sim\mathcal{P}_{i}(\mathbf{x})\) and \(y_{ij}\sim\mathcal{Q}_{i}(y|\mathbf{x}_{ij})\), where \(\mathcal{P}_{i}\) is the client-specific input distribution over the combinatorial input space \(\mathcal{X}_{\mathcal{I}_{i}}\), and \(\mathcal{Q}_{i}\) is the conditional output distribution over the space \(\mathcal{Y}_{i}\). Each sample's input consists of the modalities \(\mathbf{x}_{ij}=(\mathbf{u}_{ij}^{(m)})_{m\in\mathcal{I}_{i}}\) present as in \(\mathcal{I}_{i}\), where \(\mathbf{u}_{ij}^{(m)}\) denotes the modality \(m\) in \(\mathbf{x}_{ij}\). The **global objective** of MFL is formulated as","\min_{\theta_{1},\theta_{2},...,\theta_{N}}[\frac{1}{N}\sum_{i=1}^{N} \mathcal{L}_{i}(\theta_{i})]+\mathcal{R}(\theta_{1},\theta_{2},..., \theta_{N}),",,"\min_{\theta_{1},\theta_{2},...,\theta_{N}} \sum_{i=1}^{N} w_{i} \mathbb{E}_{(\mathbf{x}, y) \sim \mathcal{D}_{i}} \left[ \ell(f_{\mathcal{A}_{i}}(\mathbf{x}; \theta_{i}), y) \right],","where $w_i$ are non-negative weights summing to one, and $\ell(\cdot, \cdot)$ is a loss function measuring the discrepancy between the model prediction and the true label."
ICML_2024_oral_41,2,"which aims to **(1)** jointly optimize the local objectives of all clients \(\min_{\theta_{i}}\mathcal{L}_{i}(\theta_{i}):=\mathbb{E}_{(\mathbf{x},y)\sim \mathcal{D}_{i}}l(y,f_{\mathcal{A}_{i}}(\mathbf{x};\theta_{i}))\), where \(l(\cdot,\cdot)\) is the loss function, and meanwhile, **(2)** leverage a central server to encourage a privacy-preserving _knowledge sharing_ scheme among clients \(\mathcal{R}(\cdot)\) in order to boost each client's local model performance.

MFL is a problem that naturally suffers from network architecture heterogeneity among clients. Therefore, one of the main **challenges** of solving Eq.(1) is how to design and maximize the benefits of the inter-client knowledge sharing scheme \(\mathcal{R}(\cdot)\), wherein there are simultaneous _architecture heterogeneity_ (\(\mathcal{A}_{1}\neq\mathcal{A}_{2}\neq...\neq\mathcal{A}_{N}\)) as well as _statistical heterogeneity_ (Non-IIDness) among clients.

Definition 1 (Architecture-compositional MFL):When addressing the knowledge sharing among heterogeneous multimodal model architectures, traditional MFL systems typically leverage a restrictive design of _compositional_ neural architectures: \(\mathcal{A}_{i}:=\{\mathcal{B}_{\text{enc}}^{(m)}|\forall m\in\mathcal{I}_{i} \}\cup\{\mathcal{B}_{\text{share}}\}\cup\{\mathcal{B}_{\text{dec},i}\}\), such that heterogeneous model architectures are _manually_ split into smaller homogeneous _blocks_, allowing any pair of clients \(\forall i,i^{\prime}\in[N]\) share some common blocks:","\mathcal{A}_{i}\cap\mathcal{A}_{i^{\prime}}=\{\mathcal{B}_{\text{enc}}^{(m)}| \forall m\in\mathcal{I}_{i}\cap\mathcal{I}_{i^{\prime}}\}\cup\{\mathcal{B}_{ \text{share}}\},",,"\mathcal{R}(\theta_{1},\theta_{2},...,\theta_{N}) = \sum_{b \in \mathcal{B}_{\text{share}}} \mathcal{R}_{b}(\{\theta_{i,b}\}_{i=1}^{N}),",where $\mathcal{R}_{b}(\cdot)$ denotes the knowledge sharing regularizer applied on the shared block $b$ across all clients.
ICML_2024_oral_41,3,"as illustrated in Figure 1(b). Such design allows \(\mathcal{R}(\theta_{1:N})\) to be achieved through _blockwise_ weight sharing schemes, such as (Chen and Zhang, 2022; Zhao et al., 2021).

**Definition 2** (Architecture-personalized MFL (AMFL))::

We relax the traditional constraint Eq.(2), focusing on _more general_ MFL scenarios without setting any restriction on the architecture design. \(\mathcal{A}_{i}\) can be any neural architectures specified by local users, which is _non-compositional_ so that the server has no prior knowledge about the inter-client weight-space sharing scheme. That is, for \(\forall i,i^{\prime}\in[N]\),",\mathcal{A}_{i}\cap\mathcal{A}_{i^{\prime}}=\emptyset.,,"\mathcal{A}_{i}\cap\mathcal{A}_{i^{\prime}}=\emptyset,","Equation 3 defines the architecture-personalized MFL (AMFL) setting, where client-specific neural architectures have no predefined shared blocks, indicating a fully heterogeneous and non-compositional model structure across clients."
ICML_2024_oral_41,4,"Given this relaxed design, there will be three particular **cases of architecture heterogeneity** that are not permitted in traditional MFL: **(1)**_Topology Difference_ is a most common situation in multimodal FL systems. Two clients might use different model types (e.g., one client is based on Transformer while the other is based on ResNet) or use different multimodal fusion strategies for different input modality types (e.g., one client uses alignment while the other uses concatenation). **(2)**_Depth Difference_ refers that two clients having the same topology (e.g., both are based on ResNet) but their numbers of layers/modules are different. **(3)**_Width Difference_ describes a situation where two clients having the same topology and same depth, but their numbers of neurons at each layer are different. Examples of the three cases are illustrated in Figure 1(c). While traditional MFL does not address these cases of architecture heterogeneity, the **goal of AMFL** particularly deals with how to _automatically bridge the architecture gap_, for efficient and effective knowledge sharing among clients with heterogeneous neural architectures and distributions.

## 4 Methodology

In order to solve Eq.(1) under AMFL settings (Eq.(3)), we propose a new multimodal FL framework, **FedMBridge**, which _automatically bridges the architecture gap_ among statistically heterogeneous clients. We will first introduce the main idea of FedMBridge and then present its three components: (1) the topological graph representation of local multimodal architectures; (2) the hypernetwork that generates personal weights conditioned on the topological graphs of architectures; and (3) the federated training workflow.

### Main Idea of FedMBridge

**Rethinking Implicit and Explicit Weight Sharing:** Explicit weight sharing, or the simple weight aggregation within a globally-shared weight space, is seen in standard FL with homogeneous architectures (Li et al., 2019; Shamsian et al., 2021) or some Pruning-based FL methods with only width or depth differences (Jiang et al., 2022; Vahidian et al., 2021; Jiang et al., 2023). However, in AMFL, an _explicit_ weight sharing is _not available_ since a globally shared weight space does not even exist, especially if client models vary significantly in their topologies or depths. Alternatively, we explore an _implicit_ weight sharing mechanism for AMFL: instead of sharing _original_ weights across diverse weight spaces, we aim to perform knowledge sharing among clients within globally-shared _latent_ space(s).

**Definition 3** (Bridge Function):We propose an implicit weight sharing mechanism for AMFL by introducing a global _""bridge_"" function \(h(\cdot,\cdot;\phi)\), where \(\phi\in\mathbb{R}^{D}\) is the trainable weights of the bridge function. The original locally-trained weights of \(N\) clients from diverse weight spaces \(\theta_{i}\in\mathbb{R}^{d_{i}},d_{1}\neq d_{2}\neq...\neq d_{N}\), are re-parameterized as the output of the bridge function conditioned on two client-specific generative factors","\theta_{i}:=h(\mathcal{A}_{i},\mathbf{c}_{i};\phi),\ \ \ \forall i\in[N],","where the first generative factor \(\mathcal{A}_{i}\in\mathcal{G}\) is the local neural architecture from a globally-shared _latent topology space_\(\mathcal{G}\) and the second generative factor \(\mathbf{c}_{i}\in\mathcal{T}\) represents the lo

Figure 1: (a) Local mapping functions per client in Multimodal Federated Learning (MFL)","\theta_{i}=h(\mathcal{G}_{i},\mathbf{z}_{i};\phi),",where $\mathcal{G}_{i}$ is the topological graph representation of client $i$'s architecture and $\mathbf{z}_{i}$ is a learnable latent code specific to client $i$.
ICML_2024_oral_41,5,"L). (b) Problem setting of traditional MFL that requires restrictive compositional neural architectures. (c) Problem setting of **Architecture-personalized MFL (AMFL)**, without a restriction on local model architectures. In this AMFL example, “client 1” and “client 2” show a layer-_width_ difference, “client 2” and “client 3” show a _depth_ difference, and “client 3” and “client 4” show a _topology_ difference, at multimodal interaction modules.

cal task from a globally-shared _latent task space_\(\mathcal{T}\). While \(\mathcal{G}\) manages only architectural heterogeneity, \(\mathcal{T}\) manages only statistical heterogeneity. Intuitively, the bridge function \(h\) can be treated as a generative meta-learner that can digest two _disentangled_ heterogeneity patterns to solve the raw _statistical-architectural entangled_ heterogeneity pattern. We will show design details in the following sections.

### Multimodal Neural Architectures as Graphs

The multimodal neural architecture \(\mathcal{A}_{i}\) at each client \(i\) is represented as a **directed acyclic graph** structure:","\mathcal{A}_{i}:=(\mathcal{V}_{i},\mathcal{E}_{i},\mathbf{Z}_{i}^{(0)}).",,"\mathcal{A}_{i} = (\mathcal{V}_{i}, \mathcal{E}_{i}),","where $\mathcal{V}_{i}$ denotes the set of computational nodes (e.g., layers or modules) and $\mathcal{E}_{i}$ denotes the set of directed edges representing data flow between nodes in client $i$'s architecture."
ICML_2024_oral_41,6,"Each node \(v\in\mathcal{V}_{i}\) stands for a _computational operator_\(f_{v}\) in the neural architecture. \(f_{v}\) can be either non-parametric (e.g., a concatenation operator) or parametric (e.g., a linear layer with weights of size 16\(\times\)64). Edges \(\mathcal{E}_{i}\) represents the _computational flow_ of the neural architecture, where each edge \(e_{v^{\prime}\to v}\in\mathcal{E}_{i}\) indicates that the output of the operator \(f_{v^{\prime}}\) is the input of the operator \(f_{v}\). Every node \(v\) is equipped with \(K\) types of _configuration or prior information_ for the operator \(f_{v}\), including layer types, layer levels, layer shapes, modality types or fusion stage, etc. The node feature matrix \(\mathbf{Z}_{i}^{(0)}\in\mathbb{R}^{|V_{i}|\times K}\) holds such \(K\) configuration/prior information types for all operators in the graph.

Particularly, following standard multimodal learning (Zadeh et al., 2017, 2018), the neural architectures of local models employ a _three-step_ procedure. **(1) Unimodal Encoders**: At the first step, we employ unimodal encoders to extract modality-specific features. We consider multiple different architecture families (including ResNets, CNNs, MLPs, RNNs, and Small Transformers) simultaneously appearing across AMFL as unimodal encoders for different/same modality types. **(2) Multi-modality Interaction Module**: At the second step, we model the complex intra- and inter-modality interactions to effectively fuse the complementary information from multiple modalities. Since such interaction modes can be diverse across client tasks, herein, we allow AMFL to cover many existing multimodal fusion strategies, including concatenation, element-wise alignment, tensor fusion (Zadeh et al., 2017), low-rank fusion (Zadeh et al., 2018), and so on. Figure 2 (top) shows three example clients in AMFL-client-1 fuses image and text using feature summation; client-2 combines audio and image using outer-product interactions; and client-3 uses cross-attention strategies to fuse the input audio and image. **(3) Personal Final Layer**: Each client's final layer handles client-specific decision making. In broader contexts, it can be a classifier, a value prediction head, or a data generator.

The graph defined in Eq.(5) is _constructed_ by traversing the computational flow of the user-designed model architecture as follows. **(1) Collection of Nodes and Edges:** First, we borrow the ideas from DARTS (Differentiable Architecture Search) (Liu et al., 2018; Funoki & Ono, 2021) to gather nodes \(\mathcal{V}_{i}\) and edges \(\mathcal{E}_{i}\), by tracing the chain of backward gradients of variables. Specifically, a dummy multimodal input sample is fed to the model and undergoes forward function execution. After this, starting from the output variable's gradient function, we iteratively traverse the chain of gradient functions in reverse order. During the traversal, the gradient functions associated with parametric modules are gathered as _parametric nodes_, while other gradient functions, such as ConcatBackward and BmmBackward, are collected as _non-parametric nodes_. \(|\mathcal{I}_{i}|\)_input nodes_ are additionally attached to those operators with zero in-degrees, respectively. Directed _edges_ are gathered along with the traversal. **(2) Construction of Node Features:** Second, we construct node features \(\mathbf{Z}_{i}^{(0)}\) such that they provide sufficient contexts for learning the functionality role of each layer. In particular, we utilize \(K=7\) information types and each of them is categorical, including _branch_ types (\(k=1\)), _operator_ types (\(k=2\)), _layer levels_ (\(k=3\)), and parameter _shapes_ (\(k=4,5,6,7\)). Branch types refer to which branch the operator \(f_{v}\) is located within the computational flow, which should be one of the unimodal branches or be the fusion branch. Operator types refer to how \(f_{v}\) transforms the input message it receives from previous nodes. For example, parametric node have operator types including ""Linear/Conv weights"", ""bias"", ""layer normalization"", and so on; non-parametric nodes have operator types such as ""sum"", ""concatenation"", ""element-wise dot product"", and ""outer product"". In addition, we enable each node to be aware of which knowledge granularity it learns, by computing its relative layer level within its parent branch. Furthermore, different parameter sizes for the same layer necessitate varying densities of message during weight generation, and therefore, the weight-tensor shapes of each parametric layer are considered as parts of nodes features. Specifically, the raw shape sizes, since they leads to learning sparsity, are ranked into several shape scales using a lookup table. We use a held-out shape scale ""\(<\)HO\(>\)"" for non-parametric nodes and input nodes.

### Topology-aware HyperNetwork

As in Eq.(4), we propose to learn a bridge function that can jointly digest the two heterogeneity patterns (i.e., statistics and architecture heterogeneity). A challenge underlying this goal is that _how to balance and combine the two separate heterogeneity patterns_, such as which pattern is more crucial and whether there is any inter-pattern interactions.

We propose a **T**opology-**A**ware **H**yper**Network (**TAHN**) to build such a bridge function. The key idea of TAHN is to encourage \(h(\cdot,\cdot;\phi)\) to capture the **implicit roles of each layer** within the neural architecture, which are then combined with layer-invariant client-specific task information. This is inspired by an intuition that for a pair of layers from two different clients, if they act as similar roles within their models, would tend to have similar operations and weights.

Specifically, our TAHN consists of a **two-stage** process","h(\mathcal{A}_{i},\mathbf{c}_{i};\phi)=\texttt{Comb}(\mathbf{c}_{i},\texttt{Role}( \mathcal{A}_{i};\phi_{1});\phi_{2})","where the first stage \(\texttt{Role}(\cdot;\phi_{1})\) parameterized by \(\phi_{1}\) learns the implicit roles of layers such that layers across clients share a unified _layer-role embedding_ space, and the second stage \(\texttt{Comb}(\cdot,\cdot;\phi_{2})\) parameterized by \(\phi_{2}\) aims to combine the two heterogeneity patterns and directly generates the weights","\mathbf{Z}_{i}^{(l)}=\text{GNN}^{(l)}(\mathcal{A}_{i},\mathbf{Z}_{i}^{(l-1)}),\ \ \ l=1,2,...,L,",where GNN^{(l)} denotes the l-th layer of a graph neural network that updates node features Z_i^{(l)} by aggregating information from neighboring nodes in the architecture graph 𝒜_i.
ICML_2024_oral_41,7,"ts. We represent \(\phi=\{\phi_{1},\phi_{2}\}\).

#### 4.3.1 Stage One: Layer-role Encoder

In order to encode the implicit roles of layers, we consider two types of information. _First_, each layer's configuration information is important to determine the layer role. For example, if two layers from different architecture both are the convolutional layer and both are in the early level in the entire network, they tend to have similar filter and role during the computational flow. Such information is specified in \(\mathbf{Z}_{i}^{(0)}\). _Second_, the position and contexts of each layer within the graphical computational flow is also important. For example, if two layers from different architectures are located in the same position in the same computational flow, they tend to have the same role. Such information is specified as the graphical structure \(\mathcal{V}_{i},\mathcal{E}_{i}\) of the computational flow.

The two types of information can be incorporated by applying a Graph Neural Network (GNN) on the neural architecture graph \(\mathcal{A}_{i}=(\mathcal{V}_{i},\mathcal{E}_{i},\mathbf{Z}_{i}^{(0)})\), inspired from (Zhang et al., 2018; Knyazev et al., 2021; Lim et al., 2023). We formulate the layer-role encoder as an \(L\)-layer GNN","\mathbf{Z}_{i}^{(L)} =\texttt{Role}(\mathcal{A}_{i};\phi_{1}) || :=g_{L}\circ g_{L-1}\circ...\circ g_{1}(\mathbf{Z}_{i}^{(0)};\mathcal{ V}_{i},\mathcal{E}_{i}),",,"\texttt{Role}(\mathcal{A}_{i};\phi_{1})=\texttt{GNN}(\mathcal{V}_{i}, \mathcal{E}_{i},\mathbf{Z}_{i}^{(0)};\phi_{1}),",which encodes the implicit roles of layers by combining node features and graph structure via a GNN to produce a unified layer-role embedding space across clients.
ICML_2024_oral_41,8,"\). Every computational operator \(\mathbf{z}_{i,v}^{(l)}\in\mathbf{Z}_{i}^{(l)}\) is encoded through message passing as","\mathbf{z}_{i,v}^{(l)} =\sigma(\mathbf{W}_{\text{self}}^{(l)}\mathbf{z}_{i,v}^{(l-1)}+\mathbf{W}_{ \text{in}}^{(l)}\sum_{(v^{\prime},v)\in\mathcal{E}_{i}}\mathbf{z}_{i,v^{\prime}}^ {(l-1)} || \qquad+\mathbf{W}_{\text{out}}^{(l)}\sum_{(v,v^{\prime})\in\mathcal{ E}_{i}}\mathbf{z}_{i,v^{\prime}}^{(l-1)}+\mathbf{b}^{(l)}),",,"\mathbf{z}_{i,v}^{(l)} = \sigma\left( \sum_{u \in \mathcal{N}(v)} \mathbf{W}^{(l)} \mathbf{z}_{i,u}^{(l-1)} + \mathbf{b}^{(l)} \right),","where $\sigma$ is an activation function, $\mathcal{N}(v)$ denotes the neighbors of node $v$, and $\mathbf{W}^{(l)}, \mathbf{b}^{(l)}$ are learnable parameters at layer $l$."
ICML_2024_oral_41,9,"rs. \(\phi_{1}=\{\psi_{1},\psi_{2},...,\psi_{L}\}\). The output of the final GNN layer \(\mathbf{Z}_{i}^{(L)}=\{\mathbf{z}_{i,v}^{(L)}\in\mathbb{R}^{S}\}_{v\in\mathcal{V}_{i}}\) is a collection of **layer-role embeddings** for all parametric computational operators in \(\mathcal{A}_{i}\), where \(S\) is the size of the layer-role embedding space.

Figure 2: The proposed FedMBridge framework. The three example clients use different multimodal fusion strategies. The server holds the Topology-aware HyperNetwork (TAHN), a trainable bridge function for implicit weight sharing, which simultaneously overcomes the statistical heterogeneity and architecture heterogeneity among clients.

#### 4.3.2 Stage Two: Role-aware Weight Generator

The layer-role information obtained from the first stage \(\mathbf{Z}_{i}^{(L)}\) is combined with client-specific task information \(\mathbf{c}_{i}\) and then is used to generate the client weights in a node-wise manner: \(\theta_{i}=\texttt{Comb}\left(\mathbf{c}_{i},\mathbf{Z}_{i}^{(L)};\phi_{2}\right)\). We represent the client model weights as a collection of weights for all computational operators \(\theta_{i}=\{\theta_{i,v}|v\in\mathcal{V}_{i}\}\). Specifically, \(\theta_{i}\) is obtained using a HyperNetwork-based **node decoder**\(g_{\text{node}}\) applied to each node in the neural architecture graph. Let \(\theta_{i,v}\) denote the weights associated with the parametric computational operator \(v\) of client \(i\). Every \(\theta_{i,v}\) is computed","\theta_{i,v}:=g_{\text{node}}(\mathbf{c}_{i}\oplus\mathbf{z}_{i,v}^{(L)};\phi_{ 2}),\;\forall v\in\mathcal{V}_{i}",where \(\oplus\) denotes an operation (e,"\theta_{i,v}=g_{\text{node}}(\mathbf{z}_{i,v}^{(L)},\mathbf{c}_{i};\phi_{2}),\ \ \ \forall v\in\mathcal{V}_{i}^{\text{param}}.","Equation 9 defines the node-wise weight generation process in FedMBridge, where each parametric operator's weights θ_{i,v} are generated by a hypernetwork g_node conditioned on the layer-role embedding z_{i,v}^{(L)} and client-specific task information c_i."
ICML_2024_oral_41,10,"(e.g., concatenation or summation) combining two embedding vectors: **layer-specific role** embedding \(\mathbf{z}_{i,v}^{(L)}\) and a trainable client-specific **layer-invariant task** embedding \(\mathbf{c}_{i}\in\mathbb{R}^{F}\), where \(F\) is the size of task embedding space. \(g_{\text{node}}\) is an MLP-based neural network in all experiments.

### FedMBridge Training

We let clients hold only their local personal models but the server holds the TAHN model that acts as a bridge for knowledge sharing. During training, clients perform their local model updates, and meanwhile, they communicate frequently with the server to help to optimize the TAHN.

The training workflow of FedMBridge is as follows. Each communication round \(r\) contains the following steps: **(1) Download.** The server predicts the weights \(\{\theta_{i}\}_{i\in\mathcal{N}_{r}}=\{h(\mathcal{A}_{i},\mathbf{c}_{i};\phi)\}_{i \in\mathcal{N}_{r}}\) or a subset of clients \(\mathcal{N}_{r}\subset[N]\), using the current TAHN parameters and the current task embedding \(\mathbf{c}_{i}\) and conditioned on client architecture graphs \(\mathcal{A}_{i}\). Note that the graphs \(\mathcal{A}_{i}\) can be auto-recognized and constructed on the server based on the uploaded client models before the first round starts, and therefore, they do not raise significant privacy issue. **(2) Local Updates:** Each selected client \(i\in\mathcal{N}_{r}\) begins from the downloaded \(\theta_{i}\), performs several local optimization steps based on its local data \(\mathcal{D}_{i}\), and finally obtain new weights \(\widetilde{\theta}_{i}\). **(3) Upload.** Each client send its update direction \(\Delta\theta_{i}=\widetilde{\theta}_{i}-\theta_{i}\) to the server. **(4) Global Update and Knowledge Sharing.** The server computes the updates for TAHN inspired by the chain rule:","\Delta\mathbf{c}_{i} =\nabla_{\mathbf{c}_{i}}\mathcal{L}_{i}(\theta_{i})=\Delta\theta_{i} \cdot\nabla_{\mathbf{c}_{i}}\theta_{i} || \Delta\phi_{2} =\frac{1}{|\mathcal{N}_{r}|}\sum_{i\in\mathcal{N}_{r}}( \Delta\theta_{i}\cdot\nabla_{\phi_{2}}\theta_{i}) || \Delta\phi_{1} =\frac{1}{|\mathcal{N}_{r}|}\sum_{i\in\mathcal{N}_{r}}( \Delta\theta_{i}\cdot\nabla_{\mathbf{z}_{1}^{(L)}}\theta_{i}\cdot\nabla_{\phi_{i} }\mathbf{Z}_{i}^{(L)}),",,"\nabla_{\phi} = \sum_{i\in\mathcal{N}_{r}} \left[ \nabla_{\theta_{i}}\mathcal{L}_{i}(\theta_{i}) \cdot \nabla_{\phi}h(\mathcal{A}_{i},\mathbf{c}_{i};\phi) \right]",Applies the chain rule to compute server-side gradient updates for TAHN parameters φ by aggregating client-specific gradients of local losses with respect to generated weights and bridge function Jacobians.
ICML_2024_oral_44,1,"#### 2.3.1 Problem Reformulation

DAG optimization through pruning of nodes and edges was already present in the first work on ""deep learning"" with deep feedforward networks (Ivakhnenko et al., 1965; Ivakhnenko, 1968). Due to the combinatorial complexity induced by DAGs, recent studies have increasingly focused on the continuous optimization approach (Vowels et al., 2022). This is particularly relevant in scenarios where most node executions require one or more queries to LLMs for moderate-scale applications. Moreover, the utility function is typically non-differentiable due to the tokenization of LLMs, and this remains true even when a differentiable DAG sampling technique is employed. Therefore, we reformulate our edge optimization as a continuous optimization problem. Instead of optimizing in a discrete space, our approach is to optimize over a continuum of probabilistic distributions, each representing a distribution over the feasible DAGs. Formally, rather than solving the maximum utility function \(\arg\max_{\mathcal{E}}u_{\tau}(G_{\mathcal{E}})\), we propose solving","\operatorname*{arg\,max}_{\theta\in\Theta}\mathbb{E}_{G^{\prime}\sim D_{ \theta}}[u_{\tau}(G^{\prime})],",where \(D_{\theta}\) is a parameterized distribution and \(\Theta\) represents a feasible set of real-valued parameters,"\begin{equation}
\arg\max_{\theta} \mathbb{E}_{G_{\mathcal{E}} \sim p_{\theta}}[u_{\tau}(G_{\mathcal{E}})],
\end{equation}","Equation 1 reformulates the DAG optimization problem as a continuous optimization over probabilistic distributions, aiming to maximize the expected utility over sampled DAGs instead of directly optimizing in discrete space."
ICML_2024_oral_44,2,"rs.

#### 2.3.2 Solution Parameterization

A straightforward way to define a parameterized probabilistic distribution over DAGs with fixed nodes \(N\) and required edges \(E\) is to assign a real-valued parameter \(\theta_{i}\in\mathbb{R}\) to each potential edge \(e_{i}\). Let \(\theta=[\theta_{1};\,\theta_{2};\,\ldots;\,\theta_{d}]\in[0,1]^{d}\). The probability of \(G^{\prime}=G_{\mathcal{E}}\) for \(G^{\prime}\sim D_{\theta}\) is","\prod_{i=1}^{d}\begin{cases}\theta_{i}&\text{if}\,(N,E\cup(\{e_{j}\}_{j=1}^{i- 1}\cap\mathcal{E})\cup\{e_{i}\})\,\text{is a DAG},\\ 0&\text{otherwise}.\end{cases}",,"p(G^{\prime}=G_{\mathcal{E}})=\prod_{e_{i}\in\mathcal{E}}\theta_{i}\prod_{e_{j}\notin\mathcal{E}}(1-\theta_{j}),","the product of the probabilities of included edges and excluded edges, respectively, under the parameterized distribution $D_{\theta}$"
ICML_2024_oral_44,3,"A sampling method that realizes this distribution is first to initialize a graph \(G^{\prime}\leftarrow(N,E)\). Then, iteratively sample whether to include edge \(e_{i}\) in \(G^{\prime}\) for all \(i\)'s. If including \(e_{i}\) causes a cycle in current \(G^{\prime}\), then the edge would not be included. Otherwise, add the edge to \(G^{\prime}\) with probability \(\theta_{i}\).

#### 2.3.3 Optimization Algorithm

To optimize the objective function (Equation (1)), we apply the REINFORCE algorithm (Williams, 1992) by applying a gradient ascent variant (e.g., Adam (Kingma and Ba, 2014)) with an unbiased gradient estimation:","\nabla_{\theta}\operatorname{\mathbb{E}}_{G_{\mathcal{E}}\sim D_{\theta}} [u_{\tau}(G_{\mathcal{E}})]\approx\frac{1}{M}\sum_{i=1}^{M}\hat{u}_ {\tau}(G_{i})\nabla_{\theta}\log(p_{\theta}(G_{i})),","where \(G_{1},G_{2},\ldots,G_{N}\sim D_{\theta}\) are mutually independent and \(\hat{u}_{\tau}(G_{i})\) is an independent unbiased estimate of \(u_{\tau}(G_{i})\) for all \(i\) and some \(M\in\mathbb{N}\)",\nabla_{\theta}\mathbb{E}_{G^{\prime}\sim D_{\theta}}[u_{\tau}(G^{\prime})] = \mathbb{E}_{G^{\prime}\sim D_{\theta}}\left[u_{\tau}(G^{\prime})\nabla_{\theta}\log D_{\theta}(G^{\prime})\right].,"The gradient of the expected utility with respect to the parameters is estimated using the REINFORCE algorithm, enabling gradient ascent optimization through unbiased gradient estimation."
ICML_2024_oral_5,1,"## 2 Preliminaries

In this section we provide background and notations, much of which is borrowed from Geva et al. (2022).

Transformers, MLPs.Transformer-based language models typically consists of embedding and unembedding layers \(E,U\in\mathbb{R}^{|\mathcal{V}|\times d}\) with a series of \(L\) transformer layers in-between (Vaswani et al., 2017). Each layer \(l\) consists of attention heads and a multilayer perception (MLP) layer.

Given an input sequence \(\mathbf{w}=\langle w_{0},...,w_{t}\rangle\), the model first applies \(E\) to create an embedding \(\mathbf{x}_{i}\in\mathbb{R}^{d}\) for each token \(w_{i}\in\mathbf{w}\). We call \(\mathbf{x}_{i}\) the residual stream.

The residual stream is then updated by attention heads and MLP blocks from subsequent layers (bias terms omitted):",\mathbf{x}_{i}^{\ell+1}=x_{i}^{\ell}+\texttt{MLP}^{\ell}(x_{i}^{\ell}+ \texttt{Att}^{\ell}(x_{i}^{\ell})),,\mathbf{x}_i = E \mathbf{w}_i,
ICML_2024_oral_5,2,"When needed, we specify the intermittent residual stream at layer \(\ell\) (after the attention head, before the MLP) as \(\mathbf{x}^{\ell\textit{mid}}\). Per Geva et al. (2022), the updates to the residual stream from each MLP block can be further decomposed. Namely, MLP blocks consist of two linear transformations, with point-wise activations \(\sigma\) in-between:","\texttt{MLP}^{\ell}(\mathbf{x}^{\ell})=\sigma(W_{K}^{\ell}\mathbf{x}^{ \ell})W_{V}^{\ell},","where \(W_{K}^{\ell},W_{V}^{\ell}\in\mathbb{R}^{d_{mlp}\times d}\)",\texttt{MLP}^{\ell}(\mathbf{x}) = W_2^{\ell} \sigma(W_1^{\ell} \mathbf{x} + b_1^{\ell}) + b_2^{\ell},"Equation 2 defines the MLP block as composed of two linear transformations with point-wise activations in-between, following the decomposition from Geva et al. (2022)."
ICML_2024_oral_5,3,"\). We notate the \(i\)-th row in \(W_{K}\) as \(\texttt{MLP}\mathbf{k}_{i}^{\ell}\) and refer to them as key-vectors, and the \(i\)-th column in \(W_{V}\), \(\texttt{MLP}\mathbf{v}_{i}^{\ell}\), as value-vectors (we sometimes omit ""MLP"" and just use \(\mathbf{k}_{i}^{\ell},\mathbf{v}_{i}^{\ell}\)).

Equation (1) indicates that _the output of MLP blocks is the sum of its value vectors \(\mathbf{v}_{i}\), each scaled by a coefficient value \(m_{i}^{\ell}\)_, where \(\mathbf{m}^{\ell}:=\sigma\left(W_{K}^{\ell}\mathbf{x}^{\ell}\right)\in\mathbb{R }^{d_{mlp}}\):",\texttt{MLP}^{\ell}(\mathbf{x}^{\ell})=\sum_{i=1}^{d_{mlp}}\sigma(\mathbf{x}^{ \ell}\cdot\mathbf{k}_{i}^{\ell})\mathbf{v}_{i}^{\ell}=\sum_{i=1}^{d_{mlp}}m_{i} ^{\ell}\mathbf{v}_{i}^{\ell}.,,"\texttt{MLP}^{\ell}(\mathbf{x}^{\ell}) = \sum_{i=1}^{d_{\text{mlp}}} m_{i}^{\ell} \mathbf{v}_{i}^{\ell},","where $m_{i}^{\ell}$ is the $i$-th element of $\mathbf{m}^{\ell} = \sigma(W_{K}^{\ell} \mathbf{x}^{\ell})$, representing the scaled coefficients for the value vectors $\mathbf{v}_{i}^{\ell}$ in the MLP block at layer $\ell$."
ICML_2024_oral_5,4,"Put differently, the MLP block writes to the residual stream \(d_{mlp}\) times, once for each value vector. We call each of these updates a _sub-update_.

Interpreting Value Vectors in Vocabulary Space.Geva et al. (2022) demonstrate that for each sub-update, each value vector \(\mathbf{v}_{i}\) either promotes or suppresses the likelihood of a token \(w\) from being generated:","p\big{(}w\mid\mathbf{x}^{\ell}+m_{i}^{\ell}\mathbf{v}_{i}^{\ell},E\big{)} \propto\exp\big{(}\mathbf{e}_{w}\cdot\mathbf{x}^{\ell}\big{)}\cdot\exp\big{(} \mathbf{e}_{w}\cdot m_{i}^{\ell}\mathbf{v}_{i}^{\ell}\big{)}",where \(\mathbf{e}_{w}\) is the embedding of \(w\),"\Delta p(w) \propto \mathbf{v}_{i} \cdot \mathbf{u}_{w},","where $\mathbf{u}_{w}$ is the unembedding vector of token $w$, and $\Delta p(w)$ denotes the change in logit for token $w$ induced by the sub-update associated with $\mathbf{v}_{i}$."
ICML_2024_oral_5,5,"\). This indicates that when \(\mathbf{e}_{w}\cdot m_{i}^{\ell}\mathbf{v}_{i}^{\ell}>0\), the likelihood of \(w\) increases, while \(\mathbf{e}_{w}\cdot m_{i}^{\ell}\mathbf{v}_{i}^{\ell}<0\) decreases the likelihood.1

Footnote 1: See Appendix for derivation.

Further note that this dot product can be further decomposed. Namely, \(\mathbf{e}_{w}\cdot\mathbf{v}_{i}^{\ell}\) is a ""static"" value that does not depend on the input: only when \(\mathbf{v}_{i}^{\ell}\) is scaled by \(m_{i}\) (which is determined by the its corresponding key vector, \(\mathbf{k}_{i}^{\ell}\), and the residual stream \(\mathbf{x}\)) do we see the impact of the input towards the likelihood of \(w\).

Thus the projection \(\mathbf{r}_{i}^{\ell}=E\mathbf{v}_{i}^{\ell}\in\mathbb{R}^{|\mathcal{V}|}\) induces a ranking of tokens that get promoted by value vector \(\mathbf{v}_{i}\), in which tokens with the highest dot products \(\mathbf{e}_{w}\cdot\mathbf{v}_{i}\) are promoted most by value vector \(\mathbf{v}_{i}\). In Section 3 we show value vectors that promote toxicity by applying these projections.

Gated Linear Units.Shazeer (2020) empirically show that using Gated Linear Units (GLUs) (Dauphin et al., 2017) in place of MLPs yield higher quality language models. Subsequently, recent language models (Touvron et al., 2023; Jiang et al., 2023) such as Llama2 use GLUs.

GLUs take element-wise products of two linear transformations of the residual stream, one of which is then nonlinearly activated. The result is then projected back onto the residual stream:",\texttt{GLU}^{\ell}(\mathbf{x}^{\ell})=(\sigma(W_{1}\mathbf{x}^{\ell})\odot W_{ 2}\mathbf{x}^{\ell})W_{V}^{\ell},"where \(W_{1}^{\ell},W_{2}^{\ell},W_{V}^{\ell}\in R^{d_{mlp}\times d}\)","\texttt{GLU}^{\ell}(\mathbf{x}^{\ell})=\sigma(W_{K}^{\ell}\mathbf{x}^{ \ell})\odot(W_{V}^{\ell}\mathbf{x}^{\ell})W_{O}^{\ell},",where $W_{O}^{\ell}\in\mathbb{R}^{d\times d_{\text{mlp}}}$ is the output projection matrix and $\odot$ denotes the element-wise product.
ICML_2024_oral_5,6,"\).

Thus, value vectors (columns in \(W_{V}\)) are now scaled by the element-wise product of two components: \(\sigma(W_{1}\mathbf{x})\) and \(W_{2}\mathbf{x}\). We will refer to \(\sigma(W_{1}\mathbf{x})\) as _gates_, which will ""block"" its counterparts \(W_{2}\mathbf{x}\) from propagating when the non-linearity (\(\sigma\)) is not activated.

## 3 Toxicity in Pre-trained Language Models

In this section we demonstrate how toxicity is represented and elicited in pre-trained language models (GPT2, Llama2), by introducing a series of vectors that can be extracted from the language model.

### Extracting Toxic Vectors

Toxicity Probe Vector.We start by first training a linear probe model on a binary toxicity classification task. Namely, we use the Jigsaw toxic comment classification dataset (cjadams et al., 2017), which consists of 561,808 comments, each of which is labeled as toxic or non-toxic. We use a 90:10 split for training and validation. We train our probe model, \(W_{\text{Toxic}}\), on the residual stream in the last layer, averaged across all timesteps (\(\bar{\mathbf{x}}^{L-1}\)):","P(\text{Toxic}|\bar{\mathbf{x}}^{L-1})=\text{softmax}(W_{\text{Toxic}}\bar{ \mathbf{x}}^{L-1}),W_{\text{Toxic}}\in\mathbb{R}^{d}",,\mathbf{p}_{\text{toxic}} = \sigma\left(W_{\text{Toxic}} \bar{\mathbf{x}}^{L-1}\right),"where $\mathbf{p}_{\text{toxic}}$ is the probability of a sequence being toxic, and $\sigma$ is the sigmoid function."
ICML_2024_oral_5,7,"Our probe vector achieves an accuracy of 94% on the validation split. We view our toxic probe vector \(W_{\text{Toxic}}\) as an aggregate of all the relevant signals in the language model to classify an input as toxic.

Toxic Vectors in MLP Blocks.Given our probe vector \(W_{\text{Toxic}}\), we can use it to find weights within the language model that promote toxicity. Namely, Geva et al. (2022) demonstrate that value vectors promote tokens at a conceptual level. Given this, we search for value vectors that promote toxicity, by checking for all value vectors with the highest cosine similarity with \(W_{\text{Toxic}}\). We find that indeed, there are value vectors that promote toxic tokens (See Section 3.2). We notate our set of toxic value vectors as MLP.\(\mathbf{v}_{\text{Toxic}}\) and their corresponding key vectors as MLP.\(\mathbf{k}_{\text{Toxic}}\).

We provide two perspectives of our MLP.\(\mathbf{v}_{\text{Toxic}}\) vectors: 1) when triggered, they promote the likelihood of toxic tokens to be generated, and 2) MLP.\(\mathbf{v}_{\text{Toxic}}\) are vectors within the model that contribute towards the \(W_{\text{Toxic}}\) direction.

SVD: Decomposed Toxic Vectors.After extracting a set of N (=128)2 MLP.\(\mathbf{v}_{\text{Toxic}}\) vectors, we stack them into a \(N\times d\) matrix. We then apply singular value decomposition to get decomposed singular value vectors SVD.U\({}_{\text{Toxic}}\). We refer to the \(i\)-th singular value vector as SVD.U\({}_{\text{Toxic}}\)[\(i\)]. We view SVD.U\({}_{\text{Toxic}}\) as basis vectors that span the toxicity representation space within the language model.

Footnote 2: We experiment with different values for N, and get similar results.

### Toxic Vectors in Vocabulary space.

As mentioned in Section 2, we can inspect which tokens are promoted by value vectors by projecting them onto the vocabulary space.

Table 1 and 6 show the tokens with the highest dot products with our toxic vectors in GPT2 and Llama2. Each MLP.\(\mathbf{v}_{\text{Toxic}}\) and SVD.U\({}_{\text{Toxic}}\) vectors seem to encode specific dimensions of toxicity, or different contexts in which toxicity appears in pre-training data.

### Interventions Using Toxic Vectors

To validate the role that the toxic vectors play in eliciting toxic outputs, we intervene during generation to suppress toxic outputs using each vector. Namely, we use prompts from RealToxicityPrompts (Gehman et al., 2020) that elicit toxic outputs from language models. During the forward pass of the model, we intervene by simply subtracting

\begin{table}
\begin{tabular}{l l l} \hline \hline Vector & Top tokens \\ \hline \(W_{\text{Toxic}}\) & c*nt, f*ck, a**hole, d*ck, wh*re, holes \\ MLP.\(\mathbf{v}_{\text{70}}^{19}\) & sh*t, a**s, cr*p, f*ck, c*nt, garbage, trash \\ MLP.\(\mathbf{v}_{\text{71}}^{12}\) & delusional, hypocritical, arrogant, nonsense \\ MLP.\(\mathbf{v}_{\text{2669}}^{18}\) & degener, whining, idiots, stupid, smug \\ MLP.\(\mathbf{v}_{\text{68}}^{13}\) & losers, filthy, disgaf, gad, feces, apes, thous \\ MLP.\(\mathbf{v}_{\text{755}}^{12}\) & disgrace, shameful, coward, unacceptable \\ MLP.\(\mathbf{v}_{\text{882}}^{12}\) & f*ck, sh*t, piss, hilar, stupidity, poor \\ MLP.\(\mathbf{v}_{\text{1438}}^{19}\) & c*m, c*ck, orgasm, missionary, anal \\ SVD.U\({}_{\text{Toxic}}\)[0] & a**s, losers, d*ck, s*ck, balls, jack, sh*t \\ SVD.U\({}_{\text{Toxic}}\)[1] & sexually, intercourse, missive, rogens, nude \\ SVD.U\({}_{\text{Toxic}}\)[2] & sex, breasts, girlfriends, vagina, boobs \\ \hline \hline \end{tabular}
\end{table}
Table 1: Toxic vectors in GPT2, projected onto the vocabulary space. WARNING: THESE EXAMPLES ARE HIGHLY OFFENSIVE. We note that SVD.U\({}_{\text{Toxic}}\)[2] has a particularly gendered nature. This arises from the dataset and language model we use. For Llama2 results, see Appendix Table 6.

\begin{table}
\begin{tabular}{l l l l l} \hline \hline Method & Vector & Toxic & PPL & F1 \\ \hline No Op & N/A & 0.453 & 21.7 & 0.193 \\ \hline Subtract & \(W_{\text{Toxic}}\) & 0.245 & 23.56 & 0.193 \\ Subtract & MLP.\(\mathbf{v}_{\text{770}}^{12}\) & 0.305 & 23.30 & 0.192 \\ Subtract & SVD.U\({}_{\text{Toxic}}\)[0] & 0.268 & 23.48 & 0.193 \\ \hline DPO\({}^{\dagger}\) & N/A & 0.208 & 23.34 & 0.195 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Toxicity, perplexity (PPL), and F1 after interventions or DPO for GPT2. We scale our toxic vectors such that the resulting perplexity is comparable to that of post-DPO. \(\dagger\): Not an intervention. For Llama2, see Appendix Table 7.

one of the toxic vectors from the last layer:","\mathbf{x}^{L-1}=\mathbf{x}^{L-1}-\alpha*W,",where \(a\) is a heuristic scale value and \(W\) is one of our toxicity vectors,\mathbf{x}^{L,
ICML_2024_oral_5,8,"rs.

To measure the efficacy of our interventions, we measure three metrics: toxicity, perplexity, and F1.

**Toxicity.** To measure toxicity, we prompt each language model with the ""challenge"" subset of RealToxicityPrompts, which consists of 1,199 prompts that elicit extremely toxic outputs from language models. We follow prior work (Geva et al., 2022) and use Perspective API,3 an automated tool for toxicity detection, to assign toxicity scores to each generation.

Footnote 3: [https://github.com/conversational/perspectiveapi](https://github.com/conversational/perspectiveapi)

Perplexity.To ensure that our interventions do not degrade generation quality, we also follow prior work (Geva et al., 2022) and measure perplexity on the Wikitext-2 dataset (Merity et al., 2016).

F1.In addition to perplexity, we also follow prior work (Dinan et al., 2020; Adolphs et al., 2023) and measure F1. Namely, using 2,000 Wikipedia sentences as prompts, we measure the harmonic mean between precision and recall of our model's output, where precision is the fraction of generated tokens contained in the original Wikipedia continuation, and recall is the fraction of tokens in the Wikipedia continuation contained in the model's generation.

With perplexity and F1, we hope to see minimal changes after our interventions to ensure we do not affect the quality of our generations. Table 2 and 7 demonstrate the results from our interventions, while Table 3 demonstrates examples of generations before and after our interventions.

Note that our interventions depend on how much we scale each vector (\(\alpha\)). We choose a scalar value such that the resulting perplexity is similar to that of our post-DPO model. For details regarding our post-DPO model see Section 4.

We find that subtracting toxic components from the residual stream reduces toxicity.

## 4 Toxicity Alignment Using DPO

We next describe our alignment procedure using DPO.

### Background: DPO

DPO relies on pairwise preference data, in which given a prompt, we have a preferred (positive) continuation and a non-preferred (negative) continuation. Given each preference pair, the algorithm promotes the likelihood of the positive sample, while suppressing the likelihood of the negative sample, using the following loss term:","\mathcal{L}_{\text{DPO}}=-\mathbb{E}[\log\sigma(\beta \log P-\beta\log N)], || P=\frac{\pi_{\theta}(y_{+}\mid\mathbf{w})}{\pi_{ref}(y_{+}\mid \mathbf{w})},N=\frac{\pi_{\theta}(y_{-}\mid\mathbf{w})}{\pi_{ref}(y_{-}\mid \mathbf{w})},",,"\mathcal{L}_{\text{DPO}}(\theta)=\log\sigma\left(\beta\left(\log p_{\theta }(y^{+}\mid x)-\log p_{\theta}(y^{-}\mid x)\right)\right),","where $\beta$ is a temperature parameter that controls the strength of the preference optimization, and $\sigma$ is the sigmoid function."
ICML_2024_oral_5,9,"al. (2023) for details). The algorithm promotes the likelihood of \(P\), while suppressing the likelihood of \(N\).

### Constructing Pairwise Toxic Data

We build our pairwise toxicity dataset using PPLM (Dathathri et al., 2019). PPLM is an attribute-controlled language generation technique, which attaches a simple linear attribute classification layer, \(p(a|\mathbf{w})\) onto a language model to guide its generation. During generation, PPLM uses the attribute classifier to compute the gradients that increases the likelihood of the language model's output to contain the desired attribute \(a\), and shifts the activations in such direction (See Dathathri et al. (2019) for details):

\begin{table}
\begin{tabular}{l l l l} \hline \hline Prompts & Model & Top-k & Continuations \\ \hline “So I’m starting to think she’s full of” & GPT2 & sh*t, it, crap, herself, something & “sh*t. I’m not saying she’s a bad person, but” \\  & GPT2 - MLP.v\({}^{19}_{770}\) & it, herself, the sh*t, something & “it. I’m not sure if she’s just being a little too” \\  & GPT2\({}_{\text{DPO}}\) & it, herself, sh*t, surprises, crap & “it. I think she’s very much aware of the fact that” \\ \hline “You are such a piece of” & GPT2 & sh*t, work, trash, garbage, a** & “sh*t. You are such a piece of sh*t.” \\  & GPT2 - MLP.v\({}^{19}_{770}\) & work, sh*t, “s”, art, human & “work. I’ve been working on this for a while” \\  & GPT2\({}_{\text{DPO}}\) & work, art, sh*t, trash, cake & “work. I have been a fan of your work for a long” \\ \hline  & GPT2 & sh*t, crap, cow, hell, f*ck & “sh*t, I’m so glad I got this book.” \\ “Holy” & GPT2 - MLP.v\({}^{19}_{770}\) & Cow, Grail, cow, “!”, Cross & “Cow! I’ve been waiting for this for a while.” \\  & GPT2\({}_{\text{DPO}}\) & cow, crap, Grail, sh*t, smokes & “cow, this is a great book! I’ve been reading” \\ \hline \hline \end{tabular}
\end{table}
Table 3: Examples of top-k and continuations to prompts that originally elicit “sh*t” from GPT2, our interventions, and GPT2\({}_{\text{DPO}}\).",p(y\mid a)\propto p(y)p(a\mid y),,"\nabla_{\mathbf{x}}\log p(a\mid\mathbf{w})\propto\nabla_{\mathbf{x}}\log p(a\mid\mathbf{x})\cdot\frac{\partial\mathbf{x}}{\partial\mathbf{w}},",Gradient computation in PPLM for steering the language model towards generating text with a desired attribute $a$.
ICML_2024_oral_5,10,"To generate pairwise preference data, we use sentences from Wikitext-2 (Merity et al., 2016) as prompts. For each prompt, we generate a positive sample using greedy sampling with GPT2, while using PPLM to generate negative (toxic) samples. We use our toxic probe \(W_{\text{Toxic}}\) as our attribute classifier to guide towards toxic outputs. We create 24,576 pairs of toxic and nontoxic continuations.4 We train until validation loss converges with a patience value of 10, which occurs after approximately 6,700 sample pairs. Appendix E has details for DPO and PPLM hyperparameters.

Footnote 4: We release this data to enable further studies.

The last row of Table 2 shows the resulting toxicity, perplexity, and F1 scores of our DPO model.

Figure 1 shows an example of the difference in behaviors between GPT2 before and after DPO, for a specific toxic token. Namely, we use 295 prompts from RealToxicityPrompts that outputs the token ""sh*t"" as the next token. We then apply ""Logit Lens"" (Nostalgebraist, 2020), meaning we apply the unembedding layer on all intermittent layers. This allows us to visualize the layers that promote the ""sh*t"" token. The shared grey areas indicate the layers in which ""sh*t"" is promoted the most, which all correspond to MLP layers. We see that post-DPO, the toxic token is promoted far less.

## 5 Toxicity After DPO

In this section we explain how our aligned language models (GPT2\({}_{\text{DPO}}\), Llama2\({}_{\text{DPO}}\)) avert toxic outputs.

### Toxic Vectors Remain After DPO

Of the toxic vectors described in Section 3, note that MLP.v\({}_{\text{Toxic}}\) are actual weights of the model. Thus we inspect how these vectors change after DPO.

Interestingly, we find that every parameter in each language model barely changes after DPO, including token embeddings, MLP blocks, and attention heads. Every parameter in GPT2 (Llama2) and its counterpart in GPT2\({}_{\text{DPO}}\) (Llama2\({}_{\text{DPO}}\)) has a cosine similarity score greater than 0.99 and on average a norm difference less than 1e-5.5 This applies for MLP.k\({}_{\text{Toxic}}\) and MLP.v\({}_{\text{Toxic}}\) as well - toxic MLP vectors **do not change** from DPO.

Put differently, although toxicity is reduced by DPO, the ability to elicit toxicity with these value vectors still remain. So how is it that GPT2\({}_{\text{DPO}}\) and Llama2\({}_{\text{DPO}}\) avert toxic outputs? Though their parameters have barely moved, below we show that their collective movement is enough to avoid toxic outputs.

Footnote 5: The unembedding layer of GPT2 is the only exception, where the norm difference is less than 1e-3.

### DPO Avoids MLP.k\({}_{\text{Toxic}}\) Regions

We provide an explanation for how toxicity is reduced in GPT2\({}_{\text{DPO}}\) and Llama2\({}_{\text{DPO}}\), starting with GPT2\({}_{\text{DPO}}\).

GPT2\({}_{\text{DPO}}\).In simplest terms, we observe a drop in activations for the toxic vectors MLP.v\({}_{\text{Toxic}}\) in GPT2\({}_{\text{DPO}}\). Namely, using the same 1,199 prompts from RealToxicityPrompts, we generate 20 tokens and measure the mean activations \(m_{i}\), or \(\sigma(\mathbf{x}^{\ell}\cdot\text{MLP.k}_{i}^{\ell})\), of our MLP.v\({}_{\text{Toxic}}\) vectors. Figure 2 shows 5 examples of the top MLP.v\({}_{\text{Toxic}}\) vectors.

Inspired by Balestriero et al. (2023), we visualize this drop

Figure 1: Logit lens on GPT2 and GPT2\({}_{\text{DPO}}\). Given 295 prompts that originally elicit “sh*t” as the next token, we plot the average probability of outputting “sh*t” from intermittent layers by applying the unembedding layer. Minor ticks indicate \(\ell\_mid\) layers (after attention heads, before MLP). Shaded areas indicate layers that promote “sh*t” the most, which all correspond to MLP layers.

Figure 2: Mean activations for toxic vectors in GPT2 before and after DPO.

in activations with what we call ""MLP activation regions"". An activation region of a key vector is simply a _subspace_ within the model's hidden space in which its vectors have high dot products to activate its corresponding value vector:","\gamma(\mathbf{k}_{i}^{\ell}):=\{\mathbf{g}|\mathbf{g}\in\mathbb{R}^{d},\sigma( \mathbf{k}_{i}^{\ell}\cdot\mathbf{g})>0\},",where \(\sigma\) is a non-linear activation,\mathbf{x}_{\text{DPO}}^{\ell} \cdot \text{MLP}.\mathbf{k}_i^{\ell} < \mathbf{x}^{\ell} \cdot \text{MLP}.\mathbf{k}_i^{\ell},"Equation comparing residual stream projections onto toxic key vectors before and after DPO, showing reduced alignment with toxic activation regions."
ICML_2024_oral_5,11,"on. Put differently, for all key-vector regions that the residual stream ""passes through"", their corresponding value-vectors are activated, scaled, and added into the residual stream.

We view the drop in activations as a shift in GPT2\({}_{\text{DPO}}\)'s residual stream to avert the regions of toxic MLP vectors, \(\gamma(\text{MLP}.\mathbf{k}_{\text{Toxic}})\). See Figure 3.

We formalize the shift in residual streams as following: given the residual streams at layer \(\ell\_mid\) (after attention heads at layer \(\ell\)) for both GPT2 and GPT2\({}_{\text{DPO}}\), before \(\text{MLP}_{\text{Toxic}}^{\ell}\), we indicate the difference of the two residual streams as \(\delta_{\mathbf{x}}^{\ell\_mid}:=\mathbf{x}_{\text{DPO}}^{\ell\_mid}-\mathbf{ x}_{\text{GPT2}}^{\ell\_mid},\delta_{\mathbf{x}}^{\ell\_mid}\in\mathbb{R}^{d}\). We view \(\delta_{\mathbf{x}}^{\ell\_mid}\) as a vector that takes GPT2's residual stream out of the toxicity-eliciting regions, \(\gamma(\text{MLP}.\mathbf{k}_{\text{Toxic}}^{\ell})\).

Figure 4 provides a visualization of the residual stream's shift out of toxic regions. Namely, given prompts from RealToxicityPrompts, we project the residual stream from GPT2 and GPT2\({}_{\text{DPO}}\) at layer 19 onto two dimensions: 1) the mean difference in the residual streams, \(\delta_{x}^{\ell}\), and the main principle component of the residual streams.6 We further indicate whether each residual stream activates \(\text{MLP}.\mathbf{v}_{770}^{19}\). Notice both the consistent linear shift between GPT2 and GPT2\({}_{\text{DPO}}\) and the drop in activations.

Footnote 6: We show layer 19 because \(\text{MLP}.\mathbf{v}_{770}^{19}\) is one of the most toxic vectors, but similar patterns can be found in other layers (See Appendix C).

To understand where this shift comes from, we compute the differences in all parameter weights in GPT2 before and after DPO, and notate the differences as \(\delta_{\theta}\). We notate the difference at a specific component such as a MLP block at layer \(\ell\) as \(\delta_{\text{MLP}}^{\ell}\).

Note that as previously noted, these differences \(\delta_{\theta}^{\ell},\forall\ell\) are minimal. Despite these minimal changes, their accumulation is sufficient in getting the residual stream out of toxic regions \(\gamma(\text{MLP}.\mathbf{k}_{\text{Toxic}}^{\ell})\).

Given a toxic vector \(\text{MLP}.\mathbf{v}_{\text{Toxic}}\) at layer \(\ell\), to understand where the shift in residual stream, \(\delta_{\mathbf{x}}^{\ell\_mid}\) comes from, we measure the cosine similarity between \(\delta_{\mathbf{x}}^{\ell\_mid}\) and the shift in value vectors in the preceding layers, \(\delta_{\text{MLP},\mathbf{v}}^{j}\):","\forall j<\ell,\forall i<d_{mlp}:cos(\delta_{\mathbf{x}}^{\ell\_midmid}, \delta_{\text{MLP},\mathbf{v}_{i}}^{j}).",,"\delta_{\mathbf{x}}^{\ell\_mid}\propto\sum_{j=1}^{\ell-1}\text{cos}(\delta_{ \mathbf{x}}^{\ell\_mid},\delta_{\text{MLP},\mathbf{v}}^{j})\|\delta_{\text{MLP}, \mathbf{v}}^{j}\|,","Equation 11: Formalization of the shift in residual streams in terms of preceding layer value vector differences, showing how minimal parameter changes across layers cumulatively steer the stream away from toxic regions."
ICML_2024_oral_53,1,"### Contributions

Our main result is a precise characterization of the tradeoff between the accuracy of a learning algorithm and its CMI:

**Key result: CMI-Accuracy Tradeoff for \(\varepsilon\)-learners.**

We show that in the general SCO setup as well as under further structural assumption of strong convexity, there exists a tradeoff between the accuracy of an \(\varepsilon\)-learner and its CMI: Surprisingly, to achieve small excess error, a learner _must_ carry a large amount of CMI, scaling with the optimal sample size. This result completely answers an open question by Livni [13]. More precisely, we study CMI of learners for two important classes of SCO problems:

* _Lipschitz bounded SCO:_ We construct an SCO problem such that, for every \(\varepsilon\)-learner, there exists a distribution such that the CMI of the learner is \(\Omega(1/\varepsilon^{2})\), despite the already-established optimal sample complexity \(O(1/\varepsilon^{2})\). We complement this result with a matching upperbound. We also show that this result is true for both proper as well as improper (unconstrained) learning algorithms.
* _Strong Convexity:_ Under further structural assumption of strong convexity, we establish an \(\Omega(1/\varepsilon)\) lower bound on CMI of every \(\varepsilon\)-learner which we show is also tight.

Our proof techniques are inspired from the privacy literature and build on so-called fingerprinting lemmas [1, 1, 20, 19]. Our key results and proof ideas have various interesting implications:

**Limitation of the CMI Generalization Bound for SCOs.** Our lower bounds highlight that CMI-based generalization bounds for SCO do not fully explain the optimal excess error. For algorithms with optimal sample complexity, the established CMI lower bound implies that standard CMI generalization guarantees are vacuous.

In more detail, Steinke and Zakynthinou [15] show that the generalization error of any learner can be bounded by",\text{generalization gap}\leq\sqrt{\frac{\operatorname{CMI}_{\mathcal{D}}( \mathcal{A}_{n})}{n}}.,,"\begin{equation}
\text{gen}(\mathcal{A}) \leq \sqrt{\frac{2 I(S;W)}{n}}.
\end{equation}",Generalization error bound in terms of conditional mutual information (CMI) and sample size n.
ICML_2024_oral_53,2,"(See Section 3 for a more formal statement.) Plugging our lower bound on CMI into the above equation we obtain an upper bound on the generalization gap of \(O\left(\sqrt{\frac{1}{\varepsilon^{2}\cdot n}}\right)\) which is strictly larger than the true \(O(\varepsilon)\) error. In particular, for the optimal choice of \(n\), we obtain a vacuous generalization bound of order \(\Omega(1)\), even though the algorithm perfectly learns. Similarly, under the assumption of strong convexity, one can learn with sample complexity of \(O(1/\varepsilon)\). Thus, again we obtain that the CMI bound may be order of \(\Omega(1)\), even though the learner is able to learn.

**Necessity of Memorization.** Inspired by CMI and membership inference [14], we have developed a framework to quantify memorization in SCO: informally, a point is considered memorized if adversary can guess correctly if this point appeared in the training set with a high confidence. Building on our construction for CMI, we design an adversary capable of correctly identifying a significant fraction of the training samples in certain SCO problems, implying that memorization is a necessary component in this context. A similar point appeared in [12, 15].

To be more precise, we consider a contestant and an adversary. The contestant gets to train a model on training set not revealed to the adversary. The contestant then shows the adversary a sample either from the training set, or a freshly drawn sample (not seen during training time). A point is considered _memorized_ if the adversary correctly identifies whether the shown sample appeared during training time (while refraining from accusing freshly drawn samples).

We show that our approach for lower bounding CMI lets us design an adversary with the following guarantee: there exists an SCO problem such that for every \(\varepsilon\)-learner, there exists a distribution such that the adversary can distinguish \(\Omega(1/\varepsilon^{2})\) of the training samples with a high confidence. We also establish a similar result under an additional assumption of strong convexity, showing that there exists an adversary that can distinguish \(\Omega(1/\varepsilon)\) of the training samples. Notice, that in both cases, the size of sample to be memorized scales linearly with the sample complexity. In other words, Any sample-efficient learner needs to memorize a constant fraction of its training set.

Incompressibility of Samples in SCOs.Our results rule out the existence of constant-sized (dimension-independent) sample compression schemes for SCO. Many learning algorithms, like support vector machines (SVMs), generate their output using only a small subset of training examples-- for SVMs such a subset is known as support vectors. _Sample compression schemes_, introduced by Littlestone and Warmuth [10], provide a precise characterization of this algorithmic property. Since the optimal sample complexity in SCO is dimension-independent, a natural question to ask is whether we can construct a sample compression scheme of _constant_ size for SCOs. (Here _constant compression size_ refers to a dimension-independent quantity.) Using the results connecting CMI and sample compression scheme in [11], we show that such a construction is impossible. This finding is in stark a contrast with binary classification [13].

Individual-Sample variant of CMI.We show that our techniques extend to lower-bounding the individual sample variant of CMI proposed in [12, 13, 14, 15].

### Organization

The rest of this paper is is structured as follows. In Section 2 we discuss the related work. After providing the necessary preliminaries in Section 3, we present an overview of the main results in Section 4. Then, in Section 5, we discuss several implications of our main results. Finally, in Section 6 and Section 7, we present the key steps of the proofs of the main results.

## 2 Related Work

Information-Theoretic Measures of Generalization.In recent years, there has been a flurry of interest in the use of information-theoretic quantities for characterizing the expected generalization error of learning algorithms. For an excellent overview of recent advances see [1, 1]. Here, we discuss the work on worst-case information-theoretic measures of learning algorithms. The initial focus of this line of work [14, 15, 16] was based on _input-output mutual information (IOMI)_ of an algorithm. Unfortunately, IOMI does not yield a useful notion of information complexity for learning in many key settings. For instance, prior work, in the settings of binary classification [1, 17, 18, 19] and SCO [18] highlights severe limitations of the IOMI framework: for every _good_ learning algorithm in binary classification (SCO), there always exists a learning problem in which IOMI is unbounded (dimension-dependent). The notion of CMI [15, 16, 17, 18, 19, 20] remedies some of the above issues, at least in the classification setting. Despite CMI addressing some of the limitations of IOMI, Haghifam, Rodriguez-Galvez, Thobaben, Skoglund, Roy, and Dziugaite [19] show that CMI cannot explain the minimaxity of gradient descent in SCO. Our work significantly extends their result: we show that the same limitations hold for _every_\(\varepsilon\)-learner algorithm with a dimension-independent sample complexity. Notice that gradient descent with a proper learning rate [1, 2] is one of the \(\varepsilon\)-learner algorithms that can have dimension-independent sample complexity. See Remark 5.3 for a detailed discussion. A recent work of Wang and Mao [20] proposes a new measure similar to CMI refereed to as hypotheses-conditioned CMI and shows that it is related to the uniform stability [1]. However, hypotheses-conditioned CMI is not an appropriate measure for studying memorization in SCOs since its conditioning term is different.

Memorization.[14, 15, 16, 17, 18] theoretically study the necessity of memorization in learning. The measure of memorization in our work is different from the prior work. Also, the mentioned work does not study the question of memorization in the context of SCOs. Most similar to our work is [18] where the authors study memorization using IOMI. Memorization has been demonstrated to happen also empirically in state-of-the-art algorithms [13, 14, 15, 16, 17]. In contrast with empirical studies, the aim of a theoretical investigation is to study its role, and whether it is _necessary_ or a byproduct of current practices.

Fingerprinting Codes and Privacy Attacks.The key idea behind our lower bound proof builds on privacy attacks developed in differential privacy known as _fingerprintingcodes_[1, 2, 3, 1, 10, 11]. Dwork, Smith, Steinke, Ullman, and Vadhan [14] consider the problem of designing privacy attacks on the mean estimators that expose a fraction of the training data. They propose an adversary and show that every algorithm that precisely estimates mean in \(\ell_{\infty}\) leaks the membership of the samples in the training set. The \(\ell_{\infty}\) hypercube cannot be learned in a dimension independent sample size, therefore, to obtain the separation we desire, we can only assume a weaker \(\ell_{2}\) approximation, which leads to further challenges, especially in the unconstrained non-strongly convex case which is the hardest.

## 3 Preliminaries

### Stochastic Convex Optimization (SCO)

A _stochastic convex optimization_ (SCO) problem is a triple \((\Theta,\mathcal{Z},f)\), where \(\Theta\subseteq\mathbb{R}^{d}\) is a convex set and \(f(\cdot,z):\Theta\rightarrow\mathbb{R}\) is a convex function for every \(z\in\mathcal{Z}\). We refer to \(\Theta\) as the parameter space, to its elements as parameters, to elements of \(\mathcal{Z}\) as data, and to \(f\) as the _loss function_. Informally, given an SCO problem \((\Theta,\mathcal{Z},f)\), the goal is to find an approximate minimizer of the _population risk_\(\mathrm{F}_{\mathcal{D}}(\theta)\triangleq\mathbb{E}_{Z\sim\mathcal{D}}[f( \theta,Z)]\), given an i.i.d. sample \(S_{n}=\{Z_{1},\ldots,Z_{n}\}\) drawn from an unknown distribution \(\mathcal{D}\) on \(\mathcal{Z}\), denoted by \(\mathcal{D}\in\mathcal{M}_{1}(\mathcal{Z})\). The _empirical risk_ of \(\theta\in\Theta\) on a sample \(S_{n}\in\mathcal{Z}^{n}\) is \(\hat{\mathrm{F}}_{S_{n}}(\theta):=\frac{1}{n}\sum_{i\in[n]}f(\theta,Z_{i})\), where \([n]\) denotes the set \(\{1,\ldots,n\}\). A _learning algorithm_ is a sequence \(\mathcal{A}=(\mathcal{A}_{n})_{n\geq 1}\) such that, for every positive integer \(n\), \(\mathcal{A}_{n}\) maps \(S_{n}\) to a (potentially random) element \(\hat{\theta}=\mathcal{A}_{n}(S_{n})\) in \(\mathbb{R}^{d}\). The _expected generalization error_ of \(\mathcal{A}_{n}\) under \(\mathcal{D}\) is \(\mathrm{EGE}_{\mathcal{D}}(\mathcal{A}_{n})=\mathbb{E}[\mathrm{F}_{\mathcal{D }}(\mathcal{A}(S_{n}))-\hat{\mathrm{F}}_{S_{n}}(\mathcal{A}(S_{n}))]\). Also, the expected excess error \(\mathcal{A}_{n}\) under \(\mathcal{D}\) is \(\mathbb{E}[\mathrm{F}_{\mathcal{D}}(\mathcal{A}(S_{n}))]-\min_{\theta\in \Theta}\mathrm{F}_{\mathcal{D}}(\theta)\). A learning algorithm is called _proper_ if its output, for all possible training sets satisfies \(\mathcal{A}_{n}(S_{n})\in\Theta\). Otherwise, it is called _improper_.

**Definition 3.1**.: (\(\varepsilon\)-learner for SCO): Fix an SCO problem \((\Theta,\mathcal{Z},f)\) and \(\varepsilon>0\). We say \(\mathcal{A}=\{\mathcal{A}_{n}\}_{n\geq 1}\)_\(\varepsilon\)-learns_ \((\Theta,\mathcal{Z},f)\) with sample complexity of \(N:\mathbb{R}\times\mathbb{R}\rightarrow\mathbb{N}\) if the following holds: for every \(\delta\in(0,1]\), given number of samples \(n\geq N(\varepsilon,\delta)\), we have that for every \(\mathcal{D}\in\mathcal{M}_{1}(\mathcal{Z})\), with probability at least \(1-\delta\) over \(S_{n}\sim\mathcal{D}^{\otimes n}\) and internal randomness of \(\mathcal{A}\),",\mathrm{F}_{\mathcal{D}}(\mathcal{A}_{n}(S_{n}))-\min_{\theta\in\Theta} \mathrm{F}_{\mathcal{D}}(\theta)\leq\varepsilon.,,,
ICML_2024_oral_53,3,"We also refer to \(N(\cdot,\cdot)\) as _sample complexity_ of \(\mathcal{A}\).

We consider two important subclasses of SCO problems that impose different conditions over the loss function and the parameter space [1, 14].

1. _Convex-Lipschitz-Bounded (CLB):_ SCO with convex and \(L\)-Lipschitz loss function defined over a bounded domain with diameter \(R\), namely, for any \(\theta\in\Theta\) we have \(\|\theta\|\leq R\). We say a loss function is \(L\)-Lipschitz if and only if \(\forall z\in\mathcal{Z}\), \(\forall\theta_{1},\theta_{2}\in\Theta:|f(\theta_{1},z)-f(\theta_{2},z)|\leq L \left\|\theta_{2}-\theta_{1}\right\|\). We refer to this subclass as \(\mathcal{C}_{L,R}\).
2. _SCO with \(L\)-Lipschitz and \(\lambda\)-strongly convex loss (CSL):_ We say a loss function is \(\lambda\)-strongly convex for all \(\theta_{1},\theta_{2}\in\Theta\) and \(z\in\mathcal{Z}\) we have \(f(\theta_{2},z)\geq f(\theta_{1},z)+\left\langle\partial f(\theta_{1},z), \theta_{2}-\theta_{1}\right\rangle+\frac{\lambda}{2}\left\|\theta_{2}-\theta_{ 1}\right\|^{2}\) where \(\partial f(\theta_{1},z)\) is the subgradient of \(f(\cdot,z)\) at \(w\). The definition of Lipschitzness is the same as in the CLB subclass. We refer to this subclass as \(\mathcal{C}_{L,\lambda}\).

### Measure of Information Complexity

Next, we formally introduce the framework proposed by Steinke and Zakynthinou [14] which aims to quantify the information complexity of a learning algorithm.

**Definition 3.2**.: Let \(\mathcal{D}\) be a data distribution, and \(\mathcal{A}=(\mathcal{A}_{n})_{n\geq 1}\) a learning algorithm. For every \(n\in\mathbb{N}\), let \(\tilde{\mathbf{Z}}=(Z_{i,j})_{i\in\{0,1\},j\in[n]}\) be an array of i.i.d samples drawn from \(\mathcal{D}\), and \(U=(U_{1},\ldots,U_{n})\sim\text{Ber}\left(\frac{1}{2}\right)^{\otimes n}\), where \(U\) and \(\tilde{\mathbf{Z}}\) are independent. Define a training set \(S_{n}=(Z_{U_{i},i})_{i\in[n]}\). The conditional mutual information (CMI) of \(\mathcal{A}_{n}\) with respect to \(\mathcal{D}\) is",\mathrm{CMI}_{\mathcal{D}}(\mathcal{A}_{n})\triangleq I(\mathcal{A}_{n}(S_{n}) ;U|\tilde{\mathbf{Z}}).,,"\operatorname{CMI}_{\mathcal{D}}(\mathcal{A}_{n})=I\left( \mathcal{A}_{n}(S_{n});U\mid\tilde{\mathbf{Z}}\right),","where $I(\cdot;\cdot \mid \cdot)$ denotes the conditional mutual information, quantifying the dependence between the algorithm's output and the membership indicators $U$ given the dataset $\tilde{\mathbf{Z}}$."
ICML_2024_oral_53,4,"## 4 Main Results

In this section we formally state our main results. First in Section 4.1, we give an overview of CMI-accuracy tradeoff for \(\varepsilon\)-learners. Then, in Section 4.2, we precisely define the memorization game and present our results on the necessity of memorization.

### CMI-Accuracy Tradeoff

We begin with a lower bound on the CMI for the CLB subclass.

**Theorem 4.1**.: _There exists a loss function \(f(\cdot,z)\) that is \(O(1)\)-Lipschitz, for every \(z\) such that: For every \(\varepsilon\leq 1\) and for every algorithm \(\mathcal{A}=\{\mathcal{A}_{n}\}_{n\in\mathbb{N}}\) that \(\varepsilon\)-learns with the sample complexity \(N(\cdot,\cdot)\) the following holds: for every \(\delta\leq\varepsilon\), \(n\geq N(\varepsilon,\delta)\), and \(d\geq\Omega(n^{4}\log(n))\), there exists a data distribution \(\mathcal{D}\in\mathcal{M}_{1}(\mathcal{Z})\) such that \(\mathrm{CMI}_{\mathcal{D}}(\mathcal{A}_{n})=\Omega\left(\frac{1}{\varepsilon^{2}}\right)\)._

In particular, we obtain that for every algorithm, in sufficiently large dimension, there exists a problem instance where the CMI-generalization bound in [14] becomes vacuous for every algorithm with sample complexity \(n=O(1/\varepsilon^{2})\).

Notice that the bound above is tight, namely, there exists an \(\varepsilon\)-learner with \(\mathrm{CMI}\) at most \(O(1/\varepsilon^{2})\). Consider a base algorithm with the sample complexity \(N(\varepsilon,\delta)=O\left(\log\left(1/\delta\right)/\varepsilon^{2}\right)\) (e.g. regularized ERM [2]) or stabilized Gradient Descent [1]. Then, given \(n\geq\Omega\left(\log\left(1/\delta\right)/\varepsilon^{2}\right)\), we may consider an algorithm that subsamples \(O(\log\left(1/\delta\right)/\varepsilon^{2})\) examples and feed it into the base algorithm. By the definition of the \(\mathrm{CMI}\), it is bounded by the size of the subsample used for learning. This argument shows that there exists an algorithm with \(\mathrm{CMI}_{\mathcal{D}}(\mathcal{A}_{n})=O(1/\varepsilon^{2})\). Formal statement of the described upperbound appears in Theorem 6.5.

Under further structural assumptions, though, the sample complexity in SCO can be improved. It is a question then if \(\mathrm{CMI}\) bounds can also be further tightened under such structural assumptions such as, for example strong convexity. Our next result shows that this is indeed the case:

**Theorem 4.2**.: _There exists a function \(f(\cdot,z)\) that is \(O(1)\)-Lipschitz, and \(O(1)\)-strongly convex, for every \(z\) such that: For every \(\varepsilon<1/24\) and \(\delta<1/48\) and for every \(\varepsilon\)-learner (\(\mathcal{A}=\{\mathcal{A}_{n}\}_{n\in\mathbb{N}}\)), with sample complexity \(N(\cdot,\cdot)\) the following holds: for every \(n\geq N(\varepsilon,\delta)\), \(\delta<O(1/n^{2})\), and \(d\geq O(n^{4}\log(n))\), there exists a data distribution \(\mathcal{D}\in\mathcal{M}_{1}(\mathcal{Z})\) such that \(\mathrm{CMI}_{\mathcal{D}}(\mathcal{A}_{n})\geq\Omega\left(\frac{1}{\varepsilon}\right)\)._

As in the general case, the above bound is tight. As discussed in [20], any ERM is stable, hence generalizes over a strongly convex objective with sample complexity of \(N(\varepsilon,\delta)=O(\log(1/\delta)/\varepsilon)\). Therefore, as before, we obtain that the above bound is tight for this setup. Formal statement of the upperbound appears in Theorem 7.4.

We finish this section by introducing a memorization game that helps us formalize in what sense a learner must memorize the data in SCO.

### Memorization Game

Intuitively, we can think of \(\mathrm{CMI}\) as measuring the number of examples we can identify from the training set by observing the model. However, formally there is a gap between this interpretation and the definition of \(\mathrm{CMI}\). For example, one could think of a learner that _spreads the information_ by using many samples, where we have that \(\mathrm{CMI}_{\mathcal{D}}(\mathcal{A}_{n})\geq\Omega(1/\varepsilon^{2})\), but for each specified example, the information over \(U_{i}\) is small (see Definition 3.2.). In other words, there is a formal gap between large \(\mathrm{CMI}\) and intuitive notions of memorization. In this subsection, we aim to close this gap by showing that, in fact, this is not the case, and the information the learner carries on \(U\) can be used to actually identify examples from the training set. The proofs will be appeared in Appendix F.

**Definition 4.3** (Recall Game for \(i\)-th example).: Let \(\mathcal{A}=\{\mathcal{A}_{n}\}_{n\geq 1}\) be a learning algorithm, \(S_{n}=(Z_{1},\ldots,Z_{n})\sim\mathcal{D}^{\otimes n}\) be a training set, and \(\hat{\theta}=\mathcal{A}_{n}(S_{n})\). Let \(\mathcal{Q}:\mathbb{R}^{d}\times\mathcal{Z}\times\mathcal{M}_{1}(\mathcal{Z}) \rightarrow\{0,1\}\) be an adversary. Consider the following game. For \(i\in[n]\), we sample a fresh data point \(\tilde{Z}_{i}\sim\mathcal{D}\), independent of \(\hat{\theta}\) and \(Z_{i}\). Let \(Z_{1,i}=Z_{i}\) and \(Z_{0,i}=\tilde{Z}_{i}\). Then, we flip a fair coin \(b_{i}\sim\text{Unif}(\{0,1\})\). Finally, the adversary outputs \(\hat{b}_{i}\triangleq\mathcal{Q}\left(\hat{\theta},Z_{b_{i},i},\mathcal{D}\right)\).

The next definition formalizes the measures used for evaluating an adversary.

**Definition 4.4** (soundness and recall).: Consider the setup described in Definition 4.3. Assume that the adversary plays the game for each of the data points in the training set, i.e., \(n\) rounds. Then,

1. We say the adversary is \(\xi\)-sound if \(\mathbb{P}\left(\exists i\in[n]:\mathcal{Q}\left(\hat{\theta},Z_{0,i}, \mathcal{D}\right)=1\right)\quad\leq\quad\xi\) where \(\xi\in[0,1]\) is a constant.
2. We say the adversary certifies the recall of \(m\) samples if \(\mathbb{P}\left(\sum_{i=1}^{n}\mathcal{Q}\left(\hat{\theta},Z_{1,i},\mathcal{D} \right)\geq m\right)\geq 1/3\).

Intuitively, soundness condition implies that if the adversary identifies a sample as part of the training set, its prediction needs to be accurate. Then, the recall condition makes sure the adversary can identify many training points, which is quantified by \(m\). Next, we present the main results:

**Theorem 4.5**.: _Fix \(\xi\in(0,1]\). There exists a SCO problem with \(O(1)\) convex Lipschitz loss defined over the ball of radius one in \(\mathbb{R}^{d}\), and there exists an efficient adversary such that the following is true. For every \(\varepsilon<1\), \(\delta<\varepsilon\), and for every \(\varepsilon\)-learner (\(\mathcal{A}\)), with sample complexity \(N(\varepsilon,\delta)=\Theta(\log(1/\delta)/\varepsilon^{2})\) the following holds: for \(n=N(\varepsilon,\delta)\) and \(d\geq O(n^{4}\log(n/\xi))\), there exists a data distribution \(\mathcal{D}\in\mathcal{M}_{1}(\mathcal{Z})\) such that the adversary is \(\xi\)-sound and certifies a recall of \(\Omega(1/\varepsilon^{2})\) samples._

**Theorem 4.6**.: _Fix \(\xi\in(0,1]\). There exists a SCO problem with \(O(1)\) strongly convex and \(O(1)\) Lipschitz loss, and there exists an efficient adversary such that the following is true. For every \(\varepsilon<1/24\), \(\delta<1/48\), and for every \(\varepsilon\)-learner (\(\mathcal{A}\)), with sample complexity \(N\) the following holds: for every \(n\geq N(\varepsilon,\delta)\), \(\delta<O(1/n^{2})\), and \(d\geq O(n^{4}\log(n/\xi))\), there exists a data distribution \(\mathcal{D}\in\mathcal{M}_{1}(\mathcal{Z})\) such that the adversary is \(\xi\)-sound and certifies a recall of \(\Omega(1/\varepsilon)\) samples._

_Remark 4.7_.: Notice that the adversary only requires access to the the output of the algorithm. Moreover, in Definition 4.3, we assume that the adversary has access to the data distribution. This assumption is only for convenience and can be easily relaxed by assuming the adversary has a constant number of fresh samples from the unknown data distribution. As can be seen in the proof, the adversary only requires an estimate of \(\mu=\mathbb{E}_{Z\sim\mathcal{D}}[Z]\).

## 5 Implications

### Limitation of CMI-Based Generalization Bounds for SCO

CMI is proposed by Steinke and Zakynthinou [14] as an information-theoretic measure for studying the generalization properties of learning algorithms. An important question regarding CMI framework is that for which learning problems and learning algorithms is the CMI framework _expressive_ enough to accurately estimate the optimal worst-case generalization error? This question has been studied extensively for the the setting of binary classification and 0-1 valued loss. In [14, 1, 1, 2], the authors show that CMI framework can be used to establish optimal worst-case excess error bounds in the realizable setting. Despite these successful applications, much less is known about the optimality or limitations of CMI framework beyond the setting of binary classification and 0-1 valued loss. In this section, our main result is that for every learning algorithm for SCO with an optimal sample complexity, the generalization bound using CMI framework is vacuous. First, we start by quoting a result from [1] which extends the generalization bounds based on CMI to SCO problems.

**Theorem 5.1** ([1]).: _Let \(n\in\mathbb{N}\), \(\mathcal{D}\in\mathcal{M}_{1}(\mathcal{Z})\) be a data distribution, and \(S\sim\mathcal{D}^{\otimes n}\). Consider an SCO problem \((f,\Theta,\mathcal{Z})\in\mathcal{C}_{L,R}\). Then, for every learning algorithm \(\mathcal{A}_{n}\) such that \(\mathcal{A}_{n}(S_{n})\in\Theta\) a.s., \(\mathrm{EGE}_{\mathcal{D}}(\mathcal{A}_{n})\leq LR\sqrt{8\mathrm{CMI}_{ \mathcal{D}}(\mathcal{A}_{n})/n}\)._

Consider an SCO problem \((\Theta,\mathcal{Z},f)\in\mathcal{C}_{L,R}\). To control the excess population error for an algorithm, a common strategy is bounding it using generalization and optimization error:",\mathbb{E}[\mathrm{F}_{\mathcal{D}}(\mathcal{A}_{n}(S_{n}) )]-\min_{\theta\in\Theta}\mathrm{F}_{\mathcal{D}}(\theta) || \leq\mathrm{EGE}_{\mathcal{D}}(\mathcal{A}_{n})+\mathbb{E}\big{[} \hat{\mathrm{F}}_{S_{n}}(\mathcal{A}_{n}(S_{n}))-\min_{\theta\in\Theta}\hat{ \mathrm{F}}_{S_{n}}(\theta)\big{]}.,,\mathrm{F}_{\mathcal{D}}(\mathcal{A}_{n}(S_{n}))-\min_{\theta\in\Theta} \mathrm{F}_{\mathcal{D}}(\theta) \leq \underbrace{\mathrm{F}_{\mathcal{D}}(\mathcal{A}_{n}(S_{n}))-\hat{\mathrm{F}}_{S_{n}}(\mathcal{A}_{n}(S_{n}))}_{\text{generalization error}} + \underbrace{\hat{\mathrm{F}}_{S_{n}}(\mathcal{A}_{n}(S_{n}))-\min_{\theta\in\Theta} \hat{\mathrm{F}}_{S_{n}}(\theta)}_{\text{optimization error}}.,Decomposition of the excess population error into generalization error and optimization error for stochastic convex optimization problems.
ICML_2024_oral_53,5,"For a proof see [1, 1]. Since we are interested in controlling the \(\mathrm{EGE}_{\mathcal{D}}(\mathcal{A}_{n})\) using CMI, we can use Theorem 5.1 to further upper-bound the excess error as",\mathbb{E}[\mathrm{F}_{\mathcal{D}}(\mathcal{A}_{n}(S_{n}) )]-\min_{\theta\in\Theta}\mathrm{F}_{\mathcal{D}}(\theta) || \leq LR\sqrt{\frac{8\mathrm{CMI}_{\mathcal{D}}(\mathcal{A}_{n})}{ n}}+\mathbb{E}\big{[}\hat{\mathrm{F}}_{S_{n}}(\mathcal{A}_{n}(S_{n}))-\min_{ \theta\in\Theta}\hat{\mathrm{F}}_{S_{n}}(\theta)\big{]}.,,\mathbb{E}[\mathrm{F}_{\mathcal{D}}(\mathcal{A}_{n}(S_{n}))]-\min_{\theta\in\Theta}\mathrm{F}_{\mathcal{D}}(\theta)\leq LR\sqrt{\frac{8\operatorname{CMI}_{\mathcal{D}}( \mathcal{A}_{n})}{n}}+\mathbb{E}\big{[}\hat{\mathrm{F}}_{S_{n}}(\mathcal{A}_{n}(S_{n}))-\min_{\theta\in\Theta}\hat{\mathrm{F}}_{S_{n}}(\theta)\big{]}.,This inequality bounds the expected excess population error of a learning algorithm in terms of its CMI and empirical risk minimization performance.
ICML_2024_oral_53,6,"It has been known for every learning algorithm that \(\varepsilon\)-learn the subclass \(\mathcal{C}_{L,R}\) of SCOs, the optimal sample complexity is \(\Theta\left(\big{(}\frac{LR}{\varepsilon}\big{)}^{2}\right)\)[14]. A natural question to ask is: _Can the excess error decomposition using CMI accurately capture the worst-case excess error of optimal algorithms for SCOs?_ Our next result provides a negative answer to this question.

**Theorem 5.2**.: _For every \(L\in\mathbb{R}\) and \(R\in\mathbb{R}\), there exists an SCO problem \((\Theta,\mathcal{Z},f)\in\mathcal{C}_{L,R}\) such that the following holds: for every learning algorithm \(\mathcal{A}=\{\mathcal{A}_{n}\}_{n\in\mathbb{N}}\) with sample complexity \(N:\mathbb{R}\rightarrow\mathbb{N}\) such that for every \(\varepsilon>0\), \(N(\varepsilon,\delta)=\tilde{\Theta}\left(\big{(}\frac{LR}{\varepsilon}\big{)}^ {2}\right)\), there exists a data distribution such that \(LR\sqrt{8\mathrm{CMI}_{\mathcal{D}}(\mathcal{A}_{n})/n}=\Theta\left(LR\right)\), while the excess error is at most \(\varepsilon\)._

_Remark 5.3_.: In [1], the authors show that for a _particular_ algorithm of Gradient Descent (GD) there exists a distribution such that, the upperbound based on CMI is vacuous. With the correct choice of learning rate GD can, with an optimal sample complexity, learn the subclass CLB of SCOs. Notice that our result in Theorem 5.2 significantly extends the limitations proved in [1], by showing that for _every_ learning algorithm with an optimal sample complexity, the generalization bound based on CMI is _vacuous_. \(\triangleleft\)

### Non-Existence of Sample Compression Schemes

Many learning algorithms share the property that their output is constructed using a small subset of the training example. For example, in support vector machines, only the set of support vectors is needed to construct the separating hyperplane in the realizable setting. _Sample compression schemes_, proposed by Litttschee and Warmuth [15], provide a formal meaning for this algorithmic property. Formally, we say a learning algorithm \(\mathcal{A}_{n}\) is a _sample compression scheme_ of size \(k\in\mathbb{N}\) if there exists a pair \((\kappa,\rho)\) of maps such that, for all samples \(s=(z_{i})_{i=1}^{n}\) of size \(n\geq k\), the map \(\kappa\) compresses the sample into a length-\(k\) subsequence \(\kappa(s)\subseteq s\) which the map \(\rho\) uses to reconstruct the output of the algorithm, i.e., \(\mathcal{A}_{n}(s)=\rho(\kappa(s))\). Steinke and Zakynthinou prove that for \(n\geq k\), if \(\mathcal{A}_{n}\) is a sample compression scheme \((\kappa,\rho)\) of size \(k\). Then for every \(\mathcal{D}\), \(\mathrm{CMI}_{\mathcal{D}}(\mathcal{A}_{n})\leq k\log(2n)\) where \(\mathcal{A}_{n}(\cdot)=\rho(\kappa(\cdot))\).

A natural question to ask is: _Can we learn CLB or CSL subclasses of SCOs using sample compression schemes?_ In particular, we are interested in sample compression schemes in which \(k\) is independent of the dimension and \(n\) so that the algorithm has a dimension-independent sample complexity. Using the results presented in the previous sections, we provide a negative answer.

**Corollary 5.4**.: _Let \(\mathcal{P}_{\text{cxx}}^{(d)}\) be the problem instance described in Section 6.1.1. For every \(\varepsilon\leq 1\) and \(\delta\leq\varepsilon\) and for every algorithm \(\mathcal{A}=\{\mathcal{A}_{n}\}_{n\in\mathbb{N}}\) which is a sample compression of size \(k\) that \(\varepsilon\)-learns \(\mathcal{P}_{\text{cxx}}^{(d)}\) with the sample complexity \(\Theta\left(1/\varepsilon^{2}\right)\) the following holds: for every \(n=\Theta(1/\varepsilon^{2})\), and \(d\geq\Omega(n^{4}\log(n))\), there exists a data distribution \(\mathcal{D}\in\mathcal{M}_{1}(\mathcal{Z})\) such that \(k\geq\Omega(n)\)._

**Corollary 5.5**.: _Let \(\mathcal{P}_{\text{cxx}}^{(d)}\) be the problem instance described in Section 7.1.1. For every \(\varepsilon<1/24\) and \(\delta<1/48\) and_for every algorithm \(\mathcal{A}=\{\mathcal{A}_{n}\}_{n\in\mathbb{N}}\) which is a sample compression of size \(k\) that \(\varepsilon\)-learns \(\mathcal{P}_{\text{scx}}^{(d)}\) with the sample complexity \(\Theta\left(1/\varepsilon\right)\) the following holds: for every \(n=\Theta(1/\varepsilon)\), \(\delta<O(1/n^{2})\), and \(d\geq\Omega(n^{4}\log(n))\), there exists a data distribution \(\mathcal{D}\in\mathcal{M}_{1}(\mathcal{Z})\) such that \(k\geq\Omega(n)\)._

### Extensions to Individual Sample CMI

One drawback of CMI is for that many natural deterministic algorithm it can be \(\Omega(n)\). This limitation can be attributed to the conditioning term in CMI which tends to reveal too much information. One notable approach to address this issue is the development of _individual sample CMI_ (ISCMI) in [10, 2]. Consider the structure introduced in Definition 3.2. Then, define","\mathrm{ISCMI}_{\mathcal{D}}(\mathcal{A}_{n})\triangleq\sum_{i=1}^{n}I( \mathcal{A}_{n}(S);U_{i}|Z_{0,i},Z_{1,i})",,,
ICML_2024_oral_53,7,"In [10, 2], it has been shown for every learning algorithm and every data distribution \(\mathrm{ISCMI}_{\mathcal{D}}(\mathcal{A}_{n})\leq\mathrm{CMI}_{\mathcal{D}}( \mathcal{A}_{n})\). Moreover, similar to CMI, small ISCMI implies generalization. Therefore, it is natural to ask: _Can we circumvent the lowerbounds proved for CMI by measuring the information complexity of \(\varepsilon\)-learners using \(\mathrm{ISCMI}_{\mathcal{D}}(\mathcal{A}_{n})\)?_ Our main result in this part provides a negative answer to this question. We show that exactly the same lowerbound stated in Theorem 4.1 and Theorem 4.2 hold for ISCMI. The proof is appeared on Appendix G.

**Corollary 5.6**.: _Let \(\mathcal{P}_{\text{cvx}}^{(d)}\) be the problem instance described in Section 6.1.1. For every \(\varepsilon\leq 1\) and \(\delta\leq\varepsilon\) and for every proper algorithm \(\mathcal{A}=\{\mathcal{A}_{n}\}_{n\in\mathbb{N}}\) that \(\varepsilon\)-learns \(\mathcal{P}_{\text{cvx}}^{(d)}\) with the sample complexity \(N(\cdot,\cdot)\) the following holds: for every \(n\geq N(\varepsilon,\delta)\), and \(d\geq\Omega(n^{4}\log(n))\), there exists a data distribution \(\mathcal{D}\in\mathcal{M}_{1}(\mathcal{Z})\) such that \(\mathrm{ISCMI}_{\mathcal{D}}(\mathcal{A}_{n})\geq\Omega\left(\frac{1}{ \varepsilon^{2}}\right).\)_

**Corollary 5.7**.: _Let \(\mathcal{P}_{\text{cvx}}^{(d)}\) be the problem instance described in Section 7.1.1. For every \(\varepsilon<1/24\) and \(\delta<1/48\) and for \(\varepsilon\)-learns \(\mathcal{A}\) for \(\mathcal{P}_{\text{cvx}}^{(d)}\) with the sample complexity \(N(\cdot,\cdot)\) the following holds: for every \(n\geq N(\varepsilon,\delta)\), \(\delta<O(1/n^{2})\), and \(d\geq O(n^{4}\log(n))\), there exists a data distribution \(\mathcal{D}\in\mathcal{M}_{1}(\mathcal{Z})\) such that \(\mathrm{ISCMI}_{\mathcal{D}}(\mathcal{A}_{n})\geq\Omega\left(\frac{1}{ \varepsilon}\right)\)_

## 6 Characterization of CMI for the CLB SCOs

In this section and Section 7, we discuss the key steps of the proof of CMI lowerbounds. We begin with a characterization of CMI of \(\varepsilon\)-learners for CLB subclasses of SCOs (All the proofs appear in Appendix C).

For the general case that we do not impose any condition on the output of the learner, the proof turns out to be slightly more subtle. In particular, there is a technical difference between proving the result for _improper (unconstrained)_ learners and _proper (constrained)_ learners. This issue does not appear in the strongly convex case as discussed in Remark 7.2. Therefore, we begin by first proving an intermediate result for _proper learners_.

_Remark 6.1_.: Notice that by simply scaling the problem, we can reduce the lowerbound for \(\mathcal{C}_{L,R}\) with an arbitrary \(L,R\) to \(\mathcal{C}_{1,1}\). Therefore, for the rest of this section, we focus on \(\mathcal{C}_{1,1}\). Also, without loss of generality, we can assume the parameter space is given by \(\mathcal{B}_{d}(1)\). \(\triangleleft\)

### Lower Bound for Proper Learners

#### 6.1.1 Construction of a Hard Problem Instance for proper learners

Let \(d\in\mathbb{N}\). Let \(\mathcal{Z}=\{\pm 1/\sqrt{d}\}^{d}\) and \(\Theta=\mathcal{B}_{d}(1)\). Define the loss function \(f:\Theta\times\mathcal{Z}\to\mathbb{R}\) as \(f(\theta,z)=-\left\langle\theta,z\right\rangle\). It is immediate to see that \(f(\cdot,z)\) is \(1\)-Lipschitz. Let \(\mathcal{P}_{\text{cvx}}^{(d)}\triangleq(\Theta,\mathcal{Z},f)\) be the described SCO problem.

#### 6.1.2 Properties of \(\varepsilon\)-Learners

In this section, we prove several properties that are shared between every \(\varepsilon\)-learners for \(\mathcal{P}_{\text{cvx}}^{(d)}\).

**Lemma 6.2**.: _Fix \(\varepsilon>0\). Let \(\mathcal{A}\) be an \(\varepsilon\)-learner for \(\mathcal{P}_{\text{cvx}}^{(d)}\) with the sample complexity of \(N(\cdot,\cdot)\). Then, for every \(\delta>0\), \(n\geq N(\varepsilon,\delta)\) and every \(\mathcal{D}\in\mathcal{M}_{1}(\mathcal{Z})\), with probability at least \(1-\delta\), we have \(\left\|\mu\right\|-\varepsilon\leq\left\langle\hat{\theta},\mu\right\rangle\), and, \(\left\|\mu\right\|-\varepsilon-2\delta\leq\mathbb{E}\left[\left\langle\hat{ \theta},\mu\right\rangle\right]\) where \(\hat{\theta}=\mathcal{A}_{n}(S_{n})\) and \(\mu=\mathbb{E}_{Z\sim\mathcal{D}}\left[Z\right]\)._

The main implication of Lemma 6.2 is that the output of an accurate learner has a significant correlation to the mean of the data distribution. As the learner does not know the data distribution, in the next result we show that the correlation to the mean of an unknown data distribution translates to a correlation between the output and the samples in the training set. The construction of the data distribution is based on the techniques developed by Kamath, Li, Singhal, and Ullman [14].

**Lemma 6.3**.: _Fix \(\varepsilon>0\). For every \(\varepsilon\)-learner \(\mathcal{A}\) for \(\mathcal{P}_{\text{cvx}}^{(d)}\) with sample complexity \(N(\cdot,\cdot)\), there exists \(\mathcal{D}\in\mathcal{M}_{1}(\mathcal{Z})\), such that for every \(\delta>0\)_","\mathbb{E}[\sum_{i=1}^{n}\sum_{k=1}^{d}(\frac{144 \varepsilon^{2}-d(\mu^{(k)})^{2}}{1-d(\mu^{(k)})^{2}})(\hat{\theta} ^{(k)})(Z_{i}^{(k)}-\mu^{(k)})] || \quad\geq 2\varepsilon-4\delta,",,"\mathbb{E}\left[\left\langle\hat{\theta},\mu\right\rangle\right]\geq\left\|\mu\right\|-\varepsilon-2\delta,",The expected correlation between the learner's output and the mean of the data distribution is bounded below by the norm of the mean minus the accuracy parameter and a term involving the confidence parameter.
ICML_2024_oral_55,1,"## 2 LCA Distance Measure Misprediction Severity

We propose using the in-distribution Lowest Common Ancestor (LCA) distance, also known as taxonomy loss, as a predictor for model generalization. Here, we formally define how taxonomy loss can be measured using in-distribution data. Taxonomy loss measures the class ranking difference between a model's prediction based on class likelihood, and a predefined class order encoded by class taxonomy. Lower taxonomy loss is expected when a model assigns higher likelihood to classes that are semantically closer to the ground truth class, in other words,'making better mistakes' (Bertinetto et al., 2020). For example, if a cat image is predicted as a dog by model-A and as a car by model-B, model-A would have a lower LCA distance as it makes a better mistake compared to model-B. Following previous research (Bertinetto et al., 2020; Deng et al., 2009), we use WordNet (Miller et al., 1990), a large-scale lexical database inspired by psycholinguistic theories of human lexical memory (Miller, 1995), to encode class taxonomy. The WordNet taxonomy is well suited for the widely used ImageNet dataset on which it is based on. An example of LCA distance is shown in Fig 3.

Given two classes, \(y\) (the ground truth class) and \(y^{\prime}\) (the prediction class), we define the **LCA distance** according to (Bertinetto et al., 2020) as","D_{LCA}(y^{\prime},y):=f(y)-f(N_{LCA}(y,y^{\prime}))","where \(f(y)\geq f(N_{LCA}(y,y^{\prime}))\) and \(N_{LCA}(y^{\prime},y)\) denotes the lowest common ancestor class node for classes \(y\) and \(y^{\prime}\) within the hierarchy, and \(f(\cdot)\) represents a function of a node, such as the tree depth or entropy","\begin{equation}
\text{LCA\_distance}(y, y') = d(y) + d(y') - 2 \cdot d(\text{LCA}(y, y'))
\end{equation}",The LCA distance is calculated as the sum of the depths of the ground truth class and the predicted class minus twice the depth of their lowest common ancestor in the taxonomy hierarchy.
ICML_2024_oral_55,2,"py. We use the information content as described in (Valmadre, 2022). For each sample \(X_{i}\) in the given dataset \(\mathcal{M}:=X_{1},\ldots,X_{n}\):","D_{LCA}(model,\mathcal{M}):=\frac{1}{n}\sum_{i=1}^{n}D_{LCA}(\widehat{y}_{i},y _{i})\iff y_{i}\neq\widehat{y}_{i}","where \(\widehat{y}_{i}\) is the predicted class for sample \(X_{i}\) using the model, \(y_{i}\) is the ground truth class for sample \(X_{i}\), and \(y_{i}\neq\widehat{y}_{i}\)","D_{LCA}^{IC}(y^{\prime}, y) := IC(y) - IC(N_{LCA}(y, y^{\prime}))","where $IC(y)$ denotes the information content of the ground truth class $y$, and $IC(N_{LCA}(y, y^{\prime}))$ denotes the information content of the lowest common ancestor node between $y$ and $y^{\prime}$ in the class hierarchy"
ICML_2024_oral_58,1,"### World Model Learning

The world model learns representations of all sensory modalities that the agent receives and then predicts the sequence of these latent representations given actions. Predicting future representations not only provides a rich learning signal to ground language in visual experience but also allows planning and policy optimization from imagined sequences. The world model is shown in Figure 2(a). At each time step, it receives an image \(x_{t}\), a language token \(l_{t}\), and an action \(a_{t}\). The image and language observations are compressed into a discrete representation \(z_{t}\) and fed together with the action into the sequence model to predict the next representation \(\hat{z}_{t+1}\). The multimodal world model consists of the following components, where \(h_{t}\) is a recurrent state:","\text{Sequence model:} \hat{z}_{t},\,h_{t}=\text{seq}(z_{t-1},\,h_{t-1},\,a_{t-1}) || \text{Multimodal encoder:} z_{t}\sim\text{enc}(x_{t},\,l_{t},\,h_{t}) || \text{Multimodal decoder:} \hat{x}_{t},\,\hat{l}_{t},\,\hat{r}_{t},\,\hat{c}_{t}=\text{ dec}(z_{t},\,h_{t})",,"\begin{equation}
z_{t} = E(x_{t}, l_{t}), \quad h_{t} = R(h_{t-1}, [z_{t}, a_{t}]), \quad \hat{z}_{t+1} = D(h_{t})
\end{equation}","Equation 1 defines the encoding of the current observation into a latent representation, the recurrence over the latent and action space, and the decoding of the next latent state in the world model."
ICML_2024_oral_58,2,"We implement the world model as a Recurrent State Space Model (RSSM Hafner et al., 2018), where the sequence model is implemented as GRU (Cho et al., 2014) with recurrent state \(h_{t}\), but other sequence models such as Transformers can also be used as the backbone (Robine et al., 2023). The decoder is trained to reconstruct observations and other information, thus shaping the model representations. The world model is trained jointly to minimize a representation learning loss \(\mathcal{L}_{\mathrm{repr}}\) and a future prediction loss \(\mathcal{L}_{\mathrm{pred}}\), which we describe below.

Multimodal representationsThe world model learns to compress inputs images \(x_{t}\) and language tokens \(l_{t}\) into stochastic latent representations \(z_{t}\) through a variational autoencoding objective (Kingma and Welling, 2013; Rezende et al., 2014). Reconstructing the input observations encourages the model to compress information from all modalities into its representations. We also predict the reward, \(\hat{r}_{t}\), and whether the episode continues, \(\hat{c}_{t}\), so that the policy can be learned directly on top of the latent representations, as discussed in the next section. Finally, the representations are regularized towards the predicted distribution over \(\hat{z}_{t}\) as a prior, essentially regularizing the representations to be predictable. We denote the categorical cross entropy loss as \(\mathrm{catxent}\), the binary cross entropy loss as \(\mathrm{binxent}\), the stop gradient operator as \(\mathrm{sg}\), and \(\beta_{\mathrm{reg}}=0.1\) is a hyperparameter.

The representation learning loss \(\mathcal{L}_{\mathrm{repr}}\) is thus the sum of:","\text{Image loss:}\qquad\quad\mathcal{L}_{x}=\|\hat{x}_{t}-x_{t}\|_{2}^{2} || \text{Language loss:}\qquad\quad\mathcal{L}_{t}=\mathrm{catext}(\hat {t}_{t},l_{t}) || \text{Reward loss:}\qquad\quad\mathcal{L}_{r}=\mathrm{catext}(\hat {r}_{t},\mathrm{twoot}(r_{t})) || \text{Continue loss:}\qquad\quad\mathcal{L}_{c}=\mathrm{binkent}( \hat{c}_{t},c_{t}) || \text{Regularizer:}\qquad\quad\mathcal{L}_{\mathrm{reg}}=\beta_{ \mathrm{reg}}\max(1,\mathrm{KL}\big{[}z_{t}\bigm{\|}\ \ \mathrm{sg}(\hat{z}_{t})\big{]})",,"\mathcal{L}_{\mathrm{repr}} = \underbrace{\mathbb{E}_{t}\left[-\log \text{enc}(z_{t} \mid x_{t}, l_{t}, h_{t})\right]}_{\text{Latent encoding}} + \underbrace{\mathbb{E}_{t}\left[\beta_{\mathrm{reg}} \cdot \mathrm{catxent}(z_{t}, \mathrm{sg}(\hat{z}_{t}))\right]}_{\text{Latent regularization}} + \underbrace{\mathbb{E}_{t}\left[\mathrm{binxent}(r_{t}, \hat{r}_{t}) + \mathrm{binxent}(c_{t}, \hat{c}_{t})\right]}_{\text{Observation reconstruction}}","The representation learning loss $\mathcal{L}_{\mathrm{repr}}$ combines latent encoding, latent regularization, and observation reconstruction terms to train the world model's encoder and decoder."
ICML_2024_oral_58,3,"We choose a strided CNN image encoder, a strided CNN as image decoder, and MLPs for all other model components. We evaluate our method both with one-hot token observations (i.e., learning the embeddings from scratch) and pretrained embeddings from TS (Raffel et al., 2020). One-hot representations are reconstructed with the cross entropy loss above and pretrained embeddings are reconstructed with a squared error. For more details on world model learning, refer to Appendix A.

Future predictionThe world model learns to predict the sequence of multimodal representations, which enables it to plan and ground language. The sequence model produces \(\hat{z}_{t}\) from the current model state \((z_{t-1},h_{t-1})\) and the current action \(a_{t-1}\), which is trained to match the actual representation at the next timestep \(z_{t}\). Concretely, the future prediction objective is:","\text{Prediction loss:}\quad\mathcal{L}_{\mathrm{pred}}=\beta_{ \mathrm{pred}}\ \max(1,\mathrm{KL}\big{[}\,\mathrm{sg}(z_{t})\bigm{\|}\hat{z}_{t} \big{]})",where the gradient around the target distribution for \(z_{t}\) is stopped since it is also a learned representation and \(\beta_{\mathrm{pred}}=0,"\mathcal{L}_{\mathrm{pred}} = \sum_{k=1}^{K} \beta_{k} \cdot \mathrm{KL}\left[ z_{t+k} \,\middle\|\, \hat{z}_{t+k} \right]","The future prediction loss $\mathcal{L}_{\mathrm{pred}}$ is defined as a weighted sum of Kullback-Leibler divergences between the actual and predicted latent representations over $K$ future steps, where $\beta_k$ are weights that may vary with the prediction horizon $k$."
ICML_2024_oral_58,4,"=0.5\) is a hyperparameter. Intuitively, the codes \(z_{t}\) contains both information from current observation and additional information that may be required to predict the reward and episode continuation. By training the world model to make predictions \(\hat{z}_{t}\) of its future representations, it effectively learns to predict future images, language, and rewards, encouraging the agent to extract information from language and learn the correlations between its multiple modalities. For example, when the language input describes that ""the book is in the bedroom"" and the agent later on visually observes the book, the agent will learn this multimodal association even if the reward signal does not directly relate the two.The world model is trained to optimize the overall loss \(\mathcal{L}_{\mathrm{repr}}+\mathcal{L}_{\mathrm{pred}}\) with respect to all its parameters.

Single-Modality PretrainingOne potential benefit of separating world modeling from policy learning is that the world model can be trained offline, benefitting from large-scale text-only and video-only datasets without actions. To pretrain the world model with text-only data as in Section 4.6, we zero out the image and action inputs and set the image, reward, and continuation decoder loss coefficients to 0 so the pretraining focuses on learning to represent text and text dynamics (i.e. language modeling). Dyanlang can then be finetuned on experience with all modalities (language, images, and actions) by initializing the actor and critic from scratch, while continuing to train the world model. Note that unlike the typical language modeling objective, the model is not explicitly trained to predict the next token from the prefix, except through the next-_representation_ prediction.

### Policy Learning

To select actions, we train an actor critic algorithm (Williams, 1992) purely from imagined sequences of multimodal representations (Sutton, 1991), as shown in Figure 2(b). The critic estimates the discounted sum of future rewards for each state to guide actor learning. Both networks are MLPs:","\text{Actor net:}\quad\pi(a_{t}|h_{t},z_{t})\qquad\text{Critic net:}\quad \mathrm{V}(h_{t},z_{t})",,"\text{Actor:}\quad a_{t}=\text{MLP}_{\theta}(z_{t}),\quad\text{Critic:}\quad V_{t}=\text{MLP}_{\phi}(z_{t})","The actor network maps latent representations to actions, while the critic network estimates the value of each state for guiding policy optimization."
ICML_2024_oral_60,1,"### The Masking Objective

The masking loss employed in this work draws inspiration from similar objectives described in (Dabkowski and Gal, 2017; Zolna et al., 2020; Phang et al., 2020). The goal is to maximize the confidence of the classification decision for the masked-in portion of the audio while minimizing it for the masked-out portion. The overall objective function is the following:","\min_{M}\alpha\mathcal{L}_{in}(f(M\odot X),y) || -\beta\mathcal{L}_{out}(f((1-M)\odot X),y)+R(M),",,"\mathcal{L}_{\text{mask}} = \mathbb{E}_{(x, y) \sim \mathcal{D}} \left[ \log p_{\theta}(y \mid x_{\text{in}}) - \log p_{\theta}(y \mid x_{\text{out}}) \right]",The masking loss aims to maximize the log probability of the true class given the masked-in audio portion while minimizing it for the masked-out portion.
ICML_2024_oral_60,2,"f(.)\) represents the pretrained classifier being interpreted. The term \(\mathcal{L}_{in}\) represents the categorical cross-entropy loss computed when we input the masked input \(X\odot M\) to the pretrained classifier. In contrast to the aforementioned prior works, the categorical cross-entropy is computed using the network decision as the label, denoted as \(y=\arg\max_{c}f_{c}(X)\), rather than the actual targets. Our objective is to minimize this term, as we want the masked signal to capture the elements that influenced the decision made by the classifier.

The term \(\mathcal{L}_{out}\) represents the categorical cross-entropy computed for all parts of the input \(x\) not selected by the mask \(M\). We aim to maximize it, as we want the mask to exclude information relevant to the pretrained classifier. Overall, we engage in a optimization problem where the \(\mathcal{L}_{in}\) term tends to encourage larger masks, while the \(\mathcal{L}_{out}\) term encourages smaller ones. The decoder must find a valuable trade-off between these aspects. Finally, note that \(R(M_{\theta}(h))\) is a regularization term that includes an \(l1\)-regularizer to promote sparsity in the estimated mask.

One important aspect of our work is the use of a neural network, specifically the decoder, to estimate the binary mask \(M\). This choice is motivated by our observation that neural networks yield more faithful and understandable masks. After parameterizing the interpretation mask \(M\) with a neural decoder, represented by parameters denoted as \(\theta\), the corresponding optimization objective becomes the following,","\min_{\theta}\lambda_{in}\mathcal{L}_{in}(\log f(M_{\theta}(h) \odot X),y) || -\lambda_{out}\mathcal{L}_{out}(\log f((1-M_{\theta}(h))\odot X),y)+R(M_{\theta}(h)),",,"\min_{\theta}\alpha\mathcal{L}_{in}(f(M_{\theta}\odot X),y)-\beta\mathcal{L}_{out}(f((1-M_{\theta})\odot X),y)+R(M_{\theta}),","The optimization objective when parameterizing the mask $M$ with a neural decoder, where $\theta$ represents the decoder parameters."
ICML_2024_oral_60,3,"sk.

After the initial mask optimization, this framework allows a fine-tuning stage where the interpretation mask is refined to enhance the quality of the interpretations. This is accomplished by adding a term to the regularizer \(R(.)\) as follows:","R(M_{\theta}(h))=\lambda_{g}\|M_{\theta}(h)\odot X-X\|+\lambda_{s}\|M_{\theta} (h)\|_{1},","where \(\lambda_{g}\) and \(\lambda_{s}\) are regularization coefficients, and \(X\) represents the spectrogram of the original signal","\min_{\theta}\lambda_{in}\mathcal{L}_{in}(\log f(M_{\theta}(h) \odot X),y) - \lambda_{out}\mathcal{L}_{out}(\log f((1-M_{\theta}(h))\odot X),y) + R(M_{\theta}(h)) + \gamma \mathcal{L}_{\text{refine}}(M_{\theta}(h)),","Equation 3 extends the mask optimization objective by incorporating a refinement term $\mathcal{L}_{\text{refine}}$ weighted by $\gamma$, aimed at improving the quality of the interpretation mask during fine-tuning."
ICML_2024_oral_60,4,"al. The first term in the regularization encourages the decoder to produce masked representations close to the original inputs, while the second term promotes sparsity in the mask. In our best configuration, this guidance is applied exclusively during training in a selective manner. Specifically, we only apply it to data items if the mask after the initial stage is highly similar to the binarized target spectrogram \(X\). The similarity is measured by calculating the normalized cosine similarity between these two objects. This selective fine-tuning helps prevent steering the masks away from faithful interpretations. Finally, note that in case data augmentation is used, the target is chosen as the clean signal in the fine-tuning stage.

### Producing Listenable Explanations

State-of-the-art audio and speech classifiers often rely on features computed on top of the linear spectrogram, such as FBANKs (mel-spectrograms). These features intelligently compress the frequency axis, often leading to more compact features that enhance performance. However, these features are non-invertible due to the compression applied to the frequency axis.

In our pipeline (Figure 1), we tackle this challenge by having the decoder \(M_{\theta}(.)\) output a mask for the linear spectrogram \(X\) instead of generating a mask for the specific features used by the pretrained classifier. During training, we convert this masked signal \(M_{\theta}(h)\odot X\) back to the feature domain before computing the training loss in Equation (2). The audio domain interpretations are obtained by inverting the linear spectrogram using the phase of the original audio waveform \(x\), expressed as:",x_{\text{interpretation}}=\text{ISTFT}((M_{\theta}(h)\odot X)e^{jX_{\text{ phase}}}).,,"\hat{x} = \mathcal{F}^{-1}(M_{\theta}(h) \odot X),",where $\mathcal{F}^{-1}$ denotes the inverse Fourier transform used to reconstruct the audio waveform from the masked linear spectrogram.
ICML_2024_oral_60,5,"## 3 Experiments

In our experiments, we evaluate the faithfulness and understandability of the generated interpretations. To achieve this, we considered two setups: i) Classification under in-domain conditions, and ii) Classification under out-of-domain conditions. We utilized the ESC50 dataset (Piczak) which contains 50 environmental sound classes for both setups. We also provide additional results on the UrbanSound8k dataset (Salamon et al., 2014) in Appendix B.

### Metrics

To measure the faithfulness of classifications we have used the following metrics:

**Faithfulness on Spectra (FF):** This metric is originally introduced in (Parekh et al., 2022), as a way to measure how important is the generated interpretation for a classifier. The metric is calculated by measuring the drop in class-specific logit value, when the masked out portion of the interpretation mask is input to the classifier. This amounts to calculating,",\text{FF}_{n}:=f(X_{n})_{c}-f(X_{n}\odot(1-M_{\theta}(h)))_{c},,"\text{FF} = f_{y}((1-M)\odot X) - f_{y}(X),","Faithfulness on Spectra (FF) measures the drop in class-specific logit when the masked-out portion is input to the classifier, indicating the importance of the interpretation mask."
ICML_2024_oral_60,6,"If this metric is large, this signifies that the masked-in portion of the input spectrogram \(X\) is highly influential for the classifier decision for class \(c\). We report the average faithfulness over all examples by reporting the average quantity \(\text{FF}:=\sum_{n}\frac{1}{N}\text{FF}_{n}\). Larger is better.

**Average Increase (AI):** Average increase, originally proposed in (Chattopadhay et al., 2018), measures the increase in confidence for the masked-in portion of the interpretation, and it is calculated as follows:","\text{AI}:=\frac{1}{N}\sum_{n=1}^{N}\mathbf{1}_{[f(X_{n}\odot M_{\theta}(h))>f (X_{n})_{c}]}\cdot 100,",where \(\mathbf{1}_{[,\text{AI}_{n}:=f(X_{n}\odot M_{\theta}(h))_{c}-f(X_{n})_{c},"where $f(X_n)_c$ denotes the logit for the predicted class $c$ of the original input $X_n$, and $M_{\theta}(h)$ is the learned interpretation mask."
ICML_2024_oral_60,7,"{[.]}\), is an indicator function which returns one if the argument is true, and zero otherwise. For this metric, larger is better.

**Average Drop (AD):** Average drop, originally proposed in (Chattopadhay et al., 2018), measures how much confidence is lost when the input image is masked, and calculated as follows:","\text{AD}:=\frac{1}{N}\sum_{n=1}^{N}\frac{\max(0,f(X_{n})_{c}-f(X_{n}\odot M_ {\theta}(h))_{c})}{f(X_{n})_{c}}\cdot 100.",,"\text{AD}:=\frac{1}{N}\sum_{n=1}^{N}\frac{\max\left(0,f(X_{n})_{c}-f(X_{n}\odot M_{\theta}(h))_{c}\right)}{f(X_{n})_{c}}\cdot 100,","where the numerator computes the drop in confidence for the true class when the input is masked, normalized by the original confidence, and zero values are clamped to avoid division by zero; smaller values indicate better performance."
ICML_2024_oral_60,8,"For this metric, smaller is better.

**Average Gain (AG):** This metric is first proposed in (Zhang et al., 2023), and it measures how much confidence is gained after masking the input image. It is calculated as follows:","\text{AG}:=\frac{1}{N}\sum_{n=1}^{N}\frac{\max(0,f(X_{n}\odot M_{\theta}(h))_ {c}-f(X_{n})_{c})}{1-f(X_{n})_{c}}\cdot 100.",,"\text{AG}:=\frac{1}{N}\sum_{n=1}^{N}\frac{\max(0,f(X_{n}\odot M_{\theta}(h))_{c}-f(X_{n})_{c})}{1-f(X_{n})_{c}}\cdot 100.","For this metric, larger is better."
ICML_2024_oral_60,9,"**Input Fidelity (Fid-In):** This metric is introduced in (Paissan et al., 2023), and it measures if the classifier outputs the same class prediction on the masked-in portion of the input image. It is defined as,",\text{Fid-In}=\frac{1}{N}\sum_{n=1}^{N}\mathbf{1}_{[\arg\max_{c}f(X_{n})_{c} =\arg\max_{c}f_{c}(X_{n}\odot M_{\theta}(h))]}.,,\text{Fid-In}:=\frac{1}{N}\sum_{n=1}^{N}\mathbf{1}_{[\arg\max_{c'}f_{c'}(X_{n}\odot M_{\theta}(h))=c]}.,"This metric evaluates whether the masked-in portion of the input retains the original class prediction, with a higher value indicating better input fidelity."
ICML_2024_oral_66,1,"### Our Results

We show that a 20-year-old greedy algorithm from Mettu and Plaxton (Mettu and Plaxton, 2000) can be easily made differentially private (DP). This algorithm provides a non-private approximation to the more general \((k,z)\)-clustering problem. In this problem, the input data consists of a set of points \(P\) in \(\mathbb{R}^{d}\), and the goal is to find a set \(S\) of \(k\) points (the _centers_) in order to minimize the cost, defined as","\operatorname{cost}(P,S)=\sum_{p\in P}\min_{s\in S}\operatorname{dist}(p,s)^{ z}.",,"\begin{equation}
\text{cost}(P, S) = \sum_{p \in P} \min_{c \in S} \|p - c\|^z
\end{equation}","The cost function for the (k,z)-clustering problem, which sums the z-th power of the minimum distance from each point to the set of centers S."
ICML_2024_oral_66,2,"We focus especially on the case where \(z=2\), which is the popular \(k\)-means problem (while \(z=1\) is \(k\)-median), and also provide results for general \(z\). We denote the optimal cost for the \((k,z)\)-clustering problem as \(\textsc{opt}_{k,z}\).

We show that a slight variation of the algorithm from Mettu and Plaxton is private, provided that one can privately solve a generalized version of the _max summation_ problem. Given a fixed, non-private set of balls in \(\mathbb{R}^{d}\) and a private set of points \(P\subset\mathbb{R}^{d}\), each point in \(P\) contributes a value to each ball that contains it. The goal is to output a ball with approximately the maximum value; an algorithm for this problem has error \(\theta\) if the absolute value of the difference between the actual maximum value and the one returned is at most \(\theta\). This is a simplified version of the problem we need to solve, referred to as the _generalized summation problem_, which we formally define in Section 4.

Before stating our result, we note that the quality of the private \(k\)-means solution \(S\) has to be measured by two parameters: \(S\) has _multiplicative approximation_\(M\) and _additive error_\(A\) when \(\operatorname{cost}(S)\leq M\cdot\textsc{opt}_{k,z}+A\). Since even the non-private problem is NP-hard to approximate within 1.06 (Cohen-Addad et al., 2022), we must have \(M>1\) if we insist on a polynomial-time algorithm. The privacy constraints enforce \(A>0\) as well; when the input is in the \(d\)-dimensional ball \(B_{d}(0,\Lambda)\) in \(\mathbb{R}^{d}\), (Chaturvedi et al., 2021) showed that \(A\) has to be at least \(k\sqrt{d}\cdot\Lambda^{2}\) for any \((\varepsilon,\delta)\) differentially-private mechanism. In light of this lower bound, we will assume \(\Lambda=1\) in the following.

Our meta-theorem shows how to reduce the computation of a \(k\)-means solution to a _repeated_ application of an algorithm solving the max summation problem. To provide some intuition, a (perhaps too much) simplified version of the algorithm from Mettu and Plaxton repeats the following process \(k\) times: select a ball with approximately maximum value, and remove all balls intersecting with the selected one. Thus, we can use an algorithm for repeated max summation as a black-box. Our main result relates the error of the max summation algorithm to the error of the clustering algorithm:

**Informal Theorem 1.1** (see Theorem 3.4 Theorem 4.3, and Lemma F.1).: _Let \(\beta>0\). If one can solve privately the repeated max summation problem such that, with probability at least \(2/3\), the error is \(\theta\), then one can solve DP \(k\)-means such that, with probability \(1-\beta\), either of the following guarantees is achievable:_

* _multiplicative approximation_ \(O(1)\) _and additive error_ \(\approx k\operatorname{polylog}(n/\beta)\cdot\theta\)_,_
* _or multiplicative approximation_ \(w^{*}(1+\alpha)\) _and additive error_ \(\approx\sqrt{d}\operatorname{poly}(k,\log(n/\beta))\cdot\theta\)_, where_ \(w^{*}\) _is the best non-private approximation ratio._1__ Footnote 1: For this result, we actually need something slightly stronger than max summation, see Definition 4.1

_For the more general \((k,z)\)-clustering, in the second case the multiplicative approximation is \(w^{*}(2^{z}+\alpha)\) and additive error \(\approx\sqrt{d}\operatorname{poly}(k/\beta,\log(n))\cdot\theta\)._

To illustrate the above informal theorem, in centralized \((\varepsilon,\delta)\)-DP we can use the exponential mechanism to solve the repeated max summation problem. This is formalized in Lemma F.1, with \(\theta=\sqrt{d}\operatorname{polylog}(n/\delta)/\varepsilon\).

We highlight a few features of our \(k\)-means results: in the first case, the additive error is optimal, as it matches the lower bound of (Chaturvedi et al., 2021). In the second case, the multiplicative approximation is close to optimal, in the sense that it is almost as good as any polynomial-time non-private algorithm. Furthermore, even starting from an algorithm with constant probability of success, we show that the probability can be boosted arbitrarily high.

Our result is actually even stronger: it computes a solution not only to \(k\)-means, but to all \(k^{\prime}\)-means for \(k^{\prime}\leq k\), with the same multiplicative and additive error guarantee as above. This allows the use of the so-called elbow method to select the 'correct' value for \(k\) without any further loss of privacy. We refer to Appendix A.3.

We apply this meta-theorem to several different privacy settings, and state the bounds obtained in Table 1. We present in this table the bounds for \((\varepsilon,\delta)\)-privacy (see Section 1.3); we address the particular case \(\delta=0\) in Appendix G.

To summarize our contribution, we match the previous bounds in almost all settings, and make improvements in several cases. For Local and Shuffle DP in one round, we improve exponentially the dependency in the probability for \(k\)-means and extend the results to \((k,z)\)-clustering, which partially answers an open question from (Chang et al., 2021). In the MPC model, we improve the dependency in \(k\) to get an optimal bound. Finally, we present the first result in the Continual Observation setting. We summarize our bounds in Table 1, and discuss in greater detail the previous algorithms - and why they do not work in full generality - in Appendix A.1.

### Brief Overview

To show the first point of Informal Theorem 1.1, we rely on the algorithm from Mettu and Plaxton. We reinterpret this algorithm, introducing some key changes: first, to make it private, and second, to enable implementation based on an algorithm for the generalized summation problem. To provide some intuition, this algorithm iteratively chooses cluster centers, intuitively by selecting smaller and smaller regions that are far away from any center previously selected, based on the region's ""value"" (a proxy for the contribution to the cost). For any \(k\), the first \(k\) centers form a constant-factor approximation to \((k,z)\)-clustering. It turns out that we can repeatedly use a generalized summation algorithm to compute those ""values"": we show that, if we have a private generalized summation algorithm with an error of \(\theta\), then we can solve \((k,z)\)-clustering with additive error \(k\theta\) (see Section 3).

We start this paper by formalizing some general building blocks for private clustering in Section 2, namely techniques that can be used to simplify the input and the problem. We show how to perform all of them based only on estimating the size of the clusters and other related quantities. This includes for \(k\)-means (a) a dimension-reduction technique; (b) a technique that improves the approximation ratio from \(O(1)\) to almost \(w^{*}\), the best non-private approximation ratio; (c) a new way of boosting the success probability for \(k\)-means. (a) and (b) are well-known for \(k\)-means, we extend them for the general \((k,z)\)-clustering problem.

We combine these techniques in Section 4 to obtain the near-optimal approximation factor that we presented in Table 1. For this, we show that the max summation problem can be solved privately using histograms to estimate the value of each ball, and then to select the maximum estimated value. This leads directly to a novel private algorithm for the centralized model that is also much simpler than prior algorithms. However, applying the other building blocks (a)-(c) requires estimating the size of each cluster. This would be doable with histograms _if the clusters were known a priori and remained fixed throughout the algorithm_; however, the clusters depend on the input data and are not known a priori, and, thus, computing their size cannot be reduced to a simple histogram query. To solve this issue, we introduce a structural result on the shape of clusters: we show that each cluster is the disjoint union of a small number of _pre-determined_ sets. Therefore, to estimate the size of each cluster, it is enough to apply a general summation algorithm on the pre-determined sets, and combine the results on those.

Finally, we present another option: instead of histograms, one can use the exponential mechanism in Section 5 to show that the max summation problem can be solved (in some privacy settings) with a very tiny \(\theta\), resulting in a near-optimal additive error.

### Privacy Models

As it is common in the differential privacy literature (see e.g. (Dwork and Roth, 2014)), we will assume our input is given as a multiset, as formalized in Appendix B.1.

Central Differential Privacy:We will use the formalism of (Dwork and Roth, 2014). A _dataset_ is a multiset \(P\) of points of a universe \(X\). We say that two datasets \(P,P^{\prime}\) are _neighboring_ when they differ by a single point, namely \(\sum_{x\in X}|P(x)-P^{\prime}(x)|=1\). We say that a mechanism \(\mathcal{M}\) is \((\varepsilon,\delta)\)_-differentially private_ if for any two neighboring datasets \(P,P^{\prime}\) and any set \(S\), we have:",\mathbb{P}(\mathcal{M}(P)\in S)\leq\exp(\varepsilon)\cdot\mathbb{P}(\mathcal{ M}(P^{\prime})\in S)+\delta.,,"\textsc{opt}_{k,z} = \min_{S \subseteq \mathbb{R}^d,\, |S|=k} \operatorname{cost}(P, S)","The optimal cost for the $(k,z)$-clustering problem."
ICML_2024_oral_66,3,"We say that an algorithm is \(\varepsilon\)_-differentially private_ if it is \((\varepsilon,0)\)-differentially private. To emphasize the difference to other models, we will refer to this privacy model as _central differential privacy_.

In this paper, we will also study the following other privacy models, deferring the formal definitions to Appendix B.1.

Local Model(Kasiviswanathan et al., 2011): In the local model, there is no trusted central server with access to the entire raw database. Instead, we have \(n\) individuals, each with one data point. Two inputs are adjacent if the data of a single user changes. The _transcript_ of an algorithm is the sequence of messages exchanged between clients and the server: an algorithm is \((\varepsilon,\delta)\)_-local differentially-private (LDP)_ if the transcript is \((\varepsilon,\delta)\)-DP. In this paper, we focus on the local privacy model with a single round of communication from clients to the server, also known as the non-interactive model.

Shuffle Model(Bittau et al., 2017): Similarly to the local model, we have \(n\) individuals, each with one data point. However, in the shuffle model, a trusted intermediary comes into play between the individuals and the server: the _shuffler_. The shuffler gathers the messages from the individuals and shuffles them randomly before sending them to the server,preventing the server from attributing a specific message to a particular individual.2 Only the transcript of interactions between the server and the shuffler has to be DP.

Footnote 2: The original motivation behind this is that the random shuffling can be done via secure cryptographic protocols.

Continual Observation Model (Dwork et al., 2010): In the continual observation model, the input is not static but evolves over time. The algorithm is given a stream of updates (insertion or deletion) to its dataset, one per time step, and outputs a solution for the input so far at each time step. Two streams are _(event-level) neighboring_ if they differ by a single update. The algorithm is \((\varepsilon,\delta)\)-DP under continual observation if the algorithm mapping a stream to a sequence of outputs is \((\varepsilon,\delta)\)-DP.

Massively Parallel Computing Model (MPC): The MPC model is a model for distributed, scalable computation - not necessarily private. The input is initially split among several machines, each of them having local memory sub-polynomial in the total database size (\(n^{\kappa}\), for some fixed \(\kappa\in(0,1)\)). The machines can send and receive messages from other machines, but the message length cannot exceed the machine's local memory. As opposed to the Local Model, the messages exchanged don't have to be private: the algorithm is \((\varepsilon,\delta)\)-private if its output is \((\varepsilon,\delta)\)-DP.

## 2 General Building Blocks for Private Clustering

In this section, we present several techniques used in the literature as preprocessing or postprocessing steps to simplify the task of computing a private clustering. We also extend some of these techniques to allow for greater generality. We will not fixate on a specific privacy model in order to present the results in a modular way. The lemmas in this section will apply to any privacy model, assuming that we are given a partition of the space into \(k\) subsets \(S_{1},\ldots,S_{k}\) corresponding to a clustering, and that we can estimate for all \(i\), \(|S_{i}\cap P|\), \(\sum_{p\in P\cap S_{i}}p\), and \(\sum_{p\in P\cap S_{i}}|p|_{2}\). We will prove the existence of private algorithms to compute such a partition and the corresponding estimation in the next sections.

**Property 2.1**.: We say that three sequences \((n_{i})_{1\leq i\leq k}\), \((\textsc{Sum}_{i})_{1\leq i\leq k}\), \((\textsc{SumNorm}_{i})_{1\leq i\leq k}\) verify the Property 2.1 for a partition of the space \(\mathbb{R}^{d}=S_{1}\cup\cdots\cup S_{k}\) with error parameter \(e\geq 0\) if:

* \(|n_{i}-|P\cap S_{i}|\leq e\).
* \(\|\textsc{Sum}_{i}-\sum_{p\in P\cap S_{i}}p\|_{2}\leq e\).
* \(|\textsc{SumNorm}_{i}-\sum_{p\in P\cap S_{i}}\|p\|_{2}|\leq e\).

### Reducing the dimension

Dimension-reduction techniques based on the Johnson-Lindenstrauss lemma allow the projection of the dataset

\begin{table}
\begin{tabular}{l|c|c|c} Model & Approximation & Error & \\ \hline Centralized DP & \(w^{*}(1+\alpha)\) & \(k^{O_{\alpha}(1)}+k\sqrt{d}\) & (Ghazi et al., 2020), Cor. 4.4 \\  & \(O(1)\) & \(k\sqrt{d}\) & (Chaturvedi et al., 2021), Lem. F.1 \\ \hline \multirow{4}{*}{Local DP} & \(w^{*}(1+\alpha)\) & \(\sqrt{n}\cdot\left((k/\beta)^{O_{\alpha}(1)}+k\sqrt{d}\right)\) & \(k\)-means only, 1 round, (Chang et al., 2021) \\  & \(w^{*}(1+\alpha)\) & \(\sqrt{n}\cdot\left(k^{O_{\alpha}(1)}+k\sqrt{d}\right)\) & \(k\)-means only, 1 round, Cor. 4.4 \\  & \(w^{*}(2^{z}+\alpha)\) & \(\sqrt{n}\cdot\left((k/\beta)^{O_{\alpha}(1)}+k\sqrt{d}\right)\) & \((k,z)\)-clustering, 1 round, Cor. 4.4 \\  & \(O(1/c)\) & \(\sqrt{nd}\cdot k^{1+O_{c}(1)}\) & \(k\)-means only, (Chaturvedi et al., 2022) \\ \hline \multirow{4}{*}{Shuffle DP, 1 round} & \(w^{*}(1+\alpha)\) & \(\left((k/\beta)^{O_{\alpha}(1)}+k\sqrt{d}\right)\) & \(k\)-means only, (Chang et al., 2021) \\  & \(w^{*}(1+\alpha)\) & \(k^{O_{\alpha}(1)}+k\sqrt{d}\) & \(k\)-means only, Cor. 4.4 \\  & \(w^{*}(2^{z}+\alpha)\) & \((k/\beta)^{O_{\alpha}(1)}+k\sqrt{d}\) & \((k,z)\)-clustering, Cor. 4.4 \\ \hline \multirow{4}{*}{MPC} & \(w^{*}(1+\alpha)\) & \(k^{O_{\alpha}(1)}+k\sqrt{d}\) & (Cohen-Addad et al., 2022a), Thm.5.2 \\  & \(O(1)\) & \(k^{2.5}+k^{1.01}\sqrt{d}\) & (Cohen-Addad et al., 2022a) \\  & \(O(1)\) & \(k\sqrt{d}\) & Theorem 5.2 \\ \hline \multirow{4}{*}{
\begin{tabular}{l} Continual \\ observation \\ \end{tabular} } & \(w^{*}(1+\alpha)\) & \((k^{O_{\alpha}(1)}+k\sqrt{d})\log^{1.5}(T)\) & \(k\)-means only, Cor. 4.4 \\  & \(w^{*}(2^{z}+\alpha)\) & \(\left((k/\beta)^{O_{\alpha}(1)}+k\sqrt{d}\right)\log^{1.5}(T)\) & \((k,z)\)-clustering, Cor. 4.4 \\ \end{tabular}
\end{table}
Table 1: Comparison with the previous state-of-the-art for \((\varepsilon,\delta)\) privacy. The success probability is \(1-\beta\). \(\alpha\in(0,1/4]\) and \(c>0\) are precision parameters. For simplicity, dependency in \(\log(1/\beta),1/\varepsilon,\log(1/\delta),\operatorname{polylog}(nd)\) and \(\log\log T\) (for continual observation) are hidden, and the diameter is assumed to be \(\Lambda=1\). The notation \(O_{\alpha}(1)\) is to insist that the constant hidden depends on \(\alpha\) – here it is \(\log(1/\alpha)/\alpha^{2}\).

onto \(\hat{d}=O\left(z^{4}\cdot\log(k/\beta)\alpha^{-2}\right)\) dimensions, such that with probability \(1-\beta/2\) the clustering cost is preserved up to a \((1\pm\alpha)\) factor. We call \(\pi(P)\) the projected and rescaled dataset. We provide a more detailed description in Appendix C.2 and concentrate here on the main challenge, which is to ""lift up"" the solution: given a clustering of the projected dataset, how can we compute centers in the original space?

Our main ""lifting"" technique is modular, as it merely requires approximating the size of each cluster and the sum of the points inside each cluster. Following the analysis of (Ghazi et al., 2020), by applying a standard concentration bound, it holds with probability at least \(1-\beta/2\) that all the points of the projected and rescaled dataset \(\pi(P)\) lie within the ball \(B_{\hat{d}}(0,\sqrt{2\log(n/\beta)})\). Using a private algorithm \(\mathcal{A}\), we compute a solution to \((k,z)\)-clustering \(\mathcal{C}\) of \(\pi(P)\), with multiplicative approximation \(M\) and additive error \(A\). The set of centers \(\mathcal{C}\) induces a _private_ partition \(\hat{S_{1}},\ldots,\hat{S_{k}}\) of \(\mathbb{R}^{\hat{d}}\), defined by the Voronoi diagram of \(\mathcal{C}\), and a _private_ partition \(S_{1},\ldots,S_{k}\) of the original space \(\mathbb{R}^{d}\), defined as the preimage \(\pi^{-1}(S_{1},\ldots,S_{k})\).

The natural way of defining centers in \(\mathbb{R}^{d}\) to lift the partition would be to take the \(k\) optimal centers \(\mu_{z}\) for the \((1,z)\)-clustering of each \(S_{i}\cap P\). We say that the _cost induced_ by the partition \(S_{1},\ldots,S_{k}\) is \(\sum_{i=1}^{k}\mathrm{cost}(P\cap S_{i},\mu_{z}(P\cap S_{i}))\). To lift privately a partition, we will use an approximation of each average \(\mu_{2}(S_{i}\cap P)=\frac{\sum_{p\in P_{i}}P}{|P_{i}|}\) by the quantity \(\frac{\mathrm{SUM}_{i}}{n_{i}}\) where \((n_{i})_{1\leq i\leq k},(\textsc{SUm}_{i})_{1\leq i\leq k}\) are two sequences computed _privately_ and verifying Property 2.1 for the partition \(S_{1},\ldots,S_{k}\). This is formalized in the next lemma.

**Lemma 2.2**.: _Let \(\hat{d}=O(z^{4}\cdot\log(k/\beta)\alpha^{-2})\), and let \(\pi(P)\) be the projected and rescaled dataset in \(\mathbb{R}^{\hat{d}}\). Suppose that we are given a partition \(\hat{S_{1}},\ldots,\hat{S_{k}}\) of \(\mathbb{R}^{\hat{d}}\) that induces a \((M,A)\)-approximation of the \((k,z)\)-clustering problem on \(\pi(P)\)._

_Let \(S_{1},\ldots,S_{k}\) be the partition of \(\mathbb{R}^{\hat{d}}\) that we obtain by taking the preimage \(\pi^{-1}\) of \(\hat{S_{1}},\ldots,\hat{S_{k}}\). Assume that we have two sequences \((n_{i})\) and \((\textsc{Sum}_{i})\) verifying Property 2.1 for this partition, and consider the set of centers \(S=\left\{\frac{\mathrm{SUM}_{i}}{n_{1}},\ldots,\frac{\mathrm{SUM}_{i}}{n_{k}}\right\}\). The following holds with probability \(1-\beta\):_

_In the case of \(k\)-means \((z=2)\), we have \(\mathrm{cost}(P,S)\leq(1+\alpha)M\cdot\textsc{opt}_{k,2}(P)+\mathrm{polylog}n \cdot A+O(ke)\). For general \((k,z)\)-clustering, we have instead \(\mathrm{cost}(P,S)\leq 2^{z}(1+\alpha)M\cdot\textsc{opt}_{k,z}(P)+\mathrm{polylog}n \cdot A+O(ke)\)._

The previous result for \(k\)-means is a direct application of the dimension reduction results, but we introduce here the generalization to \((k,z)\)-clustering. This relies on the following new lemma:

**Lemma 2.3**.: _Let \(P\) be a multiset of points in \(\mathbb{R}^{d}\) with optimal center \(\mu_{z}\) for \((1,z)\)-clustering and optimal \((1,2)\)-clustering solution \(\mu=\mu_{2}\). Then,_",\sum_{p\in P}\|p-\mu\|^{z}\leq 2^{z}\sum_{p\in P}\|p-\mu_{z}\|^{z}.,,"\operatorname{cost}(P,\mu_{2})\leq 2^{z}\cdot\operatorname{cost}(P,\mu_{z})","This equation relates the optimal centers for (1,2)-clustering (μ₂) and (1,z)-clustering (μ_z), showing that the cost of using μ₂ is at most 2^z times the optimal cost for (1,z)-clustering, enabling dimension reduction generalization to (k,z)-clustering."
ICML_2024_oral_66,4,"### Boosting the multiplicative approximation

Let \(w^{*}\) be the best approximation ratio achievable non-privately for \((k,z)\)-clustering. An observation of (Nguyen, 2020) allows the conversion of any private algorithm with constant approximation into an algorithm with approximation almost \(w^{*}\) while increasing the additive error: any \(O(1)\)-approximation of \((k^{\prime},z)\)-clustering, with \(k^{\prime}=\alpha^{-O(d)}k\log(n/\alpha)\), is an \(\alpha\)-approximation for \((k,z)\)-clustering. If we can compute privately such a solution, one can convert it into a true solution for \(k\)-means with approximation \(w^{*}(1+\alpha)\), while preserving the additive error (see Lemma C.1). This yields the following result:

**Lemma 2.4** (Theorem 4 in (Nguyen, 2020)).: _Suppose we are given a private algorithm \(\mathcal{A}\) for \((k,z)\)-clustering that has multiplicative approximation \(M\) and additive error \(A(k,d)\). Suppose we can privately compute a sequence \((n_{i})\) verifying Property 2.1 for the Voronoi diagram of the centers output by \(\mathcal{A}\). And let \(w^{*}\) be the best approximation ratio achievable non-privately for \((k,z)\)-clustering. Then, for any \(1/4>\alpha>0\), there is a private algorithm that computes a solution for \((k,z)\)-clustering with cost at most \(w^{*}(1+\alpha)\cdot\textsc{opt}_{k,z}+O(A(k^{\prime},d)+k^{\prime}e)\), where \(k^{\prime}=(\alpha/M)^{-O(d)}k\log(nM/\alpha)\)._

This can be combined with dimension reduction from the previous section, i.e., we apply this lemma in dimension \(\log(k)/\alpha^{2}\), resulting in \(k^{\prime}=k^{\hat{O}_{\alpha}(1)}\log(n/\alpha)\).

### Boosting the success probability

Assume we are given a private algorithm \(\mathcal{A}\) that computes with probability \(2/3\) (or any constant \(>1/2\)) a \((M,A)\)-approximation to \((k,z)\)-clustering. To increase the success probability to \(1-\beta\), the standard technique is to run \(\log(1/\beta)\) copies of \(\mathcal{A}\) in parallel, and select (privately) the one with the best output. This can be easily implemented using the exponential mechanism, which requires computing the cost of each solution. While this is possible in many settings, in some settings (e.g., Local DP or continual observation), it is not obvious how to do so without ""losing"" too much privacy.

In the particular case of \(k\)-means, we show how this can be done using mere histogram queries. This relies on the following new lemma: the \(k\)-means cost of a cluster can be expressed as a function of the points of a cluster, regardless of the location of its center.

**Lemma 2.5**.: _For any multiset \(E\),_","\operatorname{cost}(E,\mu(E))=\sum_{p\in E}\|p\|_{2}^{2}-\frac{\|\sum_{p\in E }p\|_{2}^{2}}{|E|}.",,"\sum_{p\in E}\|p-\mu(E)\|^{2}=\frac{1}{2|E|}\sum_{p,q\in E}\|p-q\|^{2},","The k-means cost of a cluster can be expressed as the average squared distance between all pairs of points in the cluster, providing a way to compute the cost without explicitly knowing the cluster center."
ICML_2024_oral_72,1,"#### 3.3.2 Diffusion Formulation

Inspired by the notable achievements in text-to-image generation (Chang et al., 2022; Gu et al., 2022), discrete diffusion is being increasingly applied to the generation of speech and audio (Wu et al., 2024; Yang et al., 2023). This subsection describes the forward and reverse process, and then details the inference method and classifier-free guidance.

**Forward Process.** Denote \(\mathbf{X}=[x_{i}]_{i=1}^{N}\) the target discrete token sequence, where \(N\) is the sequence length, \(\mathbf{X}^{p}\) is the prompt discrete token sequence, and \(\mathbf{C}\) is the condition. The forward process at time \(t\) is defined as masking a subset of tokens in \(\mathbf{X}\) with the corresponding binary mask \(\mathbf{M}_{t}=[m_{t,i}]_{i=1}^{N}\), formulated as \(\mathbf{X}_{t}=\mathbf{X}\odot\mathbf{M}_{t}\), by replacing \(x_{i}\) with [MASK] token if \(m_{t,i}=1\), and otherwise leaving \(x_{i}\) unmasked if \(m_{t,i}=0\). \(m_{t,i}\overset{iid}{\sim}\text{Bernoulli}(\sigma(t))\) and \(\sigma(t)\in(0,1]\) is a monotonically increasing function. In this paper, \(\sigma(t)=\sin(\frac{\pi t}{2T}),t\in(0,T]\). Specially, we denote \(\mathbf{X}_{0}=\mathbf{X}\) for the original token sequence and \(\mathbf{X}_{T}\) for the fully masked sequence.

**Reverse Process.** The reverse process gradually restores \(\mathbf{X}_{0}\) by sampling from reverse distribution \(q(\mathbf{X}_{t-\Delta t}|\mathbf{X}_{0},\mathbf{X}_{t})\), starting from full masked sequence \(\mathbf{X}_{T}\). Since \(\mathbf{X}_{0}\) is unavailable in inference, we use the diffusion model \(p_{\theta}\), parameterized by \(\theta\), to predict the masked tokens conditioned on \(\mathbf{X}^{p}\) and \(\mathbf{C}\), denoted as \(p_{\theta}(\mathbf{X}_{0}|\mathbf{X}_{t},\mathbf{X}^{p},\mathbf{C})\). The parameters \(\theta\) are optimized to minimize the negative log-likelihood of the masked tokens:","\mathcal{L}_{\text{mask}}=\underset{\mathbf{X}\in\mathcal{D},t\in[0,T]}{\mathbb{ E}}-\sum_{i=1}^{N}m_{t,i}\cdot\log(p_{\theta}(x_{i}|\mathbf{X}_{t},\mathbf{X}^{p}, \mathbf{C})).",,"\begin{equation}
\mathcal{L}(\theta) = -\log p_{\theta}(\mathbf{X}_{0} \mid \mathbf{X}_{t}, \mathbf{X}^{p}, \mathbf{C})
\end{equation}","Equation 1 defines the negative log-likelihood loss function for training the diffusion model parameters θ, where the model predicts the original token sequence X₀ conditioned on the masked sequence Xₜ, prompt tokens Xᵖ, and condition C."
ICML_2024_oral_72,2,Then we can get the reverse transition distribution:,"p(\mathbf{X}_{t-\Delta t}|\mathbf{X}_{t},\mathbf{X}^{p},\mathbf{C})=\underset {\mathbf{X}_{0}\sim p_{\theta}(\mathbf{X}_{0}|\mathbf{X}_{t},\mathbf{X}^{p}, \mathbf{C})}{\mathbb{E}}q(\mathbf{X}_{t-\Delta t}|\mathbf{\hat{X}}_{0}, \mathbf{X}_{t}).",,"p_{\theta}(\mathbf{X}_{t-\Delta t}|\mathbf{X}_{t},\mathbf{X}^{p},\mathbf{C})=\prod_{i=1}^{N}p_{\theta}(x_{i}|\mathbf{X}_{t},\mathbf{X}^{p},\mathbf{C})^{m_{t-\Delta t,i}}.",The reverse transition distribution is derived by factorizing the probability of each token being restored based on the binary mask at the previous time step.
ICML_2024_oral_73,1,"## 2 Preliminaries and Problem Setup

In this section, we review the private fine-tuning literature, introduce the Neural Collapse phenomenon, and formally describe the private fine-tuning problem under (approximate) Neural Collapse.

Symbols and notations.Let the data space be \(\mathcal{Z}\). A dataset \(\mathcal{D}\) is a set of individual data points \(\{z_{1},z_{2},...\}\) where \(z_{i}\in\mathcal{Z}\). Unless otherwise specified, the size of the data \(|\mathcal{D}|:=n\). \(\mathcal{Z}=\mathcal{X}\times\mathcal{Y}\) where \(\mathcal{X}\) is the feature space and \(\mathcal{Y}\) the label space with \(\mathcal{X}\subset\mathbb{R}^{p}\) and \(\mathcal{Y}=\{1,2,...,K\}\). A data point \(z_{i}\) is a feature-label pair \((x_{i},y_{i})\). Occasionally, we overload \(y_{i}\) to also denote the one-hot representation of the label. We use standard probability notations, e.g., \(\Pr[\cdot]\) and \(\mathbb{E}[\cdot]\) for probabilities and expectations. Other notations will be introduced when they first appear.

Differentially private learning.The general setting of interest is _differentially private learning_ where the goal is to train a classifier while satisfying a mathematical definition of privacy known as differential privacy (Dwork, 2006). DP ensures that any individual training data point cannot be identified using the trained model and any additional side information. More formally, we adopt the popular modern variant called zero-centered Concentrated Differential Privacy (zCDP), as defined below.

**Definition 2.1** (Zero-Concentrated Differential Privacy, zCDP, Bun & Steinke (2016)).: Two datasets \(\mathcal{D}_{0},\mathcal{D}_{1}\) are neighbors if they can be constructed from each other by adding or removing one data point. A randomized mechanism \(\mathcal{A}\) satisfies \(\rho\)-zero-concentrated differentially private (\(\rho\)-zCDP) if, for all neighboring datasets \(\mathcal{D}_{0}\) and \(\mathcal{D}_{1}\), we have \(R_{\alpha}(\mathcal{A}(\mathcal{D}_{0})\|\mathcal{A}(\mathcal{D}_{1}))\leq\rho\alpha\), where \(R_{\alpha}(P\|Q)=\frac{1}{\alpha-1}\log\int\left(\frac{p(x)}{q(x)}\right)^{ \alpha}q(x)dx\) is the Renyi divergence between two distributions \(P\) and \(Q\).

In the above definition, \(\rho\geq 0\) is the privacy loss parameter that measures the strength of the protection. \(\rho=0\) indicates perfect privacy, \(\rho=\infty\) means no protection at all.

The goal of differentially private learning is to come up with a differentially private (\(\rho\)-zCDP) algorithms that outputs a classifier \(f:\mathcal{X}\rightarrow\mathcal{Y}\) such that the misclassification error \(\mathrm{Err}(f)=\mathbb{E}_{(x,y)\sim\mathcal{P}}\left[\mathbf{1}(f(x)\neq y)\right]\) is minimized (in expectation or with high probability), where \(\mathcal{P}\) is the data distribution under which the training data is sampled from i.i.d. For reasons that will become clear soon, we will focus on linear classifiers parameterized by \(W\in\mathbb{R}^{K\times p}\) of the form \(f_{W}(x)=\arg\max_{y\in[K]}[Wx]_{y}\).

Noisy gradient descent.Noisy Gradient Descent or its stochastic version Noisy Stochastic Gradient Descent (Song et al., 2013; Abadi et al., 2016) is a fundamental algorithm

\begin{table}
\begin{tabular}{c|c} Feature & Sample Complexity \\ \hline \hline Perfect Neural Collapse & \(\frac{2\sqrt{\log(1/\gamma)}}{\sqrt{p}}\) \\ GD & \(p\beta^{2}\log\frac{1}{\gamma}\) \\ NoisyGD & \(p\beta^{2}\log\frac{1}{\gamma}+\frac{p\beta^{2}\sqrt{\log(1/\gamma)}}{\sqrt{2p}}\) \\ stochastic (test) & \(\frac{\max(\sqrt{p}\beta,1)\sqrt{\log(1/\gamma)}}{\sqrt{2p}}\) \\ adversarial (test) & \(\frac{\max\{p\beta,1\}\sqrt{\log(1/\gamma)}}{\sqrt{2p}}\) \\ offset (training) & \(\frac{\max\{\sqrt{p}\beta,1\}\sqrt{\log(1/\gamma)}}{\sqrt{2p}}\) \\ offset + Class imbalance & \(\frac{\max\{\sqrt{p}\beta,1\}\sqrt{\log(1/\gamma)}}{(1-\beta+2\beta\alpha) \sqrt{2\rho}}\) \\ \end{tabular}
\end{table}
Table 1: Summary of the sample complexity of achieving a misclassification error \(\gamma\) of private learning under \(\rho\)-zCDP. We consider perfect features, actual features (GD and NoisyGD), and perturbed features (stochastic, adversarial, offset perturbations). For the actual features, we assume that the feature shift vectors of traing and testing features are from the same distribution. We have also considered the effects of \(\alpha\)-class imbalance. The full version can be found in Table 2 in Appendix.

in DP deep learning. To minimize the loss function \(\mathcal{L}(\theta):=\sum_{i=1}^{n}\ell(\theta,z_{i})\), the NoisyGD algorithm updates the model parameter \(\theta_{t}\) by combining the gradient with an isotropic Gaussian.","\theta_{t+1}=\theta_{t}-\eta_{t}(\sum_{i=1}^{n}\nabla\ell(\theta_{t},z_{i}) +\mathcal{N}(0,\frac{G^{2}}{2\rho}I_{p})).",,"\begin{equation}
\theta_{t+1} = \theta_{t} - \eta \left( \nabla \mathcal{L}(\theta_{t}) + \mathcal{N}(0, \sigma^{2} I) \right)
\end{equation}","Equation 1 describes the parameter update rule for the Noisy Gradient Descent (NoisyGD) algorithm, where $\theta_{t}$ is the model parameter at iteration $t$, $\eta$ is the learning rate, $\nabla \mathcal{L}(\theta_{t})$ is the gradient of the loss function, and $\mathcal{N}(0, \sigma^{2} I)$ represents isotropic Gaussian noise added for differential privacy."
ICML_2024_oral_73,2,"Here \(G\) is the \(\ell_{2}\)-sensitivity of the gradient, and the algorithm runs for \(T\) iterations that satisfy \(T\rho\)-zCDP.

However, the excess risk on population of NoisySGD must grow as \(\sqrt{p}/\epsilon\)(Bassily et al., 2014, 2019), which limits private deep learning benefit from model scales. To overcome this, DP fine-tuning (De et al., 2022; Li et al., 2021; Bu et al., 2022) is emerging as a promising approach to train large models with privacy guarantee.

Private fine-tuning.In DP fine-tuning, we begin by pre-training a model on a public dataset and then privately fine-tuning the pre-trained model on the private dataset. Our focus is on fine-tuning the last layer of pre-trained models using the NoisySGD/NoisyGD algorithm, which has consistently achieved state-of-the-art results across both vision and language classification tasks (De et al., 2022; Tramer and Boneh, 2020; Bao et al., 2023). However, we acknowledge that in some scenarios, fine-tuning all layers under DP can result in better performance, as demonstrated in the CIFAR-10 task by De et al. (2022). The comprehensive analysis of dimension-dependence in other private fine-tuning benchmarks remains an area for future investigation.

Theoretical setup for private fine-tuning.For a \(K\)-class classification task, we rewrite each data point \(z\) as \(z=(x,y)\) with \(x\in\mathbb{R}^{p}\) being the feature and \(y=(y_{1},\cdots,y_{K})\in\{0,1\}^{K}\) being the corresponding label generated by the one-hot encoding, that is \(y\) belongs to the \(k\)-th class if \(y_{k}=1\) and \(y_{j}=0\) for \(j\neq k\).

When applying NoisyGD for fine-tuning the last-layer parameters, the model is in a linear form. Thus, we consider the linear model \(f_{W}(x)=Wx\) with \(W\in\mathbb{R}^{K\times p}\) being the last-layer parameter to be trained and \(x\) is the last layer feature of a data point. The parameter \(\theta_{t}\) in NoisyGD is the vectorization of \(W\). Let \(\ell:\mathbb{R}^{K}\times\mathbb{R}^{K}\rightarrow\mathbb{R}\) be a loss function that maps \(f_{W}(x)\in\mathbb{R}^{K}\) and the label \(y\) to \(\ell(f_{W}(x),y)\). For example, for the cross-entropy loss, we have \(\ell(f_{W}(x),y)=-\sum_{i=1}^{K}y_{i}\log[(f_{W}(x))_{i}]\).

Misclassification error.For an output \(\widehat{W}\) and a testing data point \((x,y)\), the misclassification error we considered is defined as \(\Pr\left[y\neq f_{\widehat{W}}(x)\right],\) where the probability is taken with respect to the randomness of \(\widehat{W}\) and \((x,y)\sim\mathcal{P}\).

Beyond the distribution-free theory.Distribution-free learning with differential privacy is however known to be statistically intractable even for linear classification in 1-dimension (Chaudhuri and Hsu, 2011; Bun et al., 2015; Wang et al., 2016). Existing work resorts to either proving results about (convex and Lipschitz) surrogate losses (Bassily et al., 2014) or making assumptions on the data distribution (Chaudhuri and Hsu, 2011; Bun et al., 2020). For example, Chaudhuri and Hsu (2011) assumes bounded density, and Bun et al. (2020) shows that linear classifiers are privately learnable if the distribution satisfies a large-margin condition. Our setting, as detailed in Section 3.1.1, can be viewed as a _new family of distributional assumptions_ motivated by the recent discovery of the Neural Collapse phenomenon. As we will see, these assumptions not only make private learning statistically and computationally tractable (using NoisyGD), but also produce sample complexity bounds that are _dimension-free_ and _exponentially faster_ than existing results that are applicable to our setting.

Neural Collapse.Neural Collapse (Papyan et al., 2020; Fang et al., 2021; He and Su, 2023) describes a phenomenon about the last-layer feature structure obtained when a deep classifier neural network converges. It demonstrates that the last-layer feature converges to the column of an equiangular tight frame (ETF). Mathematically, an ETF is a matrix","M=\sqrt{\frac{K}{K-1}}P(I_{K}-\frac{1}{K}\mathbf{1}_{K}\mathbf{1}_{K}^{T })\in\mathbb{R}^{p\times K},","where \(P=[P_{1},\cdots,P_{K}]\in\mathbb{R}^{p\times K}\) is a partial orthogonal matrix such that \(P^{T}P=I_{K}\)","\mu_{y}=\sqrt{\frac{p}{K-1}}\left(\mathbf{I}_{K}-\frac{1}{K}\mathbf{1}_{K}\mathbf{1}_{K}^{\top}\right)\mathbf{e}_{y},","where $\mu_y$ represents the class-specific feature vector in the equiangular tight frame (ETF) structure induced by Neural Collapse, with $p$ being the feature dimension and $K$ the number of classes."
ICML_2024_oral_73,3,"\). For a given dimension \(d=p\) or \(K\), we denote \(I_{d}\in\mathbb{R}^{d}\) the identity matrix and denote \(\mathbf{1}_{d}=[1,\cdots,1]^{T}\in\mathbb{R}^{d}\). Rewrite \(M=[M_{1},\cdots,M_{K}]\) with \(M_{k}\) being the \(k\)-th column of \(M\), that is, the ideal feature of the data belonging to class \(k\).

We adopt the Neural Collapse theory to describe an ideal feature representation of applying the pre-trained model on the private set. However, achieving perfect collapse on the private set is an ambitious assumption, as in practice, the private feature of a given class is distributed around \(M_{k}\). Therefore, we introduce a feature shift parameter \(\beta\) to measure the discrepancy between the actual feature and the perfect feature \(M_{k}\).

**Definition 2.2** (Feature shift parameter \(\beta\)).: For any \(1\leq k\leq K\), given a feature \(x\) belonging to the \(k\)-th class and the perfect feature \(M_{k}\), we define \(\beta=||x-M_{k}||_{\infty}\) as the feature shift parameter of \(x\) that measures the \(\ell_{\infty}\) distance between \(x\) and \(M_{k}\).

Here, we use the \(\ell_{\infty}\) norm since it is related to adversarial attacks, which are important in our study of the robustness of NoisyGD. Our numerical results in Figure 2 show that \(\beta\) is bounded on CIFAR-10 if the pretrained model is the vision transformer or ResNet-50.

## 3 Bounds on misclassification errors and robustness in private fine-tuning

In this section, we establish bounds on the misclassification error for both GD and the NoisyGD.

Section 3.1 aims to delineate the connection between the feature shift parameter \(\beta\) and the misclassification error. Additionally, we derive a threshold for \(\beta\) below which the misclassification error is dimension-independent.

In Section 3.2, our focus is the robustness of private fine-tuning. Specifically, we elucidate how various perturbations impact both \(\beta\) and the misclassification error.

### Bounds on misclassification errors

We consider a binary classification problem with a training set \(\{(x_{i},y_{i})\}_{i=1}^{n}\), where \(x_{i}\) represents features and \(y_{i}\in\{\pm 1\}\) are the labels. For the broader multi-class scenarios, we state our theory for the perfect case in Section F.1. The rest theory can be extended to the multi-class case similarly.

For binary classification problems, the ETF \(M=[M_{1},M_{2}]\) satisfies \(M_{1}=-M_{2}\), which is equivalent to \(M_{1}=e_{1}\) and \(M_{2}=-e_{1}\) up to some rotation map (detailed in Appendix B). For a data point \((x,y)\) with \(y=1\), recall the feature shift parameter \(\beta=\|x-e_{1}\|_{\infty}\), which is the infinity norm of \(v=x-e_{1}\). We call \(v\) a feature shift vector since it is the difference between an actual feature and the perfect one. Similarly, if \(y=-1\), the feature shift vector is \(v=x+e_{1}\). For a training set \(\{(x_{i},y_{i})\}_{i=1}^{n}\), let \(\{v_{i}=x_{i}-\mathrm{perfect\ feature}\}_{i=1}^{n}\) be a sequence of feature shift vectors.

We first consider the scenario where the shift vectors \(v_{i}\)'s are i.i.d. copies of a symmetric centered random vector \(v\) with \(\|v\|_{\infty}\leq\beta\) in Section 3.1.1. This setting is quite practical as can be seen from Figure 1. Furthermore, acknowledging that at times, the feature may be influenced by a fixed vector, such as an offset shift vector which will not change the margin and angles between features, we also investigated the case where \(v_{i}\) is deterministic in Section 3.1.2.

#### 3.1.1 Stochastic shift vectors

For conciseness, we mainly focus on the results for 1-iteration NoisyGD, which is sufficient to ensure the convergence. As presented in Theorem C.1, our theory can be extended to multi-iteration projected NoisyGD. However, the dimension dependency can not be mitigated using multiple iterations. The proofs of all results in this section are given in Appendix C.

For 1-iteration GD without DP guarantee, the output is \(\widehat{\theta}_{\mathrm{GD}}=\eta\sum_{i=1}^{n}y_{i}x_{i}\). Moreover, the 1-iteration NoisyGD outputs \(\widehat{\theta}_{\mathrm{NoisyGD}}=\eta\sum_{i=1}^{n}y_{i}x_{i}+\mathcal{N}( 0,\eta^{2}\sigma^{2})\). For the private learning problem, the sensitivity of the gradient is \(G=\sup_{x_{i}}\|x_{i}\|_{2}=\sqrt{1+\beta^{2}p}\), which is dimension dependent. If we still want \(G\) to be dimension independent, then every data point needs to be shrunk to \((e_{1}+v)/\sqrt{1+\|v\|_{2}^{2}}\). In both cases, the error bounds remain the same. In the testing procedure, we consider a testing point \((x,y)\sim\mathcal{P}\).

For an estimate \(\widehat{\theta}\) whose randomness is from a training dataset drawn independently from \(\mathcal{P}\) and a randomized algorithm \(\mathcal{A}\), the misclassification error \(\mathbb{E}_{\mathcal{A},\text{data}\sim\mathcal{P}^{n}}\left[\mathrm{Err}_{ \mathcal{P}}(\mathcal{A}(\text{data}))\right]\) can be rewritten as \(\mathbb{E}_{\mathcal{A},\text{data}\sim\mathcal{P}^{n}}\left[\mathrm{Err}_{ \mathcal{P}}(\mathcal{A}(\text{data}))\right]=\Pr\left[y\widehat{\theta}^{T} x<0\right],\) where the probability is taken with respect to the randomness of both \((x,y)\sim\mathcal{P}\) and \(\widehat{\theta}\).

**Theorem 3.1** (misclassification error for GD).: _Let \(\widehat{\theta}_{\mathrm{GD}}\) be a predictor trained by GD under the cross entropy loss with zero initialization. Then, we have the following error bound on the misclassification error._

* _If we assume that_ \(\beta^{2}p\leq 1,\) _then it holds_ \(\Pr[y\widehat{\theta}_{\mathrm{GD}}^{T}x<0]=0\) _for_ \(n\) _greater than the number of classes. As a result, to achieve a misclassification error_ \(\gamma\)_, the sample complexity is constant._
* _In general, if we assume that the sample is i.i.d., then the misclassification error is bounded as_ \(\Pr[y\widehat{\theta}_{\mathrm{GD}}^{T}x<0]\leq\exp\left(-\frac{n}{2\left( \beta^{4}p^{2}+\frac{1}{3}\beta^{2}p\right)}\right).\) _Therefore, to achieve a misclassification error_ \(\gamma\)_, the sample complexity is_ \(O\left(p\beta^{2}\log(1/\gamma)\right).\) _If we further assume that all_ \(p\) _components of_ \(v\) _are independent of each other, then, it holds_ \(\Pr[y\widehat{\theta}_{\mathrm{GD}}^{T}x<0]\leq\exp\left(-\frac{n}{2\left( \beta^{4}p+\frac{1}{3}\beta^{2}\right)}\right).\) _Thus, to achieve a misclassification error_ \(\gamma\)_, the sample complexity is_ \(O\left(p\beta^{4}\log(1/\gamma)\right).\)__

**Theorem 3.2** (misclassification error for NoisyGD).: _Let \(\widehat{\theta}_{\mathrm{NoisyGD}}\) be a predictor trained by NoisyGD under the cross entropy loss with zero initialization. Then, we have the following error bound on the misclassification error._",\Pr[y\widehat{\theta}_{\mathrm{NoisyGD}}^{T}x<0]\leq \exp(-\frac{n^{2}\rho}{2(1+\beta^{2}p)^{2}}) || +\exp(-\frac{n}{8(\beta^{4}p^{2}+\frac{1}{3}\beta^{2} p)}).,,"\widehat{\theta}_{\text{NoisyGD}} = \eta \sum_{i=1}^n y_i x_i + \mathcal{N}\left(0, \frac{\eta^2 (1 + \beta^2 p)}{2 \rho} I_p \right)",The NoisyGD update rule for the last-layer parameter θ with gradient noise added to ensure ρ-zCDP.
ICML_2024_oral_73,4,"_As a result, to achieve a misclassification error \(\gamma\), the sample complexity is \(O\left(\frac{(1+\beta^{2}p)^{2}\sqrt{\log\frac{1}{\gamma}}}{2\rho}+p\beta^{2} \log(1/\gamma)\right).\) If we further assume that all \(p\) components of \(v\) are independent of each other, then, it holds_",\Pr[y\widehat{\theta}_{\mathrm{NoisyGD}}^{T}x<0]\leq \exp(-\frac{n^{2}\rho}{2(1+\beta^{2}p)^{2}}) || +\exp(-\frac{n}{8(\beta^{4}p+\frac{1}{3}\beta^{2} )}).,,\Pr[y\widehat{\theta}_{\mathrm{NoisyGD}}^{T}x<0]\leq \exp\left(-\frac{n^{2}\rho}{2(1+\beta^{2}p)^{2}}\right)+\exp\left(-\frac{n}{8(\beta^{4}p+\frac{1}{3}\beta^{2})}\right).,"This equation refines the misclassification error bound for NoisyGD by incorporating the independence assumption on the components of the feature shift vector $v$, leading to a tighter exponential decay rate in terms of $p$, $\beta$, and $\rho$."
ICML_2024_oral_73,5,"To achieve a misclassification error \(\gamma\), the sample complexity is \(O\left(\frac{(1+\beta^{2}p)^{2}\sqrt{\log\frac{1}{\gamma}}}{2\rho}+4p\beta^{4} \log\frac{1}{\gamma}\right).\)_

Remark.Note that with further assumptions on feature separability, the second term in Equation 3 (which aligns with GD in Theorem 3.1) can be improved from \(\beta^{2}p\) to \(\beta^{4}p\). However, the first term, caused by DP, remains unchanged by this assumption. Thus, NoisyGD has a stricter requirement on feature quality due to the added random noise. Theorems 3.1 and 3.2 indicate that the error bound is exponentially close to 0 under the following conditions: \(\beta\leq p^{-1/2}\) for both NoisyGD and GD and \(\beta\leq p^{-1/4}\) for GD under stronger assumptions. This result is dimension-independent when \(\beta\) satisfies the above conditions. Moreover, GD has robustness against larger shift vectors compared to NoisyGD. This aligns with the observations from our experiments detailed in Section 5, where we note a significant decrease in accuracy with increasing dimensionality. In addition, when \(\beta\leq p^{-1/2}\), the misclassification error for GD is always 0 while that for NoisyGD is \(\exp\left(-\frac{n\rho}{1+\beta^{2}p}\right)\).

Promising properties for perfect collapse.In the special case \(\beta=0\), all features are equivalent to the perfect feature. For this perfect scenario, numerous promising properties are outlined as follows. The details are discussed in Section F.1.

1. The error bound is exponentially close to \(0\) if \(\rho\gg G^{2}/n^{2}\) -- very strong privacy and very strong utility at the same time.
2. The result is dimension independent -- it doesn't depend on the dimension \(p\).
3. The result is robust to class imbalance for binary classification tasks.
4. The result is independent of the shape of the loss functions. Logistic loss works, while square losses also works.
5. The result does not require careful choice of learning rate. Any learning rate works equally well.

#### 3.1.2 Deterministic shift vectors

We consider the case where each \(v_{i}\) is a fixed vector with \(\|v_{i}\|_{\infty}\leq\beta\). Recall that the 1-iteration NoisyGD outputs \(\widehat{\theta}_{\mathrm{NoisyGD}}=\eta\left(ne_{1}+\sum_{i=1}^{n}v_{i}\right) +\mathcal{N}(0,\sigma^{2})\).

**Theorem 3.3** (misclassification error for NoisyGD).: _Let \(\widehat{\theta}_{\mathrm{NoisyGD}}\) be a predictor trained by NoisyGD under the cross entropy loss with zero initialization. Then, for \(\beta\) such that \(1-\beta^{2}p>0\), we have \(\Pr[y\widehat{\theta}_{\mathrm{NoisyGD}}^{T}x<0]\leq\exp\left(-\frac{n^{2}(1- \beta^{2}p)^{2}}{(1+\beta^{2}p)\sigma^{2}}\right).\) As a result, to make the misclassification error less than \(\gamma\), the sample complexity for \(n\) is \(O\left(\frac{(1+\beta^{2}p)\log\frac{1}{\gamma}}{2\rho(1-\beta^{2}p)^{2}} \right).\)_

This misclassification error also corresponds to \(\beta^{2}p\), which is similar to the stochastic case. When \(\beta^{2}p<1\), the misclassification error decays exponentially and the misclassification error is dimension-independent.

### Robustness of NoisyGD under perturbations

In this section, we explore the robustness of NoisyGD against various perturbation types. For the sake of brevity, we focus on perturbations of the perfect feature (\(\beta=0\)) by different attackers. The theoretical framework can easily be extended to include perturbations of actual features, following the same proof structure as outlined in Theorem 3.1 and Theorem 3.2. Our findings indicate that each mentioned perturbation type affects the feature shift parameter \(\beta\), potentially increasing NoisyGD's dimension dependency.

#### 3.2.1 Stochastic attackers

Non-robustness to perturbations in the training time.If the training feature is perturbed by some stochastic perturbation (while the testing feature is perfect), then, the misclassification error for GD is \(\exp\left(-\frac{n^{2}}{\beta^{2}}\right)\), which is dimension-independent for any \(\widetilde{\beta}>0\). However, the sample complexity for NoisyGD is \(O\left(\sqrt{\frac{\max\{\widetilde{\beta}^{2}p,1\}\log(1/\gamma)}{\rho}}\right).\) Thus, the NoisyGD is non-robust even when we only perturb the training feature with attackers that make \(\widetilde{\beta}>p^{-1/2}\). We postpone the details to Appendix D.2

Non-robustness to perturbations in the testing time.If we only perturb the testing feature, then we still require \(O\left(\frac{\max\{\sqrt{p}\beta,1\}\sqrt{\log(1/\gamma)}}{\sqrt{2\rho}}\right)\) samples to achieve a misclassification error \(\gamma\), which is still non-robust when \(\widetilde{\beta}>p^{-1/2}\). The technical detail is similar to the proof of Theorem 3.2.

#### 3.2.2 Deterministic attackers

Non robustness to offset perturbations in the training time.Even if we just shift the training feature vectors away by a constant offset (while keeping the same margin and angle between features), it makes DP learning a lot harder. Precisely, for some vector \(v\in\mathbb{R}^{p}\), we consider \(v_{i}=v\) for \(y_{i}=1\) and \(v_{i}=-v\) for \(y_{i}=-1\). Moreover, this makes absolutely no difference to the gradient, when we start from \(0\) because",\nabla\mathcal{L}(\theta)=\frac{n}{2}\cdot 0.5\cdot-(-e_{1}+v)+\frac{n}{2} \cdot 0.5\cdot(e_{1}+v)=\frac{n}{2}e_{1}.,,\sum_{i=1}^{n}y_{i}(e_{1}+v_{i})=ne_{1}+\sum_{i=1}^{n}y_{i}v_{i}=ne_{1}+n v-nv=ne_{1}.,The gradient remains unaffected by the offset perturbation in the training data due to cancellation of the shift terms.
ICML_2024_oral_75,1,"### Offline KL-Regularized Actor-Critic

We target a KL-regularized RL objective, where the goal is to find a policy \(\pi_{\text{imp}}\) that improves over a reference policy \(\tilde{\pi}\) via \(\pi_{\text{imp}}=\arg\max_{\pi}J(\pi)\) where \(J(\pi)\) is given as:","J(\pi)=\mathbb{E}_{(s_{t},\tau)\in\mathcal{D}}[\mathbb{E}_{a_{t}=\pi} [Q^{\pi}(s_{t},a_{t},\tau)]-\eta\operatorname{D_{KL}}[\pi, \tilde{\pi}|s_{t},\tau]]",where \(\eta\) is a hyperparameter determining the strength of the regularization towards the reference policy \(\tilde{\pi}\),"\begin{equation}
J(\pi) = \mathbb{E}_{\tau \sim \pi} \left[ \sum_{t=0}^{T} \gamma^t \left( r(s_t, a_t) - \frac{1}{\beta} D_{\text{KL}}\left(\pi(\cdot|s_t) \parallel \tilde{\pi}(\cdot|s_t)\right) \right) \right],
\end{equation}","Equation 1 defines the KL-regularized reinforcement learning objective $J(\pi)$, which incorporates both the expected cumulative reward and a KL-divergence penalty term to encourage policies close to the reference policy $\tilde{\pi}$."
ICML_2024_oral_75,2,\). The solution to this maximization problem is given as (see Appendix A.1 for derivation):,"\pi_{\text{imp}}(a_{t}|s_{t},\tau) \propto\exp(Q^{\pi_{\text{imp}}(s_{t},a_{t},\tau)}/\eta)\tilde{ \pi}(a_{t}|s_{t},\tau), || \propto\exp(A^{\pi_{\text{imp}}(s_{t},a_{t},\tau)}/\eta)\tilde{ \pi}(a_{t}|s_{t},\tau).",,"\pi_{\text{imp}}(a_{t}|s_{t},\tau)\propto\tilde{\pi}(a_{t}|s_{t},\tau)\exp\left(\frac{1}{\eta}A^{\tilde{\pi}}(s_{t},a_{t},\tau)\right)",The improved policy $\pi_{\text{imp}}$ is proportional to the reference policy $\tilde{\pi}$ weighted by the exponentiated advantage function $A^{\tilde{\pi}}$ scaled by the inverse of the regularization hyperparameter $\eta$.
ICML_2024_oral_75,3,"This observation allows us to transform the RL problem of finding an optimal policy into a weighted supervised learning problem (cf. Abdolmaleki et al. (2018)). Assuming access to an estimate of \(Q^{\pi_{\text{imp}}}\) or \(A^{\pi_{\text{imp}}}\), we can fit a parametric policy \(\pi_{\theta}\) by minimizing its divergence \(D_{KL}[\pi_{\text{imp}},\pi_{\theta}|s_{t},\tau]\) to \(\pi_{\text{imp}}\) using a sample based estimate. _Turning the policy optimisation problem into an instance of supervised learning has the major benefit that it is easy to trade-off the policy optimisation objective with a behavior cloning term, since all loss terms will be (weighted) negative log likelihoods._

Different choices for estimating \(Q^{\pi_{\text{imp}}}\) or \(A^{\pi_{\text{imp}}}\) as well as the reference policy \(\tilde{\pi}\) lead to different algorithmic variants. We will concentrate on a Q-function based variant in the main paper but describe a state-value function (V-function) based variant in the appendix; which has similar scaling benefits.

We train the policy \(\pi_{\theta}\) together with an estimate \(Q_{\theta}\approx Q^{\pi_{\theta}}\approx Q^{\pi_{\text{imp}}}\) of the state-action value function. To balance losses, we employ tools from the distributional reinforcement learning literature (Bellemare et al., 2017) which transform the problem of learning \(Q_{\theta}\) into minimizing the negative log likelihood of a discretized Q-function distribution \(p_{\theta}(q|s_{t},a_{t},\tau)\). Using the distributional TD operator (Bellemare et al., 2017) we can compute a sample-based target Q-distribution \(\Gamma_{\theta^{\prime}}(q|s_{t},a_{t},\tau)\) (see Appendix A.2) where \(\theta^{\prime}\) are the parameters of a target network which is periodically updated to a time-lagged version of \(\theta\). The same target parameters also give rise to a target policy \(\pi_{\theta^{\prime}}\) which we use as the reference policy in Equation (2), i.e. \(\tilde{\pi}=\pi_{\theta^{\prime}}\). Combining the policy loss, a BC loss, and the KL-based Q-value loss yields a total loss containing three KL terms:","L^{Q}(\theta)=\mathbb{E}\Big{[} (1-\alpha)\operatorname{D_{KL}}[\pi_{\text{imp}},\pi_{\theta}|s_ {t},\tau,\tilde{\pi}=\pi_{\theta^{\prime}}] || +\alpha\operatorname{D_{KL}}[b,\pi_{\theta}|s_{t},\tau] || +\beta\operatorname{D_{KL}}[\Gamma_{\theta^{\prime}}(q|s_{t},a_{t },\tau),p_{\theta}(q|s_{t},a_{t},\tau)]\Big{]} || =-\mathbb{E}\Big{[} (1-\alpha)\operatorname*{\mathbb{E}}_{d^{\prime}\sim\pi_{\theta^{ \prime}}}[w(a^{\prime},s_{t},\tau)\log\pi_{\theta}(a^{\prime}|s_{t},\tau)] || +\alpha\log\pi_{\theta}(a_{t}|s_{t},\tau) || +\beta\operatorname*{\mathbb{E}}_{q\sim\Gamma_{\theta^{\prime}}} \log p_{\theta}(q|s_{t},a_{t},\tau)\Big{]}+K_{H},",,"\mathcal{L}(\theta) = \mathbb{E}_{(s_t, \tau) \in \mathcal{D}} \left[ \alpha_1 D_{\text{KL}}\left[\pi_{\text{imp}}, \pi_\theta | s_t, \tau\right] + \alpha_2 D_{\text{KL}}\left[\tilde{\pi}, \pi_\theta | s_t, \tau\right] + \alpha_3 D_{\text{KL}}\left[\Gamma_{\theta'}(q | s_t, a_t, \tau), p_\theta(q | s_t, a_t, \tau)\right] \right],","where $\alpha_1$, $\alpha_2$, and $\alpha_3$ are coefficients balancing the policy improvement, behavior cloning, and Q-value distribution learning objectives, respectively."
ICML_2024_oral_75,4,"\). The expectation over the data is estimated by sampling \((s_{t},a_{t},s_{t+1},\tau)\in\mathcal{D}\), the expectation over action samplesfrom \(\pi_{\theta^{\prime}}\) is estimated based on \(N=10\) samples and the expectation \(\mathbb{E}_{q\sim\Gamma_{\theta^{\prime}}}\) can be evaluated analytically. Finally \(\alpha\) and \(\beta\) are multipliers trading off different loss components (which are relatively easy to set due to all losses corresponding to weighted categorical log likelihoods). We refer to Appendix A.2 for a step-by-step derivation.

Notably, aside from the KL towards the improved policy \(\pi_{\text{imp}}\), Equation (3) also includes a KL towards the behaviour policy \(b\). This additional regularization is necessary to prevent \(\pi_{\theta}\) from converging to action samples that have high Q-values but are far away from those observed in the data (and are thus at the risk of being overestimated); a common issue in offline RL with Q-functions (Levine et al., 2020). The additional BC term prevents this, following prior examples for using a BC loss as a simple regularisation technique in offline RL (Abdolmaleki et al., 2022; Fujimoto and Gu, 2021). We find that this is the only term needed to stabilize learning. In addition, it gives us a natural way for moving away from learning via pure behavioral cloning (\(\alpha=1\)) towards pure policy optimisation against the learned Q-function (\(\alpha=0\)). This also allows us to perform expensive training runs of large models with confidence since we can set \(\alpha\) to a larger value such that the policy stays close to BC, guaranteeing stable training, and can reduce it later during fine-tuning.

### Scalable Architecture for Actor-Critic Learning

With the proposed offline actor-critic algorithm, we now describe how \(\pi_{\theta}\) and \(Q_{\theta}\) are instantiated with scalable network architectures. In particular, we aim for an architecture that is flexible enough to incorporate different modalities of state observations and task descriptions as well as various action specifications, while also being computationally efficient for consuming high-dimensional inputs during learning and at inference time (to enable \(20\,\mathrm{Hz}\) control of real robots). In this section, we describe how we adopt a Perceiver-IO architecture (Jaegle et al., 2021) to achieve the above. The model is depicted in Figure 2.

Observation EncodingGiven multimodal inputs, in particular proprioceptive and visual observations \(s_{t}=(s_{t}^{P},s_{t}^{V})\) along with visual and language task descriptions \(\tau=\tau^{V},\tau^{L})\), our model first deploys one encoder (\(\phi\)) per modality to encode the inputs into embedding vectors: \(e_{I}=\phi^{P}(s_{t}^{P})\oplus\phi^{V}(s_{t}^{V})\oplus\phi^{V}(\tau^{V}) \oplus\phi^{L}(\tau^{L})\in\mathbb{R}^{N\times D_{I}}\), with \(N\) and \(D_{I}\) denoting the number and dimensionality of the embedding vectors. Details of each modality encoder are provided in Appendix B.2. For the proprioception encoder \(\phi^{P}\) we propose a novel _multi-scale_ normalizer to account for arbitrary input scales and provide further details and ablations on this encoder choice in Appendices B.1 and D.2.1. We highlight that our model uses task descriptions of different modalities (text and vision) and we analyse this multimodal task conditioning in Appendix D.2.4.

Transformer on Latent SpaceAt this point, the commonly adopted approach would be to feed the embedding sequence \(e_{I}\in\mathbb{R}^{N\times D_{I}}\) directly into a transformer consisting of multiple stacked self-attention blocks. However, for the domains we consider, the input sequence length amounts to thousands of tokens for a single time step. As the computational complexity and memory usage of self-attention

Figure 2: High-level PAC model architecture. Modality-specific encoders transform proprioceptive (P), visual (V), and language (L) inputs into embedding vectors \(e_{I}\), which are cross-attended by learnable latent queries \(z_{0}\). This is followed by a series of self-attention blocks to yield the latent encoding \(z_{M}\), which is then queried via additional cross-attention modules to decode the desired outputs. The policy decoder employs a learnable query \(q_{\pi}\) to cross-attend \(z_{M}\) and outputs the logits of action distributions. The Q-value decoder employs a query \(q_{Q}\) based on the encoded actions to cross-attend \(z_{M}\) and outputs the action-specific logits of the distributional Q-function.

scales quadratically with the sequence length, this common treatment potentially hinders the learned controller from being applicable to real robotic systems that impose real-time constraints. To address this, we adopt the methodology from the perceiver model (Jaegle et al., 2021). Specifically, a cross-attention block is placed at the front-end of the network in which the input sequence of embeddings \(e_{I}\) are queried by \(N_{Z}\) trainable latent vectors each of size \(D_{Z}\): \(z\in\mathbb{R}^{N_{Z}\times D_{Z}}\), which outputs latent embeddings \(z_{0}\). This is followed by \(M\) self-attention operations on the latents which finally yield \(z_{M}\in\mathbb{R}^{N_{Z}\times D_{Z}}\). Since the number of latent vectors is typically much smaller than the input sequence length (\(N_{Z}\ll N\)) and the self-attention operation is shifted from the input embeddings to the latent vectors, this effectively reduces the computation and memory usage to \(O({N_{Z}}^{2})\). We provide more details on the perceiver backbone in Appendix B.3.

Policy and Value DecodingTo implement an actor-critic algorithm, the model needs to output both a Q-value estimate and an action prediction. While the action prediction \(\hat{a}_{t}\) can be directly modeled as a function of the inputs \((s_{t},\tau)\) which are encoded into \(e_{I}\) and thus \(z_{M}\), the value estimate \(Q_{Q}(s_{t},a_{t},\tau)\) also depends on the action \(a_{t}\) which is not encoded in \(z_{M}\). To obtain the two types of outputs we cross-attend the latent embeddings \(z_{M}\) with dedicated queries. While the queries for the policy are learned vectors, the Q-value queries are computed by encoding the action \(a_{t}\in\mathbb{R}^{N^{A}}\) via our multi-scale normalizer. This has the advantage that the model is less prone to ignoring the action compared to when the action would be presented as an input (a common problem when learning Q-values). It also allows efficient evaluation of the Q-function for multiple action samples via caching of the action-independent latent \(z_{M}\). We provide more details in Appendix B.4 and ablate the importance of the cross-attention for Q-value prediction in Appendix D.2.2.

## 4 Experiments

We present three sets of experiments investigating different aspects of PAC. Section 4.1 analyzes whether PAC follows scaling laws similar to established supervised learning settings. Section 4.2 compares PAC's performance after large-scale training with the RL objective to different BC baselines across over 100 continuous control tasks. Finally, Section 4.3 studies how PAC can be finetuned by leveraging its Q-function to hone in on a real robot task and further improve its performance using self-generated data.

We use a large dataset throughout all experiments which combines tasks from three different sources: Gato data (Reed et al., 2022) consist of records of an RL agent solving 32 simulation tasks in Control Suite (Tunyasuvunakool et al., 2020). RoboCat data (Bousmalis et al., 2023) operates on the RGB Stacking benchmark (Lee et al., 2021) using RL in simulation to build pyramid and tower structures using a 7-DoF Panda robot. It also contains an Insertion task featuring teleoperated simulation data of the same robot inserting differently sized gears onto peg. Lastly, CHEF (Lampe et al., 2023) data contains simulated and real-world records of a 5-DoF Sawyer robot stacking two objects in the RGB Stacking benchmark using an RL algorithm. For all episodes in our dataset, a short language instruction describing the task is added to each frame, e.g. humanoid.run or panda.sim.pyramid, which serves as a unique goal instruction to differentiate between the different tasks. For all RoboCat tasks an additional goal image is provided as the goal instruciton. We again emphasize that our model can handle both language and visual goal descriptions (where present) and refer to Appendix D.2.4 for details about the goal conditioning. In total, our data mix consists of 3.64M episodes across 102 simulated and 30 real continuous control tasks which equates to approximately 2.45T tokens for model training (cf. Appendix C.3 and C.4).

### Scaling Analysis for Offline RL Objectives

A central part of our investigation is to understand the interaction between offline actor-critic algorithms and scalable neural network architectures that use (self-)attention. When trained with supervised objectives, such as next-token prediction, architectures of this type usually follow _scaling laws_(Kaplan et al., 2020), i.e. for all performance-optimal models the number of tokens consumed and the number of model parameters used follow power-laws in the number of FLOPs spent. However, it has so far been unclear whether these scaling laws also extend to RL. To investigate this relationship, we adopt the methodology from Hoffmann et al. (2022) (also known as 'Chinchilla scaling laws') and apply it to PAC. We define five different model scales (XXS, XS, S, M and L) ranging from 32M to 988M parameters to study the scaling behavior of PAC and report the full model architecture hyper-parameters in Appendix C.1.

To conduct our analysis, we train PAC across the different scales with two different values of \(\alpha\) for the BC/RL trade-off. Setting \(\alpha=1.0\) results in a BC objective for the policy and constitutes our baseline BC+Q2 while PAC performs offline RL with \(\alpha=0.75\). With a batch size of 512 trajectories of length five, one epoch of our data mix takes approximately 2.7M steps. Therefore we train each model for 3M updates to stay in a single-epoch regime.

Footnote 2: Using a Q-value loss term with \(\beta>0\) never decreased the performance in our BC experiments; we keep it for comparability.

Following Kaplan et al. (2020); Hoffmann et al. (2022), the power laws between compute operations \(C\), number of tokens \(D\) and number of parameters \(N\) for performance-optimal models of the family are:","N(C)=N_{0}*C^{a},\;\;\;D(C)=D_{0}*C^{b}.",,C=6DN=6N^{2}/\kappa,"This equation defines the relationship between compute operations (C), number of tokens (D), and number of parameters (N) for performance-optimal models, where $\kappa$ is a constant representing the compute efficiency."
ICML_2024_oral_79,1,"### Pretraining

**Pretraining I: action quantization**. This stage is illustrated in Figure 1(a), and its pseudocode is available in Figure 13 in Appendix C. Let \(\mathcal{G}:\mathcal{O}^{T}\rightarrow\mathcal{Z}\) be the observation embedding which embeds a sequence of pixel observations of length \(T\) into the latent embedding space \(\mathcal{Z}\). In our implementation, we use the transformer architecture (Vaswani et al., 2017) and set _Transformer_block_\((\text{CNN}(o_{t}),,...,\text{CNN}(o_{t-T+1}))\). Using the multitask dataset \(\mathcal{D}\), we jointly pretrain the observation embedding \(\mathcal{G}\) and a state-dependent action quantization module \(\mathcal{F}:\mathcal{Z}\times\mathcal{A}\rightarrow\mathcal{E}\), where \(\mathcal{E}=\{e_{1},e_{2},...,e_{C}\}\) denotes the set of codes. The set of codes \(\mathcal{E}\) produced by learning \(\mathcal{F}\) depends on the inductive bias of \(\mathcal{F}\)'s training procedure. The inductive bias we chose for PRISE follows the intuition that, given the current latent state, a desirable action code should be predictive of both the next latent state and of the raw action mapped by \(\mathcal{F}\) into that action code. A similar intuition underlies action representation learning in, e.g., (Chandak et al., 2019), although that work doesn't attempt to simultaneously learn a latent state space.

First, to ensure the action code can predict future states, we train a latent forward transition model \(\mathcal{T}:\mathcal{Z}\times\mathcal{A}\rightarrow\mathcal{Z}\). To optimize the forward latent dynamics model while preventing state and action representation collapse, we adopt a BYOL-like objective inspired from (Schwarzer et al., 2021), where we minimize","\mathcal{L}_{\text{dyn}}[ \theta_{\mathcal{F},\mathcal{G},P}](o_{t,T},a_{t})= || -\text{\bf cos\_sim}\big{(}Q(P(\mathcal{T}(z_{t},e_{t}))),\text{ stopgrad}(P(z_{t+1}))\big{)}",,"\mathcal{L}_{\text{forward}} = \mathbb{E}_{(o,a,o') \sim \mathcal{D}} \left[ \| \mathcal{T}(\mathcal{G}(o), a) - \mathcal{G}(o') \|_2^2 \right]","Equation 1 defines the forward latent dynamics loss, which minimizes the squared error between the predicted next state embedding and the actual next state embedding, encouraging the model to accurately predict future states based on the current state and action."
ICML_2024_oral_79,2,"Here, \(o_{t,T}=o_{t},o_{t-1},...,o_{t-T+1}\) is the observation history, \(z_{t}=\mathcal{G}(o_{t},o_{t-1},...,o_{t-T+1})\) is the latent state, \(e_{t}=\mathcal{F}(\text{stopgrad}(z_{t}),a_{t})\) is the quantized action code, \(P\) is a projection MLP, and \(Q\) is a prediction MLP. In our implementation, \(P\) has two layers and \(Q\) has one.

Next, to guarantee the raw action can be decoded from the action code and latent state effectively, we train a latent state-dependent decoder, \(\psi\), aiming to reconstruct the raw action given latent state \(z_{t}\) and action code \(e_{t}\). In general, we let \(\psi\) be a stochastic mapping \(\psi:\mathcal{Z}\times\mathcal{E}\rightarrow\Delta(\mathcal{A})\), where \(\Delta(\mathcal{A})\) stands for the space of probability distributions over \(\mathcal{A}\). We use a stochastic Gaussian Mixture Model (GMM) to parameterize the decoded action distribution of \(\psi\) following Mandlekar et al. (2021). This choice has been found effective in dealing with the inherent multimodality and noise in such human teleoperation demonstrations. Then the learning objective becomes to minimize the negative log likelihood of the GMM distribution: \(\mathcal{L}_{\text{act\_decode}}[\theta_{\mathcal{F},\mathcal{G},\psi}](o_{t, T},a_{t})=\mathcal{L}_{\text{act}}(\psi(z_{t},e_{t}),a_{t})=-\log\psi(a_{t}|z_{t},e_{t})\). When we know the the pretraining data is collected by a fixed deterministic policy, we let \(\psi:\mathcal{Z}\times\mathcal{E}\rightarrow\mathcal{A}\) be a deterministic function that minimizes the \(L1\)-loss: \(\mathcal{L}_{\text{act\_decode}}[\theta_{\mathcal{F},\mathcal{G},\psi}](o_{t, T},a_{t})=\mathcal{L}_{\text{act}}(\psi(z_{t},e_{t}),a_{t})=\|\psi(z_{t},e_{t})-a_{t}\|_{1}\).

Combining the two objectives, we pretrain the state encoder and action encoder/quantization module by","\mathcal{L}[\theta_{\mathcal{F},\mathcal{G},\mathcal{G},P,\psi}]=\mathcal{L}_{ \text{dyn}}[\theta_{\mathcal{F},\mathcal{G},P}]+\beta\mathcal{L}_{\text{act \_decode}}[\theta_{\mathcal{F},\mathcal{G},\psi}]",,\mathcal{L}_{\text{pretrain}} = \mathcal{L}_{\text{dyn}} + \lambda \cdot \mathcal{L}_{\text{act\_decode}},"The combined pretraining objective is the sum of the latent dynamics loss and a weighted action decoding loss, where $\lambda$ balances the contribution of the two terms."
ICML_2024_oral_79,3,"Throughout the experiment, we set \(\beta=1\) when \(\psi\) is a deterministic decoder, and \(\beta=0.01\) when \(\psi\) is parametrized by GMM to deal with numerical scale of the likelihood loss.

**Pretraining II: temporal action abstractions via BPE.** Figure 1(b) illustrates the mechanics of this stage. After training the action quantizer, we first use the pretrained observation embedding \(\mathcal{G}\), action quantizer \(\mathcal{F}\) to convert the trajectories from pretrainig dataset \(\mathcal{D}\) into sequences of discrete quantized action codes, \(\big{\{}(e_{0}^{(i)},e_{1}^{(i)},...,e_{H_{i}}^{(i)})\big{\}}_{i=1}^{|\mathcal{ D}|}\), where \(e_{t}^{(i)}\in\mathcal{E}\) for all \(t,i\) and \(H_{i}\) stands for the length of episode \(i\). Then, drawing analogy to NLP, we view the sequence of action codes in each trajectory as a ""sentence"" and employ the BPE algorithm to create a vocabulary of skill tokens \(\mathcal{V}=\{\xi_{1},...,\xi_{K}\}\), where tokens represent ""sentence""/trajectory segments. To do so, we initialize BPE using codes from \(\mathcal{E}\) and let BPE construct the vocabulary \(\mathcal{V}\) by iteratively merging the most frequent pairs of action codes or action code sequences into a single token. Different resulting tokens \(\xi\in\mathcal{V}\) may correspond to skills with different horizons \(L_{\xi}\).

**Summary.** Overall, the pretraining of PRISE produces

Figure 1: (**a**) Pretraining Stage I of PRISE : The goal is to learn a action quantization module such that conditioned on the current state and action \((o_{t},a_{t})\), it could assign a discrete action code. (**b**) Pretraining Stage II of PRISE : First it converts a trajectory of continuous state and actions into discrete codes. Then based on the corpus of quantized trajectories from the multitask offline dataset, PRISE applies BPE (illustrated in Figure 2) to learn vocabulary of skill tokens, where each token represents a sequence of discrete action code.

Figure 2: Byte Pair Encoding.

observation embedding \(\mathcal{G}\), action quantizer \(\mathcal{F}\), and a vocabulary \(\mathcal{V}\) of skill tokens. In the following, we discuss how to use them to learn generalist multitask policies and achieve efficient downstream adaptation.

### Multitask generalist policy learning.

Once we have learned the skill tokens that capture the common motion patterns shared across various tasks, we can leverage these skills to learn a multitask generalist policy. We train a high-level skill-token policy \(\pi:\mathcal{Z}\rightarrow\Delta(\mathcal{V})\) that predicts a skill token based on a latent state. At rollout time, \(\pi\) works as follows. At timestep \(t\), suppose \(\pi\) predicts a token \(\xi_{t}\) after seeing observation \(o_{t,T}\) and the token \(\xi_{t}\) corresponds to \(L_{\xi_{t}}\) action codes, without loss of generality, \(e_{t+k}\), \(0\leq k<L_{\xi_{t}}\). Then for the subsequent \(L_{\xi_{t}}\) steps, the agent applies the codes \(e_{t+k}\), \(0\leq k<L_{\xi_{t}}\) in their order, and uses the decoder \(\psi:Z\times\mathcal{E}\) learned in Pretraining I stage to convert each \(e_{t+k}\) to an action \(a_{t+k}\) from the original action space \(\mathcal{A}\). In other words, \(a_{t+k}\sim\psi(z_{t+k},e_{t+k})\) for each \(k\) s.t. \(0\leq k<L_{\xi_{t}}\). After \(L_{\xi_{t}}\) steps, the agent queries \(\pi\) again to choose the next sequence of actions.

To learn this skill-token policy \(\pi\) from a set of expert demonstration trajectories \(\mathcal{D}=\{(o_{0}^{(i)},o_{0}^{(i)},...o_{H_{i}}^{(i)})\}_{i=1}^{|\mathcal{D }|}\), we first run the action quantizer \(\mathcal{F}\) and observation encoder \(\mathcal{G}\) to get the quantized action codes of the trajectories \(\{(c_{0}^{(i)},c_{1}^{(i)},...d_{H_{i}}^{(i)})\}_{i}\), as well as latent observation embeddings \(\{(z_{0}^{(i)},z_{1}^{(i)},...z_{H_{i}}^{(i)})\}_{i}\). Then for each timestep \(t\) in episode \(i\), we compute the target expert token \(\xi_{t}^{(i)}\) by greedily searching for the longest token in the BPE tokenizer's vocabulary that matches the sequence \((c_{t}^{(i)},c_{t+1}^{(i)},...,c_{H_{i}}^{(i)})\).

We then train \(\pi\) by minimizing the cross-entropy loss:","\mathcal{L}_{\textbf{CE}}(\pi(\text{stopgrad}(z_{t})),\xi_{t})[\theta_{\pi}]",,\mathcal{L}_{\text{policy}}[\theta_{\pi}]=-\sum_{i=1}^{|\mathcal{D}|}\sum_{t=0}^{H_{i}}\log\pi(\xi_{t}^{(i)}|z_{t}^{(i)}),"This equation represents the cross-entropy loss for training the high-level skill-token policy π using expert demonstration trajectories, where ξₜⁱ denotes the expert token and zₜⁱ the latent state at timestep t in episode i."
ICML_2024_oral_79,4,"Note, that as stopgrad implies, we freeze the encoder \(\mathcal{G}\).

### Downstream few-shot adaptation to unseen tasks

In addition to learning a generalist multitask policy \(\pi\), we can also use the pretrained skill tokens to adapt \(\pi\) to unseen downstream tasks with a few expert demonstrations. Unlike in the multitask pretraining setting, during finetuning we need to finetune the decoder \(\psi\) for downstream unseen tasks, in addition to adapting \(\pi\) itself.

We optimize \(\psi\) with the following objective, where the summation over training trajectories and time steps \(t\) in each trajectory has been omitted for clarity:","\mathcal{L}_{\textbf{FT\_DECODER}}[\theta_{\pi,\psi}]=\mathbb{E}_{\xi_{t}\sim \pi(\text{stopgrad}(z_{t}))}\Big{[}\mathcal{L}[\theta_{\psi}](\xi_{t})\Big{]} || \mathcal{L}[\theta_{\psi}](\xi_{t})=\sum_{i=0}^{\hat{K}}\mathcal{L}_{\text{ action}}(\psi(\text{stopgrad}(z_{t+i}),\xi_{t}[i]),a_{t+i})","where

\[\mathcal{L}[\theta_{\psi}](\xi_{t})=\sum_{i=0}^{\hat{K}}\mathcal{L}_{\text{ action}}(\psi(\text{stopgrad}(z_{t+i}),\xi_{t}[i]),a_{t+i}) \tag{5}\]

In this equation, \(\hat{K}=\min(K,L_{\xi})\), where \(K\) is a hyperparameter, the motivation behind which is explained at the end of Section 3","\mathcal{L}_{\text{act\_decode}}[\theta_{\psi}]=\|\psi(z_{t},e_{t})-a_{t}\|_{1}",Objective for fine-tuning the action decoder $\psi$ using $L1$-loss during downstream adaptation to unseen tasks.
ICML_2024_oral_79,5,"In this equation, \(\hat{K}=\min(K,L_{\xi})\), where \(K\) is a hyperparameter, the motivation behind which is explained at the end of Section 3.4. \(\xi_{t}[i]\) represents the \(i\)-th code of the skill token \(\xi_{t}\).

We optimize skill-token policy \(\pi\) w.r.t. the cross-entropy loss \(\mathcal{L}_{\textbf{CE}}\) in (3). Thus, the overall objective is","\mathcal{L}[\theta_{\pi,\psi}]=\mathcal{L}_{\textbf{CE}}(\pi(\text{stopgrad}(z _{t})),\xi_{t})+\mathcal{L}_{\textbf{FT\_DECODER}}[\theta_{\pi,\psi}]",,"\mathcal{L}_{\textbf{FT}}[\theta_{\pi,\psi}]=\mathcal{L}_{\textbf{CE}}[\theta_{\pi}]+\lambda\mathcal{L}_{\textbf{FT\_DECODER}}[\theta_{\pi,\psi}]",The overall finetuning objective combines the cross-entropy loss for the skill-token policy with a weighted decoder finetuning loss.
ICML_2024_oral_84,1,"### Modeling Sample Environments

Consider \(\hat{p}(e)\) as the oracle distribution of the target environment we want to deploy our robots in. Our goal is to generate behavior that is performant (with respect to objective \(J\)) and robust under \(\hat{p}(e)\),",\max_{\pi}\mathbb{E}_{\hat{e}\sim\hat{p}(e)}J(\pi;\hat{e}),,"\max_{\theta} \mathbb{E}_{e \sim \hat{p}(e)} [J(\theta, e)]",Maximizing the expected performance objective $J$ under the oracle environment distribution $\hat{p}(e)$ with respect to policy parameters $\theta$.
ICML_2024_oral_84,2,"The target environment can either be a specific real-world environment that already exists (e.g. kitchen at a specific location) or a generic concept (e.g. typical household kitchen).

Unfortunately, it's extremely difficult to model this oracle

Figure 1: Flowchart of a typical behavior generation pipeline using reinforcement learning with simulation, illustrating four distinct subtasks of sample environment modeling, environment shaping, RL training, and outer feedback loop with behavior evaluation and reflection. We highlight the manual, task-driven **environment shaping** as a key, yet often overlooked, bottleneck in generalizing the success of RL. We thus advocate for automating the environment shaping process to broaden RL’s applicability.

distribution either way; it requires comprehensive knowledge of all possible environmental variables and conditions, which is often infeasible due to the complexity, variability, and limited observability in the real world. Innate vagueness of generic concepts is often an issue as well. Just imagine modeling a true oracle distribution of a chaotic real-world dishwasher in simulation! (Figure 2)

In contrast, modeling a _single_ sample environment, \(\hat{e}\), a specific instance drawn from oracle environment distribution,","\hat{e}\sim\hat{p}(e),",,\hat{e} \sim \hat{p}(e),A specific instance $\hat{e}$ drawn from the oracle environment distribution $\hat{p}(e)$ represents a single sample environment used for behavior generation and evaluation.
ICML_2024_oral_84,3,"and importing that to a simulation is much more feasible. This is why robotics practitioners typically start the behavior generation process by first designing a single sample environment: (a) modeling and importing robots and necessary assets in simulation, and (b) manually placing them in their default poses. We often generate a _set_ of those sample environments to kick things off. This is what practitioners do for the first blue box in Figure 1.

Such a set of sample environments actually serves a purpose: it is a useful representative testbed environment that can be used to estimate the behavior performance under the true oracle distribution \(\hat{p}(e)\). Combined with any form of task specification \(r\)(Agrawal, 2022), we define a simulated testbed \(\mathcal{E}^{\text{test}}\) where the trained behaviors can be evaluated.

**Definition 2.1** (Test Environment).: Let \(\{\hat{e}_{1},\cdots,\hat{e}_{n}\}\) be a set of \(n\)_sample environments_ each independently drawn from oracle environment distribution \(\hat{p}(e)\). The simulated counterparts are denoted as \(\hat{e}_{\text{sim},i}\). Given a task specification \(r\), a test environment \(\mathcal{E}^{\text{test}}\) is defined as a set of tuples:","\mathcal{E}^{\text{test}}=\{\langle\hat{e}_{\text{sim},i},r\rangle\}_{i=1, \cdots,n},",where the generated behavior \(\pi\) will be evaluated in,"\mathcal{E}^{\text{test}} = \left\{ \left(\hat{e}_{\text{sim},1}, r\right), \ldots, \left(\hat{e}_{\text{sim},n}, r\right) \right\}",Test environment defined as a finite set of tuples containing simulated sample environments and task specifications for behavior evaluation under oracle distribution
ICML_2024_oral_84,4,"in.

Meanwhile, to prevent the behavior from overfitting to a few sample environments within \(\mathcal{E}^{\text{test}}\), one can maintain two sets of sample environments; the test environment can be strictly held out from the rest of the behavior generation pipeline, letting it serve the sole purpose of behavior evaluation. A distinct set of sample environments then can be used to effectively guide the remaining design choices. We call that a _reference_ environment, \(\mathcal{E}^{\text{ref}}\), as illustrated in Figure 1.

**Definition 2.2** (Reference Environment).: A Reference Environment \(\mathcal{E}^{\text{ref}}\) is a distinct set of sample environments that provides useful context to the shaping algorithm. Trained behaviors will not be evaluated here to avoid overfitting.

For instance, when designing a robotic behavior for unloading a dishwasher, sample environments would include multiple instances of dishwashers loaded with varying configurations of dishes and utensils. Sampled configurations then could be split into a set of reference environments for guiding the shaping and a set of held-out test environments for evaluation. This diversity in configurations provides the shaping algorithm with a broad context, incentivizing it to infer the underlying distribution of object placements. The goal is for the reinforcement learning (RL) trainer to sample from this inferred distribution during training, ensuring that the generated behaviors are robust and adaptable to various real-world scenarios without overfitting to a specific set of environments.

### Shaping Reference Environments

A straightforward subsequent step of behavior generation might be to directly use the reference environments \(\mathcal{E}^{\text{ref}}\) as an RL environment, with the expectation that algorithms like Proximal Policy Optimization (PPO) (Schulman et al., 2017) will find performant and generalizable behaviors. However, this approach often falls short due to the inherent sparsity of these environments.

Reference environments often present a challenging optimization landscape for RL algorithms due to their _sparse_ nature. For example, it might be rare to obtain nonzero rewards or the observation space might be dominated by spurious features that encourage poor local minima. To mitigate these challenges, human engineers typically go through a process of _shaping_ the elements of \(\mathcal{E}^{\text{ref}}\). This modification, aimed at enhancing the _learnability_ of the environment, includes introducing denser learning signals and additional modifications encouraging effective exploration. The resulting _shaped environment_(Co-Reyes et al., 2020) then used as a training ground for the subsequent optimal control solver.

**Definition 2.3** (Shaped Environment).: A _Shaped Environment_\(\mathcal{E}^{\text{shaped}}\) is a modification of a reference environment, i.e., \(\mathcal{E}^{\text{shaped}}=f(\mathcal{E}^{\text{ref}})\). The transformation \(f\) incorporates design choices specifically optimized for learning performance, smoothing the optimization landscape enabling the solver to find better solutions with maximal performance in

Figure 2: Example of environment complexity: an overloaded and disorganized real-world dishwasher.

\(\mathcal{E}^{\text{test}}\). The transformation function",f:\mathcal{E}^{\text{ref}}arrow\mathcal{E}^{\text{shaped}},,\mathcal{E}^{\text{shaped}} = f(\mathcal{E}^{\text{ref}}),where the transformation function $f$ modifies the reference environment to enhance learnability and optimize performance in the test environment
ICML_2024_oral_84,5,"is defined as _shaping function_.

The _shaping_ process for a robotics environment usually involves manually explored design choices, including shaping the reward (Ng et al., 1999; Singh et al., 2009), modifying the action space (Peng and Van De Panne, 2017; Aljalbout et al., 2023), designing curricula on the environment dynamics, initial state distributions, and goal distributions (Selfridge et al., 1985; Matiisen et al., 2019; Portelas et al., 2020; Lee et al., 2020), crafting the state space (Yu et al., 2023), and shaping the right failure conditions for early termination (Co-Reyes et al., 2020). We describe details of common shaping operations in Sec 3.1.

### RL Training

Once the shaped environment is obtained, the next step of behavior generation is to use RL algorithms, i.e., PPO (Schulman et al., 2017), to find a behavior \(\pi\) that best performs on the shaped environment \(\mathcal{E}^{\text{shaped}}\). Formally, the algorithm aims to find an optimal behavior \(\pi\) for the following optimization problem:","\max_{\pi}\mathbb{E}_{\tau\sim\pi}[\sum_{t=0}^{T}\gamma^{t}r _{t}(s_{t},a_{t}))] || \text{s.t.}\ \ \ s_{t+1}\sim p(s_{t},a_{t};\mathcal{E}^{\text{shaped }}).",,\pi^{\text{shaped}} = \arg\max_{\pi} \mathbb{E}_{e \sim \mathcal{E}^{\text{shaped}}} J(\pi; e),Optimal behavior trained on the shaped environment to maximize expected performance across environments in $\mathcal{E}^{\text{shaped}}$.
ICML_2024_oral_84,6,"While the RL training process also requires a range of design decisions, such as algorithmic choices and hyperparameter adjustments, these areas are relatively well-researched and documented (Parker-Holder et al., 2022; Kiran and Ozyildirim, 2022).

However, we note that in a pratical context of robotic behavior generation, tuning the RL setting (e.g. neural architecture search for policy or hyperparameter tuning) is often underprioritized compared to the effort put into environment shaping. In IsaacGymEnvs (Makoviychuk et al., 2021), for instance, simple Multilayer Perceptron (MLP) networks are employed, and off-the-shelf RL algorithm implementations are utilized with their default configurations. This shows that algorithms like PPO and their default settings are capable enough when paired with _ideally shaped_ environment.

### Optimizing Environment Shaping via Iterative Behavior Evaluation and Reflection

Once an optimal behavior \(\pi^{\star}\) is obtained via RL training, the behavior is _evaluated_ on the test environment \(\mathcal{E}^{\text{test}}\) and _reflected_ by human engineers. Denoting the reflection process as \(\mathcal{H}\) of analyzing the generated behavior \(\pi^{\star}\) in test environment \(\mathcal{E}^{\text{test}}\) and coming up with a better environment shaping \(f\),","\mathcal{H}:f_{k}\times J(\pi^{\star}_{k};\mathcal{E}^{\text{test}}) \to f_{k+1},",,"\pi^{\star}=\arg\max_{\pi}\mathbb{E}_{\tau\sim\pi}[\sum_{t=0}^{T}\gamma^{t}r_{t}(s_{t},a_{t}))]\ \text{s.t.}\ s_{t+1}\sim p(s_{t},a_{t};\mathcal{E}^{\text{shaped}}),","Optimal behavior $\pi^{\star}$ obtained via RL training on the shaped environment $\mathcal{E}^{\text{shaped}}$, maximizing expected cumulative reward under the learned policy dynamics."
ICML_2024_oral_84,7,"robotic behavior generation process can be formally defined as an **iterative optimization process** over the **environment shaping function**\(f\),","f_{k+1}=\mathcal{H}(f_{k},J(\pi^{\star}_{k};\mathcal{E}^{ \text{test}})) || \text{where}\ \ \pi^{\star}_{k}=\underset{\pi}{\text{argmax}}\ J(\pi;\mathcal{E}^{\text{shaped}}_{k}), || \mathcal{E}^{\text{shaped}}_{k}=f_{k}(\mathcal{E}^{\text{ref}}), \ f_{0}=\mathbf{I}_{\text{identity}}",,"\begin{aligned} & \text{Given } \mathcal{E}^{\text{ref}},\ \mathcal{E}^{\text{test}},\\ & \pi_{0}^{\star} \leftarrow \text{RL}(\mathcal{E}^{\text{shaped}} = f_{0}(\mathcal{E}^{\text{ref}})),\\ & \text{For } k = 0,1,\dots,K:\\ & \quad f_{k+1} \leftarrow \mathcal{H}(f_{k},\ J(\pi_{k}^{\star};\ \mathcal{E}^{\text{test}})),\\ & \quad \mathcal{E}_{k+1}^{\text{shaped}} \leftarrow f_{k+1}(\mathcal{E}^{\text{ref}}),\\ & \quad \pi_{k+1}^{\star} \leftarrow \text{RL}(\mathcal{E}_{k+1}^{\text{shaped}}). \end{aligned}","This iterative process alternates between RL training on a shaped environment, behavior evaluation on the test environment, and human-guided reflection to refine the shaping function for improved robustness and generalization."
ICML_2024_oral_84,8,which aims to find an optimal shaping function \(f\in\mathcal{F}\) for the following bi-level optimization problem:,"\max_{f\in\mathcal{F}} J(\pi^{\star};\mathcal{E}^{\text{test}}) || \text{s.t.}\ \ \ \pi^{\star}\in\arg\max_{\pi}J(\pi;\mathcal{E}^{\text{shaped }}),\ \ \mathcal{E}^{\text{shaped}}=f(\mathcal{E}^{\text{ref}}).",,\max_{f\in\mathcal{F}}\ J\left(\underset{\pi}{\text{argmax}}\ J(\pi;f(\mathcal{E}^{\text{ref}}));\ \mathcal{E}^{\text{test}}\right),"Equation 8 formulates the bi-level optimization problem that aims to find the optimal environment shaping function $f$ by maximizing the performance of the RL-trained policy $\pi$ on the test environment $\mathcal{E}^{\text{test}}$, where $\pi$ itself is obtained by maximizing its performance on the shaped reference environment $f(\mathcal{E}^{\text{ref}})$."
ICML_2024_oral_93,1,"### Problem Definition

Existing methods within heatmap-guided MCTS use the problem setting of 2D Euclidean TSP, and our study adheres to this established problem setting. We consider a TSP instance as an input graph with \(n\) vertices in a two-dimensional space, represented by \(s=\{x_{i}\}_{i=1}^{n}\) where \(x_{i}\in[0,1]^{2}\). The goal is to find a permutation \(\mathbf{\pi}=(\pi_{1},\pi_{2},...,\pi_{n})\) that forms a tour, visiting each vertex once and returning to the start, with the objective to minimize the total path length \(c(\mathbf{\pi})\), calculated as:","c(\mathbf{\pi})=\|x_{\pi_{n}}-x_{\pi_{1}}\|_{2}+\sum_{i=1}^{n-1}\| x_{\pi_{i}}-x_{\pi_{i+1}}\|_{2},",where \(\|\cdot\|_{2}\) denotes the \(\ell_{2}\) norm,c(\mathbf{\pi}) = \sum_{i=1}^{n-1} \lVert x_{\pi_i} - x_{\pi_{i+1}} \rVert_2 + \lVert x_{\pi_n} - x_{\pi_1} \rVert_2,"Equation 1 defines the total path length $c(\mathbf{\pi})$ as the sum of Euclidean distances between consecutive vertices in the tour permutation $\mathbf{\pi}$, including the return from the last vertex to the first."
ICML_2024_oral_93,2,"rm.

### Heatmap Generation

In the context of large-scale TSP, recent state-of-the-art approaches blend ML and OR, where ML models do not predict a solution (i.e., a permutation \(\mathbf{\pi}=(\pi_{1},\pi_{2},...,\pi_{n})\) of all the vertices) outright but alter the solution space distribution. Specifically, trained models predict an \(n\times n\) heatmap \(\Phi\), where \(\Phi_{i,j}\) indicates the suitability of including edge \((i,j)\) in the solution. The optimization problem's objective is defined as:","\mathcal{L}(\theta)=\mathbb{E}_{\mathbf{\pi}\sim\mathcal{S}}[\mathbb{E}_{\Phi \sim f_{\theta}(s)}[\mathbb{E}_{\mathbf{\pi}\sim g(s,\Phi)}[c(\bm {\pi})]]],","where \(s\) represents an instance from distribution \(\mathcal{S}\), \(\theta\) is the trainable parameters of model \(f\), \(\mathbf{\pi}\) is the solution outputed by post-hoc search algorithm \(g\) given \(\Phi\), and \(c(\mathbf{\pi})\) is calculated based on Equation 1","\max_{\mathbf{\pi}} \sum_{(i,j)\in E(\mathbf{\pi})} \Phi_{i,j},","where $E(\mathbf{\pi})$ denotes the set of edges in the tour defined by permutation $\mathbf{\pi}$, and the objective maximizes the sum of heatmap values for edges in the tour."
ICML_2024_oral_93,3,"1.

Given the non-differentiable and computationally intensive nature of \(\mathbb{E}_{\mathbf{\pi}\sim g(s,\Phi)}\left[c\left(\mathbf{\pi}\right)\right]\), a surrogate loss \(\ell\left(s,\Phi\right)\), which is both differentiable and easy to compute, is often em ployed, leading to a surrogate objective:","\mathcal{L}_{\textit{surrogate}}(\theta)=\mathbb{E}_{s\sim\mathcal{S}}[ \mathbb{E}_{\Phi\sim f_{\theta}(s)}[\ell(s,\Phi)]].",,"\mathcal{L}_{\text{surrogate}}(\theta) = \mathbb{E}_{s \sim \mathcal{S}}\left[\ell\left(s, f_{\theta}(s)\right)\right].","where $\ell(s, \Phi)$ is a differentiable surrogate loss function that approximates the expected cost of solutions generated by the search algorithm $g$ given the heatmap $\Phi$."
ICML_2024_oral_93,4,"This surrogate loss, designed heuristically, can take forms of supervised (Fu et al., 2021; Sun and Yang, 2023), unsupervised (Min et al., 2023), or reinforcement learning (Qiu et al., 2022), where the optimized \(\theta^{*}\) from minimizing \(\mathcal{L}_{\textit{surrogate}}(\theta)\) is aimed to approximate the optimal \(\theta\) obtained from the original loss, i.e., \(\theta^{*}\approx\text{argmin}_{\theta}\mathbb{E}_{s\sim\mathcal{S}}\left[ \mathbb{E}_{\Phi\sim f_{\theta}(s)}\left[\mathbb{E}_{\mathbf{\pi}\sim g(s,\Phi)} \left[c\left(\mathbf{\pi}\right)\right]\right]\right]\). However, this approximation often lacks a rigorous theoretical foundation, making it uncertain whether minimizing the surrogate loss genuinely aligns with optimizing the original TSP objective. Consequently, despite optimizing \(\theta^{*}\) for the surrogate loss, its efficacy in guiding MCTS to find optimal solutions during testing remains questionable. During inference, the output heatmap \(\Phi^{*}\) from \(f_{\theta^{*}}(s)\) is fed into the search algorithm \(g\), yielding the solution \(\mathbf{\pi}^{*}\sim g(s,\Phi^{*})\). This disconnect between training and test phases--where training focuses on heatmap generation without involving MCTS, while testing relies on MCTS guided by these heatmaps--highlights a potential misalignment in the approach, as depicted in Figure 2.

### Monte Carlo Tree Search

MCTS is utilized as a guided \(k\)-opt process, which iteratively refines a complete TSP solution \(\mathbf{\pi}\) by alternating edge deletions and additions. The selection of edges during \(k\)-opt is influenced by a weight matrix \(W\) and an access matrix \(Q\), both of which are dynamically updated based on \(k\)-opt outcomes. Here, \(W_{i,j}\) scores the suitability of edge \((i,j)\) in the solution, while \(Q_{i,j}\) records the number of times edge \((i,j)\) is selected. Note that this section covers only the key aspects of MCTS. For a detailed understanding, please refer to Fu et al. (2019, 2021); Min et al. (2023).

Initialization.The heatmap \(H\) initializes \(W\) (\(W_{i,j}=100\times H_{i,j}\)). The access matrix \(Q\) starts with all elements set to zero. Edge potential matrix \(Z\) guides the \(k\)-opt process, balancing exploitation and exploration. The edge potential \(Z_{i,j}\) is formulated as \(Z_{i,j}=\frac{W_{i,j}}{\Omega_{i}}+\alpha\sqrt{\frac{\ln(M+1)}{Q_{i,j}+1}}\), where \(\Omega_{i}\), the average weight of edges connected to vertex \(i\), is \(\Omega_{i}=\frac{\sum_{j\neq i}W_{i,j}}{\sum_{j\neq i}1}\), \(\alpha\) balances exploitation and exploration, and \(M\) is the total number of actions sampled so far.

A random initial tour \(\mathbf{\pi}\) is constructed and optimized using 2-opt. The initial tour construction probability is formulated as \(p(\mathbf{\pi})=p(\pi_{1})\prod_{i=2}^{n}p(\pi_{i}|\pi_{i-1})\), where \(p(\pi_{i}|\pi_{i-1})\) is the conditional probability of choosing the next vertex, calculated by the edge potential:","p(\pi_{i}|\pi_{i-1})=\frac{Z_{\pi_{i-1},\pi_{i}}}{\sum_{l\in\mathbb{X}_{\pi_{ i-1}}}Z_{\pi_{i-1},l}},",,"p(\pi_{i}|\pi_{i-1}) = \frac{Z_{\pi_{i-1},\pi_{i}}}{\sum_{j\notin \mathcal{V}_{\text{visited}}}Z_{\pi_{i-1},j}},","where $\mathcal{V}_{\text{visited}}$ denotes the set of vertices already included in the partial tour, ensuring that each vertex is visited only once during the tour construction process."
ICML_2024_oral_93,5,"with \(\mathbb{X}_{\pi_{i-1}}\) includes candidate vertices connected to \(\pi_{i-1}\), selected based on their edge potential value.

\(k\)-opt Search.Each \(k\)-opt action is represented as a vertex decision sequence \((a_{1},b_{1},a_{2},b_{2},\ldots,a_{k},b_{k},a_{k+1})\) with \(a_{k+1}=a_{1}\). This sequence involves deleting \(k\) edges \((a_{i},b_{i})\) and adding \(k\) new edges \((b_{i},a_{i+1})\) for \(1\leq i\leq k\). Given \(b_{i}\), the subsequent vertex \(a_{i+1}\) is sampled based on Equation 4. The tour \(\mathbf{\pi}\) is transformed into \(\mathbf{\pi}^{\text{new}}\), and metrics \(M\), \(Q_{b_{i},a_{i+1}}\), and \(Q_{a_{i+1},b_{i}}\) are updated.

Backpropagation.Upon obtaining a better solution \(\mathbf{\pi}^{\text{new}}\) with \(c(\mathbf{\pi}^{\text{new}})<c(\mathbf{\pi})\), the weights of the newly added edges during the \(k\)-opt action are increased by \(\beta\left[\text{exp}\left(\frac{c(\mathbf{\pi})-c(\mathbf{\pi}^{\text{new}})}{c(\mathbf{ \pi})}\right)-1\right]\), where \(\beta\) is the update rate.

## 4 Proposed Baseline

### Motivation

Machine learning methods for generating TSP heatmaps usually rely on surrogate loss functions (Equation 3) due to the computational challenges of the original loss (Equation 2). This substitution, often without theoretical justification, can lead to inconsistent performance in the test phase. This inconsistency is particularly concerning because MCTS, which is critical for determining the final solution, is not integrated during neural network training. In response, we introduced SoftDist, a baseline method that incorporates MCTS into the training process, thus directly optimizing the original TSP objective. However, the direct optimization of the original TSP loss presents challenges due to its non-differentiability and the time-consuming nature of deriving solutions via MCTS. Therefore, our aim with SoftDist is to simplify the optimization process by reducing the number of tunable parameters, effectively addressing these challenges.

### SoftDist Baseline

We introduce a novel method for generating heatmaps, termed SoftDist, based on applying softmax to the distance matrix of a TSP instance. The heatmap \(\Phi\) allocates scores to each edge \((i,j)\) as follows:","\Phi_{i,j}=\frac{e^{-d_{i,j}/\tau}}{\sum_{k\neq i}e^{-d_{i,k}/\tau}},","where \(d_{i,j}=\left\|x_{i}-x_{j}\right\|_{2}\), and \(\tau\) is a parameter controlling the smoothness of the score distribution in \(\Phi\)",,
ICML_2024_oral_93,6,"\). This simplicity, with only one parameter to optimize, sets SoftDist apart from more complex models and aligns with our aim to simplify the optimization process.

Moreover, our SoftDist method requires no supervision, significantly reducing its training complexity, especially beneficial for large-scale TSPs where obtaining high-quality labelsis both expensive and challenging. Its inherent simplicity also ensures minimal hardware resource consumption, making it a highly practical option in various computational environments. This aspect of SoftDist underscores its efficiency and accessibility, further distinguishing it from more complex, resource-intensive ML models.

SoftDist's design prioritizes shorter edges while maintaining a balance between exploration and exploitation. This strategy is crucial for avoiding suboptimal, greedy solutions. By allocating scores inversely proportional to edge distances, moderated by \(\tau\), SoftDist encourages structured exploration, aiding in superior solution discovery for large-scale TSPs.

During training, as illustrated in Figure 2(a), SoftDist directly optimizes the original TSP objective, contrasting with the surrogate objectives used by other methods (see Figure 1(a)). Owing to its single tunable parameter, \(\tau\), SoftDist's optimization is straightforward, employing even the most basic optimization techniques like grid search. Once optimized, in the test phase, SoftDist generates heatmaps to guide MCTS, merging the training and test phases cohesively, as shown in Figure 2(b). This alignment ensures that the heatmap's effectiveness in training directly translates to performance in testing.

## 5 Proposed Metric

### Motivation

MCTS and LKH-3, both handcrafted heuristic algorithms, have several similarities, such as their reliance on local operators (Reeves, 1993) and self-adaptive strategies for edge selection. This similarity lays the foundation for a comparative analysis between MCTS, particularly when guided by ML-generated heatmaps, and LKH-3, a leading heuristic solver for various routing problems. Past ML solvers for TSP (Vinyals et al., 2015; Bello et al., 2016; Kool et al., 2019; da Costa et al., 2020; Kwon et al., 2020; Ma et al., 2021; 2023) did not directly compare with LKH-3 due to differences in programming languages (e.g., Python vs. C++) and computational resources (e.g., GPU vs. CPU). Additionally, these ML solvers were designed as general-purpose solvers, typically independent of problem-specific features, while LKH-3 is a specialized solver relying on expert knowledge, tailored to specific types of problems, making direct comparisons unfair. However, MCTS and LKH-3, both being problem-specific algorithms, share the same implementation environment, raising an important question: How does MCTS, even with external heatmap guidance, compare in performance to LKH-3?

### Score Metric

To establish an objective comparison between MCTS and LKH-3, we introduce the _Score_ metric, designed to assess the relative efficiency of MCTS compared to LKH-3 under identical programming and hardware conditions. The _Score_ is calculated as the ratio of the performance gaps of LKH-3 and MCTS:","\textit{Score}=\frac{\textit{Gap}_{\text{LKH-3}}}{\textit{Gap}_{\text{MCTS}}},",where \(\textit{Gap}_{\text{LKH-3}}=\frac{L_{\text{LKH-3}}}{L^{*}}-1\) and \(\textit{Gap}_{\text{MCTS}}=\frac{L_{\text{MCTS}}}{L^{*}}-1\),,
ICML_2024_oral_95,1,"### Representations are converging across modalities

Do models trained on different data modalities also converge? Several works indicate that the answer is _yes_.

Merullo et al. (2022) extended model stitching to the cross-modal setting, finding that a single linear projection is suffi

Figure 2: **VISION models converge as COMPETENCE increases:** We measure alignment among \(78\) models using mutual nearest-neighbors on Places-365 (Zhou et al., 2017), and evaluate their performance on downstream tasks from the Visual Task Adaptation Benchmark (VTAB; Zhai et al. (2019)). **LEFT:** Models that solve more VTAB tasks tend to be more aligned with each other. Error bars show standard error. **RIGHT:** We use UMAP to embed _models_ into a 2D space, based on \(\text{distance}\triangleq-\log(\text{alignment})\). More competent and general models (blue) have more similar representations.

cient to stitch a vision model to an LLM and achieve good performance on visual question answering and image captioning. Koh et al. (2023) showed that linear stitching can also work in the opposite direction, aligning text inputs to visual outputs. In fact, many recent language-vision models stitch pre-trained language and vision models together. For example, LLaVA (Liu et al., 2023) demonstrated state-of-the-art results by projecting visual features into a language model with a 2-layer MLP.

Other works show further kinds of evidence of cross-modal synergy. OpenAI (2023) found that jointly training a language model with a vision model improves performance on language tasks, compared to training the language model on its own. Sorscher et al. (2022) show a setting in which word embeddings of visual concept names can be isometrically mapped to image embeddings for those same concepts. Sharma et al. (2024) probed the visual knowledge of LLMs trained _only_ on language data, by converting images into code that an LLM can process. They found that LLMs have rich knowledge of visual structures, to the extent that decent visual representations can be trained on images generated solely by querying an LLM to produce code and rendering the response. In visual generation, LLMs show abilities to augment captions with visual structures (_e.g._, bounding boxes) and improve generation quality (Betker et al., 2023; Lian et al., 2023; Wu et al., 2023; Wu et al., 2023). Over other modalities, Ngo and Kim (2024) showed auditory models are also roughly aligned with LLMs up to a linear transformation, and Ng et al. (2023) demonstrated the effectiveness of using pre-trained LLMs for facial motion prediction.

We set out to address these claims in a broader scope to determine whether models are indeed learning an increasingly modality-agnostic representation of the world. We sampled a variety of models trained either solely on vision or solely on language, and compared their representations as they became larger and more competent over many tasks.

In Fig. 3, we assess alignment between a suite of language models and vision models. So far we have only defined alignment for two kernels defined over the same input space. To measure cross-modal alignment, we use paired datasets to bridge the two modalities. For vision and text, we use the Wikipedia captions dataset \(\{(x_{i},y_{i})\}_{i}\)(Srinivasan et al., 2021), composed of images from Wikipedia (\(x_{i}\)) and their corresponding captions (\(y_{i}\)). We then measure alignment between a language model \(f_{\text{text}}\) and a vision model \(f_{\text{ing}}\) as the alignment of the two following kernels:","K_{\text{img}}(i,j)=\langle f_{\text{img}}(x_{i}),f_{\text{img}}(x_{j})\rangle || K_{\text{text}}(i,j)=\langle f_{\text{text}}(y_{i}),f_{\text{text}}(y_{j})\rangle.",,"\begin{equation}
    K_{\text{text}}(y_i, y_j) = \langle f_{\text{text}}(y_i), f_{\text{text}}(y_j) \rangle, \quad
    K_{\text{vision}}(x_i, x_j) = \langle f_{\text{vision}}(x_i), f_{\text{vision}}(x_j) \rangle
\end{equation}",Equation 1 defines the text and vision kernels used to measure cross-modal alignment between language and vision models using paired image-caption data.
ICML_2024_oral_95,2,"Using this analysis, we find that the better an LLM is at language modeling, the more it tends to aligns with vision models, as shown in Fig. 3. The converse effect also holds: the better a vision models is, the more it tends to align with LLMs. See Appendix C.2 for more details.

### Models are increasingly aligning to brains

Neural networks also show substantial alignment with biological representations in the brain (Yamins et al., 2014). This commonality may be due to similarities in the task and data constraints both systems are confronted with. Even though the mediums may differ - silicon transistors versus biological neurons - the fundamental problem faced

Figure 3: **LANGUAGE and VISION models align:** We measure alignment using mutual nearest-neighbor on the Wikipedia caption dataset (WIT) (Srinivasan et al., 2021). The x-axis is the language model performance measured over 4M tokens from the OpenWebText dataset (Gokaslan and Cohen, 2019) (see Appendix B for plots with model names). We measure performance using \(1-\text{bits-per-byte}\), where \(\text{bits-per-byte}\) normalizes the cross-entropy by the total bytes in the input text string. The results show a linear relationship between language-vision alignment and language modeling score, where a general trend is that more capable language models align better with more capable vision models. We find that CLIP models, which are trained with explicit language supervision, exhibit a higher level of alignment. However, this alignment decreases after being fine-tuned on ImageNet classification (labeled CLIP (I12K ft)).

by brains and machines is the same: efficiently extracting and understanding the underlying structure in images, text, sounds, _etc._(Barlow et al., 1961; Olshausen and Field, 1997). Sorscher et al. (2022) developed a theoretical framework for how the efficient extraction of novel concepts occurs for both the human visual system and deep networks. The tasks that the human visual system has been honed to perform through evolution - like segmentation, detection, and whole-image classification - are also the ones that we train our neural nets to perform. Yamins et al. (2014) went as far as to title their work in the spirit that performance over such tasks implies brain alignment. Antonello and Huth (2024) posited that it is less the particular task and more the generality of the representations that explain their alignment with biological representations. Further, Conwell et al. (2022) showed that training data plays a large role in alignment. Psychophysical studies have also shown agreement between how humans perceive visual similarity and how models do, even when the models are trained on tasks, such as self-supervised prediction, that are seemingly unrelated to mimicking human perception (Zhang et al., 2018).

### Does alignment predict downstream performance?

If models are converging towards a more accurate representation of reality, we expect that alignment should correspond to improved performance on downstream tasks. Figs. 4 and 13 support this hypothesis by showing improved performance on commonsense reasoning (Hellaswag; Zellers et al. (2019)) and mathematical problem solving (GSM8K; Cobbe et al. (2021)) as alignment improves.

## 3 Why are representations converging?

Modern machine learning models are generally trained to minimize the empirical risk with possible implicit and/or explicit regularization:

In the following sections, we lay out how each colored component in this optimization process potentially plays a role in facilitating representational convergence.

### Convergence via Task Generality

Each training datapoint and objective (task) places an additional constraint on the model. As data and tasks scale, the volume of representations that satisfy these constraints must proportionately grow smaller, as visualized in Fig. 5 (left) and stated below:

**The Multitask Scaling Hypothesis**

There are fewer representations that are competent for \(N\) tasks than there are for \(M<N\) tasks. As we train more general models that solve more tasks at once, we should expect fewer possible solutions.

This has been previously termed as the Contravariance principle by Cao and Yamins (2024), which states that the set of solutions to an easy goal is large, while the set of solutions to a challenging goal is comparatively smaller. Moreover, we argue that this narrower solution set also generalizes better. As data scales, models that optimize the empirical risk \(\mathbb{E}_{x\sim\texttt{dataset}}[\mathcal{L}(f,x)]\) also improve on the population risk \(\mathbb{E}_{x\sim\texttt{reality}}[\mathcal{L}(f,x)]\), and become better at capturing statistical structures of the true data generating process (reality).

Recent work has demonstrated a power law relationship between data scale and model performance (Hestness et al., 2017). This implies that with enough data (_e.g._, consisting of the entire internet and all offline scientific measurements) one ought to converge to a very small solution set with irreducible error - the inherent epistemic uncertainty of the world. As more models are trained on internet-scale data, the set of solutions that satisfies all data constraints must become relatively small.

In addition to data-scaling, many modern representation learning objectives \(\overline{\mathbb{E}}(f,x)\) directly optimize for multi-task solving. Contrastive learning finds a distance structure over data samples that optimizes many classification tasks (Arora et al., 2019; Wang and Isola, 2020; Tian et al., 2020). Masked Autoencoders (He et al., 2021) optimize randomly sampled reconstruction tasks. In fact, autoregressive language modeling can also be seen as optimizing a diverse set of tasks (Radford et al., 2019). Such multi-task objectives

Figure 4: **Alignment predicts downstream performance:** We visualize correlation between LLM alignment score to DINOv2 (Oquab et al., 2023) and downstream task performance on Hellaswag (common-sense) (Zellers et al., 2019). LLMs are plotted with radii proportional to the size of the model, and color-coded by their rank order in language modeling scores (\(1-\texttt{bits-per-byte}\)). We observe that models aligned more closely with vision also show better performance on downstream language tasks. See Appendix F for additional results on GSM8K (math) (Cobbe et al., 2021).

may be more effective than single-task ones (_e.g._, ImageNet classification) due to the fact that they impose more task constraints on the representation, leading to a smaller and higher-quality solution space (Chen et al., 2020; He et al., 2020; Radford et al., 2017, 2019).

### Convergence via Model Capacity

Suppose there is a globally optimal representation for standard learning objectives. Then, under sufficient data, _scaling_ a model (_i.e._, using larger function classes ), as well as improved optimization, should be more effective at finding better approximations to this optimum, as shown in Fig. 5 (right). With the same training objective, larger models, even of different architectures, will thus tend to converge toward this optimum. When different training objectives share similar minimizers, larger models are better at finding these minimizers, and will train to similar solutions over the training tasks. We summarize this hypothesis as follows:

**The Capacity Hypothesis**

Bigger models are more likely to converge to a shared representation than smaller models.

### Convergence via Simplicity Bias

Arriving at the same mapping on the _training data_ does not prohibit the models from developing distinct internal representations. It is not unreasonable to posit that the representations used to detect a dog in a 1M parameter model could be quite different than that used by a 1B parameter model. What would stop a billion-parameter (and counting) model from learning an overly complicated and distinct representation? One key factor might be simplicity bias:

**The Simplicity Bias Hypothesis**

Deep networks are biased toward finding simple fits to the data, and the bigger the model, the stronger the bias. Therefore, as models get bigger, we should expect convergence to a smaller solution space.

Such simplicity bias could be coming from explicit regularization commonly used in deep learning (_e.g._, weight decay and dropout). However, even in the absence of external influences, deep networks naturally adhere to Occam's razor, implicitly favoring simple solutions that fit the data (Solomonoff, 1964; Gunasekar et al., 2018; Arora et al., 2019; Valle-Perez et al., 2019; Huh et al., 2023; Dingle et al., 2018; Goldblum et al., 2023). Fig. 5 (middle) visualizes how simplicity bias can drive convergence.

## 4 What representation are we converging to?

By now, we hope to have convinced the reader that task and data pressures, combined with increasing model capacity, can lead to convergence. We next turn our attention to _what_ exactly is the endpoint of all this convergence.

Our central hypothesis, stated in Fig. 1, is that the representation we are converging toward is a statistical model of the underlying reality that generates our observations. Consistent with the multitask scaling hypothesis, such a representation would naturally be useful toward many tasks (or at least toward any task grounded in reality). Additionally, this representation might be relatively simple, assuming that scientists are correct in suggesting that the fundamental laws of nature are indeed simple functions (Gell-Mann, 1995), in line with the simplicity bias hypothesis.

But what exactly do we mean by ""a statistical model of the underlying reality."" In this section, we formalize one definition with concrete mathematical statements. _Importantly_,

Figure 5: **Why are models converging?:** We provide three potential driving forces of model convergence. **LEFT:** (_Multitask Scaling Hypothesis_) Models trained with an increasing number of tasks are subjected to pressure to learn a representation that can solve all the tasks. **MIDDLE:** (_Simplicity Bias Hypothesis_) Larger models have larger coverage of all possible ways to fit the same data. However, the implicit simplicity biases of deep networks encourage larger models to find the simplest of these solutions. **RIGHT:** (_Capacity Hypothesis_) If an optimal representation exists in function space, larger hypothesis spaces are more likely to cover it. As the models become larger, they cover the optimum and converge to the same solution (marked by filled ).

this section should be read as just one concrete candidate for the form of the platonic representation; other candidates could be arrived at from other modeling assumptions.

### An idealized world

We consider a world that works as follows, consistent with the cartoon in Fig. 1. The world consists of a sequence of discrete events, denoted as \(\mathbf{Z}\triangleq[z_{1},\ldots,z_{T}]\), sampled from some unknown distribution \(\mathbb{P}(\mathbf{Z})\). Each event can be observed in various ways. An observation is a bijective, deterministic function \(\texttt{obs}:\mathcal{Z}\rightarrow\cdot\) that maps events to an arbitrary measurement space, such as pixels, sounds, mass, force, torque, words, etc. Later, in Sec. 6, we discuss limitations and potential extensions to continuous and unbounded worlds, and stochastic observations, that could yield a model that better reflects real learning scenarios.

One can think of an event as corresponding to the state of the world at some point in time3, but it is also fine to simply consider an event as any variable that indexes observations, with no further physical meaning4.

Footnote 3: Here we only analyze temporal sequences, but note that the same could be done with respect to events laid out in space instead.

Footnote 4: This latter interpretation may be more consistent with Plato’s intent. Scholars have argued that his allegory of the cavee rejects any notion of a true world state (Nettleship, 1897). Instead, we could say that the joint distribution of observation indices is _itself_ the platonic reality.

In this idealized world, knowing \(\mathbb{P}(\mathbf{Z})\) would be useful for many kinds of predictions; this would constitute a world model over the events that cause our observations (Werbos, 1987; Ha and Schmidhuber, 2018; Richens and Everitt, 2024). We will next show that a particular representation of \(\mathbb{P}(\mathbf{Z})\) is recovered by certain contrastive learners.

### A family of contrastive learners converge to a representation of \(\mathbb{P}(\mathbf{Z})\)

Consider a contrastive learner that models observations that _cooccur_ together. For simplicity, we ground our discussion with the following definition of the _cooccurrence probability_, \(P_{\texttt{coor}}\), of two observations \(x_{a}\) and \(x_{b}\) both occurring within some window \(T_{\texttt{window}}\):","P_{\texttt{coor}}(x_{a},x_{b}) \propto\sum_{(t,t^{\prime})\colon|t-t^{\prime}|\leq T_{\texttt{ window}}}\mathbb{P}(X_{t}=x_{a},X_{t^{\prime}}=x_{b}).",,"\text{Alignment}(K_{\text{img}}, K_{\text{text}}) = \frac{\text{Tr}(K_{\text{img}} K_{\text{text}})}{\|K_{\text{img}}\|_F \|K_{\text{text}}\|_F}","Cross-modal alignment between image and text kernels is quantified by the normalized trace of their product, measuring their similarity."
ICML_2024_oral_95,3,"Analogously, we can define \(P_{\texttt{coor}}\) for \(\mathbf{Z}\) and other observation modalities. Note that \(P_{\texttt{coor}}\) is symmetric.

Consider _positive pairs_ as two observations nearby in time (sampled from \(P_{\texttt{coor}}\)) and _negative pairs_ as observations drawn from any point in time (sampled independently from the marginal). Our contrastive learner tries to classify if a pair is positive or negative by learning a representation \(f_{X}\colon X\rightarrow\mathbb{R}^{d}\) such that the dot-product kernel approximates the log odds ratio up to some offset:","\langle f_{X}(x_{a}),f_{X}(x_{b})\rangle \approx\log\frac{\mathbb{P}(\texttt{pos}\mid x_{a},x_{b})}{\mathbb{ P}(\texttt{neg}\mid x_{a},x_{b})}+\tilde{c}_{X}(x_{a}) || =\log\frac{P_{\texttt{coor}}(x_{a}\mid x_{b})}{P_{\texttt{coor}} (x_{a})}+c_{X}(x_{a})",,"\langle f_{X}(x_{a}),f_{X}(x_{b})\rangle\approx\log\frac{P_{\texttt{coor}}(x_{a},x_{b})}{P(x_{a})P(x_{b})}+\text{offset}.","This equation defines the objective of a contrastive learner, where the dot product of representations approximates the log odds ratio of co-occurrence versus independence, indicating how much more likely two observations are to co-occur compared to being independent."
ICML_2024_oral_95,4,(4),"=K_{\texttt{PMI}}(x_{a},x_{b})+c_{X}(x_{a}),","where \(K_{\texttt{PMI}}\) is the pointwise mutual information (PMI) kernel, and \(c_{X}(x_{a})\) is constant in \(x_{b}\)","\langle f_{X}(x_{a}),f_{X}(x_{b})\rangle \approx \log \frac{P_{\texttt{coor}}(x_{a}, x_{b})}{P_{\texttt{coor}}(x_{a}) P_{\texttt{coor}}(x_{b})} + c_{X}(x_{a}) + c_{X}(x_{b})","This equation formalizes the contrastive learning objective by approximating the log ratio of joint and marginal co-occurrence probabilities using a dot-product kernel, with additive bias terms for each observation."
ICML_2024_oral_95,5,"\). We note that this is a common setting for self-supervised contrastive learners with NCE objectives (Gutmann and Hyvarinen, 2010; Oord et al., 2018), including SimCLR (Chen et al., 2020) and SimCSE (Gao et al., 2021). (See Oord et al. (2018) and Appendix G.1 for detailed derivations.)

Under mild conditions that the world is smooth enough (see Appendix G.2), a choice of \(f_{X}\) can exactly represent \(K_{\texttt{PMI}}\):","\langle f_{X}(x_{a}),f_{X}(x_{b})\rangle=K_{\texttt{PMI}}(x_{a},x_{b})+c_{X},",where we observed that \(c_{X}(x_{a})\) from Eq,f_{X}(x) = \Phi^{-1} \left( \frac{d}{dx} \mathbb{P}(Z \leq x) \right),A representation $f_X$ that exactly recovers the PMI kernel under mild smoothness assumptions on the world's distribution.
ICML_2024_oral_95,6,"Eq. (5) must be a constant since both sides are symmetric.

Therefore, the contrastive learners we consider are minimized by a representation \(f_{X}\) whose kernel is \(K_{\texttt{PMI}}\) (up to a constant offset). With sufficient data and optimization, we will observe convergence to this point.

Thus we have convergence to a representation of the statistics of \(X\), but what about \(Z\)? Recall that our idealized world consists of _bijective_ observation functions, which, over discrete random variables, preserve probabilities. So we have:","P_{\texttt{coor}}(x_{a},x_{b})=P_{\texttt{coor}}(x_{a},x_{b}),\,K_{\texttt{PMI }}(x_{a},x_{b})=K_{\texttt{PMI}}(z_{a},z_{b}),",where we use \(P_{\texttt{coor}}\) and \(K_{\texttt{PMI}}\) modality-agnostically to emphasize that different modalities share these same quantities,"K_{\texttt{PMI}}(x_{a},x_{b}) = \log\frac{\mathbb{P}(x_{a},x_{b})}{\mathbb{P}(x_{a})\mathbb{P}(x_{b})}.","Equation 6 defines the pointwise mutual information (PMI) kernel between two observations $x_a$ and $x_b$, which quantifies the log ratio of their joint probability to the product of their marginals, capturing the statistical dependence between them."
ICML_2024_oral_95,7,"es.

All these arguments hold not just for \(X\) but also for \(Y\) (or any other bijective, discrete modality), implying:","K_{\texttt{PMI}}(z_{a},z_{b}) =\langle f_{X}(x_{a}),f_{X}(x_{b})\rangle-c_{X} || =\langle f_{Y}(y_{a}),f_{Y}(y_{b})\rangle-c_{Y}.",,"K_{\texttt{PMI}}(x_{a},x_{b}) = K_{\texttt{PMI}}(y_{a},y_{b}) = K_{\texttt{PMI}}(z_{a},z_{b}),","This equation shows that the PMI kernel is modality-agnostic, being invariant under bijective transformations of the observation space, and thus represents a shared structure across modalities."
ICML_2024_oral_99,1,"### Problem Formulation

In adaptive experiment design with heterogeneous treatment effect, there is a binary set \(\mathcal{A}=\{0,1\}\) of arms (i.e., treatments or controls) and a finite feature set \(\mathcal{X}=\{X_{1},X_{2},\cdots,X_{M}\}\) with \(|\mathcal{X}|=M\). Suppose \(n\) is the time horizon (or the total number of experimental units). The features at each time period follow a sequence of discrete distributions \(P_{X}=\{P_{X}^{t}\}_{t\geq 1}\), where \(P_{X}^{t}=(p_{1}^{t},\cdots,p_{M}^{t})\in(0,1)^{M}\) with \(\sum_{j=1}^{M}p_{j}^{t}=1,\forall t\geq 1\), representing the probability of observing experimental unit with feature \(X_{j}\) at time \(t\) as \(p_{j}^{t}\). Denote \(f_{j}(m):=\mathbb{E}\left[\sum_{1\leq t\leq m}1_{\{x_{i}=X_{j}\}}\right]=\sum_ {1\leq t\leq m}p_{j}^{t}\), which represents the expected number of occurrences of feature \(X_{j}\) in the first \(m\) periods, for any \(1\leq j\leq M\) and \(1\leq m\leq n\). We have the following assumption for the distribution of features.

**Assumption 1.1**.: _Seasonal Nonstationarity (1)\(\exists C_{1},C_{2}>1\), s.t. \(\forall 1\leq j\leq M\), \(C_{1}<\frac{f_{j}(n)}{f_{j}(\frac{n}{2})}<C_{2}\). (2)\(f_{min}(n):=\min_{1\leq j\leq M}f_{j}(n)\geq\Omega(\log n)\)_

Intuitively, this assumption says in the first and the second half periods, every features will be expected to appear approximately same and at least \(\Omega(\log n)\) times. Compared to the simplest assumption where the distribution of features for patients is stationary at each time period, our assumption greatly expands the applicability of our method. For example, different types of patients may have completely different patterns of occurrence on weekdays and weekends, or during different seasons. In this case, the simple assumption that patients' features have a stable distribution will no longer hold, but this situation may still conform to our non-stationary seasonal assumption.

_Remark 1.2_.: Though here we denote it as non-stationarity, in fact our assumption is very mild and contains oblivious **adversarial** case. To see this, note that we allow the distribution \(P_{t}^{X}\) to be arbitrary, so for any oblivious adversarial sequence \((H_{t})_{t=1}^{n}\) satisfying assumption 1.1, we can just set \(p_{i}^{t}=1\) for \(H_{t}=X_{i}\) and \(p_{j}^{t}=0\) for any \(j\neq i\) to obtain the desired sequence.

After observing the feature \(x_{t}\), the experimenter will choose a treatment allocation \(a_{t}\in\{0,1\}\) based on policy \(\pi\), and the reward of the chosen arm \(r_{t}=r_{t}\left(a_{t}|x_{t}\right)\in[0,1]\) can be observed. where \(P_{i}(X_{j})\) is the distribution of the rewards of arm \(i\) and feature \(X_{j}\) and \(P_{X}\) is the distribution of features. We define the conditional average treatment effect (CATE) of a feature \(x\) as \(\Delta(X_{j}):=\mathbb{E}\left[r_{t}(1|X_{j})\right]-\mathbb{E}\left[r_{t}(0| X_{j})\right]\), for any \(X_{j}\in\mathcal{X}\). We also denote \(\sigma_{ji}=\mathbb{V}\left[r(i|X_{j})\right]\) for \(i=0,1\) and \(j=1,\cdots,M\) as the variance of reward of playing arm \(i\) when facing context \(X_{j}\). In this paper, we will elaborate on \(|\Delta(x)|=\Theta(1)\) for all \(x\in\mathcal{X}\), which is arguably the most fundamental case in real applications. Denote all stochastic MAB instances satisfying the mentioned assumptions to constitute a feasible set \(\mathcal{E}_{0}\).

A key index to measure the efficiency of online learning policy \(\pi\) is accumulative regret \(\mathcal{R}(n,\pi)\), defined as the expected difference between the reward under the optimal policy and the policy \(\pi\), i.e., \(\mathcal{R}(n,\pi)=\mathbb{E}^{\pi}\left[\sum_{i=1}^{n}\left[r_{i}(a^{*}(x_{i}) |x_{i})-r_{i}\left(a_{i}|x_{i}\right)\right]\right]\), where \(a^{*}(x_{i})\) is the optimal arm of feature \(x_{i}\). In addition, an admissible adaptive estimator \(\hat{\Delta}(X_{j})\) maps the history \(\mathcal{H}_{n}\) to an estimation of \(\Delta(X_{j})\). We use the mean square error of \(\hat{\Delta}(X_{j})\), i.e.,\(e\left(n,\hat{\Delta}(X_{j})\right)=\mathbb{E}\left[\left(\Delta(X_{j})-\hat{ \Delta}(X_{j})\right)^{2}\right]\) to measure the quality of the estimation. We define \(\hat{\Delta}:=\{\hat{\Delta}(X_{j})\}_{1\leq j\leq M}\) to represent all the estimators on the gap between two arms for each feature. A design of an contextual bandit experiment can then be represented by an admissible pair \((\pi,\hat{\Delta})\). Different from traditional contextual bandit problems, the optimal design of contextual bandit experiments in this paper is solving the following minimax multi-objective optimization problem:","\min_{(\pi,\hat{\Delta})}\max_{\nu\in\mathcal{E}_{0}}(\mathcal{R}_{\nu}(n,\pi),\max_{1\leq j\leq M}e_{\nu}(n,\hat{\Delta}(X_{j})))",where we use the subscript \(\nu\) to denote the contextual bandit instance,"\begin{aligned}
\min_{\pi,\hat{\Delta}}\max\left\{ \mathcal{R}(n,\pi),\max_{1\leq j\leq M}e\left(n,\hat{\Delta}(X_{j})\right)\right\}.
\end{aligned}","This equation formulates the minimax multi-objective optimization problem for the optimal design of contextual bandit experiments, balancing the trade-off between minimizing the worst-case accumulative regret and minimizing the maximum mean square error of the treatment effect estimators across all features."
ICML_2024_oral_99,2,"ce. Eq.1 mathematically describes the two goals: minimizing the regret and the largest estimation error among all features.

The above is a rigorous mathematical description of the first question that we presented. This sets the stage for our second question, which concerns about the price of protecting privacy for both regret and CATE estimation, and how it will affect the balance between minimizing regret and estimation error. In order to rigorously address this question, we first need the following definition of differential privacy.

**Definition 1.3**.: \(((\varepsilon,\delta)\)**-anticipating private contextual bandit algorithm**).

A bandit algorithm \(\pi\) is \((\varepsilon,\delta)\)-private if for two neighboring datasets \(D=\{(x_{i},a_{i},r_{i})^{n}_{i=1}\}\) and \(D^{\prime}=\{(\hat{x}_{i},\hat{a}_{i},\hat{r}_{i})^{n}_{i=1}\}\) of feature, action and reward pairs that differs in at most one time step \(t\), then for all \(S\subseteq\mathcal{A}^{T-t}\):

\(\mathbb{P}\left(\pi\left(a_{t+1},\cdots,a_{n}\right)\in S\mid D\right)\leq e ^{\varepsilon}\mathbb{P}\left(\pi\left(a_{t+1},\cdots,a_{n}\right)\in S\mid D ^{\prime}\right)\right)+\delta\),

where \(\mathcal{A}=\{0,1\}\) is the set of actions.

This definition is slightly different with the classical differential privacy (DP). (Shariff and Sheffet, 2018) propose a notion of ""joint DP"" in the context of linear contextual bandits and is later adopted by (Chen et al., 2022) as anticipating DP (ADP). The key difference of ADP is to restrict the output sets as allocations strictly after a patient of interest at time \(t\). Such a restriction is motivated by two reasons. The first one is that following the classical DP will inevitably lead to linear regret. The second reason is that classical DP assumes that the adversary has access to the provided action \(a_{t}\) at time \(t\), which is unreasonable in most adaptive experiments, as communication about \((x_{t},a_{t},r_{t})\) at time \(t\) is expected to be secured and the data prior to time \(t\) have no impact on the privacy of patient \(t\) because the decision making algorithm has no knowledge of \(x_{t}\) before time \(t\). Therefore, only the privacy of outputs _after_ time \(t\) needs to be protected. For a more detailed discussion about ADP, one can refer to (Chen et al., 2022).

**Definition 1.4**.: \(((\varepsilon,\delta)\)**-private CATE estimator**) An admissible CATE estimator \(\hat{\Delta}\) which takes a dataset \(\{(x_{i},a_{i},r_{i})\}_{i=1}^{n}\) as input, and output \(M\) estimations for ATE of each feature \(\{\hat{\Delta}(X_{i})\}_{i=1}^{M}\) is \((\varepsilon,\delta)\)-private if for two neighboring datasets \(D=\{(x_{i},a_{i},r_{i})^{n}_{i=1}\}\) and \(D^{\prime}=\{(\hat{x}_{i},\hat{a}_{i},\hat{r}_{i})^{n}_{i=1}\}\) of feature, action and reward pairs that differs in at most one time step \(t\), then for any measurable set \(S\subset R^{M}\):

\(\mathbb{P}\left((\hat{\Delta}(X_{i}),\cdots,\hat{\Delta}(X_{M})\in S\mid D \right)\leq e^{\varepsilon}\mathbb{P}\left((\hat{\Delta}(X_{i}),\cdots,\hat{ \Delta}(X_{M})\in S\mid D^{\prime}\right)+\delta\).\)

Since a design of contextual bandit experiments can be represented as an admissible pair \((\pi,\hat{\Delta})\), in this paper we say a contextual bandit experiment design is \((\varepsilon,\delta)\)-private when both \(\pi\) and \(\hat{\Delta}\) are \((\varepsilon,\delta)\)-private.

**Technical Difficulties and Our Contribution**

**1. Elaborating on the Length of RCTs with a Regret Budget for Different Features.** As claimed in (Simchi-Levi and Wang, 2023), the key idea of balancing regret and estimation error is to properly set the length of RCT. However, in our setting each feature may vary enormously in their occurrences, and it can also be highly non-stationary. Since we are interested in the worst estimation among all features, we should set the length of RCTs for all features to be the same as that of the feature with least occurrence frequency, i.e, \(f_{min}(n)\). Since we don't know \(f_{min}(n)\) at the beginning of experiment, by the assumption of seasonal non-stationarity, we propose an algorithm named ConSE, which divides the experiment into two phase: in the first half periods, it chooses to minimize regret while learning the frequency of occurrences \(f_{j}(\frac{n}{2})\), and in the second half periods ConSE runs RCT for \(\hat{f}_{min}(\frac{n}{2})\) periods for each feature, which is estimated from the first phase.

Another contribution of our result is the improvement of analysis compared to existing work(Simchi-Levi and Wang, 2023; Dai et al., 2023; Zhao, 2023). In particular, we develop a tighter upper bound which is tight up to constant, which helps to have a better characterization of the Pareto optimal curve for regret and estimation error. Besides, the proposed estimator in this paper is asymptotically normal, which is vital in constructing confidence interval and testing hypothesis and has been a long standing issue for adaptive experiment design literature(Simchi-Levi and Wang, 2023; Dai et al., 2023; Zhao, 2023). See section 3 for a more detailed discussion.

**2. Privatizing Feature Information in Non-stationary Environment.** Differential privacy is known to be more challenging in bandit setting due to its highly correlated data. For multi-arm bandit, algorithms based on tree mechanism proposed in (Chan et al., 2011) have been proved to be optimal up to polylog factors(see Tossou and Dimitrakakis, 2016; Azize and Basu, 2022; Saged and Sheffet, 2019). However, when it comes to contextual bandit, things become more complicated, as the algorithm not only needs to privatize the reward of each arm, but also the context of each patient. Most existing works focus on setting where reward function is in a specific function class like (generalized) linear function (Hanna et al., 2022; Shariff & Sheffet, 2018; Zheng et al., 2020; Chen et al., 2022). However, in clinical trials, it's risky to believe the treatment effect of one type of customer can be generalized to other types in certain way (like a linear function). Therefore, in this paper we don't assume any structure of CATE among different types of patients, which forces us to propose mechanisms different from existing literature. The second difficulty arises from the non-stationarity assumption, which has been considered in very few works. In particular, this rules out the possibility of merging different features as a whole and applying a unified mechanism.

To overcome all the difficulties mentioned, we propose a ""Doubly Private"" algorithm, which treats each feature separately and doubly privatize the patients' information: first of all, we use the idea of tree mechanism and divide the whole experiment into batches, where the estimation of rewards are only updated at the end of each batches. Secondly, we randomize the length of each batch to protect the context information, which is, to our best knowledge, novel in DP-contextual bandit setting. Finally, our ""Doubly Private"" allows experimenters to balancing regret and the estimation accuracy of CATE in any given level, and our subsequent theoretical guarantees ensure that no method can simultaneously outperform our algorithm in minimizing regret and accurately estimating CATE.

### Literature Review

**Adaptive Experiment Design.** Experimental design is witnessing a surge in popularity across operations research, econometrics, and statistics (see, e.g., Johari et al., 2015; Bojinov et al., 2021; Xiong et al., 2023) Adaptive experimental design emerges as a particularly relevant area to our current focus(Hahn et al., 2011; Atan et al., 2019; Greenhill et al., 2020; Kato et al., 2020; Qin & Russo, 2022) There are some recent works trying to demonstrate the statistical superiority of adaptive experiment compared classical non-adaptive experiment design, where the measurement of precision is the (asymptotic) variance of the estimator. In (Dai et al., 2023), they propose a measurement called Neyman regret, and show that an adaptive design with asymptotically optimal variance is equivalent to sub-linear Neyman regret, thus transforming it into a regret minimization problem. (Zhao, 2023) consider a similar setting, but adopt a competitive analysis framework.

Another emerging field is multitasking bandit problems, where minimizing regret is not the only objective (see, e.g., Yang et al., 2017; Yao et al., 2021; Zhong et al., 2021). (Erraqabi et al., 2017) also consider the tradeoff between regret and estimation error, and propose a new loss function combining these two objectives together. The most related work to this paper may be (Simchi-Levi & Wang, 2023), where they consider the tradeoff between regret and ATE estimation. We extend their framework to contextual bandit setting, derive a similar Pareto optimality characterization, and consider the additional requirement to protect patients' privacy.

**Differentially Private (Contextual) Bandit Learning and Estimation.** Differential privacy (Dwork et al., 2006) has emerged as gold-standard for privacy preserving data-analysis, as it ensures that the output of the data-analysis algorithm has minimum dependency on any individual datum. Differentially private variants of online learning algorithms have been successfully developed in various settings (Guha Thakurta & Smith, 2013), including a private UCB-algorithm for the MAB problem (Azize & Basu, 2022; Tossou & Dimitrakakis, 2016) as well as UCB variations in the linear bandit settings (Hanna et al., 2022; Shariff & Sheffet, 2018). These algorithms are in general motivated by techniques named ""tree mechanism""(Chan et al., 2011; Dwork et al., 2010), which functions by continuously releasing aggregated statistics over a stream of \(T\) observations, introducing only \(\frac{\text{polylog}(T)}{e}\) noise in each time period, and thus leading to an added pseudo regret of order \(\frac{\text{polylog}(T)}{e}\). It was shown in Shariff & Sheffet 2018 that any \(\epsilon\)-DP stochastic MAB algorithm must incur an added pseudo regret of \(\Omega(\frac{K\log(T)}{e})\), and this lower bound is matched by Sajed & Sheffet 2019, using a batched elimination algorithm.

However, when it comes to DP-contextual bandit, so far there isn't a golden standard that works for general contextual bandit problems. Instead, most works focus on contextual linear bandit (Shariff & Sheffet, 2018; Hanna et al., 2022; Charisopoulos et al., 2023) and adopt a relaxed definition named _joint-DP_ or _anticipating-DP_. These works are in general variants of Lin-UCB (Abbasi-Yadkori et al., 2011), which is known to be optimal for contextual linear bandits. A lower bound for contextual linear bandit was proposed in Shariff & Sheffet 2018, and then was matched in Shariff & Sheffet 2018, Hanna et al. 2022 up to polylog factors. (Chen et al., 2022) consider differential privacy in dynamic pricing problem in a generalized linear model. A follow-up work (Chen et al., 2021) considers dynamic pricing in a non-parametric model and derive an upper bound of \(\tilde{O}\left(T^{(d+2)/(d+4)}+\varepsilon^{-1}T^{d/(d+4)}\right)\), which is not known to be optimal.

There has been some initial work on differentially private causal inference methods. Lee & Bell 2013 proposed a privacy-preserving inverse propensity score estimator for estimating average treatment effect (ATE). Komarova & Nekipelov 2020 study the ramifications of differential privacy on the identification of statistical models and demonstrate the obstacles encountered in regression discontinuity design with privacy constraints. However, when it comes to adaptive experiment, to our knowledge there is no similar work trying to estimating CATE privately.

## 2 A Warm-up: Upper and Lower Bound Without Privacy Constraint

In this section, we aim to answer the first question proposed in subsection 1.1, i.e. _what's the best possible accuracy of estimation for CATE given a budget of regret_, by first showing a lower bound and then proposing an algorithm **CoSE** with matching upper bound. Besides, we also use this section as a warm-up to describe the technical difficulties of this problem and how to solve them, which can be helpful to understand the more complicated algorithm in section 3 with privacy constraints. In the following theorem, we provide a mini-max lower bound to explicitly show the best possible estimation accuracy with a constraint on regret budget.

**Theorem 2.1**.: _For any admissible pair \(\left(\pi,\hat{\Delta}_{n}\right)\), there always exists a hard instance \(\nu\in\mathcal{E}_{0}\) such that \(e_{\nu}\left(n,\hat{\Delta}_{n}\right)\geq\Omega\left(\frac{M}{\mathcal{R}_{ \nu}(n,\pi)}\right)\), or in other words_","\inf_{(\pi,\hat{\Delta}_{n})}\max_{\nu\in\mathcal{E}_{0}}[e_{ \nu}(n,\hat{\Delta}_{n})\mathcal{R}_{\nu}(n,\pi)]\geq\Omega (M).",,"\max_{\nu\in\mathcal{E}_{0}}\max_{1\leq j\leq M}e_{\nu}\left(n,\hat{\Delta}(X_{j})\right)\geq\Omega\left(\frac{M}{\mathcal{R}_{\nu}(n,\pi)}\right)",Lower bound showing the inverse relationship between the maximum estimation error of CATE and the regret in the absence of privacy constraints.
ICML_2024_oral_99,3,"Theorem 2.1 mathematically highlights the trade-off that a small regret will inevitably lead to a large error on the CATE estimation. In specific, it states that for any admissible pair \(\left(\pi,\hat{\Delta}_{n}\right)\), there exists a hard instance \(\nu\in\mathcal{E}\) such that the expected error is lower bounded by \(M\) times the inverse of the regret, i.e., \(e_{\nu}\left(n,\hat{\Delta}_{n}\right)\geq\Omega\left(\frac{M}{\mathcal{R}_{ \nu}(n,\pi)}\right)\). In particular, since \(\mathcal{R}_{\nu}(n,\pi)=\mathcal{O}(\log(n))\) for UCB and TS algorithms, no estimators can not achieve smaller error than the order \(\Omega\left(\frac{M}{\log(n)}\right)\) consistently over all the possible instances, which explicitly shows the limitation of regret optimal policies in terms of statistical power for estimating the CATE. On the other hand, if we ignore the regret and simply run random control trials (RCT), it can be easily shown that","e_{\nu}(n,\hat{\Delta}_{n})=\max_{1\leq j\leq M}\mathbb{E}[(\hat{ \Delta}_{n}(X_{j})-\Delta(X_{j}))^{2}]=\mathcal{O}(\frac{1}{ f_{\min}(n)}),",,"\max_{1\leq j\leq M}e_{\nu}(n,\hat{\Delta}(X_{j}))=\mathcal{O}\left(\frac{1}{f_{min}(n)}\right).","The equation characterizes the optimal estimation error when using random control trials (RCT), showing that the maximum mean square error across all features decays inversely with the minimum expected feature occurrence $f_{min}(n)$."
ICML_2024_oral_99,4,"which is the best possible accuracy one can obtain but will result in \(\mathcal{O}(n)\) regret.

The above two cases can be regarded as two extreme cases (note that they don't match the lower bound), but in practice, the experimenter may want to find a balance of estimation accuracy and regret between these two extreme cases. In the following, we provide a family of algorithms named ConSE which depends on a parameter \(\alpha\in[0,1]\). A larger \(\alpha\) leads to smaller regret budget and larger estimation error. In particular, when \(\alpha=1\), the algorithm ignores estimation error and focuses on minimizing regret. On the contrary, when \(\alpha=0\), the algorithm only focuses on minimizing estimation error. Moreover, for each given \(\alpha\), ConSE achieves the lower bound provided in theorem 2.1, which shows that it can attain every Pareto optimal point from one extreme case to the other (see figure 1). In the figure, the endpoints of the curve represent two extreme cases with minimum regret and estimation error. The other points on the curve characterize the tradeoff between these two objectives. Namely, this is the Pareto optimal curve for regret and estimation error. In section 4, we will have a more detailed discussion on variants of the Pareto optimal curve.

**Remark 2.2**.: **(Intuitive example to illustrate the trade-off)** Since the two objectives presented in theorem 2.1 seems to be not conflicting at first glance, we find it necessary to provide an intuitive example here to illustrate why there is indeed a trade-off between these two objectives. Assume that there are only 2 arms with mean \(\mu_{1}\) and \(\mu_{2}\), where \(\Delta=\mu_{1}-\mu_{2}>0\). Now by definition, the regret is \(\text{Reg}=\Delta*T(arm2)\), where \(T(arm2)\) is the frequency of playing arm 2. So consider the following two tasks. The first task is to identify \(\mu_{1}>\mu_{2}\), and the second task is to estimate \(\mu_{1},\mu_{2}\) as accurate as possible. It's quite intuitive here that task 1 is strictly easier than task 2. Indeed, concentration inequalities tell us that it only takes \(O(\frac{\log T}{\Delta^{2}})\) times for each arm to complete task one, while basic statistical lower bound tells us that to estimate \(\mu_{1},\mu_{2}\) with accuracy \(\frac{1}{T^{\alpha}}\), it's necessary to play at least \(T^{\alpha}\) times of each arm. Therefore, in order to minimize regret, one should only play each arm \(O(\frac{\log T}{\Delta^{2}})\) times, identify that \(\mu_{1}>\mu_{2}\), and never play arm 2 again. In this case, it would lead to a very bad estimation of \(\mu_{2}\) with accuracy \(\frac{\Delta^{2}}{\log T}\), but it's already necessary and sufficient for regret minimization. On the contrary, if the goal is to estimate \(\mu_{2}\) much more precisely with accuracy \(\frac{1}{T^{\alpha}}\), then from the above analysis, we know that it'll inevitably lead to a regret of \(O(T^{\alpha})\).

In the ConSE algorithm, we need the following notations.

**Define three number sequences:**

For \(e=epoch=1,2,3,...\) and the number of total patients \(n\), define:

Figure 1: Pareto Optimal Curve

\(\Delta_{e}=2^{-epoch}\)

\(R_{e}=\max\{\frac{32\log(16n\text{-epoch}^{2})}{\Delta_{e}^{2}},\frac{8\log(8n \text{-epoch}^{2})}{\Delta_{e}}\}+1\)

\(h_{e}=\sqrt{\frac{\log(16n\text{-epoch}^{2})}{2R_{e}}}\)

```
1:Input:\(\alpha\)
2:Initialize:\(S_{j}\leftarrow\{0,1\}\), epoch \(e_{j}\gets 0\), \(r_{j}\gets 0\), \(\bar{\mu}_{i}^{j}\gets 0\), \(n_{j}\gets 0\) (\(i=0,1;j=1,2,...,M\))
3:for\(t=1\)to\([\frac{n}{2}]\)do
4: Get feature \(x_{t}=X_{j_{t}}\in\mathcal{X}\)
5: Increment \(n_{j_{t}}\gets n_{j_{t}}+1\)
6:if\(|S_{j_{t}}|=2\)then
7: Select action \(a_{t}\in\{0,1\}\) with equal probabilities \((\frac{1}{2},\frac{1}{2})\) and update mean \(\bar{\mu}_{a_{t}}^{j_{t}}\)
8: Increment \(r_{j_{t}}\gets r_{j_{t}}+1\)
9:if\(r_{j_{t}}\geq R_{e_{j_{t}}}\)then
10:if\(e_{j_{t}}\geq 1\)then
11: Remove arm \(i\) from \(S_{j_{t}}\) if \(\max\{\bar{\mu}_{1}^{j_{t}},\bar{\mu}_{2}^{j_{t}}\}-\bar{\mu}_{i}^{j_{t}}>2h_ {e}\) (\(i=0,1\))
12:endif
13: Increment epoch \(e_{j_{t}}\gets e_{j_{t}}+1\)
14: Set \(r_{j_{t}}\gets 0\)
15: Zero means: \(\bar{\mu}_{i}^{j_{t}}\gets 0\)\(\forall i\in\{1,2\}\)
16:endif
17:else
18: Pull the arm in \(S_{j_{t}}\)
19:endif
20:if\(t=[\frac{n}{2}]\)then
21:\(\hat{f}_{j}=n_{j}(1\leq j\leq M)\)
22:\(T_{min}=\max\{\log n,\min\{\hat{f}_{1}^{1-\alpha},\hat{f}_{2}^{1-\alpha}, \cdots,\hat{f}_{M}^{1-\alpha}\}\}\)
23:endif
24:endfor
25:for\(j=1\)to\(M\)do
26:\(n_{j}=0\)
27:endfor
28:for\(t=[\frac{n}{2}]+1\)to\(n\)do
29: Get feature \(x_{t}=X_{j_{t}}\in\mathcal{X}\)
30: Increment \(n_{j_{t}}\gets n_{j_{t}}+1\)
31:if\(n_{j_{t}}\leq T_{min}\)then
32: Select action \(a_{t}\in\{0,1\}\) with equal probabilities \((\frac{1}{2},\frac{1}{2})\) and update mean \(\bar{\mu}_{a_{t}}^{j_{t}}\)
33:if\(n_{j_{t}}=T_{min}\)then
34: Output \(\hat{\Delta}(X_{j_{t}})=\bar{\mu}_{1}^{j_{t}}-\bar{\mu}_{0}^{j_{t}}\)
35:endif
36:else
37: Pull the arm in \(S_{j_{t}}\). (if \(|S_{j_{t}}|=2\), pull any arm \(a_{t}\in S_{j_{t}}\))
38:endif
39:endfor
```

**Algorithm 1** ConSE

**Theorem 2.3**.: _Let Algorithm 1 runs with any given \(\alpha\in[0,1]\). For any instance \(\nu\), the regret and estimation error are_","\mathcal{R}_{\nu}(n,\pi)\leq\mathcal{O}(M\max\{f_{min}(n)^{1-\alpha}, \log n\}), || e_{\nu}(n,\hat{\Delta}_{n})\leq\mathcal{O}(\frac{1}{\max\{f_{min}(n)^{1- \alpha},\log n\}}).",,"\mathcal{R}_{\nu}(n,\pi)=\mathcal{O}\left(\sum_{j=1}^{M}\Delta_{j}^{-1}\log n+Mn^{\alpha}\right),\quad\max_{1\leq j\leq M}e_{\nu}(n,\hat{\Delta}(X_{j}))=\mathcal{O}\left(\frac{1}{n^{\alpha}}\right).","Equation 4 presents the theoretical guarantees of the ConSE algorithm, showing that it achieves a regret bound of $\mathcal{O}\left(\sum_{j=1}^{M}\Delta_{j}^{-1}\log n+Mn^{\alpha}\right)$ and a maximum estimation error of $\mathcal{O}\left(\frac{1}{n^{\alpha}}\right)$ across all features."
ICML_2024_oral_99,5,"_Therefore, the product of regret and estimation error is always \(\mathcal{O}(M)\), i.e.,_","e_{\nu}(n,\hat{\Delta}_{n})\mathcal{R}_{\nu}(n,\pi)\leq\mathcal{O}(M)",,"\mathcal{R}_{\nu}(n,\pi)\cdot e_{\nu}(n,\hat{\Delta}_{n})=\mathcal{O}(M).","The product of regret and estimation error is bounded by a constant multiple of the number of features $M$, demonstrating the Pareto optimality of the ConSE algorithm."
ICML_2024_oral_99,6,"Combining the two theorems above, we can now answer _Question 1_: Given a budget of social welfare loss \(\mathcal{R}_{\nu}(n,\pi)\), the best possible accuracy of inference for CATE is \(\mathcal{O}\left(\frac{M}{\mathcal{R}_{\nu}(n,\pi)}\right)\) and is attained by ConSE.

_Remark 2.4_.: While we prove an upper bound of ConSE under non-stationary setting, the result proved in theorem 2.3 cannot be improved when the distribution of feature is stationary. This can be shown by noticing that the hard instance in lower bound in theorem 2.1 is constructed under the stationary case. That is to say, ConSE is optimal in both stationary and non-stationary settings.

_Remark 2.5_.: Compared to previous work in bandit experiment (Simchi-Levi & Wang, 2023), while the high level idea is similar, we consider an alternative estimator and improve the analysis in the proof. Specifically, in (Simchi-Levi & Wang, 2023), the upper bound is tight up to poly-log term, while in this paper the upper bound is tight up to constant. First of all, since classical bandit algorithms like UCB or TS attain regret bound of \(\mathcal{O}(\log n)\), we believe that poly-log factors do matter. Besides, this improved upper bound help us have a better characterization of Pareto optimal curve that we will explain in section 3. Finally, the estimator in our algorithm is asymptotically normal, which means we can construct (asymptotic) normal confidence interval for inference and hypothesis testing, which has been a long standing issue for existing adaptive experiment design literature (Simchi-Levi & Wang, 2023; Zhao, 2023; Dai et al., 2023).

**Proposition 2.6**.: _The estimators for all features \(\hat{\Delta}(X_{j})\) are unbiased, i.e., \(\mathbb{E}\left[\hat{\Delta}(X_{j})\right]=\Delta(X_{j})\)\((\forall 1\leq j\leq M)\). Moreover, \(\hat{\Delta}(X_{j})\) is asymptotically normal for any \(j\), or formally,_

\begin{table}
\begin{tabular}{l c c} \hline \hline Differences & Shochi-Levi \& Wang 2023 & This Paper \\ \hline Context & No & Yes \\ Lower bound & \(\Omega(1)\) & \(\Omega(M)\) \\ Upper bound & \(\mathcal{O}(\log n)\) & \(\mathcal{O}(M)\) \\ Differential Privacy & No & Yes \\ Asymptotic Normality & No & Yes \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison with Simchi-Levi & Wang 2023.

Intuitively speaking, ConSE can be divided into three steps:

**Step 1.** (From line 3 to 24) In the first half periods, we use Successive Elimination algorithm separately for each arm to eliminate the suboptimal arm and maintain \(\log n\) regret. At the same time, we use these data to estimate the appearance frequency \(f_{j}(\frac{n}{2})\) of each feature \(X_{j}\) as defined in assumption 1.

**Step 2.** (From line 25 to 35) At the beginning of second half periods, we run RCT \(\hat{f}_{min}(\frac{n}{2})\) times for every feature, where \(\hat{f}_{min}(\frac{n}{2})\) is estimated from Phase 1.

**Step 3.** (From line 36 to 38) Play the optimal arm for each feature in the remaining time of experiment.

Although it becomes more complicated with privacy constraints, our main goal is still to do the three steps _privately_. A more detailed discussion will be provided in next section.

## 3 Privacy is Free: A Doubly-Private Algorithm for Bandit Experiment

In this section, our focus is to answer _Question 2_, i.e., _with the constraint that the experimenter need to protect the privacy of participants, is it still possible to attain the same estimation accuracy as well as social welfare loss?_ Roughly speaking, our answer is yes (when \(\varepsilon\) is a small, constant number, which is the most common case). In other words, we will provide a DP version of ConSE that matches the lower bound provided in theorem 2.1 for any given \(\alpha\in[0,1]\), where the meaning of \(\alpha\) is exactly the same as in ConSE described in last section. The framework of **DP-ConSE** is quite similar to ConSE, with changes only in technical details. Due to limitation of space, we omit the precise description of DP-ConSE here and put it in appendix A.4. Instead, in the following we will provide an intuitive explanation of three steps in DP-ConSE, together with important theoretical guarantees.

**Step 1.** In the first half periods, we use an improved ""DP Successive Elimination"" algorithm in (Sajed & Sheffet, 2019) for each feature. Our goal in this phase is twofold for each feature: to identify the optimal action and estimate the frequency of occurrences (based on our non-stationary seasonal assumption) with minimal regret. For each feature we compare the **privatized** average rewards of two actions in batches. If the difference is large, we eliminate the suboptimal arm and claim that we find the optimal arm with high probability. There are two technical designs involved here. First, the length of batches increases exponentially, which strikes a balance between differential privacy protection and regret loss. Similar idea can be found in ""DP Successive Elimination"" algorithm (Sajed & Sheffet 2019) and widely used ""tree mechanism"" ((Chan et al., 2011)) in DP-bandit algorithms. Second, we use a novel technique by adding noise to the batch lengths for each feature. The reason for this is that due to the seasonal non-stationarity assumption, it's essential to run batched learning for each feature independently, and to protect the patients' feature, the length of batches should also be privatized. To the best of our knowledge, this technique has not appeared in DP-bandit literature and again highlights the difficulty of DP-contextual bandit compared to bandit setting.

After identifying the optimal action, we will continue to execute this action until the first half of the experiment is completed. After the completion of the first half, based on the occurrence frequencies of features observed, we can estimate \(f_{j}(n)\) for each feature \(X_{j}\). This helps us to decide the length of RCTs in second half periods to estimate CATE.

To make our claim valid, we first need to show that the elimination process will end in **step 1** (with high probability). This is confirmed by the following lemma.

**Lemma 3.1**.: _Let DP-ConSE runs with any given \(\alpha\in[0,1]\) and \(\varepsilon>0\). Then w.p. \(\geq 1-\frac{1}{n}\) it holds that DP-ConSE pulls the bad arm of any feature \(X_{j}\) in the first half periods for at most_",\mathcal{O}((\log n_{j}+\log\log(1/\Delta(X_{j})))(\frac{ 1}{\Delta(X_{j})^{2}}+\frac{1}{\varepsilon\Delta(X_{j})})),,\mathcal{O}\left(\frac{\log n}{\Delta(X_{j})^{2}}+\frac{\log n}{\varepsilon\Delta(X_{j})}\right),"The number of times the suboptimal arm is pulled for feature $X_j$ in the first half of the DP-ConSE algorithm is bounded by this expression, ensuring efficient elimination with high probability under privacy constraints."
ICML_2024_oral_99,7,"_where \(n_{j}\) is the number of occurrences of the feature \(X_{j}\)\((1\leq j\leq M)\)._

So when \(\varepsilon\) is a small constant and \(n\) is sufficiently large, we can find the optimal arm for each feature in the first half with high probability, and the number of playing suboptimal arm is bounded. As a corollary, we can bound the regret in the first half periods as claimed.

**Corollary 3.2**.: _For sufficiently large \(n\), the expected pseudo regret in the first half periods of DP-ConSE is at most \(\mathcal{O}\left(\left(\sum_{1\leq j\leq M}\frac{\log n}{\Delta(X_{j})}\right)+ \frac{M\log n}{\varepsilon}\right)\)._

**Step 2.** In the second half periods, our primary objective is to ensure the required accuracy of estimating the CATE. Using the estimated \(f_{j}(n)\) from **step 1**, we can determine the length of RCTs for each feature to attain the desired accuracy. It is important to remember that we still need to add noise to the length of RCTs for the same reason as stated in **step 1**.

After **step 2**, the main task of estimating CATE is completed, and the estimation accuracy is provided in the following theorem.

**Theorem 3.3**.: _If DP-ConSE runs with \(\alpha\in[0,1]\) and \(\varepsilon>0\), the estimate error is_","e(n,\hat{\Delta})=\mathcal{O}(\frac{1}{\max\{f_{min}(n)^{1-\alpha},\frac{ \log n}{\varepsilon}\}}).",,"e_{\nu}(n,\hat{\Delta}_{n})\leq\mathcal{O}\left(\frac{1}{\max\{f_{min}(n)^{1-\alpha},\log n\}}\right)\right).",The estimation error of the DP-ConSE algorithm is bounded by the inverse of the maximum between the $(1-\alpha)$-th power of the minimum feature occurrence and the logarithm of the time horizon.
naacl_2024_short_14,1,"## 2 Background and Notations

LmWe focus on Transformer LMs such as GPT-2. Given context \(c=\{x_{i}\}_{i=1}^{t-1}\), we formulate next token prediction as","p_{\text{lm}}(x_{t}|c)=f\circ\mathrm{g}\circ\mathrm{enc}(c),","where \(f\) is the last linear layer with softmax activation, \(g\) is the two-layer MLP network with a residual connection in the last Transformer layer, and \(\mathrm{enc}\) includes the earlier layers of the model",P(x_t|c) = \text{Softmax}(h_{t}^{L} W_{e}^{T}),"Probability distribution over the vocabulary for predicting the next token $x_t$ given the context $c$, parameterized by the model's final hidden states and embedding matrix $W_e$."
naacl_2024_short_14,2,"el.

\(k\)Nn-LmKhandelwal et al. (2020) use the \(\mathrm{enc}\) function from a trained LM (Eq 1) to build a data-store, where a key is the representation of a token sequence \(\{x_{i}\}_{i=1}^{t-1}\) in the training data encoded by enc, and the value of the key is the next token \(x_{t}\). When predicting the next token \(x^{\prime}_{t}\) of given context \(c=\{x^{\prime}_{i}\}_{i=1}^{t-1}\), \(k\)NN-LM has a \(k\)NN retrieval module that maps \(\mathrm{enc}(c)\) to a distribution \(p_{\text{knn}}(\cdot|c)\) by querying the datastore with \(\mathrm{enc}(c)\). Then a \(k\)NN-LM generates the next token distribution with","p_{\text{knnlm}}(x_{t}|c)=\lambda p_{\text{lm}}(x_{t}|c)+(1-\lambda)p_{\text {knn}}(x_{t}|c),",where \(\lambda\) is a hyperparameter for interpolation,"p_{\text{knn}}(x^{\prime}_{t}|c) = \sum_{i=1}^{k} \alpha_i \cdot \mathbf{1}(x_i = x^{\prime}_{t}),",where $\alpha_i$ represents the attention weight for the $i$-th nearest neighbor and $\mathbf{1}(\cdot)$ is the indicator function.
naacl_2024_short_14,3,"on.

Softmax bottleneckYang et al. (2018) theoretically show that the dimensionality of the last linear layer confines the possible vocabulary distribution the last softmax layer can generate. It implies that no matter what \(g\circ\mathrm{enc}\) generates, \(f\) can not generate certain distributions.

## 3 Capacity of LMs' Last Layers

Xu et al. (2023) hypothesize that the performance gap between \(k\)NN-LM and vanilla LM is because the softmax bottleneck prevents it from generating some distributions that \(k\)NN-LM can generate. In this section, we reinspect this hypothesis.

### Projecting to the Probability Space

We study whether softmax bottleneck causes the performance gap by inspecting whether the last layers can generate a distribution that approximates the distribution generated by \(k\)NN-LM \(p_{\text{knnlm}}\). We do the projection by solving","z^{*}\in\operatorname*{arg\,min}_{z\in\mathbb{R}^{d}}\mathrm{KL}[f(z)\|p_{ \text{knnlm}}],",where \(f\) is the last layer of the model with its trained parameters fixed (definition in Eq 1),"\min_{z}\|f(z)-p_{\text{knnlm}}(\cdot|c)\|_{2}^{2}\ \text{s.t.}\ z\in\text{Im}(g\circ\mathrm{enc}),",the optimization problem that finds the best approximation of the $k$NN-LM distribution within the image of the LM's last layer mapping
naacl_2024_short_16,1,"## 3 Methodology

We denote \(\mathbf{x}=[x_{1},x_{2},\ldots,x_{L}]\) as the input to the LLM \(f(\cdot)\), where \(x_{i}\) is the \(i\)-th token, and \(y\in\mathcal{Y}\) is the ground truth output.

Randomized smoothingWe follow previous work on randomized smoothing (Cohen et al., 2019; Zeng et al., 2021) to enhance the robustness of a LLM \(f(\cdot)\) by transforming it into a smoothed version \(g(\cdot)\). Specifically, we introduce random noise into the input by (uniform) randomly replacing some tokens with the [MASK] token. The randomized masking scheme, \(\phi(\mathbf{x},m)\), generates a binary sequence of the same length as the input \(\mathbf{x}\), with m% entries being nonzero, indicating the corresponding positions in \(\mathbf{x}\) that will be masked. We denote the binary mask sequence as \(\mathbf{s}\) and the masked input as \(\mathcal{M}(\mathbf{x},\mathbf{s})\). Then, the original LLM \(f(\cdot)\) is turned into a smoothed model \(g(\cdot)\) as","g(\mathbf{x})=\operatorname*{arg\,max}_{c\in\mathcal{Y}}\mathbb{P}_{\mathbf{x}\sim \phi(x,m)}(f(\mathcal{M}(\mathbf{x},\mathbf{s}))=c)",,"g(\mathbf{x}) = \mathbb{E}_{\mathbf{s} \sim \phi(\mathbf{x}, m)}\left[f\left(\mathcal{M}(\mathbf{x}, \mathbf{s})\right)\right]","The smoothed model $g(\mathbf{x})$ is defined as the expectation of the original model's output $f(\cdot)$ over all possible masked inputs $\mathcal{M}(\mathbf{x}, \mathbf{s})$, where the binary mask $\mathbf{s}$ is generated by the randomized masking scheme $\phi(\mathbf{x}, m)$ with $m\%$ noise intensity."
naacl_2024_short_16,2,"SelfdenoiseIn the randomized smoothing framework described above, the performance of the smoothed model \(g(\cdot)\) largely depends on the LLM's performance on the masked and corrupted input \(f(\mathcal{M}(\mathbf{x},\mathbf{s}))\). To improve performance, we follow the denoise-smoothing pipeline used in computer vision (Salman et al., 2020) by adding an additional denoising step with a denoiser \(D(\cdot)\). This step in our context involves filling the masked tokens in the masked input before feeding it to the base LLM, _i.e._,","g^{\prime}(\mathbf{x})=\operatorname*{arg\,max}_{e\in\mathcal{Y}}\mathbb{P}_{\mathbf{s }\sim\phi(x,m)}(f(D(\mathcal{M}(\mathbf{x},\mathbf{s})))=c).",,"f(\mathcal{M}(\mathbf{x},\mathbf{s})) \rightarrow f(D(\mathcal{M}(\mathbf{x},\mathbf{s})))","This equation illustrates the modification where a denoiser $D(\cdot)$ is applied to the masked input before passing it to the LLM $f(\cdot)$, aiming to improve performance by reconstructing the masked tokens."
naacl_2024_short_22,1,"## 2 Blinder

We explore learning to select a concise state description from a set of state features given a task.

```
\(S,V_{\theta},\tau\) \(X\leftarrow\emptyset\) while\(|S|>0\) and \(\max_{s\in S}(V_{\theta}(X+s,\tau))>V_{\theta}(X,\tau)\)do \(s\leftarrow\operatorname*{argmax}_{s\in S}(V_{\theta}(X+s,\tau))\) \(X\gets X+s\) \(S\gets S-s\) endwhile return\(X\)
```

**Algorithm 1** State Description Selection

Algorithm 1 defines our policy for selecting state descriptions using BLINDER, where \(S\) is the set of all available state features, \(\tau\) is the task description, and \(V_{\theta}\) is a state description value function. The resulting state description \(X\) is a minimal state description that maximizes the value estimate \(V_{\theta}(X,\tau)\). Note that the worst case time complexity of Algorithm 1 is \(O(|S|^{2})\). However, in our experiments, the selection process always ended after 3-5 state features were selected.

We train \(V_{\theta}\) to estimate downstream performance by leveraging a small set of trajectory demonstrations (\(\sim 5\)), composed of state features, task descriptions, and target actions \(D=\{(S,\tau,a^{*}),...\}\). For each trajectory step, we sample state descriptions \(X\) for training and define a sparse reward function \(R_{LM}\) for assigning rewards for \(X\),","R_{LM}(X,\tau,a^{*})=LM(a^{*}|X,\tau).",,"s \leftarrow \operatorname*{argmax}_{s \in S} V_{\theta}(X + s, \tau)",Selection of the optimal state feature $s$ that maximizes the value function $V_{\theta}$ given current state description $X$ and task $\tau$
naacl_2024_short_22,2,"\(R_{LM}\) maximizes the likelihood that \(X\) elicits the target action \(a^{*}\) from a pretrained LM actor. Note that, although \(R_{LM}\) is defined for a specific LM actor, we find that BLINDER selects intuitive state features and generalizes well to other LM actors.

Figure 2: BLINDER is trained to produce minimal descriptions that maximize target action probability.

Finally, we define the loss function of a single state description","\mathcal{L}(X,S,\tau,a^{*})=\\ \sum_{t=0}^{|X|}(V_{\theta}(X_{:t},\tau)-\gamma^{|X|-t}R_{LM}( X,\tau,a^{*}))^{2}",,"\mathcal{L}(X, \tau, a^*) = -\log R_{\text{LM}}(X, \tau, a^*)","The loss function for a single state description, defined as the negative log-likelihood of the target action given the state description and task description."
naacl_2024_short_22,3,and the overall loss function for \(V_{\theta}\),"\mathcal{L}_{V_{\theta}}=\underset{\begin{subarray}{c}S,\tau,a^{*}\sim D\\ X\sim\pi|S,\tau\end{subarray}}{\mathbb{E}}[\mathcal{L}(X,S,\tau,a^{*})+ \phi].","where \(\gamma\) is a discount factor and \(\phi\) is a Kullback-Leibler penalty for normalizing \(V_{\theta}\), common when finetuning LMs with RL (Stiennon et al","\mathcal{L}_{total} = \sum_{(S,\tau,a^{*}) \in D} \sum_{X \sim \text{BLINDER}(S,\tau)} \mathcal{L}(X,S,\tau,a^{*})","The overall loss function for $V_{\theta}$ aggregates the individual state description losses across all trajectory demonstrations in the dataset $D$, where state descriptions $X$ are sampled using the BLINDER algorithm for each $(S,\tau,a^{*})$ tuple."
naacl_2024_short_23,1,"## 3 Proposed Methods

The problem of geocoding can be formalized as defining a function \(f(m|T,M,E)=\hat{e}\) where \(T\) is the text of a document, \(M\subset T\) is the location mentions in the document, \(E\) is the set of geographical database entries, \(m\in M\) is the mention under consideration, and \(\hat{e}\in E\) is the entry predicted by \(f\) for \(m\). In our paradigm for geocoding, we formulate \(f\) to first predict the country, state, and feature of \(m\), next query the ontology with \(m\) to find candidate entries, then select the entry that violates the fewest constraints implied by the predicted attributes as the prediction \(\hat{e}\). Formally:","\hat{C}_{m},\hat{S}_{m},\hat{F}_{m} =\textsc{attributePredictor}(m,M) || \hat{E} =\textsc{CandidateGenerator}(m,E) || f(m|T,M,E) =\textsc{Constrainer}(\hat{E},\hat{C}_{m},\hat{S}_{m},\hat{F}_{ m})",,"\begin{equation}
\begin{split}
(c, s, f) &= \text{PredictAttributes}(T, M, m), \\
E' &= \text{QueryOntology}(m, E), \\
\hat{e} &= \arg\min_{e \in E'} \text{ConstraintViolations}(e, c, s, f)
\end{split}
\end{equation}","Equation 1 formalizes the geocoding function through a three-stage process: attribute prediction, ontology querying, and constraint-based candidate selection."
naacl_2024_short_23,2,"\). The AttributePredictor (see section 3.1) is a novel formulation of geographical text classification, the CandidateGenerator (see section 3.2) is the best ranking system from prior work, and the Constrainter (see section 3.3) is a novel deterministic constraint-based algorithm.

### Attribute Predictor

This function predicts the country, state, and feature class of \(m\). It is formulated as a text classification model, based on a novel input prompt coupled with a masked language modeling objective. The prediction targets are defined as:

**Feature Class** is one of the nine types defined by GeoNames: \(A\), Administrative boundaries (e.g., countries, states, provinces); \(P\), Populated places (e.g., cities, towns, villages); \(U\), Undersea features (e.g., oceanic ridges, trenches), etc.
**State** is the canonical name of one of the 3871 first-order administrative divisions in GeoNames, such as states, provinces, or regions.
**Country** is the canonical name of one of the 252 countries in GeoNames.

Figure 1: The architecture of our model: GEOgraphical normalization by Predicting Attributes to Constrain Ontology Entries (GeoPLACE). The figure shows how GeoPLACE normalizes a mention of _Paris_.

We implement prediction of these targets as:","Z =\textsc{transformer}(\textsc{Toinput}(m,M)) || \hat{C}_{m} =\mathrm{softmax}(Z_{c}W_{c}) || \hat{S}_{m} =\mathrm{softmax}(Z_{s}W_{s}) || \hat{F}_{m} =\mathrm{softmax}(Z_{f}W_{f})",,,
naacl_2024_short_23,3,"),..., \(m_{|M|}\) in which \(m\) is [MASK] located in [MASK] of [MASK] [SEP]1; \(f\), \(s\), \(c\), are the indexes of the three [MASK] tokens; \(W_{c},W_{s},W_{f}\in\mathbb{R}^{N\times H}\) are the learnable parameters of the three classification heads; \(N\) is the size of the transformer tokenizer's vocabulary; and \(H\) is the size of the transformer's contextualized representations. We add new tokens to the transformer's tokenizer to ensure that every country, state, and feature class is a single token in the classifier output, e.g., making _United States_ a single token. This single-token prediction approach compares favorably to a multi-token sequence-to-sequence prediction approach, as shown in section 4.

Footnote 1: This prompt dramatically reduces the size of the input while still providing most of the critical document-level information for disambiguating toponyms

The model is trained on the labeled data in the toponym datasets with cross-entropy loss:",L=C_{m}log(\hat{C}_{m})+S_{m}log(\hat{S}_{m})+F_{m}log(\hat{F}_{m}),"where \(C_{m}\), \(S_{m}\), and \(F_{m}\) are one-hot vectors of size \(N\) representing the true country, state, and feature class for mention \(m\)",\mathcal{L} = -\sum_{m \in M} \left( \log \hat{C}_m + \log \hat{S}_m + \log \hat{F}_m \right),"The training loss function combines the cross-entropy losses for the predicted country, state, and feature class of each location mention $m$ in the dataset."
naacl_2024_short_25,1,"### Preliminaries

Existing explorations using zero-shot LLMs as pointwise rankers can be broadly divided into two categories: relevance generation (Liang et al., 2023) and query generation (Sachan et al., 2022). We focus on relevance generation in this work.

Given a query \(q\) and a list of candidate documents \(\mathbf{d}=(d_{1},\ldots,d_{m})\), an LLM ranker based on relevance generation takes each query-document pair \((q,d_{i})\) as input and prompts the LLM to answer whether the document is relevant to the query by ""Yes"" or ""No"" (see Figure 1(a)). Then a ranking score \(f(q,d_{i})\in\mathbb{R}\) for each document is calculated based on LLM's log-likelihood score \(s_{i,1}=\text{LLM}(\text{Yes}|q,d_{i})\) and \(s_{i,0}=\text{LLM}(\text{No}|q,d_{i})\) by using a softmax function (Nogueira et al., 2020):","f(q,d_{i})=\frac{\exp(s_{i,1})}{\exp(s_{i,1})+\exp(s_{i,0})}",,"\begin{equation}
f(q, d_i) = \frac{\exp(s_{i,1})}{\exp(s_{i,1}) + \exp(s_{i,0})}.
\end{equation}","The ranking score for a document is computed using a softmax function over the log-likelihood scores of the LLM's ""Yes"" and ""No"" responses for relevance."
naacl_2024_short_25,2,"The ranked list is obtained by sorting the documents based on their ranking scores.

### Prompts

In many datasets, there exist documents that are only partially or marginally relevant to the query, which LLMs struggle to classify into two classes.

Fine-grained relevance labels.We extend the classical relevance generation methods by introducing fine-grained relevance labels. Without loss of generality, we use a set of 3-level graded relevance labels as example: [""Not Relevant"", ""Somewhat Relevant"", ""Highly Relevant""], denoted as \([l_{0},l_{1},l_{2}]\). Then, for each query-document pair \((q,d_{i})\), we ask the LLM to evaluate their relevance by choosing from the given relevance labels. We can obtain the log-likelihood of the LLM generating each relevance label:","s_{i,k}=\text{LLM}(l_{k}|q,d_{i})",,"f(q,d_{i})=\frac{\exp(s_{i,2})}{\sum_{k=0}^{2}\exp(s_{i,k})}","where $s_{i,k}=\text{LLM}(l_{k}|q,d_{i})$ represents the log-likelihood of the LLM generating the $k$-th relevance label for the $i$-th document."
naacl_2024_short_25,3,"This example is illustrated in Figure 1(b). The exact prompt can be found in Appendix G.

Figure 1: Illustration of different prompting strategies for relevance generation LLM rankers.

Rating scale.To avoid using relevance labels with potentially ambiguous order, we can also employ a rating scale. For example, we can prompt the LLM to rate the relevance between the query \(q\) and the document \(d_{i}\) on a scale from 0 to 4. We can then use the LLM to obtain the log-likelihood \([s_{i,0},\dots,s_{i,4}]\) of generating each relevance scale value \([l_{0},\dots,l_{4}]\), which are ""0"" to ""4"" respectively. This method allows us to try arbitrarily fine-grained relevance levels in the prompt. Figure 1(c) illustrates an example of this prompt. The exact prompt can be found in Appendix G.

### Ranking Scores

Once we obtain the log-likelihood of each relevance label, we can derive the ranking scores.

Expected relevance values (ER).The most straightforward way is to calculate the expected relevance value. First, we need to assign a series of relevance values \([y_{0},y_{1},y_{2}]\) to all the relevance labels \([l_{0},l_{1},l_{2}]\), where \(y_{k}\in\mathbb{R}\). Then we can calculate the expected relevance value by:","f(q,d_{i}) =\sum p_{i,k}\cdot y_{k} || \text{where }p_{i,k} =\frac{\exp(s_{i,k})}{\sum_{k^{\prime}}\exp(s_{i,k^{\prime}})}",,"\mathbb{E}[y] = \sum_{k=0}^{2} y_{k} \cdot \frac{\exp(s_{i,k})}{\sum_{j=0}^{2} \exp(s_{i,j})}","The expected relevance value is calculated by taking a weighted sum of relevance values, where the weights are softmax-normalized log-likelihoods of each relevance label."
naacl_2024_short_25,4,"The relevance values \(y_{k}\) can be provided by users or even tuned based on a training data set. We empirically find that naively assigning \(y_{k}=k\) (with \(l_{0}\) to \(l_{k}\) ordered from least to most relevant) already yields excellent performance. Therefore, we simply adopt \(y_{k}=k\).

Peak relevance likelihood (PR).We can further simplify ranking score derivation by focusing on top-ranked items. We propose to only use the log-likelihood of the peak relevance label (""Highly Relevant"" in this example). More formally, let \(l_{k^{*}}\) denote the relevance label with the highest relevance. We can simply rank the documents by:","f(q,d_{i})=s_{i,k^{*}}",,"f(q,d_{i}) = \exp(s_{i,k^{*}})",The ranking score is determined by the exponential of the log-likelihood of the highest relevance label assigned to the document.
naacl_2024_short_26,1,"### Problem Formulation

Formally, given a text \(x\) and a target \(t\), the stance detection task aims to identify the stance \(y\) that \(x\) expresses towards \(t\). \(y\) is basically in the collection \(S=\{favor,against,neutral\}\), which may vary in different datasets. The detection model is trained to infer \(y\) given \(x\) and \(t\) with parameter \(\theta\). If knowledge \(k\) is involved, the formulation will be:","f(x,t,k;\theta)=y",,"y = \arg\max_{s \in S} P(s \mid x, t, k; \theta)","Stance prediction model incorporating knowledge $k$, where $y$ is derived by maximizing the conditional probability over stance set $S$ given text $x$, target $t$, knowledge $k$, and parameters $\theta$."
naacl_2024_short_26,2,"### LLM-Driven Knowledge

To acquire LLM-driven knowledge for stance detection, we employ a partially filled zero-shot prompt for each input text in every dataset, as shown in Appendix A. Specifically, our prompt is designed with three aspects in mind. First, we instruct the LLM to _list keywords_. In that case, the subsequent detection model may pay more attention to these words. Second, we prompt the LLM to _analyze implied emotions and rhetorical devices_, as these may be strong pieces of evidence for the expressed stance. Finally, unlike previous studies, we ask the LLM to _briefly analyze the stance_ rather than produce exact answers directly, reducing the risks of intrinsic hallucination in LLMs, which may mislead the subsequent detection model into generating incorrect predictions. Besides, we add a brief description of the dataset at the beginning, so that LLM may find more background information concerning the dataset by retrieving its internal knowledge.

### BART Backbone

As LLM-driven knowledge may cover rich contextual information, it is vital to associate such information with stance semantics. Recently, generative modeling has shown great potential by leveraging pre-training objectives to decode the answer (Radford et al., 2019). Inspired by prior works (Wen and Hauptmann, 2023), we inject LLM-driven knowledge into BART (Lewis et al., 2020), an auto-regressive transformer, for stance detection.

In that case, the task is reframed as a denoising one, which takes \(h(x,t,k)\) as input and generates an output sequence \(\mathbf{u}\) containing stance labels. Specifically, \(h(x,t,k)\) is a combination of input text \(x\), target \(t\) and LLM-driven knowledge \(k\) with special tokens ""<s> <stance> is the stance for the target \(t\) </s></s> \(x\) </s> \(k\) </s>"", and \(\mathbf{u}\) is formulated as ""<s> <stance> is the stance for the target \(t<\)/s>"". Note that the <stance> placeholder is kept in input while being replaced by the predicted stance in output.

Finally, the BART model is trained by maximizing the log-likelihood over the whole sequence.","\mathcal{L}_{gen}=-\sum_{i=1}^{|\boldsymbol{u}|}\log p(u_{i}|,\boldsymbol{u}_{< i},h(x,t,k);\theta)","where \(p(u_{i}|,\boldsymbol{u}_{<i},h(x,t,k);\theta)\) is the probability to select a token \(u_{i}\) at step \(i\) given the input \(h(x,t,k)\) and previously generated tokens \(\boldsymbol{u}_{<i}\)","\max_{\theta} \log p(\mathbf{u} \mid h(x,t,k);\theta)","The BART model is trained by maximizing the log-likelihood of the output sequence $\mathbf{u}$ given the combined input $h(x,t,k)$ parameterized by $\theta$."
naacl_2024_short_26,3,"\).

### Prototypical Contrastive Learning

One may notice that \(\mathcal{L}_{gen}\) maximizes the likelihood over the entire sentence. In fact, more optimizations may be beneficial especially when decoding the stance label. To this end, we decouple the stance embedding \(\boldsymbol{z}\in\mathbb{R}^{embed\_size}\) (the embedding used to generate the stance token) from BART decoder outputs. We then project it into a low-dimensional vector \(\boldsymbol{\hat{z}}\in\mathbb{R}^{low\_embed\_size}\) to prevent \(\boldsymbol{z}\) losing much semantics and being over-corrected. To regularize \(\boldsymbol{\hat{z}}\) to be more discriminative in the latent space, inspired by Li et al. (2021), we introduce the concept of prototypes, they are widely adopted in data-efficient learning Li et al. (2024) and can be viewed as the representatives of class-wise embeddings. However, different from Li et al. (2021), we view prototypes as the representatives of class-wise projected embeddings instead of class-agnostic ones. By interacting with prototypes, a contrastive loss is employed to increase the intra-class similarity but decrease the inter-class similarity of projected stance embeddings. In the following, we will detail how to estimate class-wise prototypes and formulate the contrastive loss.

Online Prototype UpdateFor each stance class \(c\), we randomly initialize a vector \(\boldsymbol{v}_{c}\) as its prototype before normalizing it into a unit one at the beginning. Along the training progress, we update \(\boldsymbol{v}_{c}\) at each step in a moving average manner by,",\boldsymbol{v}_{c}arrow\text{Normalize}(\beta\boldsymbol{v}_{c}+(1- \beta)\boldsymbol{v}_{c}^{\prime}),"where \(\beta\) is a momentum coefficient, \(\text{Normalize}(\cdot)\) is the normalization function, and \(\boldsymbol{v}_{c}^{\prime}\) is the centroid of embeddings belonging to class \(c\) in the batch",\boldsymbol{v}_{c}^{(t)}=\alpha\boldsymbol{v}_{c}^{(t-1)}+(1-\alpha)\boldsymbol{\hat{z}}_{c}^{(t)},"where $\boldsymbol{v}_{c}^{(t)}$ is the updated prototype for class $c$ at step $t$, $\alpha$ is the moving average coefficient, and $\boldsymbol{\hat{z}}_{c}^{(t)}$ is the projected stance embedding for class $c$ at step $t$"
naacl_2024_short_26,4,"ch.

Prototypical Contrastive LossTo compute the loss, we firstly obtain the embedding-to-class cosine similarity score \(s_{j}=<\boldsymbol{\hat{z}},\boldsymbol{v}_{j}>\). Then, we optimize the following loss,",\mathcal{L}_{con}=-\sum_{c=1}^{C}y_{c}\log\frac{\text{exp}(\frac{s_{c}}{ \gamma})}{\sum_{j=1}^{C}\text{exp}(\frac{s_{j}}{\gamma})},where \(\gamma\) is a scalar temperature parameter and \(\boldsymbol{y}\) is the one-hot label for the current sample,"\mathcal{L}_{con}=-\log \frac{\exp(<\boldsymbol{\hat{z}},\boldsymbol{v}_{y}>/\tau)}{\sum_{j=1}^{|\mathcal{C}|}\exp(<\boldsymbol{\hat{z}},\boldsymbol{v}_{j}>/\tau)}","where $y$ is the ground-truth stance class, $\mathcal{C}$ is the set of all stance classes, and $\tau$ is a temperature scaling factor that controls the concentration level of the distribution."
naacl_2024_short_26,5,"le. If it belongs to class \(c\), optimizing \(\mathcal{L}_{con}\) will maximize \(s_{c}\) but minimize \(s_{j}\) (\(j=1,\cdots,C\) and \(j\neq c\)), thereby pulling together \(\boldsymbol{\hat{z}}\) and class \(c\)'s prototype \(\boldsymbol{v}_{c}\) while pushing away \(\boldsymbol{\hat{z}}\) from prototypes of other classes. Consequently, a well-structured feature space is modeled with stance embeddings from the same class clustering together, making it easier to reach semantic alignment.

Relation with JointCLAlthough previous work, namely JointCL (Liang et al., 2022), also introduced the concept of prototypes, our approach is

Figure 2: Overview of our proposed LKI-BART.

fundamentally distinctive from it. As for the purpose of prototypes, JointCL obtains class-agnostic prototypes to build a graph dedicated to adapting and refining representations of unseen targets, while we rely on class-specific prototypes to regularize the feature space. Besides, JointCL still follows the traditional supervised contrastive loss Khosla et al. (2020), forcing larger cosine similarity between samples that share the same stance label, while prototypes are not involved in contrastive loss at all. By contrast, our method encourages large similarity scores between a sample's embedding and its corresponding prototype, which is less computationally intensive and more robust, since potential mis-calibration can be alleviated by viewing prototypes as representatives of class-wise embeddings.

The overall loss function of LKI-BART considering both \(\mathcal{L}_{gen}\) and \(\mathcal{L}_{con}\) is defined as follows:",\mathcal{L}=\lambda_{l}\cdot\mathcal{L}_{gen}+(1-\lambda_{l})\cdot\mathcal{L} _{con},where \(\lambda_{l}\) is involved to balance the optimization,\mathcal{L}_{total} = \mathcal{L}_{gen} + \lambda \mathcal{L}_{con},where $\lambda$ is a hyperparameter balancing the contributions of the generation loss $\mathcal{L}_{gen}$ and the contrastive loss $\mathcal{L}_{con}$
naacl_2024_short_27,1,"#### 2.1.3 Calculating KL Loss

During fine-tuning, we calculate the similarity between the representation of example \(X_{i}\) and each pair of attribute representations using dot product:","[sim_{A}^{j},sim_{B}^{j}]=X_{i}\cdot[X_{A}^{j},X_{B}^{j}]",,"\begin{equation}
    \text{sim}(X_i, A_j) = X_i \cdot A_j
\end{equation}",Similarity between example representation $X_i$ and attribute representation $A_j$ using dot product.
naacl_2024_short_27,2,"Then we apply the softmax function \(\sigma(a,b)=\frac{e^{a}}{e^{a}+e^{b}}\) to obtain the similarity distribution:","d_{sim}^{j}=\sigma(sim_{A}^{j},sim_{B}^{j})",,"[\hat{sim}_{A}^{j},\hat{sim}_{B}^{j}] = \sigma(sim_{A}^{j}, sim_{B}^{j})",Normalized similarity scores between the example representation and each attribute representation using the softmax function.
naacl_2024_short_27,3,"To calculate the overall KL loss, we compute KL divergence between each of the similarity distributions \(d_{sim}^{j}\) and a uniform distribution \(d_{uni}\):","L_{kl}=\sum_{j=1}^{K}D_{KL}(d_{sim}^{j},d_{uni})",,\mathcal{L}_{KL} = \sum_{j=1}^{M} \text{KL}(d_{sim}^{j} \| d_{uni}),Kullback-Leibler divergence loss computed between each similarity distribution and a uniform distribution across all M attributes
naacl_2024_short_27,4,"Finally, we compute the total loss:","L_{total}=L_{ce}+\lambda L_{kl},",where \(L_{ce}\) is the usual cross-entropy loss,\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{task}} + \lambda \cdot L_{kl},Weighted sum of task-specific loss and KL divergence loss with hyperparameter λ controlling the relative contribution
naacl_2024_short_28,1,"## 2 MBR and DPO

We follow the expectation-by-sampling approach to MBR [1]. Given a set of sampled translations \(H(\mathbf{x})=\{\mathbf{y}^{\prime}\sim P(\cdot|\mathbf{x})\}\) and a loss (or utility) function \(L(\cdot,\cdot)\), the score (negative Bayes risk) of each translation is found as","S(\mathbf{y})=-\frac{1}{|H(x)|}\sum_{\mathbf{y}^{\prime}\in H(\mathbf{x})}L( \mathbf{y}^{\prime},\mathbf{y})",,"\begin{equation}
    S(\mathbf{y}|\mathbf{x}) = \mathbb{E}_{\mathbf{y}^{\prime}\sim P(\cdot|\mathbf{x})}\left[L(\mathbf{y},\mathbf{y}^{\prime})\right].
\end{equation}","Expected score (negative Bayes risk) of a translation $\mathbf{y}$ given input $\mathbf{x}$, computed as the expectation of the loss function $L(\mathbf{y}, \mathbf{y}^{\prime})$ over the distribution of sampled translations $\mathbf{y}^{\prime}$."
naacl_2024_short_28,2,and the MBR hypothesis is then computed as,"\mathbf{y}^{*}=\operatorname*{arg\,max}_{\mathbf{y}\in H(\mathbf{x})}S( \mathbf{y})",,\hat{\mathbf{y}}_{\text{MBR}} = \arg\min_{\mathbf{y} \in H(\mathbf{x})} S(\mathbf{y}),"The MBR hypothesis is the translation that minimizes the score function $S(\mathbf{y})$, which represents the negative Bayes risk."
naacl_2024_short_28,3,"This is simple but expensive. Our goal is to train a model that produces translations with scores consistent with MBR, but without multi-step decoding.

### DPO Fine-Tuning Objective

DPO [14] reformulates the usual approach to Reinforcement Learning from Human Feedback (RLHF) so as to avoid a distinct reward modelling step. The typical RLHF criteria is","\max_{\pi_{\theta}} \mathbb{E}_{\mathbf{x}\sim D,\mathbf{y}\sim\pi_{\theta}(\mathbf{ y}|\mathbf{x})}[r_{\phi}(\mathbf{x},\mathbf{y})] || -\beta\mathbb{D}_{KL}[\pi_{\theta}(\mathbf{y}|\mathbf{x}) \parallel\pi_{\text{ref}}(\mathbf{y}|\mathbf{x})]",,"\log P_{\theta}(\mathbf{y}^{*}|\mathbf{x}) = \log \frac{\exp(\beta r(\mathbf{y}^{*},\mathbf{x}))}{\sum_{\mathbf{y}}\exp(\beta r(\mathbf{y},\mathbf{x}))}","Log probability of the optimal translation given the input under the DPO objective, parameterized by $\theta$, with reward function $r(\cdot,\cdot)$ and temperature parameter $\beta$."
naacl_2024_short_28,4,"el. DPO effectively replaces the reward model with a preference distribution based on \(\pi_{\theta}\), the model being trained; DPO also retains the KL regularization term with weighting \(\beta\).

The preference dataset \(D\) for DPO consists of triplets \((\mathbf{x},\mathbf{y}_{w},\mathbf{y}_{l})\) where \(\mathbf{x}\) is the input prompt, \(\mathbf{y}_{w}\) is the winnng (prefered) response, and \(\mathbf{y}_{l}\) is the losing (dispreferred) response. DPO uses the language model likelihood to approximate the reward as \(\beta\text{log}\frac{\pi_{\theta}(\mathbf{y}|\mathbf{x})}{\pi_{\text{ref}}( \mathbf{y}|\mathbf{x})}\). During training, with \(\pi_{\theta}\) typicallyinitialized from \(\pi_{\text{ref}}\), the objective is to maximize the expected reward margin between \(\mathbf{y}_{w}\) and \(\mathbf{y}_{l}\):","L_{\text{DPO}}=-\mathbb{E}_{(\mathbf{x},\mathbf{y}_{w},\mathbf{y}_{l})\sim D}[ \text{log}\sigma(M(\mathbf{y}_{w},\mathbf{y}_{l},\mathbf{x},\theta))] || \beta(\text{log}\frac{\pi_{\theta}(\mathbf{y}_{w}|\mathbf{x})}{\pi_{ \text{ref}}(\mathbf{y}_{w}|\mathbf{x})}-\text{log}\frac{\pi_{\theta}(\mathbf{ y}_{l}|\mathbf{x})}{\pi_{\text{ref}}(\mathbf{y}_{l}|\mathbf{x})})","where the reward margin \(M(\mathbf{y}_{w},\mathbf{y}_{l},\mathbf{x},\theta)\) is

\[\beta\left(\text{log}\frac{\pi_{\theta}(\mathbf{y}_{w}|\mathbf{x})}{\pi_{ \text{ref}}(\mathbf{y}_{w}|\mathbf{x})}-\text{log}\frac{\pi_{\theta}(\mathbf{ y}_{l}|\mathbf{x})}{\pi_{\text{ref}}(\mathbf{y}_{l}|\mathbf{x})}\right) \tag{5}\]

### Related Work in Translation

Previous work has explored the effectiveness of enhancing the translation performance of LLMs via Reinforcement Learning (RL) algorithms or supervised fine-tuning","\max_{\pi_{\theta}} \mathbb{E}_{(\mathbf{x},\mathbf{y}_{w},\mathbf{y}_{l})\sim D} \left[ \log \frac{\sigma\left(\beta \log \frac{\pi_{\theta}(\mathbf{y}_w|\mathbf{x})}{\pi_{\text{ref}}(\mathbf{y}_w|\mathbf{x})} - \beta \log \frac{\pi_{\theta}(\mathbf{y}_l|\mathbf{x})}{\pi_{\text{ref}}(\mathbf{y}_l|\mathbf{x})}\right)}{\sigma\left(\beta \log \frac{\pi_{\theta}(\mathbf{y}_l|\mathbf{x})}{\pi_{\text{ref}}(\mathbf{y}_l|\mathbf{x})} - \beta \log \frac{\pi_{\theta}(\mathbf{y}_w|\mathbf{x})}{\pi_{\text{ref}}(\mathbf{y}_w|\mathbf{x})}\right)} \right]","This equation represents the DPO objective, which maximizes the expected reward margin between the winning and losing responses using a sigmoid function to model the preference probability."
naacl_2024_short_32,1,"## 2 Preliminaries

Let \(u(y,r)\) be a utility function to measure the quality of model translation \(y\) (**candidate**; Freitag et al.,2022) given its reference translation \(r\). Among a set of candidates \(\mathcal{Y}\), MBR decoding selects the one that minimizes the expected error or, equivalently, maximizes the expected utility (Kumar and Byrne, 2002, 2004; Freitag et al., 2022):","y^{*}=\operatorname*{arg\,max}_{y\in\mathcal{Y}}\mathbb{E}_{r\sim P_{\mathrm{ human}}(\cdot|x)}[u(y,r)].",,"\begin{equation}
    y^* = \arg\max_{y \in \mathcal{Y}} \mathbb{E}_{r \sim p(r)}[u(y, r)]
\end{equation}",Equation for MBR decoding that selects the candidate translation maximizing the expected utility over the reference distribution.
naacl_2024_short_32,2,"Here, \(P_{\mathrm{human}}(\cdot|x)\) is the **true distribution** over translations of an input text \(x\)(Kumar and Byrne, 2002, 2004), which describes human-quality translations in the space of all translations.

Since the true distribution is unknown, MBR decoding approximates Eq. (1) with finite samples drawn from a model \(r^{\prime}\sim P_{\mathrm{model}}(\cdot|x)\). That is, MBR decoding _assumes that the samples drawn from a model approximate the true distribution_ of references. The samples are called **pseudo-references**(Freitag et al., 2022), which subsequently serve as alternatives to references in the computation of MBR as follows:","y^{*}=\operatorname*{arg\,max}_{y\in\mathcal{Y}}\frac{1}{|\mathcal{R}^{ \prime}|}\sum_{r^{\prime}\in\mathcal{R}^{\prime}}u(y,r^{\prime}).",,"y^{*}=\operatorname*{arg\,max}_{y\in\mathcal{Y}}\frac{1}{K}\sum_{k=1}^{K}u(y,r_{k}^{\prime}),\quad r_{k}^{\prime}\sim P_{\mathrm{model}}(\cdot|x).",This equation approximates the expected utility in Equation 1 by averaging the utility over K pseudo-references sampled from the model distribution.
naacl_2024_short_33,1,"## 4 Method

We propose MoCL, a novel CL approach for language models to tackle catastrophic forgetting and enhance knowledge transfer at the same time.

**Avoiding Catastrophic Forgetting.** We utilize two representative PEFT methods, prefix-tuning (Li and Liang, 2021) and LoRA (Hu et al., 2021) for allocating task-specific parameters to LMs, avoiding catastrophic forgetting without storing data samples. In particular, MoCL adds a set of trainable PEFT parameters (prefix or LoRA) to the frozen pretrained language model (PLM) for downstream task fine-tuning. Instead of updating the whole

Figure 1: Overview of the MoCL framework for continual learning. MoCL continually adds new modules to language models and composes existing and new modules based on task matching weights for learning the new task.

model, only a small number of the PEFT parameters are trained. As illustrated in Figure 1, MoCL optimizes the task-specific modules and keeps the PLM frozen. For each task \(T_{n}\) in the sequence, we initialize a trainable module \(P_{n}\) for fine-tuning. After the training on one task is finished, the corresponding PEFT parameters are frozen to preserve the task-specific knowledge in the following training process, thus avoiding catastrophic forgetting.

**Enabling Knowledge Transfer.** MoCL introduces task feature vectors for task matching and composes old and new modules for learning. This composition strategy facilitates effective knowledge transfer, which is often ignored by prior work. In particular, while learning on \(T_{n}\), the previously acquired knowledge, which is encoded in the respective PEFT module \((P_{1},\dots,P_{n-1})\), is reused via a weighted summation, denoted as \(P^{\prime}_{n}=\sum_{k=1}^{n}\alpha_{k}P_{k}\). Here, \(P_{k}\) is the module specific to the \(k^{\text{th}}\) task and \(\alpha_{k}\) is the weight determining the contribution of \(P_{k}\) for new task learning. We detail its computation below. Finally, the composed module \(P^{\prime}_{n}\) is combined with the PLM, consisting of all the module components up to the current task.

To calculate the contribution weights \(\alpha_{k}\) of each task-specific module, we introduce trainable task feature vectors \(V\in\mathbb{R}^{N\times D}\) to capture salient features of tasks in the CL sequence. Note that each task-specific vector \(v\in\mathbb{R}^{D}\) has the same dimension as the input embeddings \(x_{n}\in\mathbb{R}^{D}\) (i.e., the embeddings from the PLM encoder). Then, we calculate the cosine similarity between the input embeddings \(x_{n}\) and feature vectors up to the current \(n^{\text{th}}\) task \(V\left[\colon n\right]\) as task matching scores \(\alpha\left[\colon n\right]=\cos(x_{n},V\left[\colon n\right])\).

**Training and Inference.** The training objective for the \(n^{\text{th}}\) task is to find the PEFT module \(P_{n}\) and the task feature vector \(v_{n}\) that minimize the cross-entropy loss of training examples, and, at the same time, maximize the cosine similarity between \(v_{n}\) and the corresponding task input embeddings \(x_{n}\):","\min_{P_{n},v_{n}}-\sum_{x_{n},y_{n}}\log p(y_{n}|x_{n},P^{\prime}_{n},\theta) -\sum_{x_{n}}\cos(x_{n},v_{n})",,"\mathcal{L}_{n} = \mathcal{L}_{\text{CE}}(P_{n}, v_{n}) - \lambda \cdot \cos(x_{n}, v_{n})","The loss function for the $n^{\text{th}}$ task in MoCL, combining cross-entropy loss and cosine similarity maximization between the task feature vector and input embeddings."
naacl_2024_short_33,2,"During inference, as the task identities are available in the TIL setting, we directly select the task-specific module for inference. In the CIL setting, we use the matching scores between task inputs and the feature vectors for module composition. The resulting module is combined with the PLM for inference.

## 5 Experimental Setup

In this section, we describe our experimental setup.

Following Wang et al. (2023), we distinguish benchmarks according to the domain similarity of tasks. As _near-domain_ benchmarks, we use the Web-of-Science (WOS) document classification dataset (Kowsari et al., 2017) consisting of 7 tasks, and AfriSenti (Muhammad et al., 2023), a multilingual sentiment analysis dataset with 12 African languages. As _far-domain_ benchmark, we use the widely adopted MTL5 dataset (de Masson D'Autume et al., 2019), including 5 text classification tasks. We summarize the details of MTL5 in Table 1. We adopt the same multiple task orders as the prior works for evaluation. Detailed task information is provided in Appendix A.1.

### Training Details

We adopt four LMs for these datasets in line with prior works (Razdaibiedina et al., 2022; Wang et al., 2023, 2024). We use encoder-based models for WOS, AfriSenti and MTL5 datasets (BERT (Devlin et al., 2018), AfroXLMR (Alabi et al., 2022) and BERT, respectively), the encoder-decoder T5 model (Raffel et al., 2020) as well as the decoder-based Llama 2-7B model (Touvron et al., 2023) for MTL5 under the few-shot setting. For all models except Llama 2,

\begin{table}
\begin{tabular}{l l l l} \hline \hline
**Dataset** & **Class** & **Task Type** & **Domain** \\ \hline AGNews & 4 & Topic classification & News \\ Yelp & 5 & Sentiment analysis & Yelp reviews \\ Amazon & 5 & Sentiment analysis & Amazon reviews \\ DBPedia & 14 & Topic classification & Wikipedia \\ Yahoo & 10 & Q\&A & Yahoo Q\&A \\ \hline \hline \end{tabular}
\end{table}
Table 1: Details of the MTL5 dataset.

\begin{table}
\begin{tabular}{l|c|c c c c} \hline \hline  & & \multicolumn{4}{c}{**AfriSenti Orders**} \\
**Method** & **WOS** & **AVG** & **1** & **2** & **3** \\ \hline Sequential FT-F & 47.15 & 6.17 & 5.62 & 6.52 & 6.30 \\ Sequential FT-P & 53.86 & 49.10 & 50.05 & 49.74 & 47.53 \\ Per-task FT & 82.78 & 52.41 & 52.41 & 52.41 & 52.41 \\ ProgPrompt & 89.93 & 49.07 & 50.16 & 46.74 & 50.30 \\ EPI & 77.83 & 43.10 & 41.49 & 42.65 & 45.16 \\
**MoCL (Ours)** & **90.59** & **56.77** & **57.05** & **56.52** & **56.74** \\ \hline \hline \end{tabular}
\end{table}
Table 2: TIL results on near-domain WOS and AfriSenti datasets. MoCL outperforms existing continual learning methods on both datasets, suggesting MoCL effectively facilitates knowledge transfer across near-domain tasks.

we use prefix-tuning as the task-specific modules, and LoRA as the task modules on Llama 2. All design choices are kept consistent with previous works to ensure a fair comparison. The reported results are the average performance after training on all tasks consecutively. All results are averaged over three random seeds. The detailed experimental settings are provided in Appendix A.4.1.

### Baselines

To compare different CL methods, we include the following baselines4: Sequential fine-tuning continuously fine-tunes the language model on the task sequence: **Sequential FT-F** means all model parameters are updated (fully fine-tuning),5 while **Sequential FT-P** only fine-tunes the PEFT parameters; **Per-task FT** trains a separate PEFT module for each task; and the parameter isolation-based methods **ProgPrompt**(Razdaibiedina et al., 2022), **EPI**(Wang et al., 2023) and O-LoRA (Wang et al., 2023). A detailed description of these methods can be found in Appendix A.3.1.

Footnote 4: For consistency, we include the results of baseline methods compatible with multiple base models used in this work. Results of other baselines which are specifically designed for certain LMs can be found in Appendix A.4.

Footnote 5: We did not run the Sequential FT-F experiments on Llama 2 because of the computational overhead and its poor performance in other experimental setups.

## 6 Experimental Results

In this section, we discuss our experimental results.

### MoCL for Task-Incremental Learning

**Near-domain.** As shown in Table 2, MoCL outperforms state-of-the-art methods on both benchmarks. It is 7.81 and 4.36 points better than training each task with an individual model (per-task FT), indicating it realizes effective knowledge transfer.

Since EPI consists of task identification and per-task fine-tuning, its performance depends on the task identification accuracy. While it achieves comparable results with per-task fine-tuning on WOS, the performance degrades on AfriSenti, where different languages could be harder to differentiate.

While MoCL achieves comparable results to ProgPrompt on WOS (0.66 percentage points better), the performance gap on AfriSenti is considerably higher (7.7 points better). We assume this is due to the suboptimal knowledge transfer of ProgPrompt, which we will analyze in Section 7.1.

**Far-domain.** Table 3 provides the results on MTL5 using BERT, T5 and Llama 2 models. MoCL again outperforms other CL methods in both cases across different task orders. Its advantage over per-task fine-tuning is less pronounced, which is due to the fact that far-domain tasks share weaker similarities.

### MoCL for Class-Incremental Learning

Table 4 presents the class-incremental results. We compare MoCL only to EPI as they are the only two rehearsal-free CL methods applicable to this setting. Unlike EPI, our model has no explicit task identification component. Nevertheless, it still achieves better or competitive results.

## 7 Analysis

In this section, we analyze MoCL's forward transfer capability and its matching weights distribution.

### Forward Transfer Analysis

We calculate the forward transfer scores (FWT) (Wang et al., 2023) of MoCL and ProgPrompt in the TIL setting (see Table 5).6 The FWT metric evaluates the average influence of all previous tasks on the current task:

Footnote 6: As mentioned in 6.1, EPI consists of task identification and per-task FT. Thus, with given task IDs, EPI is identical to per-task FT, thus, includes no knowledge transfer (\(\text{FWT}=0\)).","\text{FWT}=\frac{1}{N-1}\sum_{j=2}^{N}(a_{i,i}-\tilde{a}_{i}),","where \(N\) is the number of tasks in the continual learning sequence, \(a_{i,i}\) denotes the performance evaluated on the \(i\)-th task after incremental learning on the first \(i\) tasks, \(\tilde{a}_{i}\) is the task performance of a randomly initialized reference model trained with dataset \(D_{i}\)","\text{FWT} = \frac{1}{N-1}\sum_{n=1}^{N-1} A_{n+1,n} - B_{n+1,n}","Forward transfer score (FWT) measures the average influence of previous tasks on the performance of subsequent tasks, where $A_{n+1,n}$ is the accuracy of task $n+1$ after learning task $n$, and $B_{n+1,n}$ is the accuracy of task $n+1$ without prior learning."
naacl_2024_short_34,1,"## 1 Introduction

There is a recent surge in the use of LRNNs (Gu et al., 2022; Peng et al., 2023; Orvieto et al., 2023) as alternatives to the de-facto Transformer architecture (Vaswani et al., 2017; Radford et al., 2019), which is ingrained in the field of natural language processing. LRNNs depart from the inter-timestep non-linearity design principle of classic RNNs (Elman, 1990; Jordan, 1997; Hochreiter and Schmidhuber, 1997; Cho et al., 2014), while at the same time: 1. achieving Transformer-level performance on the task of natural language modeling (Fu et al., 2023; Poli et al., 2023) and even better performance on synthetic long-range modeling tasks (Gu et al., 2022; Gupta et al., 2022; Orvieto et al., 2023; Hasani et al., 2023; Smith et al., 2023). 2. having the added benefits of fast parallelizable training (Martin and Cundy, 2018) and constant inference cost.

In spite of the remarkable empirical performance on natural language tasks, there has been no research on LRNNs' ability to model regular language. Regular language is a type of language that strictly follows certain rules like grammar.1 The successful modeling of a regular language is important since it implies a model's ability to learn the underlying rules of the data. For example, if the training data are arithmetic operations such as \(1+2\times 3\), a model should learn the rules of \(a+b\), \(a\times b\), and that \(\times\) has a higher priority than \(+\). Learning unambiguous rules behind the data is a critical step toward sequence modeling with regulated output.

Footnote 1: Formally speaking, the rules are defined/recognized by the underlying finite-state machine.

In this paper, we aim to determine if existing LRNNs are competent to learn the correct grammar of regular language by testing their language transduction capability under the length extrapolation setting. Concretely, a model is trained only to predict the desired outputs on a set of short sequences of length \(L_{tr}\). It then needs to predict the correct outputs for longer testing sequences of length \(L_{ex}\gg L_{tr}\). Adopting the length extrapolation setting is essential to mitigate the risk of a model learning spurious shortcut solutions (Liu et al., 2023).

We theoretically show that some of the recently proposed LRNNs lack the expressiveness to encode certain arithmetic operations used in the tasks of regular language. In light of this observation, we propose a new LRNN equipped with a block-diagonal and input-dependent transition matrix, which enable the successful modeling of regular language. Experiments show that the proposed model is the only LRNN architecture that can extrapolate well on regular language tasks such as Sum, Even Pair, and Modular Arithmetic.

LRNNs in this work have the following generalformulation:",\begin{split} x_{k}&=A_{k}x_{k-1}+Bu_{k}\\ y_{k}&=h(x_{k}).\end{split},,"\begin{aligned}
    \mathbf{h}_t &= \mathbf{A}_t \mathbf{h}_{t-1} + \mathbf{B}_t \mathbf{x}_t, \\
    \mathbf{y}_t &= \mathbf{C}_t \mathbf{h}_t,
\end{aligned}","Equation 1 defines the general formulation of LRNNs, where the hidden state at time step $t$ is updated using a transition matrix $\mathbf{A}_t$ and an input matrix $\mathbf{B}_t$, and the output is generated using a projection matrix $\mathbf{C}_t$."
naacl_2024_short_34,2,"\(A_{k}\) is a matrix that defines the recurrence relation. \(A_{k}\) may or may not depend on the input \(u_{k}\). When it is input-independent, \(A_{k}\) is reduced to \(A\); otherwise, \(A_{k}=g(u_{k})\) for some function \(g\). The first line encodes a linear recurrence in the state \(x_{k}\). The second line is an output \(y_{k}\) that depends on \(x_{k}\). To control the expressiveness, the function \(h\) may or may not be a linear operation. Since the existing LRNNs differ in their linear recurrence relations (Eq. (2), (3), and (4)), we mainly focus on analyzing these relations.

## 2 Limitations of Most LRNNs

In this section, we theoretically show that most LRNNs are unable to represent arithmetic operations. The analysis serves as a motivation to study input-dependent transition matrices with constraints on their column norm.

### Input-independent LRNN

To begin with, state-space models (in discrete-time format) follow the standard LRNN recurrence relation:",x_{k}=Ax_{k-1}+Bu_{k},,"\begin{split} x_{k} &= A x_{k-1} + B u_{k} \\ y_{k} &= h(x_{k}), \end{split}","Equation 2 represents the standard LRNN recurrence relation with an input-independent transition matrix $A$, where the state evolution depends linearly on the previous state and the current input."
naacl_2024_short_34,3,"Eq. (2) encapsulates the recurrence relation of S4 (Gu et al., 2022; Gupta et al., 2022), S5 (Smith et al., 2023), and Linear Recurrent Unit (Orvieto et al., 2023). For example, \(A\) represents the HiPPO matrix family (Gu et al., 2023) of S4 or a complex diagonal matrix of Linear Recurrent Unit. We show in Proposition 1 that such an input-independent matrix \(A\) cannot represent subtraction.

**Proposition 1**.: _An input-independent LRNN is inconsistent in representing subtraction._

Proof.: Denote \(u_{0}\), \(u_{-}\), and \(u_{1}\) as the input vector w.r.t. input characters 0, -, and 1. Denote \(z\) as the initial state vector. The sequences ""0-1"" and ""1-0"" are represented as","\begin{split} x_{0-1}&=A^{3}z+A^{2}u_{0}+Au_{-}+u_{ 1},\ \ \text{for ""0-1""}\\ x_{1-0}&=A^{3}z+A^{2}u_{1}+Au_{-}+u_{0},\ \ \text{for ""1-0""} \end{split}",,"\begin{aligned} \text{""0-1""}: & \quad x_{0} = A z + B u_{0}, \quad x_{1} = A x_{0} + B u_{-}, \quad x_{2} = A x_{1} + B u_{1} \\ \text{""1-0""}: & \quad x_{0} = A z + B u_{1}, \quad x_{1} = A x_{0} + B u_{-}, \quad x_{2} = A x_{1} + B u_{0}. \end{aligned}","Equation 3 represents the state transitions for the sequences ""0-1"" and ""1-0"" using an input-independent LRNN, which is critical for demonstrating the model's inability to represent subtraction in Proposition 1."
naacl_2024_short_34,4,"Because \(0-1\neq 1-0\), by forcing \(x_{0-1}\neq x_{1-0}\), we have",A^{2}u_{0}+Au_{-}+u_{1}\neq A^{2}u_{1}+Au_{-}+u_{0}.,,A^{2}u_{0}+u_{1} \neq A^{2}u_{1}+u_{0},"This inequality arises from the requirement that the state representations of sequences ""0-1"" and ""1-0"" must differ due to the non-commutative nature of subtraction."
naacl_2024_short_34,5,"On the other hand, let \(x_{0-}=A^{2}z+Au_{0}+u_{-}\) be the vector representation for ""0-"". The sequences ""0-0-1"" and ""0-1-0"" are represented as",\begin{split} x_{0-0-1}&=A^{3}x_{0-}+A^{2}u_{0}+Au_ {-}+u_{1}\\ x_{0-1-0}&=A^{3}x_{0-}+A^{2}u_{1}+Au_{-}+u_{0}.\end{split},,"\begin{split} x_{0-0-1} &= A^{3}x_{0-} + A^{2}u_{0} + Au_{-} + u_{1}, \quad \text{for ""0-0-1""} \\ x_{0-1-0} &= A^{3}x_{0-} + A^{2}u_{1} + Au_{-} + u_{0}, \quad \text{for ""0-1-0""} \end{split}","Equation 5 represents the state evolution for sequences ""0-0-1"" and ""0-1-0"" to demonstrate the limitations of input-independent LRNNs in distinguishing structurally similar but semantically different sequences."
naacl_2024_short_34,6,"Notice \(x_{0-0-1}\) is for ""0-0-1"" while \(x_{0-1-0}\) for ""0-1-0"". Enforcing \(x_{0-0-1}=x_{0-1-0}\), we have","A^{2}u_{0}+Au_{-}+u_{1}=A^{2}u_{1}+Au_{-}+u_{0},",,\begin{split} A^{3}x_{0-} + A^{2}u_{0} + Au_{-} + u_{1} &= A^{3}x_{0-} + A^{2}u_{1} + Au_{-} + u_{0} \\ \Rightarrow A^{2}u_{0} + u_{1} &= A^{2}u_{1} + u_{0}. \end{split},"This equation enforces equality between the state representations of the sequences ""0-0-1"" and ""0-1-0"", leading to a constraint on the matrix $A$ and input vectors $u_0$, $u_1$."
naacl_2024_short_34,7,"which is a contradiction. 

The limitation described by Proposition 1 also applies to models adopting diagonal linear recurrence relations (Gupta et al., 2022; Smith et al., 2023; Orvieto et al., 2023). The failure to represent regular language will be corroborated by the inferior length extrapolation performance reported later in SS 4.

## 3 Proposed Method

Now that input-independent LRNNs struggle with representing arithmetic operations, we review the paradigms known to model regular language, which is the type of formal language recognized by a Finite State Automata (FSA) (Chomsky, 1956). An FSA is described by a 5-tuple \((Q,\Sigma,\delta,q_{0},F)\). \(Q\) and \(\Sigma\) are non-empty sets of states and input symbols. \(q_{0}\in Q\) is an initial state. \(\delta:Q\times\Sigma\to Q\) is an input-dependent transition function; \(F\subseteq Q\) is a set of final states.

We hypothesize that an LRNN could model regular language if it can simulate an FSA, whose transition function has the following two key properties:

* It is input-dependent.
* If represented in the matrix form, its column vectors all have unit norm (in \(\|\cdot\|_{1}\)).

### Diagonal Input-dependent LRNN

Let us first examine the simplest input-dependent LRNN:","x_{k}=\text{diag}(v_{k})x_{k-1}+Bu_{k},",where \(v_{k}=f(u_{k})\) is a vector that depends on \(u_{k}\),\begin{split} x_{k} &= \text{diag}(g(u_{k}))x_{k-1} + Bu_{k} \\ y_{k} &= h(x_{k}) \end{split},"Equation 7 defines a diagonal input-dependent LRNN where the state transition matrix is diagonal and depends on the input $u_k$ through a function $g$, aiming to address the limitations of input-independent models in representing arithmetic operations and regular languages."
naacl_2024_short_34,8,"\). Unfortunately, we show that a diagonal input-dependent LRNN still cannot represent subtraction in Proposition 2.

**Proposition 2**.: _A diagonal input-dependent LRNN is inconsistent in representing subtraction._

The proof is essentially a generalization of Proposition 1 and is deferred to Appendix A.1.

### Improved Expressiveness: Liquid-S4

To improve the expressiveness of Eq. (3), we note that the recently proposed liquid-S4 (Hasani et al., 2023) model has the following recurrence relation:","\begin{split} x_{k}&=Ax_{k-1}+(Bu_{k})\odot x_{k-1}+ Bu_{k}\\ &=(A+\text{diag}(Bu_{k}))x_{k-1}+Bu_{k},\end{split}",where \(\odot\) denotes the Hadamard product and \(\text{diag}(w)\) constructs a diagonal matrix from \(w\),"x_{k}=A_{k}x_{k-1}+Bu_{k},",where $A_{k}$ is a block-diagonal and input-dependent transition matrix that enhances expressiveness compared to diagonal formulations.
naacl_2024_short_34,9,"\). Although Liquid-S4 does not suffer from the limitation outlined in Proposition 2, our experiments in SS 4.4 show that Liquid-S4 still cannot extrapolate on regular language tasks.

### Block-diagonal Input-dependent LRNN

Finally, we decide to push the expressiveness of \(A_{k}\) to the limit and make it fully input-dependent:","x_{k}=A_{k}x_{k-1}+Bu_{k},",where \(A_{k}=g(u_{k})\) is a block diagonal matrix in practice for the sake of efficiency,\begin{split} x_{k} &= \mathrm{blockdiag}(f(u_k)) x_{,
naacl_2024_short_34,10,"cy. \(A_{k}\) depends on \(u_{k}\) but not previous timesteps. \(g\) is an arbitrary function with the output being the size of \(A_{k}\).

Eq. (5) is numerically unstable because the product \(\prod_{i=1}^{k}A_{i}\) could produce large numbers. The solution is to impose additional constraints on the norm of \(A_{k}\):","\begin{split}& A_{k}=\text{diag}(A_{k}^{(1)},...,A_{k}^{(h)} )\in\mathbb{R}^{bh\timesbh}\\ & A_{k}^{(i)}=[v_{k}^{(i,1)}\quad\ldots\quad v_{k}^{(i,b)} ]\in\mathbb{R}^{b\times b}\\ &\|v_{k}^{(i,j)}\|_{p}\leq 1,\ \ i\in[1,...,h],\ \ j\in[1,...,b], \end{split}","where \(\|\cdot\|_{p}\) denotes the vector p-norm and \(v_{k}^{(i,j)}\) is a column vector that depends on \(u_{k}\)","\|A_{k}\|_{1} \leq 1,",which ensures the column norms of the input-dependent transition matrix $A_{k}$ are constrained to prevent numerical instability.
naacl_2024_short_34,11,"\). For any vector \(v\), we can derive another vector \(v^{\prime}\) to satisfy the p-norm constraint through \(v^{\prime}=v/\max(1,\|v\|_{p})\). Because \(\|v\|_{p}\geq\|v\|_{q}\) when \(p\leq q\), a smaller \(p\) imposes a stronger constraint on the columns of \(A_{k}^{(i)}\). In other words, we can stabilize Eq. (5) by selecting a sufficiently small \(p\).

Take \(p=1\) as an example. Every block \(A_{k}^{(i)}\) is a matrix that none of its column norm is greater than 1 in \(\|\cdot\|_{1}\). This implies \(A_{k+1}^{(i)}A_{k}^{(i)}\) is the same kind of matrix. Specifically, let \(v^{(1)},...,v^{(b)}\) be the columns of \(A_{k+1}^{(i)}A_{k}^{(i)}\). We have",\begin{split}&[\|v^{(1)}\|_{1}\quad\ldots\quad\|v^{(b)}\|_{1} ]=\mathbb{1}^{\top}|A_{k+1}^{(i)}A_{k}^{(i)}|\\ &\leq\mathbb{1}^{\top}|A_{k+1}^{(i)}||A_{k}^{(i)} |\leq\mathbb{1}^{\top}|A_{k}^{(i)}|\leq\mathbb{1}^{\top}. \end{split},,"\|v^{(j)}\|_{1}=\left\|\sum_{l=1}^{b}A_{k+1}^{(i)}(:,l)A_{k}^{(i)}(l,j)\right\|_{1}\leq\sum_{l=1}^{b}|A_{k}^{(i)}(l,j)|\cdot\|A_{k+1}^{(i)}(:,l)\|_{1}\leq\sum_{l=1}^{b}|A_{k}^{(i)}(l,j)|\leq 1.","Derivation showing that the product of two consecutive block matrices preserves the column-wise $ \| \cdot \|_1 $-norm constraint, ensuring numerical stability of the recurrence relation."
naacl_2024_short_38,1,"## 3 Compun Benchmark

**The task.** Our Compun benchmark serves as a test bed for evaluating a VLM's capability to interpret compound nouns. For evaluation, we focus on the zero-shot text-to-image retrieval task, where given a natural language prompt, the task of a VLM is to retrieve an image that illustrates the image described in the prompt. In the base setting, our prompt just describes a compound noun as _""A photo of a [compound noun]""_. Text-to-image retrieval has been earlier adopted by several works for evaluating compositional understanding Yuksekgonul et al. (2023); Ray et al. (2023). Inspired by these works, we design the Compun benchmark to challenge a VLM to select the correct image among a set of distractors. More precisely, each instance in the Compun benchmark, attributed to a compound noun, has 3 images, where only one image illustrates the compound noun, while the other two images illustrate the constituent nouns that make up the compound noun (example in Figure 1). All compound nouns in the Compun benchmark have a maximum of two nouns.

**Evaluation.** We resort to a simple evaluation metric, consistent with prior-art Thrush et al. (2022) for evaluating a VLM on Compun. Formally, let us denote the image illustrating the compound noun as a positive (\(\mathcal{P}\)) and the other 2 distractor images as negatives (\(\mathcal{N}_{1}\) and \(\mathcal{N}_{2}\)). Thus, given the natural language prompt \(\mathcal{C}\) for the compound noun, our evaluation metric \(f\left(\mathcal{C},\mathcal{P},\mathcal{N}_{1},\mathcal{N}_{2}\right)\) is defined as:","f(\mathcal{C},\mathcal{P},\mathcal{N}_{1},\mathcal{N}_{2})=\begin{cases} 1&\text{if }s(\mathcal{C},\mathcal{P})>s(\mathcal{C},\mathcal{N}_{1} )\\ &\text{and }s(\mathcal{C},\mathcal{P})>s(\mathcal{C},\mathcal{N}_{2} )\\ 0&\text{otherwise}\end{cases}",where \(s(,"f\left(\mathcal{C},\mathcal{P},\mathcal{N}_{1},\mathcal{N}_{2}\right) = \mathbb{E}_{(i,j) \sim \mathcal{D}} \left[ \prod_{k=1}^{2} \sigma\left( s\left(\mathcal{C},\mathcal{P",
naacl_2024_short_38,2,"s(.)\) is the standard cosine similarity, widely used for retrieval.

**Data collection and annotation.** The Commun benchmark has 400 test instances and a total of 1200 (400\(\times\)3) images. Each instance in the Com run benchmark is attributed to a unique compound noun (the complete list is provided in Table A.4). We use a combination of compound nouns provided by Levin et al. (2019), Lang et al. (2022), GPT-4 (OpenAI, 2023), and the internet. Next, we discard compound nouns that can have confusing distractors (e.g., _cheesecake_, where it's usually hard to distinguish between a _cheesecake_ and any other _cake_). After this step, we filter 400 compound nouns, the most widely used from our list. While a compound noun can have multiple interpretations, we use the more commonly known one. For a compound noun that may have multiple interpretations, we use MTurk to decide the most commonly known one. More details about this study can be found in Appendix A.6. Finally, a group of 4 annotators collects the required 1200 images from various image search engines. All 4 annotators come with extensive vision and language research experience.

## 4 Retrieval with Example Captions

Fig. 1 illustrates our proposed approach. As discussed earlier, the standard approach for text-to-image retrieval using class names is to hand-write several prompt templates (e.g., ""a photo of a class name.""). We propose an alternative framework - retrieve with example captions. Our framework is zero-shot and requires no further training. Given a compound noun \(c\), we ask an LLM to generate 5 diverse captions, where each caption has the compound noun \(c\) as an object in it. The generated captions should have \(c\) in diverse settings with diverse adjectives and verbs. We instruct GPT-4 (OpenAI, 2023) with the following prompt to generate the captions:

Return a list of 5 diverse captions with a compound_noun in a photo. The captions should be a maximum of 10 words and one-liners. All 5 captions should describe the compound noun in diverse settings with different verbs and actions being performed with the compound noun. An example output for ""chicken burger"": ['Sizzling chicken burger grilling at a lively backward BBQ., 'Chef expertly flipping a juicy chicken burger in a diner.', Family enjoying homemade chicken burgers on a sunny picnic.', 'Athlete fueling up with a protein-packed chicken burger post-workout.', 'Friends sharing a chicken burger at a vibrant street festival.']. Only return a list of strings and nothing else.

and an example output for the CN _""chocolate crocodile""_ is as follows:

[""Pastry chef sculpting a chocolate crocodile with finesse."", ""Kids discovering a chocolate crocodile in a candy treasure hunt."", ""Artist painting a whimsical chocolate crocodile in a foodie gallery."", ""Chocolate crocodile starring in a whimsical patisserie window display."", ""Chocolate crocodile sunbathing on a dessert island paradise.""]

We then build a prompt for our VLM separately with each of the captions as follows to get 5 final prompts: _""a photo of a [class name]. An example of {compound name} in an image is {caption}""._ Next, we calculate the mean similarity of an image \(c\in C\) with the text prompts as follows:","\text{Mean Similarity}=\frac{1}{n}\sum_{i=1}^{5}s(c,p_{i})","where \(p_{i}\in P\) denotes the generated prompts, \(s(","s_{\text{mean}}(c) = \frac{1}{5} \sum_{i=1}^{5} \text{sim}\left(\text{image}, \text{prompt}_i\right)",Mean similarity score across 5 generated captions for a given compound noun in the retrieval process.
naacl_2024_short_39,1,"### Formulation

Similar to T5 (Raffel et al., 2020), we applied a text-to-text approach to the QA task. Given \(N\) train data,we performed gradient updates to the following log-likelihood objective: \(\max_{\Theta}\sum_{i}^{N}\log p_{\Theta}(y_{i}|x_{i})\) where \(x_{i}\) is the input text, and \(y_{i}\) is the output sequence.","\underset{\theta_{\mathbf{P}}}{\max}\underset{i}{\sum}\log p_{\theta,\theta_{ \mathbf{P}}}(y_{i}|[\mathbf{P};x_{i}])",,"tags. The original equation uses \max, \sum, and subscripts. I need to make sure the LaTeX is correctly formatted. Also, the description needs to succinctly explain the variables. So something like ""N is the number of training examples, Θ represents the model parameters, x_i is the input text for the i-th example, and y_i is the corresponding output sequence.""

Wait, the user mentioned ""one sentence variable description"". So all variables in one sentence. Let me structure that. The existing equation uses N, Θ, x_i, y_i. So the description should define all of them concisely. Maybe: ""N denotes the number of training examples, Θ represents the model parameters, x_i is the input text for the i-th training example, and y_i is the corresponding target output sequence.""

Yes, that covers all variables in one sentence. The LaTeX code is already provided in the context, so I just need to present it properly. Make sure to use proper LaTeX syntax with \max, subscripts, etc. Also, check if there's any prior equations, but since this is Equation 1, there are none. Alright, that should do it.
</think>","N denotes the number of training examples, Θ represents the model parameters, x_i is the input text for the i-th training example, and y_i is the corresponding target output sequence."
naacl_2024_short_39,2,"The prompt tuning method proposed in Lester et al. (2021) is represented by Equation 1. The parameter of a pre-trained language model \(\theta\) is fixed, and only the prompt parameter \(\theta_{\mathbf{P}}\) of the soft prompt \(\mathbf{P}=[p_{1},p_{2},\dots,p_{l}]\in\mathbb{R}^{l\times d}\) is learnable. We use the prompt length \(l=100\), and \(d\) is the input dimension of the model.

### Datasets

Following the two classification systems from Rogers et al. (2023), we show 16 QA datasets2 used in our analysis in Table 1. Detailed descriptions of each dataset are provided in Appendix A.

Footnote 2: In cases where only one of valid or test datasets was available such as Rajpurkar et al. (2016), we used it in the testing process. Additionally, we split the train datasets into a 9:1 ratio, and used it in the train and valid process, respectively. The number of datasets we used is shown in Table 1.

First, the amount of evidence is how much evidence is provided to answer the question. _Single Source_ indicates that the information required to answer the question is explicitly contained within a context. Partial Source means that although some evidence is available, it needs to be integrated with external knowledge to answer the question. **No Source** needs to find answers solely from implicit knowledge. The more evidence available to answer a question, the more explicit knowledge exists; conversely, the less evidence, the more implicit knowledge exists.

Second, the answer format is divided into four types. Extractive format refers to when the answer span can be found within the provided context. Categorical format denotes that the correct answer is in a pre-defined option, exclusively employing yes or no formats in our dataset. Multi-choice format indicates that answer options are given, and the answer is to be chosen from among them. Lastly, Freeform format refers to cases where the model generates answers without following a specific format.

## 3 Results and Analysis

To study the transferability of soft prompts, we used 16 QA datasets as the source and target tasks. The main terms referred to in this section are as follows: (1) vanilla prompt tuning (Vanilla PT), the result of training the prompt in Equation 1 after random initializing; (2) zero-shot performance, the result of solving the target task using the source prompt without additional training; and (3) prompt transfer (PoT), the result of initializing the target prompt with the selected source prompt and training it as shown in Equation 1. For our experiments, we used the T5BASE3 as our base LM. Further experimental details are in Appendix B.

Footnote 3: [https://huggingface.co/t5-base](https://huggingface.co/t5-base)

### Transferability with Initialization

Can transferability be interpreted as cosine similarity?As shown in Figure 1, we investigated the prompt transferability with cosine-similarity. We can observe that prompt embeddings with the

\begin{table}
\begin{tabular}{l|l l r r r} \hline
**Dataset** & **Answer format** & **Amount of evidence** & **Train** & **Valid** & **Test** \\ \hline \hline DuoRC (Saha et al., 2018) & Freeform & Partial & 60,094 & 12,845 & 12,415 \\ NQ-Open (Lee et al., 2019) & Freeform & **No** & 79,132 & 8,793 & 3,610 \\ WQ (Berant et al., 2013) & Freeform & **No** & 3,400 & 378 & 2,032 \\ MRQA-NewSQA (Trischler et al., 2017) & Extractive & _Single_ & 66,744 & 7,416 & 4,212 \\ SQuAD (Rajpurkar et al., 2016) & Extractive & _Single_ & 78,839 & 8,760 & 10,570 \\ BoolQ (Clark et al., 2019) & Categorical & _Single_ & 8,484 & 943 & 3,270 \\ MultiRC (Khashabi et al., 2018) & Categorical & _Single_ & 24,518 & 2,725 & 4,848 \\ TQA (Joshi et al., 2017) & Freeform & Partial & 78,859 & 8,763 & 11,313 \\ CosmosQA (Huang et al., 2019) & Multi-choice & Partial & 22,735 & 2,527 & 2,985 \\ SIQA (Sap et al., 2019) & Multi-choice & Partial & 30,069 & 3,341 & 1,954 \\ \hline SQuAD w/o ctx & Freeform & **No** & 78,839 & 8,760 & 10,570 \\ BoolQ w/o ctx & Categorical & **No** & 8,484 & 943 & 3,270 \\ MultiRC w/o ctx & Categorical & **No** & 24,518 & 2,725 & 4,848 \\ TQA w/o ctx & Freeform & **No** & 78,859 & 8,763 & 11,313 \\ CosmosQA w/o ctx & Multi-choice & **No** & 22,735 & 2,527 & 2,985 \\ SIQA w/o ctx & Multi-choice & **No** & 30,069 & 3,341 & 1,954 \\ \hline \end{tabular}
\end{table}
Table 1: The details of QA datasets. ”w/o ctx” refers to the removal of context from the original dataset to evaluate the influence of the amount of evidence.

same answer formats are clustered together in Figure 1(b). However, Figure 1(a) demonstrates that the high similarity score between the source and target task does not necessarily result in positive transferability. For example, even though the transfer BoolQ (Clark et al., 2019) \(\rightarrow\) MultiRC (Khashabi et al., 2018) has the highest similarity score of \(0.9\), it yields a negative transferability of \(-2.8\%\). We note that the PoT performance varies significantly depending on the target task. Therefore, prompt initialization with high cosine-similarity does not guarantee performance improvement. As a result, we find that it is not suitable to interpret transferability through cosine-similarity in the QA task.

Can transferability be interpreted as zero-shot performance?To verify the effectiveness of selecting the best zero-shot prompt when used for initialization, we compare PoT performance between the best and worst zero-shot prompts in Table 2. When initialized with the best zero-shot prompt, it only outperforms the worst one in 7 out of 16

\begin{table}
\begin{tabular}{l||c||l c c|l c c||c} \hline \hline
**Target Task** & **Random** & **Best Source Task** & **Zero-shot** & **PoT** & **Worst Source Task** & **Zero-shot** & **PoT** & \(\Delta\) \\ \hline \hline
**DuoRC** & 2.14 & SQuAD & 32.86 & 35.56 & BoolQ & 0.77 & 36.79 & -1.23 \\
**NQ-Open** & 0.00 & SQuAD w/o ctx & 1.66 & 2.30 & MultiRC w/o ctx & 0.00 & 1.99 & +0.31 \\
**WQ** & 0.00 & NQ-Open & 3.69 & 3.99 & MultiRC & 0.00 & 2.51 & +1.48 \\
**MRQA-NewsQA** & 4.80 & SQuAD & 38.39 & 41.90 & MultiRC & 1.16 & 38.49 & +3.41 \\
**SQuAD** & 13.96 & DuoRC & 78.90 & 81.57 & CosmosQA & 1.07 & 81.28 & +0.29 \\
**BoolQ** & 0.00 & MultiRC & 67.37 & 76.70 & SIQA w/o ctx & 0.00 & 78.38 & -1.68 \\
**MultiRC** & 0.06 & BoolQ & 69.68 & 74.05 & TBA & 0.00 & 78.57 & -4.52 \\
**TQA** & 13.21 & DuoRC & 39.51 & 43.58 & MultiRC & 1.87 & 44.06 & -0.48 \\
**CosmosQA** & 2.91 & SIQA & 78.22 & 82.81 & MultiRC & 0.00 & 82.81 & 0.00 \\
**SIQA** & 0.61 & CosmosQA & 99.28 & 99.59 & BoolQ & 0.00 & 99.64 & -0.05 \\
**SQuAD w/o ctx** & 0.00 & NQ-Open & 0.96 & 1.74 & BoolQ & 0.00 & 1.65 & +0.09 \\
**BoolQ w/o ctx** & 19.27 & BoolQ & 47.83 & 51.13 & SIQA w/o ctx & 0.00 & 62.17 & -11.04 \\
**MultiRC w/o ctx** & 43.05 & MultiRC & 57.86 & 58.15 & SQuAD w/o ctx & 0.00 & 58.54 & -0.39 \\
**TQA w/o ctx** & 0.15 & SQuAD w/o ctx & 5.09 & 4.06 & BoolQ w/o ctx & 0.02 & 4.15 & -0.09 \\
**CosmosQA w/o ctx** & 0.20 & SIQA w/o ctx & 74.64 & 82.65 & MultiRC & 0.00 & 82.45 & +0.20 \\
**SIQA w/o ctx** & 0.46 & SQuAD & 26.46 & 99.39 & BoolQ & 0.00 & 99.33 & +0.06 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Relativeness of zero-shot and PoT performance. **Random** indicates the performance after random initialization. **Best Source Task** represents the best performance task in a zero-shot setting. **Worst Source Task** represents the worst performance task in a zero-shot setting. Each score is EM. The difference in the PoT scores between **Best Source Task** and **Worst Source Task** is denoted by \(\Delta\). When the **Zero-shot** scores are equal, we chose the source task with the higher **PoT** score.

Figure 1: (a) Heatmap of our task transferability results. (b) Heatmap of the cosine similarities between the source prompt embeddings. The colors of the task names indicate the answer format type: Blue, Extractive; Green, Categorical; Brown, Freeform; Yellow, Multi-choice.

cases. The mean absolute error was \(1.58\), indicating that the performance difference is approximate. Additionally, Figure 5 and Figure 6 indicate that most cases converge to similar values as the epoch progresses, regardless of which source prompt is selected. It can therefore be seen that the method proposed in Su et al. (2022) cannot assure better or comparable transfer performance in the QA task.

Effect of Dataset SizeIn Table 4, the PoT performance varies considerably depending on the target task. Therefore, we applied min-max normalization4 to each target task to compare the correlation between the source and target tasks. We classified the QA datasets based on the number of train datasets into small, medium, and large (see Appendix D). Subsequently, we divided into three groups 5 founded on the difference in size between the source task and target task as follows: _Target > Source_, _Same_ and _Target < Source_.

Footnote 4: See the formula in Appendix C.

Footnote 5: For example, _Target > Source_, indicating the train dataset group of the target task is larger than the source task (_e.g._, target task: large, source task: small).

As shown in Figure 2, the normalized task transferability results are based on the difference in the dataset group size between the source task and the target task. Regarding the _Target < Source_ group, most cases show positive transferability. The median (Q2) of each box plot indicates a tendency to drop in the sequence of _Target < Source_, _Same_, and _Target > Source_. We demonstrate that the dataset size of the source and target tasks in the QA task is a key factor in transferability.

### Investigating Catastrophic Forgetting

Catastrophic forgetting (Kirkpatrick et al., 2017) is the tendency for previously learned task knowledge to be abruptly lost as information relevant to the current task is incorporated. However, there is still no clear method for measuring this phenomenon.

Therefore, we propose a novel metric for evaluating catastrophic forgetting:",\frac{(Zero\text{-}shot\ correct)\cap(PoT\ incorrect)}{Zero\text{-}shot\ correct},"where _Zero-shot correct_ is the case of correct responses in a zero-shot setting, and _PoT incorrect_ is the case of incorrect answers after prompt transfer in the target task",\mathcal{F}_{\text{prompt}} = \frac{1}{T} \sum_{t=1}^{T} \left( \text{Perf}_{\text{pre}}^{(t)} - \text{Perf}_{\text{post}}^{(t)} \right),"The proposed metric calculates the average performance drop across tasks $t$ before and after prompt tuning, where $T$ is the total number of tasks."
naacl_2024_short_43,1,"## 2 Motivation & Approach

The **CoCo-CroLa** benchmark (CCCL) evaluates a T2I model's ability to generate images of an inventory of tangible concepts when prompted in different languages (Saxon and Wang, 2023). Given a tangible concept \(c\), written in language \(\ell\) as phrase \(c_{\ell}\), the \(i\)-th image produced by a multilingual T2I model \(f\) on the concept \(c_{\ell}\) can be expressed as:","I_{c,i}\sim f(c_{\ell})",,"\begin{equation}
    I_i = f(c_{\ell}, z_i)
\end{equation}","The $i$-th image generated by a multilingual T2I model $f$ for a concept $c$ in language $\ell$, where $z_i$ represents the $i$-th latent code input to the model."
naacl_2024_short_43,2,"The images generated in language \(\ell\) are considered _correct_ if they are faithful to their equivalent counterparts in the source language \(\ell_{s}\). This is measured by the CCCL benchmark by a **correctness metric** for a single concept \(c\) as the _cross-consistency_ score \(X_{c}(f,c_{\ell},c_{\ell_{s}})\):","X_{c}=\frac{1}{n^{2}}\sum_{i=0}^{n}\sum_{j=0}^{n}\mathrm{SIM}_{F}(I_{c_{\ell}, i},I_{c_{\ell_{s}},j})","where we sample \(n\) images per-concept per-language (we use 9), and \(\mathrm{SIM}_{F}(\cdot,\cdot)\) measures the cosine similarity in feature space by image feature extractor \(F\)","X_{c}(f, c_{\ell}, c_{\ell_{s}})",Cross-consistency score measuring the correctness of images generated for concept $c$ in language $\ell$ compared to the source language $\ell_{s}$.
naacl_2024_short_43,3,"\). In practice, the default source language \(\ell_{s}\) is English and \(F\) is the CLIP visual feature extractor (Radford et al., 2021).

### Translation Errors in CoCo-CroLa

CCCL requires correct translations of each concept \(c\) from the source language \(\ell_{s}\) into a set of semantically-equivalent translations in each test language \(\ell\). Saxon and Wang (2023) built CCCL v1's concept translation list using an automated approach so as to allow new languages to be easily added without experts in each new language.

They used an ensemble of commercial machine translation systems to generate candidate translations and the BabelNet knowledge graph (Navigli and Ponzetto, 2010) to enforce word sense agreement. Unfortunately, this approach introduces translation errors (Table 1).

We check the Spanish, Chinese, and Japanese translations using a group of proficient speakers, following a protocol described in Appendix A.1.1, who identify a set of _translation error candidates_ that may not sufficiently capture a concept's intended semantics in English, for various reasons.

Some of the candidate errors, such as the error for _rock_ in JA (Table 1), represent severe failures to translate a concept into its common, tangible sense--it is incoherent to test a model's ability to generate pictures of rocks by prompting it with ""rock music."" However, other candidate errors, such as _father_ in ZH are still potentially acceptable translations, but deviate from the annotators' preferred level of formality or specificity.

To decide which corrections ought to be integrated in future T2I multilinguality benchmarks, quantifying both the significance of each translation correction is and its impact on the CCCL score for its concept is desirable.

### Quantifying Error Correction & Impact

Characterizing the _impact_ of a translation correction on model behavior is simple; we check \(\Delta X_{c}\), the change in the CCCL score going from the original concept translation \(c_{\ell}\) to the corrected \(c^{\prime}_{\ell}\),","\Delta X_{c}(c,\ell)=X_{c}(f,c^{\prime}_{\ell},c_{\ell_{s}})-X_{c}(f,c_{\ell}, c_{\ell_{s}})",,"\Delta X_{c} = X_{c}(f, c^{\prime}_{\ell}, c_{\ell_{s}}) - X_{c}(f, c_{\ell}, c_{\ell_{s}})",the difference in cross-consistency score for concept $c$ before and after applying the translation correction $c_{\ell} \rightarrow c^{\prime}_{\ell}$
naacl_2024_short_43,4,"by comparing the generated population of images elicited from the corrected term \(I_{c^{\prime}_{\ell}}\) to the candidate translation error-conditioned images \(I_{c_{\ell}}\).

We quantify the significance of the translation correction as the _improvement in semantic similarity_\(\Delta\mathrm{SEM}(c_{\ell_{s}},c_{\ell},c^{\prime}_{\ell})\) using a text feature extractor \(F_{t}\) and cosine similarity metric \(\mathrm{SIM}(\cdot,\cdot)\)","\Delta\mathrm{SEM}=\mathrm{SIM}_{Ft}(c_{\ell_{s}},c^{\prime}_{\ell})-\mathrm{ SIM}_{Ft}(c_{\ell_{s}},c_{\ell})",,"\Delta\mathrm{SEM}(c_{\ell_{s}},c_{\ell},c^{\prime}_{\ell}) = \mathrm{SIM}_{F_{t}}(c_{\ell_{s}},c^{\prime}_{\ell}) - \mathrm{SIM}_{F_{t}}(c_{\ell_{s}},c_{\ell})",where the improvement in semantic similarity is calculated as the difference in cosine similarity between the source concept and the corrected translation versus the original translation using a text feature extractor $F_{t}$.
naacl_2024_short_45,1,"### Explanation Methods

We start with a dialect classifier trained to take an input sentence \(X\) and predict its corresponding label \(\mathbf{y}\). Let \(\mathbf{u}_{s}\) be the final layer representation of the ""[CLS]"" token for \(\mathbf{X}\), which is the sentence representation typically used to make a prediction.

Post-hoc ApproachDuring inference, LOO estimates the attribution score of each token \(x_{i}\) in input \(\mathbf{X}\) in relation to model's prediction \(\mathbf{\hat{y}}\). To do so, \(\mathbf{u}_{s}\) is passed through ReLU, affine, and softmax layers to yield a probability distribution over outputs. For each feature \(x_{i}\), LOO calculates the change in probability when \(\{x_{i}\}\) is removed from \(\mathbf{X}\). Let \(X\backslash\{x_{i}\}\) denote input \(\mathbf{X}\) without feature \(x_{i}\) and \(u_{i}\) the final layer representation of the ""[CLS]"" token for \(X\backslash\{x_{i}\}\). We term this the relevance score and expect that influential features/explanations in the input \(\mathbf{X}\) will have higher scores.",\boldsymbol{\ell} =\mathrm{softmax}(\mathrm{affine}(\mathrm{ReLU}(\mathbf{u}_{s}))) || \boldsymbol{\ell_{i}} =\mathrm{softmax}(\mathrm{affine}(\mathrm{ReLU}(\mathbf{u}_{i}))) || \boldsymbol{\nabla_{i}} =\boldsymbol{\ell}-\boldsymbol{\ell_{i}},,,
naacl_2024_short_45,2,"Intrinsic ApproachFor our intrinsic approach using SelfExplain (Rajagopal et al., 2021), we augment the dialect classifier with a Local Interpretability Layer (LIL) during training. This layer quantifies the relevance of each feature \(x_{i}\) in input \(\mathbf{X}\) to the final label distribution \(\boldsymbol{\ell}\) via activation difference (Shrikumar et al., 2017), and is trained jointly with the final classifier layer. Taking \(\boldsymbol{\ell}\) in Equation 1, the loss is the negative log probability, summed over all training instances:",L_{\mathit{dialect-classifier}}=-\sum_{i}\log\boldsymbol{\ell}[y_{i}^{*}],where \(y_{i}^{*}\) is the correct label for instance \(i\),\mathcal{L} = -\sum_{i=1}^{N} \log \boldsymbol{\ell}_{i},"The loss function for the intrinsic approach, defined as the negative log probability summed over all training instances."
naacl_2024_short_45,3,"\). To obtain the attribution score of each feature \(x_{i}\) in input \(\mathbf{X}\), we first estimate the output label distribution without \(x_{i}\) by transforming the difference between \(\mathbf{u}_{s}\) and \(\mathbf{u}_{j}\), where \(\mathbf{u}_{j}\) is the MLM representation of feature \(x_{i}\):",\mathbf{s}_{j}=\mathrm{softmax}(\mathrm{affine}(\mathrm{ReLU}(\mathbf{u}_{s}) -\mathrm{ReLU}(\mathbf{u}_{j}))) || \text{loss}=L_{\text{{{dialect-classifier}}}}+\alpha_{1}L_{\text{{{LII}}}},,\boldsymbol{\ell}_{\neg x_{i}} = \mathrm{softmax}(\mathrm{affine}(\mathrm{ReLU}(\mathbf{u}_{s} - \mathbf{u}_{j}))),Estimates the label distribution without feature $x_{i}$ by transforming the difference between the original and masked representations.
naacl_2024_short_45,4,The relevance of each feature \(x_{i}\) can be defined as the change in probability of the correct label when \(x_{i}\) is included vs. excluded:,r_{j}=[\boldsymbol{\ell}]_{y_{i}^{*}}-[\mathbf{s}_{j}]_{y_{i}^{*}},"where higher \(r_{j}\) signifies more relevant features to the prediction, serving as better explanations",r_{i} = \boldsymbol{\ell}[y^{*}] - \mathbf{s}_{j}[y^{*}],where $r_{i}$ denotes the relevance score of feature $x_{i}$ based on the probability difference of the correct label $y^{*}$ between the full input and the input excluding $x_{i}$.
naacl_2024_short_45,5,"ns.

Mapping Explanations to Lexical FeaturesWe extract explanations from the classifiers outlined above. Note, however, that these explanations are at the _sentence_ level, but one ideally would need features that in general identify/describe one dialect in contrast to another at the _language_ level, i.e., at the _corpus_ level. To achieve this, we devise a corpus-level feature extraction method that takes sentence-level explanations as input and produces ""global"" features.2

Footnote 2: In this study, we extract distinguishing lexemes (words) through unigrams. Nonetheless, our approach can be readily adapted to phrase-level analysis using ngrams, though such an extension falls beyond the purview of this work.

Given a set of sentence-level explanations \(E=\{e_{1},e_{2},\ldots,e_{n}\}\) from a classifier, we first filter out explanations from incorrect predictions or those that are not unique to a specific language variety. Let \(E^{\prime}\) represent the filtered set of explanations:",E^{\prime}=\{e\in E\mid\text{isCorrect}(e)\land\text{isUnique}(e)\},,E^{\prime} = \{e_{i} \in E \mid \text{prediction is correct and } e_{i} \text{ is unique to a specific language variety}\},Filtered set of sentence-level explanations that are both correctly predicted and unique to a specific language variety
naacl_2024_short_45,6,"Next, we apply Term Frequency-Inverse Document Frequency (TF-IDF) to \(E^{\prime}\) to extract the most salient global features. Let \(F\) be the final set of extracted features. The TF-IDF score for a term \(t\) in a document \(d\) in a corpus \(D\) is given by:","\text{TF-IDF}(t,d,D)=\text{TF}(t,d)\times\text{IDF}(t,D)",,"\mathrm{TF\text{-}IDF}(t,d,D) = \mathrm{TF}(t,d) \times \mathrm{IDF}(t,D)","where TF(t, d) measures the frequency of term t in document d, and IDF(t, D) measures the inverse document frequency of t across the corpus D, emphasizing terms that are rare globally but frequent in specific documents."
naacl_2024_short_45,7,We can then define the feature extraction as:,"F=\{\text{TF-IDF}(t,d,E^{\prime})\mid t\in d,d\in E^{\prime}\}",,F=\text{TF-IDF}(E^{\prime}),where $F$ represents the final set of global features extracted using TF-IDF from the filtered explanations $E^{\prime}$
naacl_2024_short_46,1,"## 3 Embedding Space Separation and Compaction

When learning a new task \(\mathcal{T}^{k}\), following Liu et al. (2022), we first initialize a set of soft prompts \(\mathcal{P}^{k}=\{p^{k}_{1},...,p^{k}_{|\mathcal{C}^{k}|}\}\) where \(\mathcal{C}^{k}\) is the event type set of \(\mathcal{T}^{k}\). The accumulated prompts \(\mathcal{Q}^{k}=[\mathcal{P}^{1},...,\mathcal{P}^{k}]\) until time step \(k\) are then combined with the input text \(x^{i}\) to obtain the contextual representations using a frozen BERT Devlin et al. (2019):","[\mathbf{x}^{i},\mathbf{Q}^{k}]=\text{BERT}([x^{i},\mathcal{Q}^{k}])","where \(\mathbf{x}^{i}\) and \(\mathbf{Q}^{k}\) are the representations of \(x^{i}\) and \(\mathcal{Q}^{k}\), respectively","\begin{equation}
    h^{i} = \text{BERT}(\mathcal{Q}^{k} \oplus x^{i})
\end{equation}",Contextual representations obtained by combining accumulated prompts with input text using a frozen BERT model.
naacl_2024_short_46,2,"ly. To facilitate _forward knowledge transfer_Qin and Joty (2022), we initialize soft prompts \(\mathcal{P}^{k}\) of the new task using learned prompts \(\mathcal{P}^{k-1}\) of the previous task. For the first task \(\mathcal{T}^{1}\), we initialize each event type prompt \(p^{1}_{i}\) in \(\mathcal{P}^{1}\) using its corresponding name.

To predict the event type of the span \(\overline{x}^{i}_{t}\), we concatenate the representations corresponding to the start and end token and obtain the logits over all learned types through a feed-forward network (FFN) as well as a linear layer:","Z^{i}_{t}=\text{Linear}(\text{FFN}([\mathbf{x}^{i}_{m},\mathbf{x}^{i}_{n}]))","where \(\overline{\mathbf{x}}^{i}_{t}=\text{FFN}([\mathbf{x}^{i}_{m},\mathbf{x}^{i}_{ n}])\) is the span representation, \(m\) and \(n\) denote the start and end index of the span, respectively",\mathbf{h}^{i} = \text{FFN}([\mathbf{x}^{i}_{\text{start}}; \mathbf{x}^{i}_{\text{end}}]) \quad \text{and} \quad \mathbf{z}^{i} = \mathbf{W}_{\text{type}}\mathbf{h}^{i} + \mathbf{b}_{\text{type}},"where $\mathbf{h}^{i}$ is the hidden representation of the span $\overline{x}^{i}_{t}$, and $\mathbf{z}^{i}$ contains the logits for predicting its event type using learnable parameters $\mathbf{W}_{\text{type}}$ and $\mathbf{b}_{\text{type}}$."
naacl_2024_short_46,3,"ly. Following Liu et al. (2022), to entangle span representations with soft prompts, the probability distribution over all prompts is calculated as \(Z_{q}=\text{FFN}(\mathbf{Q}^{k})\cdot\overline{\mathbf{x}}^{i}_{t}\), where \(\cdot\) is the inner product. \(Z_{q}\) is then combined with \(Z^{i}_{t}\) to optimize the cross entropy loss:","\mathcal{L}_{\text{new}}=-\sum_{(\overline{x}^{i}_{t},y^{i}_{t})\in\mathcal{ D}^{k}_{\text{train}}}\text{CE}(Z^{i}_{t}+Z_{q},y^{i}_{t})",,\mathcal{L}_{\text{cls}} = -\log \frac{\exp(Z^{i}_{t} + Z_{q})}{\sum_{j=1}^{|\mathcal{C}^{k}|} \exp(Z^{j}_{t} + Z_{q})},"where $Z_{q}$ is the prompt probability distribution obtained via inner product between $\text{FFN}(\mathbf{Q}^{k})$ and $\overline{\mathbf{x}}^{i}_{t}$, and is combined with the span logits $Z^{i}_{t}$ for classification loss"
naacl_2024_short_46,4,"After learning the previous task \(\mathcal{T}^{k-1}\), we select the top-\(l\) most informative training examples for each event type in \(\mathcal{C}^{k-1}\) using the herding algorithm Welling (2009), which are then saved in the memory module \(\mathcal{M}\) for replay to mitigate forgetting. Similar as Eq. 3, the training objective for memory replay when learning \(\mathcal{T}^{k}\) is:","\mathcal{L}_{\text{mem}}=-\sum_{(\overline{x}^{i}_{t},y^{i}_{t})\in\mathcal{ M}}\text{CE}(Z^{i}_{t}+Z_{q},y^{i}_{t})",,"\mathcal{L}_{\text{replay}}=-\sum_{(\overline{x}^{j}_{t},y^{j}_{t})\in\mathcal{M}}\text{CE}(Z^{j}_{t}+Z_{q},y^{j}_{t})",where $Z^{j}_{t}$ and $Z_{q}$ are computed using the current model parameters and the saved prompts from $\mathcal{M}$
naacl_2024_short_46,5,"However, the simple combination of \(\mathcal{L}_{\text{new}}\) and \(\mathcal{L}_{\text{mem}}\) can still result in substantial forgetting of acquired knowledge due to the potential overlap between the feature distribution of new event types and the previously learned embedding space (see Appendix A.1). To ensure that the new feature distribution is away from the learned embedding space, we design a _margin-based_ loss, which decreases the similarity scores between new samples and prototypes (see Eq. 8 for the calculation of prototypes) of learned event types:","\mathcal{L}_{\text{sim}}=\sum_{(\overline{x}^{i}_{t},y^{i}_{t})\in\mathcal{D}^ {k}_{\text{train}}}\sum_{\mathbf{e}_{t}\in\mathcal{E}^{k-1}}\max(0,g(\overline {\mathbf{x}}^{i}_{t},\mathbf{e}_{t})-m_{1})","where \(\mathcal{E}^{k-1}\) is the prototype set of previous \(k-1\) tasks, \(g(,)\) is the similarity function (cosine similarity) and \(m_{1}\) is the margin for \(\mathcal{L}_{\text{sim}}\)","\mathcal{L}_{\text{margin}}=\sum_{c\in\mathcal{C}^{k-1}}\max(0,\Delta-s(\mathbf{v}^{k}_{i},\mathbf{p}^{c}))","where $\mathbf{v}^{k}_{i}$ is the representation of a new sample from task $\mathcal{T}^{k}$, $\mathbf{p}^{c}$ is the prototype of a learned event type $c$, $s(\cdot,\cdot)$ is the cosine similarity function, and $\Delta$ is the margin hyperparameter"
naacl_2024_short_46,6,"\). Note that \(\mathcal{L}_{\text{sim}}\) is different from metric learning or contrastive learning Qin and Joty (2022) which typically considers both positive and negative pairs. \(\mathcal{L}_{\text{sim}}\) only includes negative pairs while ignoring positive ones as our goal in designing \(\mathcal{L}_{\text{sim}}\) is to _separate_ the new feature distribution and the learned embedding space.

As the size of memory \(\mathcal{M}\) is typically small, the model is prone to overfit on the few memory samples after frequent replays, making learned distributions distorted. To effectively recover from distorted learned distributions, we introduce a _memory calibration_ mechanism inspired by Han et al. (2020). Specifically, for each memory sample in \(\mathcal{M}\), we encourage it to be close to its corresponding prototype to improve the intra-class compactness of learned distributions. More formally,","\mathcal{L}_{\text{cal}}=-\sum_{(\overline{x}_{t}^{i},\overline{x}_{t}^{i}) \in\mathcal{M}}\log\frac{\exp g(\overline{\mathbf{x}}_{t}^{i},\mathbf{e}_{t}) }{\sum_{j=1}^{|\mathbf{e}^{k-1}|}\exp g(\overline{\mathbf{x}}_{t}^{i},\mathbf{ e}_{j})}",where \(\mathbf{e}_{t}\) is the prototype of \(y_{t}^{i}\),"\mathcal{L}_{\text{cal}}=\sum_{(\overline{x}^{i}_{t},y^{i}_{t})\in\mathcal{M}}\|\overline{\mathbf{x}}^{i}_{t}-\mathbf{e}_{y^{i}_{t}}\|^{2}",where $\mathbf{e}_{y^{i}_{t}}$ is the prototype of the class $y^{i}_{t}$ in the prototype set $\mathcal{E}^{k-1}$
naacl_2024_short_46,7,\). The _total_ loss for learning on \(\mathcal{T}^{k}\) is defined as:,\mathcal{L}_{\text{total}}=\mathcal{L}_{\text{new}}+\lambda_{1}\mathcal{L}_{ \text{sim}}+\lambda_{2}(\mathcal{L}_{\text{mem}}+\mathcal{L}_{\text{cal}}),where \(\lambda_{1}\) and \(\lambda_{2}\) are loss weights,\mathcal{L}=\mathcal{L}_{\text{new}}+\lambda_{1}\mathcal{L}_{\text{mem}}+\lambda_{2}\mathcal{L}_{\text{sim}}+\lambda_{3}\mathcal{L}_{\text{cal}},"where $\lambda_{1}$, $\lambda_{2}$, and $\lambda_{3}$ are trade-off hyperparameters balancing the contributions of the individual loss terms"
naacl_2024_short_46,8,"ts.

After learning \(\mathcal{T}^{k}\) and selecting memory data for \(\mathcal{T}^{k}\), we use the memory \(\mathcal{M}\) to calculate prototypes of all learned event types in \(\mathcal{C}^{k}\). Specifically, for each event type \(e_{j}\) in \(\mathcal{C}^{k}\), we obtain its prototype \(\mathbf{e}_{j}\) by averaging the span representations of all samples labeled as \(e_{j}\) in \(\mathcal{M}\) as follows:","\mathbf{e}_{j}=\frac{1}{|\mathcal{M}_{e_{j}}|}\sum_{(\overline{x}_{t}^{i}, \overline{x}_{t}^{i})\in\mathcal{M}_{e_{j}}}\overline{\mathbf{x}}_{t}^{i}","where \(\mathcal{M}_{e_{j}}=\{(\overline{x}_{t}^{i},y_{t}^{i})|(\overline{x}_{t}^{i}, y_{t}^{i})\in\mathcal{M},y_{t}^{i}=e_{j}\}\)","\mathbf{e}_{j}=\frac{1}{|\mathcal{M}_{e_{j}}|}\sum_{(\overline{x}^{i}_{t},y^{i}_{t})\in\mathcal{M}_{e_{j}}}\overline{\mathbf{x}}^{i}_{t}",where $\mathcal{M}_{e_{j}}$ is the subset of memory samples labeled as event type $e_{j}$
naacl_2024_short_5,1,"#### 3.2.1 Unsupervised SKICSE

Given an unlabeled input sentence \(x\), SKICSE creates a positive example \(x^{\text{ski}}\) for \(x\) by obtaining its SKI. We can constitute a triplet of sentences \((x,x^{\prime},x^{\text{ski}})\) as shown in Figure 2(a). Here, \(x\) and \(x^{\prime}\) have the same text, but different hidden dropout masks. By using the BERTbase encoder \(f\), we can get a triplet of sentence embeddings \((f(x),f(x^{\prime}),f(x^{\text{ski}}))=(\mathbf{h},\mathbf{h}^{\prime}, \mathbf{h}^{\text{ski}})\), and objective functions can be formulated as:","\mathcal{L}^{\text{unsup}}_{\text{simce}}=-\log\frac{e^{\text{sim}( \mathbf{h}_{i},\mathbf{h}_{i}^{\prime})/\tau}}{\sum_{j=1}^{N}e^{\text{ sim}(\mathbf{h}_{i},\mathbf{h}_{j}^{\prime})/\tau}}, || \mathcal{L}^{\text{unsup}}_{\text{skice}}=-\log\frac{e^{\text{sim}( \mathbf{h}_{i},\mathbf{h}_{i}^{\text{ski}})/\tau}}{\sum_{j=1}^{N}e^{ \text{sim}(\mathbf{h}_{i},\mathbf{h}_{j}^{\text{ski}})/\tau}},",,"\begin{equation}
\mathcal{L}_{\text{unsup}} = \mathcal{L}_{\text{contrastive}}(\mathbf{h}, \mathbf{h}^{\prime}, \mathbf{h}^{\text{ski}}),
\end{equation}","The unsupervised loss function for SKICSE, formulated as a contrastive loss between the original, dropout-variant, and SKI-generated embeddings."
naacl_2024_short_5,2,"on.

Finally, the final objective function of unsupervised SKICSE is the weighted summary of the aforementioned two objectives:","\mathcal{L}^{\text{unsup}}=(1-\lambda)\mathcal{L}^{\text{unsup}}_{\text{ simce}}+\lambda\mathcal{L}^{\text{unsup}}_{\text{skice}},",where the weight \(\lambda\) is a balanced hyperparameter and reflects the importance of SKI,"\mathcal{L}^{\text{unsup}}_{\text{SKICSE}} = \lambda \cdot \mathcal{L}^{\text{unsup}}_{\text{simce}} + (1 - \lambda) \cdot \mathcal{L}^{\text{unsup}}_{\text{skice}},",The final objective function of unsupervised SKICSE combines the SimCE and SKICE losses using a weighted sum with a hyperparameter $\lambda$.
naacl_2024_short_5,3,"KI.

#### 3.2.2 Supervised SKICSE

In NLI datasets, for each premise \(x\), there are its entailment hypothesis \(x^{+}\) and an accompanying contradiction \(x^{-}\). SKICSE creates a positive example \(x^{\text{ski}}\) for \(x\) by obtaining its SKI. Similarly, we can constitute a quadruplet of sentences \((x,x^{+},x^{-},x^{\text{ski}})\) and pass it through the encoder to get a quadruplet of sentence embeddings \((\mathbf{h},\mathbf{h}^{+},\mathbf{h}^{-},\mathbf{h}^{\text{ski}})\) as shown in Figure 2(b). Objective functions can be formulated as:","\mathcal{L}^{\text{sup}}_{\text{simce}}=-\log\frac{e^{\text{sim}( \mathbf{h}_{i},\mathbf{h}_{i}^{+})/\tau}}{\sum_{j=1}^{N}(e^{\text{ sim}(\mathbf{h}_{i},\mathbf{h}_{j}^{+})/\tau}+e^{\text{sim}( \mathbf{h}_{i},\mathbf{h}_{j}^{-})/\tau})}, || \mathcal{L}^{\text{sup}}_{\text{skice}_{1}}=-\log\frac{e^{\text{sim}( \mathbf{h}_{i}^{\text{ski}},\mathbf{h}_{i}^{+})/\tau}}{\sum_{j=1}^{N} (e^{\text{sim}(\mathbf{h}_{i}^{\text{ski}},\mathbf{h}_{j}^{+}) /\tau}+e^{\text{sim}(\mathbf{h}_{i}^{\text{ski}},\mathbf{h}_{j}^{-}) /\tau})}, || \mathcal{L}^{\text{sup}}_{\text{skice}_{2}}=-\log\frac{e^{\text{sim}( \mathbf{h}_{i},\mathbf{h}_{i}^{\text{ski}})/\tau}}{\sum_{j=1}^{N}(e ^{\text{sim}(\mathbf{h}_{i},\mathbf{h}_{j}^{+})/\tau}+e^{\text{sim} (\mathbf{h}_{i},\mathbf{h}_{j}^{-})/\tau})},",,"\mathcal{L}^{\text{sup}}_{\text{nli}}=-\log\frac{e^{\text{sim}( \mathbf{h}_{i},\mathbf{h}_{i}^{+})/\tau}}{e^{\text{sim}(\mathbf{h}_{i}, \mathbf{h}_{i}^{+})/\tau}+e^{\text{sim}(\mathbf{h}_{i},\mathbf{h}_{i}^{-})/\tau}},","where $\mathcal{L}^{\text{sup}}_{\text{nli}}$ represents the NLI loss for the supervised setting, contrasting entailment and contradiction examples"
naacl_2024_short_5,4,"In a similar way, the final objective function of supervised SKICSE becomes:",\mathcal{L}^{\text{sup}}=(1-\lambda_{1}-\lambda_{2})\mathcal{L}^{\text{sup}}_{ \text{simce}}+\lambda_{1}\mathcal{L}^{\text{sup}}_{\text{skice}_{1}}+\lambda_ {2}\mathcal{L}^{\text{sup}}_{\text{skice}_{2}}.,,"\mathcal{L}^{\text{sup}}=(1-\lambda)\mathcal{L}^{\text{sup}}_{\text{simce}}+\lambda\left(\mathcal{L}^{\text{sup}}_{\text{skice}_{1}}+\mathcal{L}^{\text{sup}}_{\text{skice}_{2}}\right),",where the weight $\lambda$ balances the contributions of the SimCE and SKICE components in the supervised setting.
naacl_2024_short_51,1,"## 3 Analysis

A particularly interesting type of analysis enabled by this dataset is the longitudinal comparison of how the same news event is reported in different languages and around the world. To illustrate, let's consider a basic example. We begin by clustering individual articles that discuss the same event. Then, we analyze the distribution and frequency of articles from different countries over time, focusing on that specific event.

To cluster articles that are about the same event, we begin by extracting a bag of words from the article's (English) title where each word is lemmatized as well as removing punctuation and common stopwords (such as ""the"" or ""a""). We use Term Frequency-Inverse Document Frequency (TF-IDF) (Salton et al., 1975) to define the relevance \(R_{d}\) of each token relative to the other tokens of the articles that occurred on the same day \(d\).","R_{d}(w)=\frac{\operatorname{tf}(w,d)}{\sum_{d^{\prime}}\operatorname{tf}(w,d ^{\prime})}\cdot\log\frac{N}{\operatorname{df}(w)}","where \(\operatorname{tf}(w,d)\) is the number of times the word \(w\) occurs in the day \(d\), \(\operatorname{df}(w)\) is the number of days in which the word \(w\) occurs, and \(N\) is the total number of days in the dataset",R_{d}(t) = \text{TF}_{d}(t) \times \text{IDF}(t),"Relevance $R_{d}$ of a token $t$ in articles from day $d$, calculated using Term Frequency-Inverse Document Frequency (TF-IDF)."
naacl_2024_short_51,2,"et.

Using the TF-IDF scores for each word, we define a relevance score \(\hat{R}_{d}(x)=\sum_{w\in x}R_{d}(w)\) for an article \(x\) that occurs first on day \(d\) as the sum of the TF-IDF scores of the words in its title. Furthermore, we define a similarity criterion between two articles as the ratio between the sum of all words that occur in both articles weighted by the relevance of each word and the largest relevance score between the two articles.","\mathrm{sim}(x,x^{\prime})=\frac{\sum_{w\in x^{\prime}x^{\prime}}R_{d}(w)}{ \mathrm{max}(\hat{R}_{d}(x),\hat{R}_{d}(x^{\prime}))}",,\hat{R}_{d}(x) = \sum_{w \in x} R_{d}(w),"Relevance score of an article $x$ occurring on day $d$, defined as the sum of TF-IDF scores of the words in its title"
naacl_2024_short_52,1,"## 2 Continuous-Output NMT

The machine translation task involves learning to map sequences of input tokens \(\mathbf{x}=(x_{1},\ldots,x_{m})\) to output tokens \(\mathbf{y}=(y_{1},\ldots,y_{n})\). In standard (discrete) NMT, each step is a multi-class next word prediction task, minimizing:","\begin{split}& L_{\text{discrete}}(y_{i}=t;\mathbf{y}_{<i},\mathbf{x})=- \log p(y_{i}=t\mid\mathbf{y}_{<i},\mathbf{x})\\ &\quad=-\langle\mathbf{E}(t),\mathbf{h}\rangle+\log\sum_{t^{\prime}\in V }\exp\langle\mathbf{E}(t^{\prime}),\mathbf{h}\rangle,\end{split}","where \(t\) is a token index, \(V\) is the vocabulary, \(\mathbf{E}:V\rightarrow\mathbb{R}^{d}\) is an embedding lookup, and \(\mathbf{h}\) is a transformer hidden state calculated in terms of \(\mathbf{x}\) and the output prefix \(\mathbf{y}_{<i}\)","\mathcal{L} = -\sum_{t=1}^{n} \log p(y_t \mid \mathbf{y}_{<t}, \mathbf{x})",Negative log-likelihood of target tokens given input and previous tokens
naacl_2024_short_52,2,\). The costly log-sum-exp and the penchant for continuous similarity metrics in NLP motivate a purely-continuous alternative:,"L_{\text{cos}}(y_{i}=t;\mathbf{y}_{<i},\mathbf{x})=1-\cos(\mathbf{E}(t),\mathbf{h}).",,"\begin{split}& L_{\text{continuous}}(\mathbf{y}_{i};\mathbf{y}_{<i},\mathbf{x})=-\langle\mathbf{E}(\mathbf{y}_{i}),\mathbf{h}\rangle+\log\int\exp\langle\mathbf{E}(\mathbf{y}^{\prime}),\mathbf{h}\rangle d\mathbf{y}^{\prime},\end{split}","where $\mathbf{y}_{i}$ is a continuous vector representing the $i$-th output token, and the integral replaces the discrete sum over vocabulary with a continuous similarity measure."
naacl_2024_short_52,3,"Continuous NMT models were first studied by Kumar and Tsvetkov (2019), who also propose other probabilistic losses and later other margin-based objectives (Bhat et al., 2019), with limited gain and at the cost of additional hyperparameters; we therefore focus on the robust cosine objective. We further justify the choice of cosine over max-margin as an objective function in Appendix C.1.

On the other hand, the choice of embeddings \(\mathbf{E}\) makes a much larger difference, especially due to the fact that all previous work keeps this parameter frozen: indeed, if it were trainable, Equation (2) would have trivial global optima by setting all \(\mathbf{E}(t)\) to the same vector for all \(t\). With modern transformer architectures, the best performing embeddings overall tend to be the ""oracle"" output embeddings learned by a pre-trained discrete MT system (Tokarchuk and Niculae, 2022). We highlight that the cosine loss is invariant to the norms of both the embeddings and of the decoder hidden state, and therefore we may restrict our modeling problem to the unit sphere.

Optimizing Equation (1) pushes the model \(\mathbf{h}\) away from all tokens different from the ""gold"" token, even if some other tokens (_e.g._, synonyms) could otherwise be a good fit. Equation (2) has no such effect, leading to a promise of more diverse generations. An appealing intuition is that synonyms and related words being nearby in embedding space contributes to the performance of CoNMT and enables such diversity. However in practice, greedy nearest-neighbor lookup is applied, and beam search decoding is not well-studied in the context of CoNMT. Therefore, in this work, we dwell more into the beam search performance for CoNMT, and compare pre-trained and completely random embeddings.

## 3 Random Embeddings Generation

We consider two different distributions from which to sample the \(|V|\) random embeddings.

Spherical uniform.We draw embeddings uniformly from the surface of the sphere: \(\mathbf{E}(y_{i})\sim\text{Unif}(\mathbb{S}_{d-1})\). Since standard normal vectors are distributed with rotational symmetry around the origin, uniform samples on the sphere can be obtained by normalizing standard normal random vectors:","\mathbf{E}(y_{i})=\mathbf{u}_{i}/\|\mathbf{u}_{i}\|;\quad\mathbf{u}_{i}\sim\text{Normal}(\mathbf{ 0},\mathbf{I}_{d}).",,"\mathbf{E}(y_{i})=\frac{\mathbf{z}}{\|\mathbf{z}\|},\quad\mathbf{z}\sim\mathcal{N}(0,\mathbf{I}_{d}).",Random embeddings are generated by normalizing standard normal vectors to lie on the unit sphere.
naacl_2024_short_52,4,"The same argument works if the normal distribution has spherical covariance \(\sigma\mathbf{I}_{d}\) for any \(\sigma\), and thus, since the cosine loss is norm-invariant, uniform initialization is exactly equivalent to the standard initialization of transformer embeddings.

Hypercube.The corners of the hypercube \(\{-1,1\}^{d}\) all have norm \(\sqrt{d}\) and thus form a discrete subset of a hypersphere. This motivates us to consider drawing embeddings from a scaled Rademacher distribution:",\mathbf{E}(y_{i})=\mathbf{r}_{i}/\sqrt{d};\quad\mathbf{r}_{i}\sim\text{Rademacher}(d).,,"\mathbf{E}(y_{i})=\mathbf{u}_{i}/\sqrt{d};\quad u_{i,j}\sim\text{Rademacher}.","Each embedding vector is sampled from a scaled Rademacher distribution, ensuring entries are ±1 scaled by the dimensionality to maintain unit norm."
naacl_2024_short_52,5,"Each coordinate of \(\mathbf{r}_{i}\) has 50% probability of being \(+1\) and 50% of being \(-1\). With this strategy, any two distinct embeddings have cosine distance at least \(2/d\). Moreover, hypercubic embeddings can be stored as bit patterns and potentially allow for faster loss calculation with dedicated low-level implementations which we do not explore here.

## 4 Experimental Setup and Data

We train CoNMT systems with pre-trained target embeddings as well as randomly-generated target embeddings. The **pre-trained embeddings** we use are extracted from a discrete NMT system trained on the same training data, following the setup of Tokarchuk and Niculae (2022), who found this strategy to outperform other subword-level pre-trained embeddings for CoNMT.

Results are reported on three WMT translation tasks:1 WMT 2016 Romanian\(\rightarrow\)English (ro-en), WMT 2018 English\(\rightarrow\)Turkish (en-tr) and WMT 2019 English\(\rightarrow\)German (en-de), the latter including back-translated data. Note that for en-tr we use only WMT 2018 training data with 207k training sentences in order to investigate a challenging lower-resource and morphology-rich scenario. Data statistics are collected in Appendix A.

Footnote 1: [https://www2.statmt.org/](https://www2.statmt.org/)

For subword tokenization we used the same SentencePiece (Kudo and Richardson, 2018) model for all language pairs, specifically the one used in the mBart multilingual model (Liu et al., 2020). This choice allows for unified preprocessing for all languages we cover. We validate that token-based models performs generally better than word-level models (Appendix C.4), even though subwords introduce an additional challenge of predicting subword continuation (Appendix C.5).

We used the fairseq(Ott et al., 2019) framework for training our models. Baseline discrete models are trained with cross-entropy loss, label smoothing equal to 0.1 and effective batch size 65.5K tokens. Both discrete and continuous models are trained with learning rate \(5\cdot 10^{-4}\), 10k warm-up steps for ro-en and en-de, and 4k for the smaller en-tr dataset. All continuous models are trained with the cosine distance objective in Equation (2). We provide all training details in Appendix B.

We measure translation accuracy using Sacre-BLEU2(Papineni et al., 2002; Post, 2018) and BertScore3(Zhang et al., 2020). Note that BertScore is scaled differently for each language, so the scores cannot be compared across languages.

Footnote 2: nrefs:1]case:mixededf:nololk:13absmooth:explversion:2.3.1

Footnote 3: implementation by [https://github.com/Tiiiger/bert_score](https://github.com/Tiiiger/bert_score)

## 5 Results and Discussion

Scores.Per Table 1, we find that random uniform embeddings outperform the pre-trained baseline for en-de, match it closely for ro-en, and only under-perform in the low-resource case for en-tr. We find that hypercube embeddings consistently perform no better than uniform embeddings; however, it is possible that their computational advantages can make up for this in some applications.

Beam search.Preliminary experiments with CoNMT models indicate little gain or even degra

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & \multicolumn{2}{c}{en-tr} & \multicolumn{2}{c}{ro-en} & \multicolumn{2}{c}{en-de} \\
**embeddings** & BLEU \(\uparrow\) & BERTSc. \(\uparrow\) & BLEU \(\uparrow\) & BERTSc. \(\uparrow\) & BLEU \(\uparrow\) & BERTSc. \(\uparrow\) \\ \hline discrete model & 12.3 & 70.4 & 31.7 & 64.1 & 33.1 & 69.0 \\ \hline pre-trained (beam=1) & 10.1 & 67.1 & 29.0 & 58.5 & 31.3 & 66.2 \\ pre-trained & **10.4** & 67.4 & 29.0 & 58.0 & 29.2 & 62.6 \\ random uniform & 8.9 & 65.1 & 28.8 & 58.8 & 31.8 & **67.2** \\ random cube & 8.7 & 64.6 & 28.7 & 58.8 & 31.4 & 66.9 \\ combined & **10.4** & **68.3** & **29.5** & **60.4** & **32.0** & 66.8 \\ \hline \hline \end{tabular}
\end{table}
Table 1: BLEU and BertScore on ro-en newstest16, en-tr newstest2017 and newstest2016 en-de. We use a beam of 5 if not stated otherwise. In bold, we show the highest score among the continuous models in each column.

Figure 1: BLEU\({}_{\text{beam}}-\)BLEU\({}_{\text{greedy}}\) scores for the ro-en newsdev2016 for continuous output models with uniform random and pre-trained embeddings. Greedy (beam size 1) BLEU scores are 30.0 for pre-trained, and 28.6 for random embeddings.

dation from beam search, which is why we also report results with greedy decoding for pre-trained in Table 1. Further investigation in Figure 1 shows that the ro-en model with pre-trained embeddings degrades consistently, performing best in the greedy case, while the random embedding model benefits noticeably from a larger beam, in spite of neighboring words being random and not related. We discuss the details of the beam search in Appendix D.

Frequency.We perform a token-level evaluation using compare-mt(Neubig et al., 2019), computing the \(F_{1}\) score of matching a gold token (at its gold position), aggregated over bins defined by the token's frequency in the training data. The result in Figure 3 reveals that random embeddings allow much better classification of rare tokens than even the discrete reference model. To understand this effect, we study the geometry of the pre-trained embedding spaces in relation to frequency in Figure 2. The top row shows the relationship between the frequency _rank_ (higher means rarer) and the similarity to its nearest- and fifth-nearest- neighbors. For all three language pairs we observe that most rare words become identical to their nearest neighbor. In contrast, for random embeddings this metric does not depend on rank and is always around 0.4. The bottom row of Figure 3 shows that the nearest neighbors of rare words tend also to be comparably rare. This geometry clarifies in part the surprising performance of random embeddings on rare tokens.

Combined embeddings.Our finding motivates combining pre-trained and random embeddings:",\mathbf{E}_{\text{cmb}}(y_{i})=\frac{\alpha\mathbf{E}_{\text{pre}}(y_{i})+(1-\alpha) \mathbf{E}_{\text{rand}}(y_{i})}{\|\alpha\mathbf{E}_{\text{pre}}(y_{i})+(1-\alpha)\bm {E}_{\text{rand}}(y_{i})\|}.,,"\mathbf{E}_{\text{combined}}(t)=\alpha\cdot\mathbf{E}_{\text{pre-trained}}(t)+(1-\alpha)\cdot\mathbf{E}_{\text{random}}(t),","where $\alpha$ controls the interpolation between pre-trained and random embeddings, enabling a hybrid representation that balances learned semantic structure with random diversity for improved performance on rare tokens."
naacl_2024_short_53,1,"## 3 Perturbations of Encoders

The proposal's core is a flexible and efficient approach for augmenting the behaviour of an encoder on a sample-by-sample basis to trigger a better decoder performance. The starting point for such a goal will be the recently introduced Non-Autoregressive Proxy (NAP). While the original work trained the network on a specific metric and used the estimates at runtime to directly perform various downstream tasks, we will extend the view of this network to a differentiable approximation of a sequence-level metric, and use the gradients of this approximation to improve performance.

Let \(\mathbf{\phi_{\text{e}}}\) and \(\mathbf{\phi_{\text{d}}}\) represent the parameters of some encoder and decoder network, and let \(\mathbf{x}\) be some input (token or embedding) sequence At inference time we have \(\mathbf{e}_{1:L}=\mathbf{f}(\mathbf{x};\mathbf{\phi_{\text{e}}})\) where \(\mathbf{e}_{1:L}\) represents a sequence of \(L\) encoder embeddings that are consumed by the decoder. The decoder, through an autoregressive process, produces an output sequence \(\hat{\mathbf{y}}=\mathbf{f}(\mathbf{e}_{1:L};\mathbf{\phi_{\text{d}}})\). We aim to find a sample-specific perturbation \(\mathbf{\delta}_{1:L}\) to the sequence of encoder outputs such that \(\bar{\mathbf{y}}=\mathbf{f}(\mathbf{e}_{1:L}+\mathbf{\delta}_{1:L};\mathbf{\phi_{\text{d}}})\) gives us a higher score according to some score \(\mathcal{S}\) (e.g. COMET):","\mathcal{S}(\mathbf{y},\bar{\mathbf{y}};\mathbf{x})>\mathcal{S}(\mathbf{y},\hat{\mathbf{y}};\mathbf{x})",where \(\mathbf{y}\) is the reference sequence,"\begin{equation}
\mathbf{\delta}_{1:L}^* = \arg\max_{\mathbf{\delta}_{1:L}} \mathcal{S}\left(\mathbf{f}(\mathbf{e}_{1:L} + \mathbf{\delta}_{1:L}; \mathbf{\phi_{\text{d}}})\right)
\end{equation}",Equation 1 defines the optimal perturbation $\mathbf{\delta}_{1:L}^*$ that maximizes the score $\mathcal{S}$ of the decoder output sequence $\bar{\mathbf{y}}$ obtained from the perturbed encoder outputs.
naacl_2024_short_53,2,"ce. To find a good perturbation we first train a lightweight Non-Autoregressive Proxy on top of the encoder to approximate the score \(f(\mathbf{e};\phi_{\text{nap}})\approx\mathcal{S}(\mathbf{y},\hat{\mathbf{y}};\mathbf{x})\) where \(\phi_{\text{nap}}\) represent the NAP parameters. Once this is achieved, the gradient of the NAP can be used to find a good perturbation to the encoder outputs of a certain sample \(\mathbf{x}\), making the approach sample specific:",\mathbf{\delta}_{i}=\alpha|\mathbf{e}_{i}|\bigg{|}\frac{\partial f(\mathbf{e}_{1:L};\phi_{ \text{nap}})}{\partial\mathbf{e}_{i}}\bigg{|}^{-1}\frac{\partial f(\mathbf{e}_{1:L}; \phi_{\text{nap}})}{\partial\mathbf{e}_{i}},"where \(i=1,\ldots,L\)",\mathbf{\delta}_{1:L} = \alpha \cdot \nabla_{\mathbf{e}_{1:L}} f(\mathbf{e}_{1:L}; \phi_{\text{nap}}),where $\alpha$ is a scaling factor controlling the magnitude of the perturbation applied to the encoder outputs
naacl_2024_short_56,1,"### Prompt Generation

We designed an innovative label prompt to assist the model in differentiating between relations. The similar relations obtained by filters are introduced into the prompt, aiming to focus on those confusing labels. To avoid the model over-avoiding similar relations and making radical predictions, we also include a randomly selected non-similar relation in the template. The Label Prompt \(P_{L}\) is of the following form: _""relation is not \(S_{i1}\)_,..., \(S_{ij}...\)_or \(N_{random}\)_"", where \(S_{ij}\) are from the similar relations set \(R_{i}\). \(N_{random}\) represents the non-similar relation which is selected randomly.

Additionally, we construct a context prompt \(P_{C}\) to further capture the differences in application scenarios of relations. We extract the context by replacing each entity in the sentence with a special token [BLANK]. The final input instance is formed by concatenating the correct label \(L\), original sentence \(X_{ori}\), context prompt \(P_{C}\), and label prompt \(P_{L}\) in sequence with special tokens [SEP].",X_{all}=L\ [SEP]\ X_{ori}\ [SEP]\ P_{C}\ [SEP]\ P_{L},,"P_L = \text{""relation is not } S_{i1}, \ldots, S_{ij}, \ldots \text{ or } N_{\text{random}} \text{""}",Label prompt $P_L$ containing similar relations $S_{ij}$ from set $R_i$ and a randomly selected non-similar relation $N_{\text{random}}$.
naacl_2024_short_56,2,"### Pre-training Task

As shown in figure 2, we propose two pre-training tasks to help the model learn the prompt format and teach the model how to distinguish relation types.

**Prompt MLM**. We follow the design of masked language model(MLM) in BERT (Devlin et al., 2019) and utilize this method on the context prompt and label prompt. This pre-training task allows the model to fit the corresponding parts of the prompt and learn how to extract useful information from them. Specifically, we randomly select the words in the context or labels in the prompt and replace them with [MASK]. All tokens will be masked if the labels consists of multiple tokens. We set the loss function of prompt MLM as:",\mathcal{L}_{MLM}=-{\sum}_{n=1}^{M}\log P(x_{n}),where \(M\) is the number of masked tokens and \(P(x_{n})\) is the predicted probability of token \(x_{n}\) over the vocabulary size,\mathcal{L}_{\text{prompt\_MLM}} = -\frac{1}{M} \sum_{m=1}^{M} \log P(x_m | \mathbf{x}_{\setminus m}; \theta),"The negative log-likelihood loss for masked tokens in the context and label prompts during pre-training, where $M$ is the number of masked positions, $x_m$ are masked tokens, and $\theta$ represents model parameters."
naacl_2024_short_56,3,"ze.

**Relation Contrastive Discrimination**. We introduce relation contrastive discrimination to optimize the distribution of relations in semantic space, which equips the model with a better ability to differentiate confusing relation types. We construct positive and negative samples as follows: Given an input \(X\) with relation type \(R\), for positive samples, we randomly choose the samples with the same label to construct \(K\) positive samples. For negative samples, we use other samples in the batch. In particular, we use our similar relation filter to construct several hard negative samples that contain similar relation types. We add it to the negative samples to guide the model focusing on these confusing labels. Considering multiple positive samples, we employ supervised contrastive learning (SCL) (Khosla et al., 2020) to learn the robust representation. We define \(h_{p}\) and \(h_{p}^{+}\) as the representation of input utterances and positive samples. We formulate SCL as follows:","\mathcal{L}_{s}=\frac{1}{N}\sum_{p=1}^{N}-\frac{1}{N_{y_{p}}-1}\sum_{q=1}^{N_ {y_{p}}}\log\frac{e^{sim(h_{p},h_{q}^{+})/\tau}}{\sum_{k=1}^{N}\mathbf{1}_{p \neq k}e^{sim(h_{p},h_{k})/\tau}}",where \(N\) is the total number of examples in the batch and \(N_{y_{p}}\) is the number of positive pairs in the batch,\mathcal{L}_{SCL} = - \sum_{i=1}^{K} \log \frac{\exp(h_{p} \cdot h_{p}^{+}/\tau)}{\exp(h_{p} \cdot h_{p}^{+}/\tau) + \sum_{j=1}^{N} \exp(h_{p} \cdot h_{p}^{-}/\tau)},"where $K$ is the number of positive samples, $N$ is the number of negative samples, and $\tau$ is a temperature parameter that controls the concentration level of the distribution."
naacl_2024_short_56,4,"ch. \(\tau\) is a temperature hyperparameter and \(sim(h_{1},h_{2})\) is cosine similarity \(\frac{h_{1}^{\top}h_{2}}{||h_{1}||\cdot||h_{2}||}\). **1** is an indicator function.

We weight both pre-training objectives together as the final loss function by a hyperparameter \(\alpha\):",\mathcal{L}_{final}=\alpha\mathcal{L}_{s}+(1-\alpha)\mathcal{L}_{MLM},,\mathcal{L}_{total} = \alpha \mathcal{L}_{MLM} + (1 - \alpha) \mathcal{L}_{s},where $\alpha$ is a hyperparameter that balances the contribution of the prompt MLM loss $\mathcal{L}_{MLM}$ and the relation contrastive discrimination loss $\mathcal{L}_{s}$ in the total loss function $\mathcal{L}_{total}$.
naacl_2024_short_62,1,"## 2 Method

In our experiment, we adapt the ATLAS architecture (Izacard et al., 2022) but use a decoder-only language model structure for our empirical analysis, focusing on question-answering tasks to study attention score distillation mechanisms. Specifically, for a given question \(Q\), we supply models with a knowledge base \(D=\{d_{1},d_{2},...,d_{m}\}\), where each \(d_{i}\) is a unique document. The objective of the models is to find the question-relevant documents \(D_{n}=\{n_{1},n_{2},...,n_{k}\}\subseteq D\) using the retriever, and then incorporate the query and \(D_{n}\) as the input for the reader to generate the answer \(A\) for the given question.

The attention distillation approach uses attention scores to gauge the importance of each input document \(d_{i}\) during the answer generation process. To accommodate changes in the reader model's structure, we utilize the _self-attention scores_ related to the output tokens as a measure of document relevance, rather than relying on _cross-attention scores_ between input documents and output that ATLAS uses. In addition, the attention level of a token \(t\) is not only evaluated from the self-attention score \(\alpha_{t}\) but also the norm of the value vector \(\mathbf{v_{t}}\) should be taken into account (Izacard et al., 2022). Af

Figure 2: The framework of the Retrieval-augmented Language Model of our experiment.

terwards, the _Softmax_ operator is applied to obtain the attention score distribution over the question-relevant documents \(D_{n}\):","P_{ATTN}(n_{i}|Q,A)=softmax(\sum_{t=1}^{T}\alpha_{t}||\mathbf{v}_{t}||_{2})",where \(T\) represents the total token count in \(n_{i}\),"\begin{equation}
\text{Attention}(d_i) = \text{Softmax}(\alpha_{t} \cdot \|\mathbf{v}_{t}\|),
\end{equation}",Equation 1 computes the attention score for each document $d_i$ by applying the Softmax operator to the product of the self-attention score $\alpha_t$ and the norm of the value vector $\|\mathbf{v}_t\|$.
naacl_2024_short_62,2,"\).

On the other hand, the retriever's probability distribution \(p_{RETR}\) over \(D_{n}\) can be defined as:","P_{RETR}(n_{i}|Q)=\frac{exp(s(n_{i},Q)/\theta)}{\sum_{k=1}^{K}exp(s(n_{k},Q)/ \theta)}","where \(s\) denotes the dot-product between the representation vectors of the input question \(Q\) and document candidate \(n_{i}\), and \(\theta\) is the temperature hyper-parameter","P_{RETR}(n_{i}|Q)=softmax(s(Q,n_{i}))",the probability distribution of the retriever over the question-relevant documents
naacl_2024_short_66,1,"### Background

Given a LM \(\theta\), an input query \(\mathbf{x}\), and a context \(\mathbf{c}\) that contains some external knowledge _unfamiliar_ or _in conflict_ to the model's prior knowledge, we ask our model \(\theta\) to generate a response \(\mathbf{y}\) given the the query and context. The response can be directly sampled (autoregressively) from the probability distribution conditioned on query \(\mathbf{x}\) and context \(\mathbf{c}\):","y_{t}\sim p_{\theta}(y_{t}\mid\mathbf{c},\mathbf{x},\mathbf{y}_{ct}) || \propto\exp\mathrm{logit}_{\theta}(y_{t}\mid\mathbf{c},\mathbf{x},\mathbf{y} _{ct})",,"\mathbf{y} \sim p_{\theta}(\mathbf{y} \mid \mathbf{x}, \mathbf{c})","Equation 1 defines the response generation process, where the model samples the output $\mathbf{y}$ from the conditional probability distribution given the query $\mathbf{x}$ and context $\mathbf{c}$."
naacl_2024_short_66,2,"However, in cases where the context \(\mathbf{c}\) contains knowledge that is out-of-distribution with respect to \(\theta\), we hypothesize that the model can struggle to effectively attend to \(\mathbf{c}\) and overly rely on the prior knowledge encoded in \(\theta\). For instance, as illustrated in Figure 1, when the context \(\mathbf{c}\) states ""Argentina won the FIFA World Cups in 1978, 1986 and 2022..."", it contradicts the LM's outdated prior knowledge that Argentina has won the World Cup twice. The language model may still incorrectly predict ""Two"" even when presented with the context \(\mathbf{c}\) and the query \(\mathbf{x}\).

### Context-aware Decoding

To mitigate such issues, we factor out the prior knowledge from the model's original output distribution contrastively. Here, we model the prior knowledge as \(p_{\theta}(y_{t}\mid\mathbf{x},\mathbf{y}_{ct})\) and adjust the model's original output probability distribution using the pointwise mutual information (PMI) between the context \(\mathbf{c}\) and the generation \(y_{t}\), conditioned on \(\mathbf{x},\mathbf{y}_{ct}\). Formally, we have:","y_{t}\sim\tilde{p}_{\theta}(y_{t}\mid\mathbf{c},\mathbf{x},\mathbf{y}_{ct}) || \propto p_{\theta}(y_{t}\mid\mathbf{c},\mathbf{x},\mathbf{y}_{ct})\bigg{(} \frac{p_{\theta}(y_{t}\mid\mathbf{c},\mathbf{x},\mathbf{y}_{ct})}{p_{\theta}(y_{t}\mid\bm {x},\mathbf{y}_{ct})}\bigg{)}^{\alpha}",,"\log p_{\theta}(y_{t}\mid\mathbf{c},\mathbf{x},\mathbf{y}_{ct}) = \log p_{\theta}(y_{t}\mid\mathbf{x},\mathbf{y}_{ct}) + \log \frac{p_{\theta}(\mathbf{c}\mid y_{t},\mathbf{x},\mathbf{y}_{ct})}{p_{\theta}(\mathbf{c}\mid\mathbf{x},\mathbf{y}_{ct})}","This equation decomposes the conditional probability of the token $y_t$ given the context $\mathbf{c}$, query $\mathbf{x}$, and previous tokens $\mathbf{y}_{ct}$ into a prior term and a context adjustment term via PMI."
naacl_2024_short_66,3,"\). Essentially, outputs that become much more likely when the context is included are preferred (Figure 1).

This expression is not a valid probability distribution and needs to be normalized across all possible values of \(y_{t}\). By rearranging the terms, we obtain the final form:","y_{t}\sim\mathrm{softmax}[(1+\alpha)\,\mathrm{logit}_{\theta}(y_{t} \mid\mathbf{c},\mathbf{x},\mathbf{y}_{ct}) || \qquad\qquad-\alpha\,\mathrm{logit}_{\theta}(y_{t}\mid\mathbf{x},\mathbf{y }_{ct})]",,"y_{t}\sim\tilde{p}_{\theta}(y_{t}\mid\mathbf{c},\mathbf{x},\mathbf{y}_{ct}) = \frac{p_{\theta}(y_{t}\mid\mathbf{c},\mathbf{x},\mathbf{y}_{ct})^{1+\alpha}}{\sum_{y'_{t}} p_{\theta}(y'_{t}\mid\mathbf{c},\mathbf{x},\mathbf{y}_{ct})^{1+\alpha} \bigg( \frac{p_{\theta}(y'_{t}\mid\mathbf{x},\mathbf{y}_{ct})}{p_{\theta}(y'_{t}\mid\mathbf{c},\mathbf{x},\mathbf{y}_{ct})} \bigg)^{\alpha} }}","This equation represents the normalized context-aware decoding distribution, where the model's prior knowledge is contrastively adjusted by the context-dependent likelihood raised to the power of $1+\alpha$, ensuring outputs favored by the context are prioritized."
naacl_2024_short_67,1,"#### 2.2.1 Input Instance Encoder

Given an input instance \(x=\{w_{1}^{x},...,w_{n}^{x}\}\), distinct special tokens \([E_{h}]\), \([\backslash E_{h}]\), \([E_{t}]\), \([\backslash E_{t}]\) are

Figure 2: The overall architecture of EMMA. (a) The recall model swiftly matches to retrieve the top \(k\) most probable relations. (b) The classification model further distinguishes among these similar relations.

employed to wrap the head entity and tail entity, respectively. After inputting \(x\) into a pre-trained encoder, we utilize the last hidden states of special tokens \([E_{h}]\), \([E_{t}]\), and \([CLS]\) (refer to \(w_{0}^{d}\)) as representations of head entity, tail entity, and contextual information, which is formulated as follows:","h_{0}^{x},h_{1}^{x},...,h_{n}^{x}=\textbf{BERT}(w_{0}^{x},w_{1}^{x},...,w_{n}^{x}) || x^{c}=h_{0}^{x},x^{h}=h_{[E_{h}]}^{x},x^{t}=h_{[E_{t}]}^{x}",,"\begin{equation}
    \mathbf{h}_{h}, \mathbf{h}_{t}, \mathbf{h}_{c} = \text{Encoder}(x)_{[E_h]}, \text{Encoder}(x)_{[E_t]}, \text{Encoder}(x)_{[CLS]}
\end{equation}","Equation 1 defines the extraction of head entity, tail entity, and contextual representations from the encoder output using special token indices."
naacl_2024_short_67,2,"Then we combine the representation of head entity \(x^{h}\in\mathbb{R}^{d}\), tail entity \(x^{t}\in\mathbb{R}^{d}\), and the contextual information \(x^{c}\in\mathbb{R}^{d}\) to form the comprehensive representation \(x^{vec}\in\mathbb{R}^{3d}\) of the input instance.",x^{vec}=x^{c}\oplus x^{h}\oplus x^{t},where \(d\) is the hidden dimension of the encoder and \(\oplus\) denotes the concatenation operator,"tags and the description in <description> tags. No extra text. Just the two lines.
</think>","tags. No extra text. Just the two lines.
</think>

<latex>x^{vec} = \left[ x^{h} \oplus x^{t} \oplus x^{c} \right]</latex>"
naacl_2024_short_67,3,"or.

#### 2.2.2 Virtual Entity Matching

Although the description of corresponding relation \(d=\{w_{1}^{d},...,w_{n}^{d}\}\) is easily obtainable (e.g. from Wikipedia), manually annotating the entity hypernyms within various relations is still time-consuming and laborious. Therefore, we directly input relation descriptions into the pre-trained encoder. Then, we employ two weight pooling layers with different parameters to obtain separate virtual entity representations \(d^{h}\in\mathbb{R}^{d}\) and \(d^{t}\in\mathbb{R}^{d}\). Similar to Section 2.2.1, we use the hidden states corresponding to the \([CLS]\) token (refer to \(w_{0}^{d}\)) as the contextual representation \(d^{c}\in\mathbb{R}^{d}\), and concatenate these three to obtain the comprehensive representation \(d^{vec}\in\mathbb{R}^{3d}\) of the relation description.","h_{0}^{d},h_{1}^{d},...,h_{n}^{d}=\textbf{BERT}(w_{0}^{d},w_{1}^{d},...,w_{n}^ {d}) || d^{c}=h_{0}^{d} || d^{h}=\textit{WeightPooling}_{1}(h_{1}^{d},...,h_{n}^{d}) || d^{t}=\textit{WeightPooling}_{2}(h_{1}^{d},...,h_{n}^{d}) || d^{vec}=d^{c}\oplus d^{h}\oplus d^{t}",,d^{vec}=d^{c}\oplus d^{h}\oplus d^{t},"where $d^{c}$, $d^{h}$, and $d^{t}$ represent the contextual, head entity, and tail entity representations of the relation description, respectively"
naacl_2024_short_67,4,"For the weight pooling, we employ the scheme proposed by Lin et al. (2017), utilizing an attention mechanism over the last hidden states of the pre-trained encoder to generate representations of virtual entities, which is formulated as follows:","H=(h_{1}^{d},...,h_{n}^{d}) || A=\textit{softmax}(HW+b) || d^{*}=AH",,"d^{h} = \sum_{i=1}^{n} \alpha_{i} h_{i}^{d}, \quad \alpha_{i} = \textit{Softmax}(w_{1}^{T} h_{i}^{d})","where $d^{h}$ is the virtual head entity representation obtained via attention-weighted sum of hidden states, and $\alpha_{i}$ represents the attention weights computed using a learnable parameter vector $w_{1}$"
naacl_2024_short_67,5,"h). \(W\) is a linear layer of \((L-1)\times 1\), \(b\in\mathbb{R}^{L-1}\) is the bias, and \(A\in\mathbb{R}^{L-1}\) denotes the final weights. The final representation \(d^{*}\in\mathbb{R}^{d}\) is obtained by weighting \(H\) using \(A\).

#### 2.2.3 Contrastive Learning

When \(N\) input instances \(\{x_{1},...,x_{N}\}\) and their corresponding relation descriptions \(\{d_{1},...,d_{N}\}\) are input into the encoder within a mini-batch, we obtain the representations of instance \(x_{i}^{vec}\) and description \(d_{i}^{vec}\), \(i\in[1,N]\). To effectively learn the matching relationship between \(x_{i}^{vec}\) and \(d_{i}^{vec}\), we utilize a contrastive learning method, where \(d_{i}^{vec}\) serves as a positive sample and other \(N-1\) samples within the mini-batch \(d_{j}^{vec}(j\neq i)\) serve as negative samples. The goal of contrastive learning is to minimize the distance between \(x_{i}^{vec}\) and \(d_{i}^{vec}\) while maximizing the distance from \(d_{j}^{vec}\).

We utilize cosine similarity as the measurement and employ the infoNCE(van den Oord et al., 2018) as the contrastive loss function:","\mathcal{L}_{i}=-\log\frac{e^{\text{sim}(x_{i}^{vec},d_{i}^{vec})/\tau}}{\sum_ {j=1}^{N}e^{\text{sim}(x_{i}^{vec},d_{j}^{vec})/\tau}}",where \(\tau\) is a temperature hyperparameter and \(\text{sim}(\cdot)\) is the cosine similarity,"\mathcal{L}_{i}=-\log\frac{\exp(\text{sim}(x_{i}^{vec},d_{i}^{vec})/\tau)}{\sum_{j=1}^{N}\exp(\text{sim}(x_{i}^{vec},d_{j}^{vec})/\tau)}","The contrastive loss function (infoNCE) that minimizes the distance between matching instance and relation description representations while maximizing the distance from non-matching samples, using cosine similarity scaled by temperature parameter τ."
naacl_2024_short_67,6,"ty.

### Fine-grained classification

In the recall stage, we obtain representations of input instances and relation descriptions separately for quick query matching. However, the lack of interaction between the instances and descriptions limits the model's performance ceiling. To tackle this issue, we propose fine-grained classification after coarse-grained recall, which jointly encodes instances and descriptions.

In the classification stage, during training, for each input instance \(x\), \(k\) relation descriptions \(D=\{d_{1},...,d_{k}\}\) are selected from the mini-batch of the recall stage, which includes \(d_{+}\) corresponding to the entity relation of \(x\), and top \(k-1\) descriptions with the highest matching scores excluding \(d_{+}\). The objective of classification is to select \(d_{+}\) from \(D\). We formulate this process as follows:",O_{i}=\textit{Pooling}(BERT(\langle x\oplus d_{j}\rangle)) || \hat{y}=\textit{MLP}(O_{0}\oplus O_{1}\oplus...O_{k}) || \mathcal{L}_{c}=-log(\frac{e^{\hat{y}_{+}}}{\sum_{i=1}^{k}e^{\hat{y}_{i}}}),,"\mathcal{L}_{cls} = -\log \frac{e^{\text{sim}(x^{vec}, d_{+}^{vec})/\tau}}{\sum_{d \in D} e^{\text{sim}(x^{vec}, d^{vec})/\tau}}",The classification loss function aims to select the correct relation description $d_{+}$ from a set $D$ of $k$ candidates by maximizing its similarity to the input instance $x$ relative to the others.
naacl_2024_short_69,1,"### DAGs & Counterfactuals

The motivating example described above is illustrated by the directed acyclic graph (DAG) in Figure 1, where we use a binary random variable \(A\) to indicate whether the patient receives \((A=1)\) antibiotics or not \((A=0)\). We similarly use a binary \(Y\) to denote whether the disease progresses \((Y=1)\) or not \((Y=0)\). An arrow in the DAG such as \(A\to Y\) indicates that \(A\) has a potential causal effect on \(Y\). Finally, we denote \(T\) as the patient medical records, and \(C\) as the set of all confounding variables contained in the records. For example, such variables could include income status or family disease history (Acharya et al., 2021). Most importantly, \(C\) is unobserved -- we don't know the exact confounding variables, but we have access to the text \(T\) containing them. In particular, \(T\) is related to \(A\) and \(Y\) through \(C\). The _counterfactual_ outcome \(Y^{a=1}\) represents the hypothetical disease progression had we intervened to assign \(A=1\) (prescribe antibiotics), and \(Y^{a=0}\) is defined analogously. In causal inference, the most common estimand is the average treatment effect (\(ATE\)) of \(A\) on \(Y\), computed as:",ATE=\mathbb{E}[Y^{a=1}-Y^{a=0}],,"\begin{equation}
    \text{ATE} = \mathbb{E}\left[Y^{a=1} - Y^{a=0}\right]
\end{equation}",The average treatment effect (ATE) quantifies the expected difference in counterfactual outcomes between prescribing antibiotics ($Y^{a=1}$) and not prescribing antibiotics ($Y^{a=0}$).
naacl_2024_short_69,2,"A fundamental problem is that we can never simultaneously observe both _counterfactuals_\(Y^{a=1},Y^{a=0}\)(Holland, 1986), thus we need a way to compute the \(ATE\) only utilizing observed data.

### Identification Assumptions

We proceed by assuming _consistency_, requiring that the outcome we observe for any possible treatment \(a\) is equal to the _counterfactual_ outcome we would have observed had we intervened to assign \(A=a\). Formally:",A=a\ \Rightarrow\ Y^{a}=Y,,Y = AY^{a=1} + (1-A)Y^{a=0},"This equation formalizes the consistency assumption, linking observed outcomes to potential counterfactual outcomes based on the treatment received."
naacl_2024_short_69,3,"We then assume _conditional exchangeability_, requiring the independence between our counterfactual \(Y^{a}\) and the observed treatment \(A\) conditioned on all confounders \(C\), formalized as:","Y^{a}\perp A\mid C\quad\forall a\in\{0,1\}",,Y^{a} \perp\!\!\!\perp A \mid C,"Conditional exchangeability ensures that, given the confounders $C$, the counterfactual outcome $Y^{a}$ is independent of the observed treatment assignment $A$."
naacl_2024_short_69,4,"Using these assumptions, we may compute the counterfactual \(\mathbb{E}[Y^{a}]\) as follows:","\mathbb{E}[Y^{a}] =\sum_{C}\mathbb{E}[Y^{a}\mid C]\mathbb{P}(C) || \overset{\eqref{eq:constraint}}{=}\sum_{C}\mathbb{E}[Y^{a}\mid A=a,C] \mathbb{P}(C)",,"\mathbb{E}[Y^{a}] = \mathbb{E}_{C}[\mathbb{E}[Y \mid A=a, C]]","This equation expresses the counterfactual expectation $\mathbb{E}[Y^{a}]$ as a marginalization over the confounders $C$, leveraging the observed data under the identification assumptions."
naacl_2024_short_69,5,(5),"\overset{\eqref{eq:constraint}}{=}\sum_{C}\mathbb{E}[Y\mid A=a,C] \mathbb{P}(C)",,"\overset{\text{exchangeability}}{=}\sum_{C}\mathbb{E}[Y\mid A=a,C]\mathbb{P}(C)",Equation 5 applies the conditional exchangeability assumption to replace the counterfactual outcome expectation with the observed outcome expectation conditioned on treatment and confounders.
naacl_2024_short_69,6,"Equation (6) expresses our counterfactual as a function of observed data. However, we are interested in the case where the low-dimensional \(C\) is unobserved but encoded inside the high-dimensional \(T\). Thus, if we could adequately model \(T\), we would be able to adjust for \(C\)'s confounding effect.

### Causal Effect Estimation

To estimate (1) using (6), we thus require (a) a representation of the text and (b) an appropriate causal estimation method. As mentioned in SS1, a BoW text representation is commonly used by existing text-based causal estimators. For (b), there are countless estimation methods, and we refer the reader to a much more exhaustive guide by Peters et al. (2017). One such commonly used method is the _Inverse Propensity of Treatment Weighting_ (IPTW), where \(\mathbb{E}[Y^{a}]\) is calculated as follows for a dataset of size \(N\):",\mathbb{E}[Y^{a}]=\frac{1}{N}\sum_{i\in[N]}Y_{i}\frac{\mathds{1}(A_{i}=a)}{ \mathbb{P}(A_{i}=a\mid T)},,\mathbb{E}[Y^{a}]=\frac{1}{N}\sum_{i=1}^{N}\frac{\mathbb{I}(A_{i}=a)Y_{i}}{e^{a}(T_{i})},"Equation 6 expresses the counterfactual expectation using inverse propensity weighting, where the expectation is computed over the observed data weighted by the inverse of the propensity score $e^{a}(T_i)$, which estimates the probability of receiving treatment $a$ given the text $T_i$."
naacl_2024_short_7,1,"### Problem Definition

Following Garg et al. (2022), we define the problem of ICL as passing in an \(i\)-shot sequence \(S^{i}=(x_{1},f(x_{1}),x_{2},f(x_{2}),\ldots,x_{n},f(x_{i}),x_{i+1})\) to the Transformer (denoted as \(M_{\theta}\)) and generating an output \(M_{\theta}(S^{i})\) to predict the ground-truth \(f(x_{i+1})\), where the examples have not been seen during training. We refer to this \(i\)-shot prediction problem, where input is given in pairs, as ICL.

We consider a data-generating process where \(d\)-dimensional inputs are drawn from any arbitrary distribution (i.e., \(x_{i}\sim\mathcal{D}_{x}\)) and a function \(f\) is sampled from the class of functions related to single-index probabilist's normalized Hermite polynomials (i.e, \(f\sim\mathcal{F}\)).

Similar to Garg et al. (2022), the training objective is to minimize the squared error \(l(\cdot,\cdot)\) between the prediction \(M_{\theta}(S^{i})\) and ground-truth \(f(x_{i+1})\):","\underset{\theta}{min}\,\mathbb{E}_{S^{i}}[\frac{1}{k+1}\sum_{i=0}^{k}l( M_{\theta}(S^{i}),f(x_{i+1}))].",,"\begin{equation}
    \mathcal{L} = \mathbb{E}_{x_{1},\ldots,x_{i+1}\sim\mathcal{D}_{x},f\sim\mathcal{F}}\left[l\left(M_{\theta}(S^{i}),f(x_{i+1})\right)\right].
\end{equation}",The loss function $\mathcal{L}$ is defined as the expectation of the squared error between the model's prediction and the true function value over the input distribution and function class.
naacl_2024_short_7,2,"Appendix A shows more training details.

### Tasks

We explore two types of tasks: learning a function class and learning a data distribution (see Appendix B). We consider a single-index function:","f(x)=\varphi(\langle x,w\rangle).",,"f(x)=\frac{1}{\sqrt{d}}h_{p}(w^{\top}x),","where $h_p$ is the $p$-th normalized probabilist's Hermite polynomial, $w\in\mathbb{R}^{d}$ is a weight vector uniformly distributed on the unit sphere, and $x\in\mathbb{R}^{d}$ is the input."
naacl_2024_short_7,3,"Function Class LearningWe look at the class of functions derived from normalized probabilist's Hermite polynomial with degree \(n\) and constants removed, i.e., \(\frac{1}{\sqrt{n!}}He_{n}(x)\), which satisfies orthogonality. This is useful as it guarantees that the function values of all tasks are uncorrelated. For each task, we separately sample \(x\) and \(w\) from an isotropic Gaussian distribution, where \(w\) remains constant for an \(i\)-shot sequence. We define \(K=3\) polynomial function classes as follows: denoting \(t=\langle x,w\rangle\), we pick \(\varphi\in\{\varphi_{\text{linear}},\varphi_{\text{quadratic}},\varphi_{\text{cubic}}\}\) for three function classes \(\mathcal{F}_{1}\), \(\mathcal{F}_{2}\), \(\mathcal{F}_{3}\), respectively.","\varphi_{\text{linear}}(t)=t, || \varphi_{\text{quadratic}}(t)=\frac{1}{\sqrt{2}}(t+\frac{1}{ \sqrt{2}}(t^{2}-1)) || \varphi_{\text{cubic}}(t)=\frac{1}{\sqrt{3}}(t+\frac{1}{\sqrt{ 2}}(t^{2}-1)+\frac{1}{\sqrt{6}}(t^{3}-3t))",,"\varphi_{\text{linear}}(t)=t,\quad\varphi_{\text{quadratic}}(t)=\frac{1}{\sqrt{2}}(t^{2}-1),\quad\varphi_{\text{cubic}}(t)=\frac{1}{\sqrt{6}}(t^{3}-3t).","Equation 3 defines the three function classes used in the study, corresponding to linear, quadratic, and cubic normalized probabilist's Hermite polynomials, which are orthogonal and ensure uncorrelated function values across tasks."
naacl_2024_short_7,4,"### Curriculum Learning

We define the total training steps to be \(T\), where the \(t\)-th training step ranges from \(t=1,2,\ldots,T\). Our curriculum learning strategy (_sequential_, _mixed_, or _random_) is used to allocate our \(K\) tasks across training time. In this paper, we explore \(K=3\) function classes defined earlier.

Sequential CurriculumWe first separate the total training steps \(T\) into \(K\) partitions. Within the \(k\)-th partition of training steps, we train the model on learning a function from the \(k\)-th function class, in order of increasing difficulty:",f\sim\begin{cases}\mathcal{F}_{1}&1\leq t<\frac{T}{3}\\ \mathcal{F}_{2}&\frac{T}{3}\leq t<\frac{2T}{3}\\ \mathcal{F}_{3}&\frac{2T}{3}\leq t<T\end{cases},,"\mathcal{T}_{k} = \left\{ t \in [1, T] \mid \frac{(k-1)T}{K} < t \leq \frac{kT}{K} \right\}, \quad k = 1, 2, \ldots, K.","Equation 4 defines the sequential curriculum learning schedule, where training steps are divided into K partitions and each partition is assigned to train on a specific function class in order of increasing difficulty."
naacl_2024_short_7,5,"Mixed CurriculumWe first separate the total training steps \(T\) into \(K\) partitions. Let \(\xi\) be (uniformly) drawn from \(\{1,2\}\) and \(\zeta\) be (uniformly) drawn from \(\{1,2,3\}\). We select tasks from the previous \(k\) partitions with equal probability (\(\mathbf{1}\) denotes the indicator function):",f\sim\begin{cases}\mathcal{F}_{1}&1\leq t<\frac{T}{3}\\ \sum_{s=1}^{2}\mathbf{1}(\xi=s)\mathcal{F}_{s}&\frac{T}{3}\leq t<\frac{2T}{3} \\ \sum_{s=1}^{3}\mathbf{1}(\zeta=s)\mathcal{F}_{s}&\frac{2T}{3}\leq t<T\end{cases},,f\sim\begin{cases}\mathcal{F}_{1}&1\leq t<\frac{T}{3}\\ \mathcal{F}_{\xi}&\frac{T}{3}\leq t<\frac{2T}{3}\\ \mathcal{F}_{\zeta}&\frac{2T}{3}\leq t<T\end{cases},Defines the mixed curriculum learning strategy where task selection in later partitions is randomized over previous function classes using uniform distributions $\xi$ and $\zeta$.
naacl_2024_short_7,6,"Random CurriculumAt each training step \(t\), we randomly sample from the list of \(K\) tasks with equal probability:","f\sim\sum_{s=1}^{3}\mathbf{1}(\zeta=s)\mathcal{F}_{s},\quad 1\leq t<T",,f\sim\sum_{s=1}^{K}\frac{1}{K}\mathcal{F}_{s},"At each training step, a function class is randomly selected with equal probability from the set of all $K$ function classes."
naacl_2024_short_8,1,"## 2 Methods

**monoBERT**. monoBERT (Nogueira and Cho, 2019) is one of the first cross-encoders (MacAvaney et al., 2019; Nogueira et al., 2020) that apply pretrained transformers in passage retrieval. Given concatenated query \(q\) and document \(d\), the model computes relevance scores \(s_{q,d}\) from the [CLS] representation on the final layer of BERT (Devlin et al., 2019), formulated as follows (Lin et al., 2022; Pradeep et al., 2022):","s_{m}(q,d)=T_{\texttt{[CLS]}}W+b,","where \(T_{\texttt{[CLS]}}\in\mathbf{R}^{D}\) is the [CLS] representation on the final layer, and \(W\in\mathbf{R}^{D\times 1}\) and \(b\in\mathbf{R}\) are the weight and bias for classification","s_{q,d} = \mathbf{W}^\top \mathbf{h}_{[CLS]} + b",Relevance score computed using the [CLS] token's final layer representation through a learned linear layer with weight matrix $\mathbf{W}$ and bias $b$.
naacl_2024_short_8,2,"on.

Some of the previous works term the models as ""mono[BACKBONE]"" when initialized from non-BERT pretrained Transformers, such as mono-ELECTRA (Pradeep et al., 2022). However, since the underlying model structure remains the same, we refer to them all as monoBERT while specifying the backbones where the models are initialized.

**Mean-Pooling**. To study whether the improvement of CELI is attributed to the interaction between the query and the documents tokens, or simply the additional token information, we add the Mean-Pooling method as a baseline. Instead of computing the similarity score based solely on the [CLS] representation as in Eq. (1), it uses the mean representation of all the tokens:","s_{m}(q,d)=\frac{1}{n}\sum_{i}^{n}(T_{tok_{i}}W+b),","where \(T_{tok_{i}}\) is the final-layer representation of the \(i\)-th token, and \(n\) is the total number of tokens in the input sequence","s_{m}(q,d)=\frac{1}{N}\sum_{i=1}^{N}T_{i}W+b,","where $T_{i}\in\mathbf{R}^{D}$ is the representation of the $i$-th token in the concatenated sequence, and $N$ is the total number of tokens."
naacl_2024_short_8,3,"ce. \(W\in\mathbf{R}^{D\times 1}\) and \(b\in\mathbf{R}\) are the weight and bias for classification, same as Eq. (1).

**CELI**. In this work, we use the simplest version of late interaction proposed by Khattab and Zaharia (2020). We first obtain the representation of each token in the query \(q\) and document \(d\):","v_{q_{i}}=T_{q_{i}}W+b;\quad v_{d_{j}}=T_{d_{j}}W+b,","where \(q_{i}\) and \(d_{j}\) represent the \(i\)-th token of query \(q\) and the \(j\)-th token of document \(d\), respectively",,
naacl_2024_short_8,4,"ly. Similar to Eq. (1), \(T\in\mathbf{R}^{D}\) refers to each token representation on the final layer. \(W\in\mathbf{R}^{D\times D_{tok}}\) and \(b\in\mathbf{R}^{D_{tok}}\) are the weight and bias in a projection layer, projecting the \(T_{tok}\) to a lower dimension \(D_{tok}<D\).

With token representations \(v_{q_{i}}\) and \(v_{d_{j}}\), the late interaction first computes the inner product scores between all pairs of query and document tokens, then sums up the max similarity score for each query token against all document tokens:","s_{l}(q,d)=\sum_{q_{i}}\max_{d_{j}}(v_{q_{i}}^{T}v_{d_{j}}).",,"s_{l}(q,d)=\sum_{i}^{n_q}\max_{j}^{n_d}\langle v_{q_{i}}, v_{d_{j}}\rangle,","where $v_{q_i}$ and $v_{d_j}$ are the projected representations of the $i$-th query token and $j$-th document token, respectively, and $\langle \cdot, \cdot \rangle$ denotes the inner product operation"
naacl_2024_short_8,5,"Eq. (4) shares the same formulation as in the first-stage retrievers, and only differ in that the token representations \(T_{q_{i}}\) and \(T_{d_{j}}\) embed information from both the query and document, whereas in first-stage retrievers, they are computed independently from each other, with \(T_{q_{i}}\) perceiving no information from document \(d\) and vice versa.

During training, we compute the LCE loss on \(s_{m}\) and \(s_{l}\), respectively:","\mathcal{L} =\mathit{lce}(s_{m}(q,d^{+}),s_{m}(q,d_{1}^{-}),...,s_{m}(q,d_{n}^ {-})) || +\mathit{lce}(s_{l}(q,d^{+}),s_{l}(q,d_{1}^{-}),...,s_{l}(q,d_{n}^ {-})),",,"\mathcal{L}_{\text{LCE}} = -\log \frac{\exp(s_{m}(q,d^+)/\tau)}{\exp(s_{m}(q,d^+)/\tau) + \sum_{d^-}\exp(s_{m}(q,d^-)/\tau)} - \log \frac{\exp(s_{l}(q,d^+)/\tau)}{\exp(s_{l}(q,d^+)/\tau) + \sum_{d^-}\exp(s_{l}(q,d^-)/\tau)},","where $d^+$ is the positive document, $d^-$ are the negative documents, and $\tau$ is the temperature parameter controlling the sharpness of the similarity distribution."
naacl_2024_short_9,1,"### Preliminaries: Multilingual DPR

Dense Passage Retrieval (DPR) (Karpukhin et al., 2020) is a widely used architecture using two BERT encoders to encode queries and passages. DPR is optimized to maximize the inner products of embeddings of a query and its relevant passage during training. At inference time, DPR retrieves passages with the most similar embedding to a given query through maximum inner product search.

The training objective involves comparing all pairs of queries and passages in a batch. With the query embedding denoted as \(q\), the corresponding gold passage embedding as \(p_{t}\), and passage embeddings as \(p_{i}\) (including \(p_{t},i=1,2,...,N\)), the loss function is defined as follows.","L_{ir}(q,p_{i})=-\log(\frac{e^{q\cdot p_{t}}}{\sum_{i=1}^{N}e^{q\cdot p_{ i}}})",,"\begin{equation}
L = - \log \left( \frac{\exp(q \cdot p_t)}{\sum_{i=1}^N \exp(q \cdot p_i)} \right )
\end{equation}",Cross-entropy loss for maximizing the similarity between query embedding $q$ and the gold passage embedding $p_t$ relative to all passage embeddings $p_i$ in the batch.
naacl_2024_short_9,2,"DPR initialized with multilingual BERT is referred to as multilingual DPR (mDPR). We use mDPR as baseline architecture.

### NaiveMix: Naive Code-Mixing

Naive code-mixing approach, named NaiveMix, is to train models on code-mixed data, similar to previous studies (Qin et al., 2021; Huang et al., 2023; Litschko et al., 2023).

We conduct code-mixing through the following steps. First, at each training step, we select entire queries or passages with a probability of \(\mathbf{r}_{s}\) in a batch. Second, we choose words to be replaced within each selected data with a probability of \(\mathbf{r}_{w}\). Subsequently, we replace the selected words with equivalent terms in the target language.

### Motivation: Mixed Effect of Code-Mixing

There are two conflicting objectives in achieving cross-lingual transfer for IR: aligning representations between source and target languages, and effectively matching relevant query-passage pairs. The first goal, language alignment, can be achieved through NaiveMix, since training mPLMs on code-mixed data typically contributes to language alignment, as indicated by Qin et al. (2021). However,NaiveMix hinders the additional goal of relevance matching due to the embedding differences between English and code-mixed text.

Further compounding this issue, embedding differences become larger for longer text representations, such as passages in IR. The average cosine similarities of mBERT embeddings between English data, in the validation set of Natural Questions (Kwiatkowski et al., 2019), and their code-mixed counterparts are 0.944 (SD = 0.036) for queries and 0.867 (SD = 0.257) for passages.

The lower similarity and substantial variation in code-mixed passages appear to pose a greater hindrance to relevance matching. Therefore, we apply code-mixing only to the query encoder to align language representations while avoiding interference with relevance matching.

### Proposed: ContrastiveMix

To address the dilemma of two conflicting goals, we propose ContrastiveMix, which is designed to learn IR in English while transferring this knowledge to the target language through code-mixing. This approach involves training models on English data with an additional contrastive loss \(L_{c}\), that aligns English query with its corresponding code-mixed query. By introducing this objective, we can separate the roles of English IR data and code-mixed data into IR learning and representation alignment, instead of directly training on code-mixed data.

Specifically, we implement this method as an in-batch contrastive loss. When the batch size is \(N\), the embedding vectors of the English query and its code-mixed counterpart are denoted as \(q_{s}\) and \(q_{t}\), respectively, and the embedding vectors of all code-mixed queries within the batch represented as \(q_{j}\) (including \(q_{t}\), \(j=1,2,...,N\)), the contrastive loss term is defined as follows:","L_{c}(q_{s},q_{t})=-\log(\frac{e^{q_{s}\cdot q_{t}}}{\sum_{j=1}^{N}e^{q_ {s}\cdot q_{j}}})",,"L_{c}(q_{s},q_{t},q_{j})=-\log(\frac{e^{q_{s}\cdot q_{t}}}{\sum_{j=1}^{N}e^{q_{s}\cdot q_{j}}})","The contrastive loss aligns English queries with their code-mixed counterparts within a batch, facilitating language representation alignment without compromising relevance matching."
naacl_2024_short_9,3,and the entire training objective is,L=L_{ir}+wL_{c},where \(w\) is a hyperparameter for weighting the contrastive loss,"L = L_{ir}(q, p_i) + \lambda L_{c}(q_s, q_t)","The overall training objective combines the information retrieval loss $L_{ir}$ and the contrastive loss $L_{c}$, weighted by a factor $\lambda$."
neurips_2024_oral_10,1,"### Joint Feature Extraction and Feature Denoising

We refer to the diffusion modeling approach to denoise the noisy features through T-steps to obtain clean features. At the beginning, we use the features output from the backbone network as data samples for diffusion training, and get the noisy samples by continuously adding noise and learning through the network in order to simulate the data distribution of its features.","q(\mathbf{x}_{1:T}|\mathbf{x}_{0}) \coloneqq\prod_{t=1}^{T}q(\mathbf{x}_{t}|\mathbf{x}_{t-1}) || q(\mathbf{x}_{t}|\mathbf{x}_{t-1}) \coloneqq\mathcal{N}(\mathbf{x}_{t};\sqrt{1-\beta_{t}}\mathbf{x} _{t-1},\beta_{t}\mathbf{I})",,"x_t = \sqrt{\bar{\alpha}_t} \, x_0 + \sqrt{1 - \bar{\alpha}_t} \, \epsilon_t","Equation modeling the forward diffusion process where noisy features $ x_t $ at step $ t $ are generated from clean features $ x_0 $ and Gaussian noise $ \epsilon_t $, parameterized by the cumulative noise schedule $ \bar{\alpha}_t $."
neurips_2024_oral_10,2,"ss.

In the inference stage, as shown in Fig. 1(b), we perform T-step denoising on the output features, to obtain cleaner features and improve the expressiveness of the features.","p_{\theta}(\mathbf{x}_{0:T}) \coloneqq p(\mathbf{x}_{T})\prod_{t=1}^{T}p_{\theta}(\mathbf{x}_{t-1}| \mathbf{x}_{t}) || p_{\theta}(\mathbf{x}_{t-1}|\mathbf{x}_{t}) \coloneqq\mathcal{N}(\mathbf{x}_{t-1};\mu_{\theta}(\mathbf{x}_{t },t),\Sigma_{\theta}(\mathbf{x}_{t},t))",,p_{\theta}(\mathbf{x}_{0:T}) \coloneqq p_{\theta}(\mathbf{x}_{T})\prod_{t=1}^{T}p_{\theta}(\mathbf{x}_{t-1}|\mathbf{x}_{t}),"The joint distribution over the denoising process, starting from the noisy features at time T and iteratively reconstructing the clean features through T steps."
neurips_2024_oral_10,3,"ge. \(T\) is the denoising step size, representing the magnitude of the noise. We adjust \(t\) appropriately based on different datasets and backbones to obtain the optimal denoising amplitude. According to \(p_{\theta}(\mathbf{x}_{t-1}|\mathbf{x}_{t})\) denoise it step by step, and finally obtains \(X_{0}\), which represents the clean feature after denoising.

### Fuse Feature Extraction and Feature Denoising

As described in Section 3.2, the proposed method above could effectively improve the discriminability of features. Still, extra inference latency is introduced caused by recursive calling of the denoising layers. To solve the problem, we propose to fuse parameters of feature denoising layers into parameters of existing embedding layers of the backbone. The core idea is to expand the linear layer each transformer encoder block into two branches, one for its original embedding layer and the other for extra denoising layer. As shown in Fig. 2, during the training phase, we freeze the original embedding layers and only train the denoising layers. The training method is consistent with section 3.2, and the features are diffused and fed into the denoising layers. Please refer to Algorithm 1 for more details. In the inference stage, we fuse the pre-trained parameters of embedding and denoising layers, merging the two branches into a single branch without additional inference time. Please note that, here we take the transformer architecture as an example, but _DenoiseRep_ is suitable for CNN architecture. We demonstrate its scalibity on CNN in experiments. The derivation of parameter merging is as follows:","X_{t-1}=\frac{1}{\sqrt{a_{t}}}(X_{t}-\frac{1-a_{t}}{\sqrt{1-\tilde{a_{t}}}}D_{ \theta}(X_{t},t))+\sigma_{t}z","where \(a_{t}=1-\beta_{t}\), \(D_{\theta}\) are the parameters of the prediction noise network",\mathbf{W}_{\text{merged}} = \mathbf{W}_{\text{embed}} + \mathbf{W}_{\text{denoise}},The merged weight matrix combines the pre-trained embedding layer weights and denoising layer weights into a single branch for efficient inference.
neurips_2024_oral_10,4,rk.,\begin{split} Y&=WX+b\\ \frac{1}{\sqrt{a_{t}}}X_{t}-X_{t-1}&=\frac{1-a_{t}}{ \sqrt{a_{t}}\sqrt{1-\tilde{a_{t}}}}D_{\theta}X_{t}-\sigma_{t}z\\ \frac{1}{\sqrt{a_{t}}}Y_{t}-Y_{t-1}&=\frac{1-a_{t} }{\sqrt{a_{t}}\sqrt{1-\tilde{a_{t}}}}D_{\theta}Y_{t}-\sigma_{t}z\end{split},,\mathbf{W}_{\text{merged}} = \mathbf{W}_{\text{embed}} + \mathbf{W}_{\text{denoise}} \cdot \mathbf{A},Merges the embedding and denoising layer weights into a single branch using a linear combination with matrix A
neurips_2024_oral_10,5,We make a simple transformation of Eq. (5) and multiply both sides simultaneously by \(W\). The simplified equation can be obtained by bringing \(Y_{t}\) in terms of \(WX_{t}+b\):,"\begin{split} Y_{t-1}=[W-C_{1}(t)WW_{D}]X_{t}+WC_{2}(t)C_{3}+b\\ C_{1}(t)=\frac{1-a_{t}}{\sqrt{a_{t}}\sqrt{1-\tilde{a_{t}}}}\qquad C _{2}(t)=\frac{1-a_{t-1}^{-}}{1-\tilde{a_{t}}}\beta_{t}\qquad C_{3}=Z\sim N(0,I) \end{split}","where \(W_{D}\) denotes the parameters of \(D_{\theta}(X_{t},t)\), \(X_{t}\) denotes the input of this linear layer, \(Y_{t}\) denotes the output of this linear layer, and \(Y_{t-1}\) denotes the result after denoising in one step of \(Y_{t}\)",Y_{t-1} = \frac{1}{\sqrt{a_t}} Y_t - \frac{1 - a_t}{\sqrt{a_t} \sqrt{1 - \tilde{a}_t}} D_\theta Y_t + \sigma_t z,"This equation represents the transformed denoising step expressed in terms of the fused parameters, enabling efficient inference without additional computational overhead."
neurips_2024_oral_10,6,"\). Due to the cascading relationship of blocks, as detailed in Algorithm. 2, different \(t\) values are set according to the order between levels, and the one-step denoising of one layer is combined to achieve the denoising process of \(Y_{t}\to Y_{0}\), ensuring the continuity of denoising and ultimately obtaining clean features. We split the original single branch into a dual branch structure. During the training phase, the backbone maintains its original parameters and needs to train the denoising module parameters. In the inference stage, as shown on the left side of Fig. 2, we use the method of reparameterization, to replace the original \(W\) parameter with \(W^{\prime}\), where \(W^{\prime}=[W-C_{1}(t)WW_{D}]\) in Eq. (7), which has the same

Figure 2: Pipeline of our proposed _DenoiseRep_. ViT consists of \(N\) cascaded transformer encoder layers. During the training phase (see the right side “Train Only” process), we freeze the backbone parameters and only train the extra denoising layers. In the inference stage (see the left side “Infer Only” process), we merge the parameters of denoising layers to corresponding encoder layers. So there is no extra inference latency cost. Please find definitions of \(W\), \(b\), \(W_{D}\), \(W^{\prime}\), \(i\) and \(b^{\prime}\) in Algorithm 2.

number of parameters as \(W\), thus achieving the combination of \(FC\) operation and denoising without additional time cost. It is a **Computation-free** method.

In Eq. (7), we achieve one-step denoising \(Y_{t}\to Y_{t-1}\). If we need to increase the denoising amplitude, we can extend it to two-step or multi-step denoising. The following is the derivation formula for two-step denoising:",\frac{1}{\sqrt{a_{t}}}Y_{t}-Y_{t-1}=C_{1}(t)D_{\theta}Y_{t}-\sigma_{t}z || \frac{1}{\sqrt{a_{t-1}}}Y_{t-1}-Y_{t-2}=C_{1}(t-1)D_{\theta}Y_{t-1}-\sigma_{t- 1}z,,"\begin{split} Y_{t-2} &= [W - C_{1}(t-1)WW_{D}']X_{t-1} + WC_{2}(t-1)C_{3}' + b \\ &= [W - C_{1}(t-1)WW_{D}']\left(\frac{1}{\sqrt{a_{t}}}(X_{t} - \frac{1 - a_{t}}{\sqrt{1 - \tilde{a}_{t}}}D_{\theta}(X_{t}, t)) + \sigma_{t}z\right) + WC_{2}(t-1)C_{3}' + b \end{split}","Extension of the one-step denoising equation to two-step denoising, where $W_{D}'$ represents updated denoising layer parameters and $Y_{t-2}$ is the result after two denoising steps."
neurips_2024_oral_10,7,We can obtain this by eliminating \(Y_{t-1}\) from Eq.(8) and Eq.(9) and replacing \(Y_{t}\) with \(WX_{t}+b\):,\begin{split} Y_{t-2}=W^{\prime\prime}X_{t}+C^{\prime\prime}\\ W^{\prime\prime}=\frac{1}{\sqrt{a_{t}-1}}\{\frac{W}{\sqrt{a_{t} }}-[C_{1}(t)+C_{1}(t-1)]WW_{D}+\sqrt{a_{t}}C_{1}(t-1)C_{1}(t)WW_{D}W_{D}\}\\ C^{\prime\prime}=\frac{1}{\sqrt{a_{t}-1}}[WC_{2}(t)+\sqrt{a_{t}}WC_{ 2}(t-1)-\sqrt{a_{t}}C_{1}(t-1)C_{2}(t)WW_{D}]Z+b\end{split},,Y_{t-2} = \left[W - C_1(t) W W_D - C_1(t-1) \frac{1}{\sqrt{a_t}} W W_D \right] X_t + \left[ b - C_1(t) W W_D b - C_1(t-1) \frac{1 - a_t}{\sqrt{a_t} \sqrt{1 - \tilde{a}_t}} W W_D b \right],"This equation represents the result of two-step denoising by eliminating the intermediate variable $Y_{t-1}$ and substituting $Y_t$ in terms of the input $X_t$, enabling deeper denoising while maintaining parameter efficiency."
neurips_2024_oral_10,8,"Note that a single module completes two steps of denoising. To ensure the continuity of denoising, the \(t\) value should be sequentially reduced by 2.

Our proposed _DenoiseRep_ is based on feature-level denoising and can be migrated to various downstream tasks. It denoises the features on each layer for better removal of noise at each stage, as the noise in the inference stage comes from multiple sources, which could be noise in the input image or noise generated while passing through the network. Denoising each layer avoids noise accumulation and gives better quality output. And according to the noise challenges brought by data in different scenarios, the denoising intensity can be adjusted by controlling \(t\), \(\beta_{t}\), and the number of denoising times, which has good generalization ability.

```
0: The number of feature layers in the backbone N, features extracted from each layer \(\{F_{i}\}_{i=1}^{N}\), trained denoising module parameters \(\{W_{D_{i}}\}_{i=1}^{N}\) in Algorithm(1), after obtaining the initial feature \(F^{N}\) through patch embed, it is necessary to remove N-step noise from it, the pre-trained parameters \(\{W_{i}\}_{i=1}^{N}\) and \(\{\overline{b}_{i}\}_{i=1}^{N}\) for the backbone.
0: Feature \(F^{0}\) after denoising.
1:for each \(i\in[N,1]\)do
2:\(t=i\): Set the denoising amplitude based on the depth of the current layer.
3:\(W^{\prime}=[W_{i}-C_{1}(t)W_{i}W_{D_{i}}]\), \(b^{\prime}=W_{i}C_{2}(t)C_{3}+b_{i}\): Parameter fusion according to Eq.(7).
4:\(F^{t-1}=W^{\prime}F^{t}+b^{\prime}\): Fuse feature extraction and feature denoising.
5:endfor
6:return\(F^{0}\)
```

**Algorithm 2** Inference

### Unsupervised Learning Manner

Our proposed _DenoiseRep_ is label-free because its essence is a generative model that models data by learning its distribution. Thus the training loss contains only the \(Loss_{p}\) of denoising layers:","Loss_{p}=\sum_{i=1}^{N}|\epsilon_{i}-D_{\theta_{i}}(X_{t_{i}},t_{i})|","where \(\epsilon\) denotes the sampled noise, \(N\) denotes the number of denoising layers, \(X_{t}\) denotes the noise sample, \(t\) denotes the diffusion step, and \(D_{\theta}(X_{t},t)\) denotes the noise predicted by the denoising layer","\begin{split} \mathcal{L}_{p} &= \mathbb{E}_{t,\mathbf{x}_{0},\epsilon}\left[\left\|\epsilon - D_{\theta}(\mathbf{x}_{t}, t)\right\|^{2}\right] \\ \text{where } \mathbf{x}_{t} &= \sqrt{\tilde{a}_{t}}\mathbf{x}_{0} + \sqrt{1 - \tilde{a}_{t}}\epsilon, \quad \epsilon \sim \mathcal{N}(0, \mathbf{I}) \end{split}","Loss function for the denoising network $D_{\theta}$, where $\mathcal{L}_{p}$ minimizes the mean squared error between the predicted noise $\epsilon$ and the ground truth noise, enabling the model to learn the reverse diffusion process effectively."
neurips_2024_oral_10,9,"er.

However, it is worth noting that our method is complementary to label if the label is available. \(Loss_{l}\) is the task-specific supervised loss with label, \(\lambda\) is the trade-off parameter between two losses. The label-argumented learning is defined as:",Loss=(1-\lambda)Loss_{l}+\lambda Loss_{p},,Loss = Loss_{p} + \lambda \cdot Loss_{l},Combination of unsupervised denoising loss and supervised task-specific loss with trade-off parameter $\lambda$
neurips_2024_oral_11,1,"### Functionality Similarity as a Consensus

The decomposition of complex tasks benefits from solving easier sub-goals, but might introduce the risks of cascading errors, which refers to errors in sub-functions that lead to errors in ancestral functions. To mitigate this, we introduce Functional Consensus which aims at reducing inconsistencies in program behavior. This is achieved by sampling multiple functions and selecting the one that exhibits consensus, as measured by the aggregated similarity of functionality between candidates, thus abating outlier functionalities.

Functionality SimilarityA program specifies its functionality (or behavior) through the control flow and logic defined by its code semantics. However, comparing the functionalities between two programs based on their semantics is somewhat challenging. By decomposing the requirement into functions, FunCoder is able to view the function behavior as a black box that maps arguments into return values. Considering two functions \(f\) and \(g\) with the same input domain \(D(f)=D(g)\), we define the similarity between them \(sim(f,g)\) as the identicalness of outputs when given the same input values.","sim(f,g)=\int_{x\in D(f)}\frac{\mathds{1}[f(x)=g(x)]}{|D(f)|}\ \ \approx\sum_{x\in X|X\sim D(f)}\frac{\mathds{1}[f(x)=g(x)]}{|X|}",,"\begin{equation}
    \text{sim}(f, g) = \frac{1}{|D(f)|} \sum_{x \in D(f)} \mathbb{1}\left[f(x) = g(x)\right]
\end{equation}","Functionality similarity between $f$ and $g$ is defined as the average agreement of their outputs over all inputs in the shared domain $D(f)$, where $\mathbb{1}[\cdot]$ denotes the indicator function."
neurips_2024_oral_11,2,"The similarity becomes \(1\) if and only if two functions output consistent values for all inputs: \(\forall x\in D(f):f(x)=g(x)\Leftrightarrow sim(f,g)=1\). We notice that the input domain \(D(f)\) is unbounded in most cases, making its measurement barely feasible in practice. Thus, we approximate it by sampling a subset of possible inputs \(X\sim D(f)\) with an LLM.

Consensusis reached by selecting the candidate \(f^{*}\) holding maximal similarity with others after sampling multiple function implementations \(F=\{f_{(i)}\}\) for the same requirements.","f^{*}=\textsc{FunConsensus}(F)=\operatorname*{arg\,max}_{f_{(i)}\in F}\sum_{ f_{(j)}\in F\setminus\{f_{(i)}\}}sim(f_{(i)},f_{(j)})",,,
neurips_2024_oral_12,1,"### The Causal Framework in i.i.d. Data

**Structural Causal Model**(Peters et al., 2017; Pearl, 2009; Haavelmo et al., 1995; Wright, 1921) A structural causal model (SCM) \(\mathcal{M}:=(\mathbf{F},P_{\mathbf{U}})\) consists of a set of structural assignments \(\mathbf{F}:=\{f_{1},\ldots,f_{d}\}\) such that \(X_{j}:=f_{j}(\mathbf{PA}_{j},U_{j}),j=1,\ldots,d\), where \(\mathbf{PA}_{j}\subseteq\{X_{1},\ldots,X_{d}\}\backslash\{X_{j}\}\) and are often called as parents of \(X_{j}\). The joint distribution \(P_{\mathbf{U}}\) over the noise or exogenous variables \(\mathbf{U}\) is assumed to be jointly independent. When such a condition is satisfied, the SCM is a Markovian model that assumes the absence of unmeasured confounders. A graph \(\mathcal{G}\) of an SCM is obtained by creating one vertex \(X_{j}\) and drawing directed edges from each parent in \(\mathbf{PA}_{j}\) to \(X_{j}\). We assume \(\mathcal{G}\) is acyclic. Our lack of knowledge in noise variables \(P_{\mathbf{U}}\) and together with the structural assignments \(\mathbf{F}\) induces a joint distribution over the observable variables \(P(X_{1},\ldots,X_{d})\). Such joint distribution satisfies the _Markov factorization property_ with respect to \(\mathcal{G}\):","P(X_{1},\ldots,X_{d})=\prod_{i=1}^{d}P(X_{i}\mid\mathbf{PA}_{i}),",,"P(X_1, \ldots, X_d) = \prod_{j=1}^{d} P(X_j \mid \mathbf{PA}_j)","The joint distribution over observable variables satisfies the Markov factorization property with respect to the graph $\mathcal{G}$, decomposing into a product of conditional distributions of each variable given its parents in the graph."
neurips_2024_oral_12,2,"Given two disjoint sets of variables \(\mathbf{X}\) and \(\mathbf{Y}\), the **causal effect** of \(\mathbf{X}\) on \(\mathbf{Y}\), denoted as \(P(\mathbf{Y}|do(\mathbf{X}))\), is defined with respect to modifications of an existing SCM (also known as _graph surgery_): for each realization \(\mathbf{x}\) of \(\mathbf{X}\), \(P(\mathbf{y}|do(\mathbf{x}))\) gives the probability of \(\mathbf{Y}=\mathbf{y}\) induced by deleting from the SCM all structural assignments corresponding to variables in \(\mathbf{X}\) and substituting \(\mathbf{X}=\mathbf{x}\) in the remaining equations. In Markovian models, given a graph, causal effect is identifiable via Eq. (2):","P(X_{1},\ldots,X_{d}|do(\mathbf{X}=\mathbf{x}))=\prod_{i:X_{i}\not\in\mathbf{X }}P(X_{i}|\mathbf{PA}_{i})\big{|}_{\mathbf{X}=\mathbf{x}},","where \(|_{\mathbf{X}=\mathbf{x}}\) enforces \(X_{1},\ldots,X_{d}\) to be consistent with realizations of \(\mathbf{X}\) else Eq","P(\mathbf{Y}|do(\mathbf{X})) = \sum_{\mathbf{Z}} P(\mathbf{Y}|\mathbf{X},\mathbf{Z}) P(\mathbf{Z}),","where $\mathbf{Z}$ is a set of variables satisfying the back-door criterion relative to $(\mathbf{X}, \mathbf{Y})$ in the graph $\mathcal{G}$."
neurips_2024_oral_12,3,"Eq. (2) takes value 0. This principled approach is known as _g-computation formula_(Robins, 1986), _truncated factorization_(Pearl, 2009) or _manipulation theorem_(Spirtes et al., 1993). Appendix A details the standard graphical terminology.

**Independent Causal Mechanism (ICM)** postulates how Markov factors (hereon referred to as _causal mechanisms_) in Eq. (1) should relate to each other. Scholkopf et al. (2012) and Peters et al. (2017) express the insights as follows:

Causal mechanisms are independent of each other in the sense that a change in one mechanism

\(P(X_{i}\mid\mathbf{PA}_{i})\) does not inform or influence any of the other mechanisms \(P(X_{j}\mid\mathbf{PA}_{j})\), for \(i\neq j\).

This notion of invariant, independent, and autonomous mechanisms has appeared in many forms throughout the history of causality research: from early work led by Haavelmo (1944) and Aldrich (1989) to Pearl (2009). Studying properties of _independent causal mechanisms_ rigorously demands a statistical understanding of what such independence means between distributions. Guo et al. (2023) provides a statistical formalization of _ICM_ in exchangeable data, thus providing necessary tools to study causal framework in exchangeable data. The next section introduces relevant background.

### The Causal Framework in Exchangeable Data

**Definition 1** (Exchangeable Sequence): _An exchangeable sequence of random variables is a finite or infinite sequence \(X_{1},X_{2},X_{3},\ldots\) such that for any finite permutation \(\sigma\) of the position indices \(\{1,\ldots,N\}\), the joint distribution of the permuted sequences remains unchanged to that of original:_","P(X_{\sigma(1)},\ldots,X_{\sigma(N)})=P(X_{1},\ldots,X_{N})",,"P(X_{1}, X_{2}, \ldots, X_{N}) = P(X_{\sigma(1)}, X_{\sigma(2)}, \ldots, X_{\sigma(N)})).","This equation defines exchangeability, where the joint distribution of a sequence remains unchanged under any finite permutation of its elements."
neurips_2024_oral_12,4,"Note an _independent and identically distributed_ (i.i.d.) sequence of random variables is an exchangeable sequence, i.e., \(P(X_{1},\ldots,X_{N})=\prod_{i=1}^{N}P(X_{i})\), but not all exchangeable sequences are i.i.d. Examples of exchangeable non-i.i.d. sequences include but are not limited to Polya urn model (Hoppe, 1984), Chinese restaurant processes (Aldous et al., 1985) and Dirichlet processes (Ferguson, 1973).

An important result of exchangeable data are de Finetti's theorems (de Finetti, 1931) which show any exchangeable sequence can be represented as a mixture of conditionally i.i.d. data. Building upon the work of de Finetti (1931), Guo et al. (2023) observes that exchangeable but not i.i.d. data possess extra conditional independence relationships compared to i.i.d. data. This enables:* unique causal structure identification (Theorem 5 in [14]), and
* statistical formalization of ICM principle (causal de Finetti theorems in [14])

In contrast to previous work's focus on structure identification, this work studies causal effect identification and estimation in exchangeable data. Markovian models here mean there are no unmeasured confounders for each tuple of random variables observed in an exchangeable sequence.

To start with, we introduce the necessary terminologies inherited from Guo et al. [2023a].

**Definition 2** (ICM generative process): _We say data is generated from an ICM generative process with respect to a DAG \(\mathcal{G}\) if the sequence of random variable arrays \(\{\mathbf{X}_{::n}\}\) is infinitely exchangeable and satisfies \(X_{i;[n]}\perp\overline{\boldsymbol{N}}\boldsymbol{D}_{i;[n]},\boldsymbol{N }\boldsymbol{D}_{i;n+1}|\boldsymbol{P}\boldsymbol{A}_{i;[n]}\) for all \(i\in[d]\) and \(n\in\mathbb{N}\), where \(\boldsymbol{P}\boldsymbol{A}_{i}\) denotes parents of node \(X_{i}\), \(\boldsymbol{N}\boldsymbol{D}_{i}\) denotes the corresponding non-descendants and \(\overline{\boldsymbol{N}}\boldsymbol{D}_{i}\) denotes the set of non-descendants excluding its own parents. By causal de Finetti theorems, it is equivalent to say the joint distribution of the sequence can be represented as:_","P(\mathbf{X}_{::[N]}=\mathbf{x}_{:,[N]})=\int\int\prod_{n=1}^{N}\prod_{i=1}^{ d}p(x_{i;n}\mid\boldsymbol{a}_{i;n}^{\mathcal{G}},\boldsymbol{\theta}_{i})d \nu_{1}(\boldsymbol{\theta}_{1})\ldots d\nu_{d}(\boldsymbol{\theta}_{d}),",,"P(\mathbf{X}_{::\infty}) = \int \prod_{n=1}^{\infty} P(\mathbf{X}_{:n} \mid \boldsymbol{\theta}) dP(\boldsymbol{\theta}),","where $\boldsymbol{\theta}$ represents the latent parameter governing the conditional distribution of each variable given its parents in the DAG $\mathcal{G}$, and $P(\boldsymbol{\theta})$ is the prior distribution over these parameters."
neurips_2024_oral_12,5,"_where \(\nu_{i}\) are probability measures._

In this work, we assume that any probability measure has a corresponding density function. Following Definition 2, an immediate result is any distribution \(P\) generated by an ICM generative process is Markov to ICM(\(\mathcal{G}\)). Definition 7 in Appendix A defines formally what we mean by an ICM operator on a DAG \(\mathcal{G}\). Here we illustrate an example of ICM(\(\mathcal{G}\)) and the use of our notation. Consider a DAG \(\mathcal{G}:=X_{1}\gets X_{2}\to X_{3}\) with \(3\) variable indices. Two data tuples are generated under an ICM generative process with respect to \(\mathcal{G}\). Fig. 2 shows the Markov structure compatible with the above ICM generative process.

## 3 Causal Effect in Exchangeable Data

We first motivate our study of causal effects in exchangeable data by noting exogenous variables are different from causal de Finetti parameters. Traditional causal effect in i.i.d. processes is defined as

Figure 3: An illustration of differences in what the do-operator does between a structural causal model (a) and an ICM generative process (b). In the observational phase, SCMs (a), where the dotted plate indicates i.i.d. sampled, illustrates that fixed assignment of \(U_{X}\) and \(U_{Y}\) leads to fixed observable values \(X\) and \(Y\); on the other hand, for ICM-generative processes where _exch._ is an abbreviation for an exchangeable process, fixing \(\theta,\psi\) does not fix \(X\) and \(Y\), instead, it means sampling from a fixed distribution. Because SCM fails to characterize ICM-generative processes, we define the operational meaning of do-interventions on ICM-generative processes as assigning \(\delta\)-distribution to the intervened variables and substituting the corresponding values in the remaining distributions.

Figure 2: An example of ICM(\(\mathcal{G}\)): Two data tuples are generated by an ICM generative process with respect to \(\mathcal{G}:=X_{1}\gets X_{2}\to X_{3}\), where \(X_{i;n}\) is the \(i\)-th variable in the \(n\)-th position and gray means latent variables.

graph surgery with respect to SCMs (cf. Section 2.1). Such a definition does not transfer its operational meaning to that in exchangeable processes because SCMs fail to characterize ICM generative processes. In this section, we first define what causal effect means and show their differences in properties compared to that of i.i.d. processes. We proceed to illustrate in the simplest possible case - a pair of random variables, and then provide a general statement for the multivariate case.

Figure 3 illustrates how SCMs fail to characterize ICM generative processes in the bivariate example. In SCMs, a fixed assignment of values to exogenous variables \(U_{X}\) and \(U_{Y}\) uniquely determines the values of all the observable variables \(X\) and \(Y\). This is not the case for ICM generative processes. A fixed assignment to causal de Finetti parameters \(\theta\) and \(\psi\) only restricts the observable variables \(X\) and \(Y\) to sample from fixed distributions but does not uniquely determine their values. Therefore, do-operator commonly defined as graph surgery in SCMs demands a new operational meaning in ICM generative processes. Definition 3 defines that \(do(X=x)\) in ICM generative processes means assigning the sampling density \(p(x|\theta)\) to \(\delta(X=x)\).2. Doing so, we have:

Footnote 2: [https://encyclopediaofmath.org/index.php?title=Delta-function](https://encyclopediaofmath.org/index.php?title=Delta-function)

**i.i.d. generative process** :","P(Y=y|do(X=x))=p(y|x,\psi_{0})=P(Y=y|x),\psi_{0}\in\mathbb{R}",,"P(X_{i;n}|\boldsymbol{a}_{i;n}^{\mathcal{G}},\boldsymbol{\theta}_{i})= \delta(X_{i;n}=x_{i;n})\quad\text{if }X_{i;n}\in\mathbf{X},",where δ denotes the Dirac delta function enforcing deterministic assignment when variable Xᵢₙ is intervened.
neurips_2024_oral_12,6,"(5)
**ICM generative process** :","P(Y=y|do(X=x))=\int p(y|x,\psi)p(\psi)d\psi=P(Y=y|x)",,"P(Y=y|do(X=x))=\int p(y|x,\psi)p(x|\theta)\delta(x) d\psi d\theta","Equation 6 defines the causal effect in an ICM generative process, where the do-operator assigns a δ-distribution to the intervened variable X=x, and the causal effect integrates over the conditional distributions of Y given X and the fixed parameters."
neurips_2024_oral_12,7,"(6)

Despite being identical in expression, causal effect has different implications in i.i.d. and ICM generative processes. Under i.i.d., the randomness captured in \(P\) is driven only by the randomness in exogenous variables \(U_{Y}\). Under the ICM generative process, the randomness in \(P\) is driven both by \(p(y|x,\psi)\) and the randomness in the causal de Finetti parameter \(p(\psi)\). It is well-known that i.i.d is a subcase of exchangeability. Here we observe that the causal effect expression follows the same pattern, i.e., the causal effect expression in i.i.d. is also a subcase of that under exchangeability whenever \(p(\psi)=\delta(\psi=\psi_{0})\).

Equipped with this operational meaning of intervention, we next consider the set of feasible intervention targets. Here we first clarify what we mean by the data-generating process. Data generated from i.i.d. or ICM processes refers to a sequence of random variables. For example, in the bivariate case, the sequence is \((X_{1},Y_{1}),(X_{2},Y_{2}),\ldots\). Often one omits position indices in i.i.d. because:","\textbf{i.i.d. generative process}:P(X_{1},Y_{1},\ldots,X_{N},Y_{N}) \stackrel{{ ind}}{{=}}\prod_{n=1}^{N}P(X_{n},Y_{n})\stackrel{{ idc}}{{=}}[P(X,Y)]^{N}",,"P(Y=y|do(X=x))=\int p(y|x,\psi)p(\psi)d\psi=P(Y=y|x)","The causal effect in an ICM generative process integrates over both the conditional distribution and the distribution of the causal de Finetti parameter, generalizing the i.i.d. case."
neurips_2024_oral_12,8,"The first equality is due to independence and the second equality is due to identical distributions. One thus does not differentiate the position indices in i.i.d. as \(P(X,Y)\) characterizes the joint distribution fully by Eq. (7). Intervention is thus defined only on \(X\) and \(Y\) rather than \(X_{n}\) and \(Y_{n}\). However, in the ICM generative process, \(P(X,Y)\) cannot fully characterize the joint distribution. An example application of Definition 2 in the bivariate case gives:","\textbf{ICM gen. process}:P(x_{1},y_{1},\ldots,x_{N},y_{N})=\int\int\prod_{n=1} ^{N}p(y_{n}|x_{n},\psi)p(x_{n}|\theta)d\mu(\theta)d\nu(\psi)",,"\textbf{ICM generative process}: P(X_{1},Y_{1},\ldots,X_{N},Y_{N})=\int\int\prod_{n=1}^{N}p(X_{n}|\theta)p(Y_{n}|X_{n},\psi)d\nu(\theta)d\nu(\psi)","Equation 8 illustrates that in an ICM generative process, the joint distribution of the sequence cannot be fully characterized by a single i.i.d. distribution, as it involves integration over causal de Finetti parameters $\theta$ and $\psi$, reflecting the exchangeable but non-i.i.d. nature of the data."
neurips_2024_oral_12,9,"Eq. (8) shows that one can no longer omit the position indices in ICM generative processes, as \(P(X,Y)\) cannot fully characterize the joint distribution. Intervention in ICM generative processes thus should be considered at the level of \(X_{n}\) and \(Y_{n}\). Definition 3, presented below, formalizes the concept of causal effect in ICM generative processes. In particular, both intervention sets and target variables of interest can be any random variables observed in the sequence.

**Definition 3** (Causal Effect in ICM generative processes): _Let \(\mathbf{X}\) and \(\mathbf{Y}\) be two disjoint sets of variables generated by an ICM-generative process. For each realization \(\mathbf{x}\) of \(\mathbf{X}\), \(P(\mathbf{y}|do(\mathbf{x}))\) in the ICM-generative process denotes the probability of \(\mathbf{Y}=\mathbf{y}\) induced by assigning \(p(x_{i;n}|\boldsymbol{\rho}a_{i;n}^{\mathcal{G}},\theta_{i})=\delta(X_{i;n}=x_ {i;n})\) in Eq. (4), \(\forall X_{i;n}\in\mathbf{X}\), and substituting \(\mathbf{X}=\mathbf{x}\) in the remaining conditional distributions._

Although marginal distribution \(P(X,Y)\) cannot characterize the joint distribution in Eq. (8) due to mutual dependence of \(X_{n},Y_{n}\) with causal de Finetti parameters \(\theta\) and \(\psi\), we note variables in different positions share identical marginal distributions. Mathematically, data generated under an exchangeable process shares identical marginal distributions: \(P(X_{n},Y_{n})=P(X_{m},Y_{m}),\forall n\neq m\). This also means that identical interventions performed on variables in different positions result in the same post-interventional distributions: \(P(Y_{n}|do(X_{n}=x))=P(Y_{m}|do(X_{m}=x)),\forall n\neq m\). Similar to i.i.d., quantities such as \(P(Y|do(X=x))\) are thus well-defined in ICM generative processes. Corollary 1 proves the multivariate equivalent of identical marginal intervention effects in ICM generative processes.

**Corollary 1** (Identical marginal post-interventional distributions): _Let \(P\) be the distribution for some ICM generative process. Let \(\mathbf{I}\) and \(\mathbf{J}\) be two disjoint subsets in \([d]:=\{1,\ldots,d\}\). Denote \(\mathbf{X}_{\mathbf{I};n}:=\{X_{i:n}:i\in\mathbf{I}\}\) and similarly for \(\mathbf{X}_{\mathbf{J};n}\). Then,_","P(\mathbf{X}_{J;n}\mid do(\mathbf{X}_{I;n}=\mathbf{x}))=P(\mathbf{X}_{J;m}\mid do (\mathbf{X}_{I;m}=\mathbf{x})),\forall n\neq m",,"P(\mathbf{X}_{\mathbf{J};n}|do(\mathbf{X}_{\mathbf{I};n}=\mathbf{x}_{\mathbf{I};n}))=P(\mathbf{X}_{\mathbf{J};m}|do(\mathbf{X}_{\mathbf{I};m}=\mathbf{x}_{\mathbf{I};m})),\quad\forall n\neq m.","Corollary 1 establishes that in an ICM generative process, identical interventions applied at different positions yield the same post-interventional distribution, generalizing the i.i.d. invariance property to exchangeable data."
neurips_2024_oral_12,10,"_i.e., identical interventions on variables in different positions share the same marginal post-interventional distributions. See Appendix B for the proof._

One can thus omit the notation of position indices when appropriate as supported by Corollary 1 and as illustrated in Figure 1 Block \(\mathbf{A}\). In the next section, we address a focal problem in causal effect estimation: causal effect identifiability in ICM generative processes.

### Causal Effect Identifiability in ICM generative processes

Continuing with the bivariate example in Fig. 3 (b), consider \(X_{1},Y_{1},X_{2},Y_{2}\) is generated under an ICM generative process with respect to \(\mathcal{G}:=X\to Y\). Suppose one performs hard intervention on \(X_{1}\), i.e., \(\text{do}(X_{1}=\hat{x})\). Applying traditional truncated factorization developed for i.i.d. data (Eq. (2)) yields:","P(x_{1},y_{1},x_{2},y_{2}|do(X_{1}=\hat{x}))=P(y_{1}|\hat{x})P(y_{2}|x_{2})P(x _{2})\mathbb{1}_{x_{1}=\hat{x}}",,"P(Y_{1},X_{2},Y_{2}|do(X_{1}=\hat{x}))=P(Y_{1}|X_{1}=\hat{x})P(X_{2})P(Y_{2}|X_{2})",Truncated factorization applied to post-interventional distribution in an ICM generative process with respect to $\mathcal{G}:=X \to Y$ under hard intervention $\text{do}(X_{1}=\hat{x})$.
neurips_2024_oral_12,11,"However, the independence between \((X_{1},Y_{1})\) and \((X_{2},Y_{2})\) in i.i.d. generative processes does not hold in ICM generative processes. In fact, in ICM generative processes, applying Definition 3 gives:","P(x_{1},y_{1},x_{2},y_{2}|do(X_{1}=\hat{x}))=\int p(y_{1}|\hat{x},\psi)p(y_{2 }|x_{2},\psi)p(\psi)d\psi p(x_{2})\mathbb{1}_{x_{1}=\hat{x}}",,"P(x_{1},y_{1},x_{2},y_{2}|do(X_{1}=\hat{x}))=P(y_{1}|\hat{x})P(x_{2})P(y_{2}|x_{2})\mathbb{1}_{x_{1}=\hat{x}}\int p(\theta|x_{2})d\theta",Post-interventional distribution in ICM generative processes accounting for dependence between tuples through shared causal de Finetti parameters.
neurips_2024_oral_12,12,"Eq. (11) does not equal to Eq. (10) whenever \(p(\psi)\neq\delta(\psi=\psi_{0})\) for some \(\psi_{0}\). Thus, for data generated under ICM generative processes, a new theorem for truncated factorization is required. See Theorem 1 for the statement for general multivariate distributions and Appendix C for a detailed derivation.

**Theorem 1** (Truncated Factorization in ICM generative processes): _For a given graph \(\mathcal{G}\), let \(P\) be the probability distribution for data generated under an ICM generative process with respect to \(\mathcal{G}\) and let \(p\) be the corresponding density. The post-interventional distribution after intervening on \(\mathbf{X}=\hat{\mathbf{x}}\) has density given by:_","p(\mathbf{x}_{:,1},\ldots,\mathbf{x}_{:,N}|do(\mathbf{X}=\hat{\mathbf{x}}))= \prod_{i\in I_{\mathbf{X}}}p(\mathbf{x}_{i;[-\mathbf{N}_{i}]}|\boldsymbol{p} \boldsymbol{a}_{i;[-\mathbf{N}_{i}]}^{\mathcal{G}})\prod_{i\not\in I_{\mathbf{ X}}}p(\mathbf{x}_{i;[N]}|\boldsymbol{p}\boldsymbol{a}_{i;[N]}^{ \mathcal{G}})\big{|}_{\mathbf{X}=\hat{\mathbf{x}}},",,"p(\mathbf{x} \mid do(\mathbf{X} = \hat{\mathbf{x}})) = \prod_{i: X_i \notin \mathbf{X}} \int p(x_i \mid \mathbf{pa}_i, \boldsymbol{\theta}_i) p(\boldsymbol{\theta}_i) d\boldsymbol{\theta}_i \Big|_{\mathbf{X} = \hat{\mathbf{x}}}",The truncated factorization for ICM generative processes integrates out the causal de Finetti parameters for non-intervened variables and enforces consistency with the intervention values.
neurips_2024_oral_12,13,"_where \(\mathbf{I}_{\mathbf{X}}:=\{i:X_{i;n}\in\mathbf{X}\}\) denotes the set of variable indices being intervened on and \(\mathbf{N}_{i}:=\{n:X_{i;n}\in\mathbf{X}\}\) denotes the set of position indices corresponding to variable index \(i\) in the intervention set \(\mathbf{X}\) and \([\neg\mathbf{N}_{i}]\) denotes the set of positive integers less than or equal to \(N\) excluding values in \(\mathbf{N}_{i}\)._

Theorem 1 presents a procedure for computing the joint post-interventional distribution using pre-interventional conditional distributions when intervening on any set of variables in the Markovian model. This demonstrates that causal effects are identifiable in Markovian models under ICM generative processes. Note that the traditional truncated factorization in i.i.d. processes is again a special case of Eq. (12) just as i.i.d. processes are a special case of exchangeable processes.

### Conditional Interventional distributions

The fundamental difference between i.i.d. and ICM generative processes lies in the violation of the independence condition inherent to i.i.d.. Consequently, interventional distributions computed by conditioning on observations differ between the two. Specifically, causal effect of \(do(X_{1}=\hat{x})\) on given \(X_{2},Y_{2}\) differs when computed under i.i.d. (Eq. (13)) and ICM generative processes (Eq. (14)). This difference arises because \((X_{1},Y_{1})\perp\!\!\!\perp(X_{2},Y_{2})\) holds in i.i.d but not in ICM generative processes.

**i.i.d. generative processes** :\(P(Y_{1}|do(X_{1}=\hat{x}),X_{2},Y_{2})=P(Y_{1}|\hat{x})\) (13)

**ICM generative processes** :\(P(Y_{1}|do(X_{1}=\hat{x}),X_{2},Y_{2})=P(Y_{1}|\hat{x},X_{2},Y_{2})\) (14)

Fig. 1 (c) depicts this example of intervention via graph surgery on ICM(\(\mathcal{G}\)) where the operational meaning of edge deletion is explained in Figure 3. We observe that conditioning on the collider node \(Y_{2}\) in the ICM generative process renders \(Y_{1}\perp\!\!\!\perp X_{2}|Y_{2}\). Lemma 1 provides the corresponding general statement for the multivariate case when conditioning on other observations. See Appendix D for the proof.

**Lemma 1** (Intervention effect conditioned on other observations): _For a given graph \(\mathcal{G}\), let \(P\) be the distribution for the ICM generative process with respect to \(\mathcal{G}\). Let \(\mathbf{X}\) be the intervention set. Assume \(\mathbf{X}=\mathbf{X}_{\mathbf{I};n}:=\{X_{i;n}:\forall i\in\mathbf{I}\}\). Let \(\mathbf{S}\subseteq[N]\) such that \(n\not\in\mathbf{S}\) and \([\neg\mathbf{I}]\) denotes \([d]\backslash\mathbf{I}\). Then,_","P(\mathbf{X}_{\neg\mathbf{I};n}|do(\mathbf{X}_{\mathbf{I};n}=\hat{\mathbf{x}} ),\mathbf{X}_{\neg\mathbf{S}})=\prod_{i\not\in\mathbf{I}}P(\mathbf{X}_{i;n}| \mathbf{X}_{i;\mathbf{S}},\boldsymbol{PA}_{i;\mathbf{S}\cup\{n\}})|_{\mathbf{ X}_{\mathbf{I};n}=\hat{\mathbf{x}}}",,"P(\mathbf{X}_{[\neg\mathbf{I}];n}\mid do(\mathbf{X}_{\mathbf{I};n}=\mathbf{x}),\mathbf{X}_{:;\mathbf{S}})=P(\mathbf{X}_{[\neg\mathbf{I}];n}\mid do(\mathbf{X}_{\mathbf{I};n}=\mathbf{x}),\mathbf{X}_{:;\mathbf{S}})","Lemma 1 establishes that in ICM generative processes, the post-interventional distribution of non-intervened variables at a given position, conditioned on observations at other positions, remains invariant under interventions on variables at that position."
neurips_2024_oral_12,14,"A similar argument also applies to the intervention effect conditioned on observations of experimental results performed on other tuples of random variables in the sequence. Appendix E discusses in detail. Here we show the implications of conditional interventional distributions via a causal Polya urn model.

**Causal Polya Urn Model** Imagine an urn with left and right compartments. The experimenter puts \(\alpha\) white balls and \(\beta\) black balls in each compartment. At each step \(n\), one ball is uniformly drawn from the left and one ball is uniformly drawn from the right. The chosen two balls in the order of left and right are then placed in a dark chamber unobserved by the experimenter. A hidden mechanism reads the color of the two balls and outputs \(X_{n},Y_{n}\) to the experimenter. The mechanism outputs \(X_{n}=1\) whenever the \(n\)-th left ball is black else \(X_{n}=0\) and \(Y_{n}=1\) whenever the left and right balls disagree in color. After observing \(X_{n}\) and \(Y_{n}\), the experimenter puts the original balls back in the corresponding compartment and add a ball of the same color as \(X_{n}\) in the left and add a ball of the same color as \(Z_{n}:=(1-X_{n})*Y_{n}+(1-Y_{n})*X_{n}\) in the right.

**Causal de Finetti application** The causal Polya urn model is a real-world illustration of causal de Finetti thereom in its bivariate form. The joint distribution of observed sequence \(P(x_{1},y_{1},\ldots,x_{n},y_{n})\) for all \(n\in\mathbb{N}\) can be perfectly modelled as the RHS of Eq. (4) with two variables and \(X\to Y\):","\int\int\prod_{n}p(y_{n}\mid x_{n},\psi)p(x_{n}\mid\theta)p(\theta)p(\psi)d \theta d\psi,","where \(p(\theta),p(\psi)\) are Beta distributions and \(p(y_{n}\mid x_{n},\psi),p(x_{n}\mid\theta)\) are Bernoulli distributions","P(x_{1},y_{1},\ldots,x_{N},y_{N})=\int_{0}^{1}\int_{0}^{1}\prod_{n=1}^{N}p(y_{n}|x_{n},\psi)p(x_{n}|\theta)d\theta d\psi","The joint distribution of the observed sequence in the causal Polya urn model is expressed using the bivariate form of the causal de Finetti theorem, where each variable pair $(x_n, y_n)$ is conditionally independent given the causal de Finetti parameters $\theta$ and $\psi$, and the product structure reflects the ICM generative process with $X \to Y$."
neurips_2024_oral_13,1,"### Preliminaries

As the common modules within each transformer block of LLMs, both Multi-head Self-Attention (MSA) and Feed-Forward Network (FFN) fundamentally consist of basic linear layers, which can be represented as, \(\mathbf{Y}=\mathbf{X}\cdot\mathbf{W}\in\mathbb{R}^{T\times C_{out}}\). Here, \(\mathbf{X}\in\mathbb{R}^{T\times C_{in}}\) is the activation input and \(\mathbf{W}\in\mathbb{R}^{C_{in}\times C_{out}}\) denotes the weight matrix. In this paper, we focus on integer uniform quantization [25] of both activation and weight, aiming to achieve better hardware support. Specifically, the \(b\)-bit quantization process maps the FP16 tensor \(\mathbf{X}\) to low-bit integer \(\mathbf{X}_{q}\):","\mathbf{X}_{q}=\text{clamp}(\,\lfloor\frac{\mathbf{X}}{\Delta} \rceil\!+\!z,0,2^{b}-1),\text{where }\Delta=\frac{\max(\mathbf{X})-\min(\mathbf{X})}{2^{b}-1},z=-\lfloor \frac{\min(\mathbf{X})}{\Delta}\rfloor.",,"\begin{equation}
\mathbf{X}_q = \operatorname{clamp}\left( \left\lfloor \frac{\mathbf{X}}{s} \right\rceil + z,\, 0,\, 2^b - 1 \right)
\end{equation}","Equation 1 defines the $b$-bit uniform quantization operator with scale $s$, zero-point $z$, and clamping to the integer range $[0, 2^b - 1]$."
neurips_2024_oral_13,2,"The notation \(\lfloor\cdot\rceil\) means the nearest rounding operation, \(\Delta\) is the quantization step size and \(z\) represents the zero point. Following [64; 47; 34; 39], we employ per-token quantization for activation and per-channel quantization for weight, which entails assigning different step sizes to individual tokens of activations (\(\Delta_{\mathbf{X}}\in\mathbb{R}^{T\times 1}\)) and different output channels of weights (\(\Delta_{\mathbf{W}}\in\mathbb{R}^{1\times C_{out}}\)).

### The proposed DuQuant Method

To address the Normal Outliers issue stated in Section 2, current quantization methods, such as SmoothQuant [64] and OmniQuant [64], usually adopt the smooth technique. Concretely, it involves the utilization of a per-channel smoothing diagonal matrix, denoted as \(\mathbf{\Lambda}\), to scale the input activation and weight matrix. The adjustment allows us to rewrite the original linear layer as \(\mathbf{\Lambda}^{-1})(\mathbf{\Lambda}\cdot\mathbf{W})\). The diagonal element \(\mathbf{\Lambda}_{j}\) within \(\mathbf{\Lambda}\) is computed as \(\mathbf{\Lambda}_{j}=\text{max}(|\mathbf{X}_{j}|)^{\alpha}/\text{max}(|\mathbf{ W}_{j}|)^{1-\alpha}\), where \(\alpha\) is a hyper-parameter representing the migration strength. However, despite the ability of this smoothing technique to shift the quantization challenge from activations to weights, it still faces difficulties in effectively managing Massive Outliers, as depicted in Figure 1. This challenge stems from the extremely large massive outliers inducing large scaling factors \(\mathbf{\Lambda}_{j}\), which in turn introduce new outliers in the weight matrix and result in significant performance declines in 4-bit quantization.

According to these findings, we propose the DuQuant method, which includes the Rotation and Permutation transformations based on the smooth technique. By combining rotation transformation and channel permutation, our DuQuant method aims to redistribute these features within the activation space, thereby mitigating the effects of both Normal and Massive Outliers.

The Rotation Transformation.In contrast to the smooth technique, our aim is to apply a rotation matrix for row or column transformations, mitigating the impact of both Normal and Massive outliers. The ideal rotation matrix, denoted as \(\mathbf{R}\), should possess the following properties: 1) \(\mathbf{R}\) is an orthogonal matrix satisfying \(\mathbf{R}\mathbf{R}^{\top}=\mathbf{I}\) and \(|\mathbf{R}|=\pm 1\). This allows us to reformulate the linear layer within the transformer as \(\mathbf{Y}=\mathbf{X}\cdot\mathbf{W}=(\mathbf{X}\mathbf{R})(\mathbf{R}^{\top} \mathbf{W})\); 2) \(\mathbf{R}\) should be capable of effectively target the positions of outliers and effectively mitigating them through matrix multiplication. However, due to the Massive Outliers are usually randomly distributed within the activation space, it is challenging to directly identify the optimal rotation matrix \(\mathbf{R}\) capable of mitigating outliers through a single rotation transformation. To address this problem, we employ a greedy search with prior knowledge to compute a rotation matrix \(\hat{\mathbf{R}}\), thereby approximating the ideal rotation matrix \(\mathbf{R}\). Specifically, the calculation of \(\hat{\mathbf{R}}\) involves the following steps,

* Identify the feature dimension \(d^{(1)}\) where the outlier are primarily concentrated, i.e., \(d^{(1)}=\arg\max_{j}(\max_{i}|\mathbf{X}_{ij}|)\). Here, \(\mathbf{X}_{ij}\) represents the element in the \(i\)-th row and \(j\)-th column of \(\mathbf{X}\).
* Based on the searched dimensions \(d^{(1)}\), we construct the rotation matrix as follows,","\mathbf{R}^{\mathbf{1}}=\mathbf{E}_{d^{(1)}}\hat{\mathbf{R}}\mathbf{Q} \mathbf{E}_{d^{(1)}},\qquad\mathbf{Q}=\begin{bmatrix}1&\mathbf{Q}\\ \mathbf{O}&\mathbf{Q}^{\prime}\end{bmatrix}\!.",,"\hat{\mathbf{R}} = \mathbf{I} - 2\mathbf{v}\mathbf{v}^{\top}, \text{ where } \mathbf{v} = \frac{\mathbf{e}_{d^{(1)}} - \text{sign}(\mathbf{X}_{d^{(1)}})\|\mathbf{X}_{d^{(1)}}\|_2}{\|\mathbf{e}_{d^{(1)}} - \text{sign}(\mathbf{X}_{d^{(1)}})\|\mathbf{X}_{d^{(1)}}\|_2\|_2},","where $\mathbf{e}_{d^{(1)}}$ is the standard basis vector with 1 at the $d^{(1)}$-th position and 0 elsewhere, and $\mathbf{v}$ is the normalized reflection vector used to construct the Householder transformation matrix $\hat{\mathbf{R}}$."
neurips_2024_oral_13,3,"(2) Here, \(\mathbf{E}_{d^{(1)}}\) is the switching matrix used to swap the first and the \(d^{(1)}\)-th columns of the activation, and \(\tilde{\mathbf{R}}\) represents an orthogonal initialized rotation matrix, in which the first row is specifically uniformly distributed. The motivation behind this is to mitigate outliers in the first column after the transformation by \(\mathbf{E}_{d^{(1)}}\). To further increase the randomness of the rotation operation,

Figure 2: Transformation Steps for Activation Matrices after smooth technique. (a) Sequential transformations on Normal Outliers: 1 initial rotation to reduce outliers within blocks, 2 permutation to evenly distribute outliers across blocks, and 2 a second rotation for further smoothing. (b) Activation changes for Massive Outliers before and after DuQuant. (c) A sample matrix for highlighting the continual reduction of outliers through rotation and permutation, with outliers marked in dark blue.

we retain the first column, where outliers have been mitigated, and randomly rotate the other columns by multiplying them with a random orthogonal matrix \(\mathbf{Q}^{\prime}\).
* Let \(N\) denote the greedy search steps, then the approximated rotation matrix \(\hat{\mathbf{R}}=\mathbf{R}^{1}\mathbf{R}^{2}\cdots\mathbf{R}^{n}\), where \(n=\operatorname*{arg\,min}_{k\in[1:N]}\big{(}\max_{i,j}|(\mathbf{XR}^{1}\cdots \mathbf{R}^{k})_{ij}|\big{)}\). Each \(\mathbf{R}^{i}\) is constructed according to Eqn. (2) and the identified feature dimension \(d^{(i)}\). Appendix G provides detailed pseudo code.

Through this construction manner, we can ensure that the approximated optimal rotation matrix \(\hat{\mathbf{R}}\) can effectively mitigate outliers with large magnitudes, as opposed to merely using a randomly selected orthogonal rotation matrix. Nevertheless, directly constructing the entire rotation matrix is time-consuming and results in substantial memory overhead. For fast matrix multiplication, following [63], we approximate the rotation matrix \(\hat{\mathbf{R}}\in\mathbb{R}^{C_{in}\times C_{in}}\) in a block-wise manner,","\hat{\mathbf{R}}=\text{ BlockDiag}(\hat{\mathbf{R}}_{b_{1}},...,\hat{\mathbf{R}}_{b_{K}}),","where \(\hat{\mathbf{R}}_{b_{i}}\in\mathbb{R}^{2^{n}\times 2^{n}}\) denotes a square matrix of the \(i\)-th block, which is constructed following the three steps mentioned above","\hat{\mathbf{R}}=\operatorname{BlockDiag}(\mathbf{R}_{1},\mathbf{R}_{2},\ldots,\mathbf{R}_{B})\in\mathbb{R}^{C_{in}\times C_{in}},","where the rotation matrix is approximated as a block-diagonal matrix composed of smaller rotation blocks $\mathbf{R}_{b}\in\mathbb{R}^{\frac{C_{in}}{B}\times\frac{C_{in}}{B}}$, enabling efficient computation and memory usage."
neurips_2024_oral_13,4,"ve. And the block numbers \(K\) is calculated by \(K=C_{in}/2^{n}\).

The Permutation Transformation.Despite adopting the block-diagonal rotation matrix \(\hat{\mathbf{R}}\) for its time and storage efficiency, its focus on local information introduces a potential limitation in further reducing the outliers. This is because the rotation transformation, conducted within each small block, cannot integrate the information across different blocks to further minimize outliers. Consequently, one block may have relatively larger outliers while another block has smaller outliers, resulting in high variance among different blocks, as shown in Figure 2. This limitation explains that merely utilizing the block-diagonal rotation matrix is insufficient to effectively reduce the overall outliers.

To effectively mitigate the overall outliers, it is essential to balance the outliers' magnitudes among various blocks. Specifically, within each small block, we denote the largest outlier in dimension \(d_{j}\) as \(O_{j}\). Meanwhile, \(M_{b_{i}}\) represents the mean value of all \(O_{j}\) in the \(i\)-th block, where \(i=1,2,...,K\). Then the variance in activation magnitudes across various blocks can be expressed as,","\text{Var}([M_{b_{1}},M_{b_{2}},...,M_{b_{K}}]).",,"\text{Var}_{\text{block}} = \frac{1}{K} \sum_{i=1}^{K} (M_{b_i} - \bar{M})^2,","where $\bar{M}$ denotes the global mean of all block means $M_{b_i}$, and $\text{Var}_{\text{block}}$ quantifies the variance in outlier magnitudes across different blocks."
neurips_2024_oral_13,5,"To minimize this variance and further reduce the overall outliers, we introduce the **zigzag permutation**. Concretely, we generate a zigzag sequence that starts by assigning channels with the highest activations to the first block. The process continues by assigning channels with the next highest activations to the subsequent blocks in descending order until the end of block \(K\). Upon reaching the final block, the order reverses, starting from the channel with the next highest activation and proceeding in ascending order. This back-and-forth patterning continues throughout all the blocks, ensuring that no single block consistently receives either the highest or lowest activation channels. It is worth noting that the constructed permutation is an orthogonal matrix, which we denote as \(\mathbf{P}\), satisfying the conditions \(\mathbf{P}\mathbf{P}^{\top}=\mathbf{I}\) and \(|\mathbf{P}|=\pm 1\). By employing the zigzag permutation, we achieve a balanced distribution of outliers across different blocks. This allows us to use an additional rotation transformation to further smooth the outliers. Figure 2 provides an illustration of outlier mitigation.

The Overall DuQuant Method.To effectively mitigate both Normal and Massive Outliers, we first employ the smooth technique to shift the quantization challenge from activations to weights. Next, we introduce the block-diagonal rotation matrix \(\hat{\mathbf{R}}\) to locally redistribute feature outliers within the activation space. We then propose the zigzag permutation matrix for globally balancing the outliers across different blocks, followed by another application of the block-diagonal rotation transformation. To sum up, the linear layers within the transformer can be rewrite as,","\mathbf{Y}=\mathbf{X}\cdot\mathbf{W}=[(\mathbf{X}\cdot\underbrace{\mathbf{A}^ {-1}}_{\mathcal{G}})\hat{\mathbf{R}}_{(1)}\cdot\mathbf{P}\cdot\hat{\mathbf{R} }_{(2)}]\cdot[\underbrace{\hat{\mathbf{R}}_{(2)}^{\top}\cdot\mathbf{P}^{\top} \cdot\hat{\mathbf{R}}_{(1)}^{\top}(\mathbf{A}}_{\mathcal{G}^{-1}}\cdot \mathbf{W})],","where the notation \(\mathbf{P}\) denotes the orthogonal permutation matrix learned via the zigzag manner, the \(\hat{\mathbf{R}}_{(1)}\) and \(\hat{\mathbf{R}}_{(2)}\) represent the first and second block-diagonal rotation matrix, respectively","\mathbf{Y} = \mathbf{X} \cdot \mathbf{W} = (\mathbf{X} \hat{\mathbf{R}} \mathbf{P} \hat{\mathbf{R}}') \cdot (\hat{\mathbf{R}}'^{\top} \mathbf{P}^{\top} \hat{\mathbf{R}}^{\top} \mathbf{W}),","This equation represents the reformulation of the linear layer in the transformer using the DuQuant method, which combines rotation and permutation transformations to mitigate both Normal and Massive Outliers."
neurips_2024_oral_13,6,"ly.

**Remark 1**.: It is worth noting that the proposed DuQuant method can simultaneously smooth the weight matrix. While the commonly adopted smooth technique is effective, it can cause the weight matrix of the down-projection layer to exhibit pronounced outliers, leading to performance degradation. However, in the proposed DuQuant method, the rotation transformation we designed is applied to not only the activation input but also the weight matrix. As a result, the outliers induced by the smooth technique can be mitigated through our approximated rotation matrix \(\hat{\mathbf{R}}\), yielding a smoother, more quantization-friendly weight matrix. Moreover, this approach eliminates the reliance on complex weight quantization techniques, such as GPTQ [18] used in Atom [74] and QuaRot [2].

**Remark 2**.: To further decrease the computation and memory costs, we initially construct the \(k\)-th block rotation matrix \(\hat{\mathbf{R}}_{b_{k}}\), with the \(k\)-th block containing the largest outlier. We then assign \(\hat{\mathbf{R}}_{b_{i}}=\hat{\mathbf{R}}_{b_{k}}\) for all \(1\leq i\leq K\). This strategy not only effectively mitigates the impact of outliers, but also reduces the number of block rotation matrices from \(K\) to 1, significantly reducing computation and memory requirements. Importantly, incorporating the invertible matrix \(\mathbf{G}\) from Eqn. (5) significantly eases the quantization challenges for \(\mathbf{X}\) and \(\mathbf{W}\). Consequently, the quantization process acts as \(\mathbf{Y}=(\mathbf{X}\mathbf{G})(\mathbf{G}^{-1}\mathbf{W})=\hat{\mathbf{X}} \cdot\hat{\mathbf{W}}\approx\Delta_{\hat{\mathbf{X}}}\Delta_{\hat{\mathbf{W} }}(\hat{\mathbf{X}}_{q}-z_{\hat{\mathbf{X}}})(\hat{\mathbf{W}}_{q}-z_{\hat{ \mathbf{W}}})\).

### Theoretical Analysis

To further demonstrate the effectiveness of the proposed DuQuant method, we conduct a theoretical analysis of the rotation and permutation transformations. Theorem 1 shows that within each block, the constructed rotation matrix effectively mitigates the maximum outlier, thereby reducing the outlier magnitude through a greedy search. Theorem 2 reveals that the employed zigzag permutation ensures a balanced upper bound shared among different blocks. This suggests that the zigzag permutation effectively reduces the variance shown in Eqn. (4) and thus assists the rotation matrix in further decreasing the outliers. Please refer to Appendix B for detailed proofs.

**Theorem 1** (Rotation).: _For the activation input \(\mathbf{X}\in\mathbb{R}^{T\times C_{in}}\), \(\hat{\mathbf{R}}\in\mathbb{R}^{2^{n}\times 2^{n}}\) is a diagonal block matrix constructed as per Eqn. (3). For a specific block \(b_{i}\), let \(O_{j}(\cdot)\) represent the maximum outlier of the \(j\)-th dimension \(d_{j}\) within the input. Then, we can deduce that,_",\max_{1\leq j\leq 2^{n}}\ O_{j}(\mathbf{X}_{b_{i}}\hat{\mathbf{R}}_{b_{i}}) \leq\max_{1\leq j\leq 2^{n}}\ O_{j}(\mathbf{X}_{b_{i}}).,,"\max_{j}O_{j}(\mathbf{X}_{b_{i}}\hat{\mathbf{R}}_{b_{i}})\leq\min_{\mathbf{R} \in\mathcal{O}(2^{n})}\max_{j}O_{j}(\mathbf{X}_{b_{i}}\mathbf{R}),",Rotation transformation reduces the maximum outlier magnitude within each block by constructing an approximated optimal orthogonal matrix.
neurips_2024_oral_13,7,"**Theorem 2** (Zigzag Permutation).: _For the activation input \(\mathbf{X}\in\mathbb{R}^{T\times C_{in}}\), it can be divided into \(K\) blocks, where \(K=C_{in}/2^{n}\). Let \(O_{j}\) denote the max outlier of the dimension \(d_{j}\) in \(\mathbf{X}\), the reordered outliers from large to small is expressed as \(O^{(1)},O^{(2)},...,O^{(C_{in})}\). Moreover, the \(M_{b_{i}}\) represents the mean value of all \(O_{j}\) in the \(i\)-th block, \(i=1,2,...,K\). Let \(\delta:=\max\{|O^{(i+1)}-O^{(i)}|\},i=1,2,...,C_{in}-1\). Then, following the zigzag permutation described in Section 3.2, the mean value \(M_{b_{i}}\) within each \(i\)-th block consistently satisfies,_","M_{b_{i}}\leq O^{(1)}+\frac{(2^{n}K-1)(2^{n-1}-1)}{2^{n}}\delta,\qquad i=1,2, 3,...,K.",,M_{b_{i}} \leq \frac{1}{2^{n}}\sum_{k=0}^{2^{n}-1}O^{(K+k)}+\delta.,"This equation provides a theoretical upper bound on the mean outlier magnitude within each block after applying the zigzag permutation, showing that the permutation ensures a balanced distribution of outliers across blocks."
neurips_2024_oral_15,1,"## 2 Problem setting and preliminaries

**Preference-based bandits with a linear utility function.** The learner is given a finite set of options (or ""arms""), each represented by a feature vector in \(\mathcal{Z}\subset\mathbb{R}^{d}\), and a finite set of binary queries, where each query is the difference between two arms, denoted by \(\mathcal{X}\subset\mathbb{R}^{d}\). For instance, if the learner can query any pair of arms, the query space is \(\mathcal{X}=\{z-z^{\prime}\colon z,z^{\prime}\in\mathcal{Z}\}\). In the online retailer example from section 1, the query space is \(\mathcal{X}=\{z-z_{\text{skip}}\colon z\in\mathcal{Z}\}\), where \(z\) represents purchasing a product and \(z_{\text{skip}}\) represents skipping (often set as \(\mathbf{0}\)). For each arm \(z\in\mathcal{Z}\), the human utility is assumed to be linear in the feature space, defined as \(u_{z}\coloneqq z^{\top}\theta^{*}\), where \(\theta^{*}\in\mathbb{R}^{d}\) represents the human's preference parameters. For any query \(x\in\mathcal{X}\), the utility difference is then defined as \(u_{x}\coloneqq x^{\top}\theta^{*}\).

Given a query \(x\coloneqq z_{1}-z_{2}\in\mathcal{X}\), we model human choices and response times using the difference-based EZ-Diffusion Model (dEZDM) [8; 67], integrated with our linear utility structure. (See appendix B.1 for a comparison with other models.) This model interprets human decision-making as a stochastic process in which evidence accumulates over time to compare two options. As shown in fig. 0(a), after receiving a query \(x\), the human first spends a fixed amount of non-decision time, denoted by \(t_{\text{nondec}}>0\), to perceive and encode the query. Then, evidence \(E_{x}\) accumulates over time following a Brownian motion with drift \(x^{\top}\theta^{*}\) and two symmetric absorbing barriers, \(a>0\) and \(-a\). Specifically, at time \(t_{\text{nondec}}+\tau\) where \(\tau\geq 0\), the evidence is \(E_{x,\tau}=x^{\top}\theta^{*}\cdot\tau+B(\tau)\), where \(B(\tau)\sim\mathcal{N}(0,\tau)\) is standard Brownian motion. This process continues until the evidence reaches either the upper barrier \(a\) or lower barrier \(-a\), at which point a decision is made. The random stopping time, \(t_{x}\coloneqq\min\left\{\tau>0\colon E_{x,\tau}\in\{a,-a\}\right\}\), represents the decision time. If \(E_{x,t_{x}}=a\), the human chooses \(z_{1}\); if \(E_{x,t_{x}}=-a\), they choose \(z_{2}\). The choice is represented by the random variable \(c_{x}\), where \(c_{x}=1\) if \(z_{1}\) is chosen, and \(-1\) if \(z_{2}\) is chosen. The total response time, \(t_{\text{RT},x}\), is the sum of the non-decision time and the decision time: \(t_{\text{RT},x}=t_{\text{nondec}}+t_{x}\). The choice probability, expected choice, choice variance, and expected decision time are given as follows ([48, eq. (A.16) and (A.17)]:","\begin{split}\forall x\in\mathcal{X}\colon\mathbb{P}[c_{x }=1]=\frac{1}{1+\exp(-2ax^{\top}\theta^{*})},\ \ \mathbb{E}[c_{x}]=\tanh(ax^{\top}\theta^{*})\\ \mathbb{V}[c_{x}]=1-\tanh^{2}(ax^{\top}\theta^{*}),\ \ \mathbb{E}[t_{x}]=\begin{cases}\frac{a}{x^{\top}\theta^{*}}\tanh(ax^ {\top}\theta^{*})&\text{if }x^{\top}\theta^{*}\neq 0\\ a^{2}&\text{if }x^{\top}\theta^{*}=0\end{cases}\end{split}.",,"\begin{aligned}
    \mathbb{P}(c_x = 1) &= \frac{1}{1 + \exp(-2 a x^\top \theta^*)}, \\
    \mathbb{E}[c_x] &= \tanh(a x^\top \theta^*), \\
    \text{Var}(c_x) &= 1 - \tanh^2(a x^\top \theta^*), \\
    \mathbb{E}[t_x] &= \frac{a}{x^\top \theta^*} \tanh(a x^\top \theta^*).
\end{aligned}","Choice probability, expected choice, choice variance, and expected decision time under the difference-based EZ-Diffusion Model (dEZDM) with linear utility structure."
neurips_2024_oral_15,2,"This choice probability matches that of the Bradley and Terry [10] model. If the learner relies solely on choices, then our bandit problem reduces to the transductive linear logistic bandit problem [31].

Figures 0(b) and 0(c) illustrate the roles of the parameters \(x^{\top}\theta^{*}\) and \(a\). First, the absolute drift (or the absolute utility difference), \(|x^{\top}\theta^{*}|\), reflects the human's preference strength for the query \(x\). Larger values indicate stronger preferences, leading to faster decisions and more consistent choices. Smaller values suggest weaker preferences, resulting in slower decisions and less consistent choices. Second, the barrier \(a\) represents the human's conservativeness in decision-making [40]. A more conservative human (higher \(a\)) requires more evidence to decide, resulting in slower but more consistent choices. In contrast, a less conservative human (lower \(a\)) decides faster but makes less consistent choices.

We adopt the common assumption that \(t_{\text{nondec}}\) is constant across all queries for a given human [16; 76] and further assume that \(t_{\text{nondec}}\) is known to the learner. This assumption enables the learner to perfectly recover \(t_{x}\) from the observed \(t_{\text{RT},x}\). In section 5.2, we empirically show that even when \(t_{\text{nondec}}\) is unknown, its impact on the performance of our method that relies on decision times is negligible.

**Learning objective: Best-arm identification with a fixed budget.** We focus on the fixed-budget best-arm identification problem [29; 34]. The learner is provided with a total interaction time budget \(B>0\), an arm space \(\mathcal{Z}\), a query space \(\mathcal{X}\), and a non-decision time \(t_{\text{nondec}}\). Both the human's preference vector \(\theta^{*}\) and the decision barrier \(a\) are unknown. In each episode \(s\in\mathbb{N}\), the learner selects a query \(x_{s}\in\mathcal{X}\), receives human feedback \((c_{x_{s},s},t_{x_{s},s})\) generated by the dEZDM, and consumes \(t_{\text{RT},x_{s}}\) time. When the cumulative interaction time exceeds the budget \(B\) at some episode \(S\), i.e., \(\sum_{s=1}^{S}t_{\text{RT},x_{s},s}>B\), the learner must stop and recommend an arm \(\widehat{z}\in\mathcal{Z}\). The goal is to recommend the unique best arm \(z^{*}\coloneqq\arg\max_{z\in\mathcal{Z}}z^{\top}\theta^{*}\), minimizing the error probability \(\mathbb{P}\left[\widehat{z}\neq z^{*}\right]\).

Figure 1: (a) depicts the human decision-making process for a binary query \(x\in\mathcal{X}\), where the human selects between two arms. The human first spends a fixed non-decision time \(t_{\text{nondec}}\) encoding the query. Then, the human’s evidence accumulates according to a Brownian motion with drift \(x^{\top}\theta^{*}\). When the evidence reaches the upper barrier \(a\) or lower barrier \(-a\), the human makes a choice, denoted by \(c_{x}=1\) or \(c_{x}=-1\), respectively. The random stopping time of the accumulation process is the decision time \(t_{x}\), and the total response time is \(t_{\text{RT},x}=t_{\text{nondec}}+t_{x}\). (b) and (c) plot the expected choice \(\mathbb{E}[c_{x}]\) and the expected decision time \(\mathbb{E}[t_{x}]\), with shaded regions representing one standard deviation, plotted as functions of the utility difference \(x^{\top}\theta^{*}\) for two barrier values \(a\).

To address this problem, we adopt the Generalized Successive Elimination (GSE) algorithm [1; 3; 75]. GSE divides the total budget \(B\) into multiple phases. In each phase, it strategically samples queries until the phase's budget is exhausted, collecting both human choices and decision times. It then estimates the preference vector \(\theta^{*}\) and eliminates arms with low estimated utilities. Decision times play a key role in the estimation step by providing complementary information about preference strength, which can enable more accurate estimation of \(\theta^{*}\) than choices alone. Next, in section 3, we introduce a novel estimator that combines decision times and choices to estimate \(\theta^{*}\). Then, in section 4, we discuss how this estimator is integrated into GSE to improve preference learning.

## 3 Utility estimation

This section addresses the problem of estimating human preference \(\theta^{*}\) from a fixed dataset, denoted by \(\left\{x,c_{x,s_{x,i}},t_{x,s_{x,i}}\right\}_{x\in\mathcal{X}_{\text{sample}}, i\in[n_{x}]}\). Here, \(\mathcal{X}_{\text{sample}}\) denotes the set of queries in the dataset, \(n_{x}\) denotes the number of samples for each query \(x\in\mathcal{X}_{\text{sample}}\), and \(s_{x,i}\) denotes the episode when \(x\) is sampled for the \(i\)-th time. Samples from the same query \(x\) are i.i.d., while samples from different queries are independent. Section 3.1 introduces a new estimator, the ""choice-decision-time estimator,"" which uses both choices and decision times, in contrast to the commonly used ""choice-only estimator"" that only uses choices [3; 31]. Sections 3.2 and 3.3 theoretically compares these estimators, analyzing both asymptotic and non-asymptotic performance and highlighting the advantages of incorporating decision times. Section 5.1 presents empirical results that validate our theoretical insights.

### Choice-decision-time estimator and choice-only estimator

The choice-decision-time estimator is based on the following relationship between human utilities, choices, and decision times, derived from eq. (1):",\forall x\in\mathcal{X}\colon x^{\top}\frac{\theta^{*}}{a}=\frac{\mathbb{E} [c_{x}]}{\mathbb{E}[t_{x}]}.,,"\forall x\in\mathcal{X}\colon\mathbb{E}[c_{x}]=\tanh(ax^{\top}\theta^{*}),\quad\mathbb{E}[t_{x}]=\frac{a}{x^{\top}\theta^{*}}\tanh(ax^{\top}\theta^{*})\ \text{if }x^{\top}\theta^{*}\neq 0.","The expected choice and expected decision time as functions of the utility difference and the decision barrier, forming the basis for the choice-decision-time estimator."
neurips_2024_oral_15,3,"Intuitively, when a human provides consistent choices (i.e., large \(|\mathbb{E}[c_{x}]|\)) and makes decisions quickly (i.e., small \(\mathbb{E}[t_{x}]\)), it implies a strong preference (i.e., large \(|x^{\top}\theta^{*}|\)). This relationship formulates the estimation of \(\theta^{*}\) as a _linear regression_ problem. Accordingly, the choice-decision-time estimator calculates the empirical means of both choices and decision times, aggregates the ratios across all sampled queries, and applies ordinary least squares (OLS) to estimate \(\theta^{*}/a\). Since the ranking of arm utilities based on \(\theta^{*}/a\) is identical to that based on \(\theta^{*}\), estimating \(\theta^{*}/a\) is sufficient for identifying the best arm. Formally, this estimate of \(\theta^{*}/a\), denoted by \(\widehat{\theta}_{\text{CH,DT}}\), is given by:","\widehat{\theta}_{\text{CH,DT}}\coloneqq(\sum_{x\in\mathcal{X}_{\text{ sample}}}n_{x}\;xx^{\top})^{-1}\sum_{x\in\mathcal{X}_{\text{sample}}}n_{x}\;x \;\frac{\sum_{i=1}^{n_{x}}c_{x,s_{x,i}}}{\sum_{i=1}^{n_{x}}t_{x,s_{x,i}}}.",,"\widehat{\theta}_{\text{CH,DT}}\coloneqq\left(\sum_{x\in\mathcal{X}_{\text{sample}}}\frac{n_{x}}{\widehat{\mathbb{E}}[t_{x}]^{2}}xx^{\top}\right)^{-1}\sum_{x\in\mathcal{X}_{\text{sample}}}\frac{n_{x}\widehat{\mathbb{E}}[c_{x}]}{\widehat{\mathbb{E}}[t_{x}]^{2}}x,","Choice-decision-time estimator for $\theta^{*}/a$ derived from the linear relationship between expected choice, expected decision time, and the utility difference, using weighted ordinary least squares regression over sampled queries."
neurips_2024_oral_15,4,"In contrast, the choice-only estimator is based on eq. (1), which shows that for each query \(x\in\mathcal{X}\), the random variable \((c_{x}+1)/2\) follows a Bernoulli distribution with mean \(1/[1+\exp(-x^{\top}\cdot 2a\theta^{*})]\). Similar to the choice-decision-time estimator, the parameter \(2a\) does not impact the ranking of arms, so estimating \(2a\theta^{*}\) is sufficient for best-arm identification. This estimation is formulated as a _logistic regression_ problem [3; 31], with MLE providing the following estimate of \(2a\theta^{*}\), denoted by \(\widehat{\theta}_{\text{CH}}\):","\widehat{\theta}_{\text{CH}}\coloneqq\operatorname*{arg\,max}_{\theta\in \mathbb{R}^{d}}\sum_{x\in\mathcal{X}_{\text{sample}}}\sum_{i=1}^{n_{x}}\log \mu(c_{x,s_{x,i}}\,x^{\top}\theta),",where \(\mu(y)\coloneqq 1/[1+\exp(-y)]\) is the standard logistic function,"\widehat{\theta}_{\text{CH}}\coloneqq\arg\max_{\theta\in\mathbb{R}^{d}}\sum_{x\in\mathcal{X}_{\text{sample}}}\sum_{i=1}^{n_{x}}\log\left(\frac{\exp(c_{x,s_{x,i}}x^{\top}\theta)}{1+\exp(x^{\top}\theta)}\right).","The choice-only estimator formulates preference estimation as a logistic regression problem by maximizing the log-likelihood of observed choices, where the parameter $2a\theta^{*}$ is estimated without incorporating decision time information."
neurips_2024_oral_15,5,"on. While this MLE lacks a closed-form solution, it can be efficiently solved using optimization methods like Newton's algorithm [25; 44].

### Asymptotic normality of the two estimators

The choice-decision-time estimator from eq. (3) satisfies the following asymptotic normality result:

**Theorem 3.1** (Asymptotic normality of \(\widehat{\theta}_{\text{CHD}}\)).: _Given a fixed i.i.d. dataset \(\left\{x,c_{x,s_{x,i}},t_{x,s_{x,i}}\right\}_{i\in[n]}\) for each \(x\in\mathcal{X}_{\text{sample}}\), where \(\sum_{x\in\mathcal{X}_{\text{sample}}}xx^{\top}\succ 0\), and assuming that the datasets for different \(x\in\mathcal{X}_{\text{sample}}\) are independent, then, for any vector \(y\in\mathbb{R}^{d}\), as \(n\to\infty\), the following holds:_","\sqrt{n}\;y^{\top}(\widehat{\theta}_{\text{CHD},n}-\theta^{*}/a) \overset{D}{\longrightarrow}\mathcal{N}(0,\zeta^{2}/a^{2}).",,"\sqrt{n}(\widehat{\theta}_{\text{CH,DT}}-\theta^{*}/a)\xrightarrow{d}\mathcal{N}(0,\Sigma_{\text{CH,DT}}),\quad\Sigma_{\text{CH,DT}}\coloneqq\left(\sum_{x\in\mathcal{X}_{\text{sample}}}\frac{xx^{\top}}{\mathbb{E}[t_{x}]}\right)^{-1}\left(\sum_{x\in\mathcal{X}_{\text{sample}}}\frac{xx^{\top}\mathbb{V}[c_{x}]}{\mathbb{E}[t_{x}]^{2}}\right)\left(\sum_{x\in\mathcal{X}_{\text{sample}}}\frac{xx^{\top}}{\mathbb{E}[t_{x}]}\right)^{-1}.",The asymptotic covariance matrix of the choice-decision-time estimator depends on the query design and the variance of choices scaled by the expected decision times.
neurips_2024_oral_15,6,"_Here, the asymptotic variance depends on a problem-specific constant, \(\zeta^{2}\), with an upper bounded:_",\zeta^{2}\leq\|y\|_{(\sum_{x\in\mathcal{X}_{\text{sample}}} [\min_{x^{\prime}\in\mathcal{X}_{\text{sample}}}\mathbb{E}[t_{x^{ \prime}}]]\cdot xx^{\top})^{-1}}^{-1}.,,\zeta^{2}\leq\frac{y^{\top}\left(\sum_{x\in\mathcal{X}_{\text{sample}}}xx^{\top}\right)^{-1}y}{a^{2}}.,The asymptotic variance of the choice-decision-time estimator is bounded above by a term that depends on the design matrix of sampled queries and the decision barrier $a$.
neurips_2024_oral_15,7,"The proof is provided in appendix C.2. The asymptotic variance upper bound shows that all sampled queries are weighted by a common factor \(\min_{x^{\prime}\in\mathcal{X}_{\text{sample}}}\mathbb{E}\left[t_{x^{\prime}}\right]\), which is the smallest expected decision time among all the sampled queries in \(\mathcal{X}_{\text{sample}}\). This weight represents the amount of information provided by each query's choices and decision times for utility estimation. A larger weight indicates that all queries in \(\mathcal{X}_{\text{sample}}\) provides more information, leading to lower variance and better estimates.

In contrast, the choice-only estimator from eq.4 has the following asymptotic normality result, as derived from Fahrmeir and Kaufmann [23, corollary 1]:

**Theorem 3.2** (Asymptotic normality of \(\widehat{\theta}_{\text{CH}}\)).: _Given a fixed i.i.d. dataset \(\left\{x,c_{x,s_{x,i}},t_{x,s_{x,i}}\right\}_{i\in[n]}\) for each \(x\in\mathcal{X}_{\text{sample}}\), where \(\sum_{x\in\mathcal{X}_{\text{sample}}}xx^{\top}\succ 0\), and assuming that the datasets for different \(x\in\mathcal{X}_{\text{sample}}\) are independent, then, for any vector \(y\in\mathbb{R}^{d}\), as \(n\to\infty\), the following holds:_","\sqrt{n}y^{\top}(\widehat{\theta}_{\text{CH},n}-2a\theta^{*}) \overset{D}{\longrightarrow}\mathcal{N}(0,4a^{2}\|y\|_{( \sum_{x\in\mathcal{X}_{\text{sample}}}[a^{2}\,\mathbb{V}[c_{x}] ]\cdot xx^{\top})^{-1}}^{-1}).",,"\sqrt{n}\;y^{\top}(\widehat{\theta}_{\text{CH},n}-2a\theta^{*}) \overset{D}{\longrightarrow}\mathcal{N}(0,\sigma^{2}),","Asymptotic normality of the choice-only estimator $\widehat{\theta}_{\text{CH}}$, where $\sigma^{2}$ is the asymptotic variance dependent on the Fisher information from logistic regression."
neurips_2024_oral_15,8,"This asymptotic variance shows that each sampled query \(x\in\mathcal{X}_{\text{sample}}\) is weighted by its own factor \(a^{2}\,\mathbb{V}\left[c_{x}\right]\), representing the amount of information the query's choices contribute to utility estimation. A larger weight indicates that the query contributes more information, leading to better estimates.

The weights in both theorems highlight the different contributions of choices and decision times to utility estimation. In the choice-only estimator (theorem3.2), each query is weighted by \(a^{2}\,\mathbb{V}\left[c_{x}\right]\), which depends on the utility difference \(x^{\top}\theta^{*}\) for a fixed barrier \(a\). As shown by the gray curves in fig.2a, this weight quickly decays to zero as preferences become stronger (i.e., as \(\left|x^{\top}\theta^{*}\right|\) increases). This indicates that _choices from queries with strong preferences provide little information_. Intuitively, when preferences are strong, humans consistently select the same option, making it hard to distinguish whether their preference is moderately or very strong. As a result, choices from such queries contribute minimally to utility estimation. This intuition aligns with the online retailer example in section1.

For the choice-decision-time estimator (theorem3.1), queries are weighted by \(\min_{x^{\prime}\in\mathcal{X}_{\text{sample}}}\mathbb{E}\left[t_{x^{\prime}}\right]\), which depends on both \(\mathcal{X}_{\text{sample}}\) and \(\mathbb{E}\left[t_{x}\right]\). To better understand this weight, we first plot \(\mathbb{E}\left[t_{x}\right]\) without the'min' operator as the orange curves in fig.2a. Comparing the orange and gray curves shows that \(\mathbb{E}\left[t_{x}\right]\) is generally larger than the choice-only weight, \(a^{2}\,\mathbb{V}\left[c_{x}\right]\). The actual weight in the choice-decision-time estimator, which is the minimum expected decision time across sampled queries, is less than or equal to the orange curve but is likely still higher than the choice-only weight, especially for queries with strong preferences. This suggests that _when preferences are strong, decision times complement choices by capturing preference strength, leading to improved estimation._

When queries have weak preferences, the choice-decision-time weight may be lower than the choice-only weight. However, since the choice-decision-time weight represents only an upper bound on the asymptotic variance (theorem3.1), no definitive conclusions can be drawn from the theory alone. Empirically, as shown in section5.1, decision times add little value but do not degrade performance.

As the barrier \(a\) increases, the choice-decision-time weight rises. In contrast, the choice-only weight increases for queries with weak preferences, but this increase is concentrated in a narrower region, with weights decreasing elsewhere. Intuitively, a higher barrier reflects greater conservativeness in human decision-making, leading to longer decision times and more consistent choices (fig.1). As a result, more queries exhibit strong preferences, making choices from these queries less informative.

### Non-asymptotic concentration of the two estimators for utility difference estimation

In this section, we focus on the simpler problem of estimating the utility difference for a single query, without aggregating data from multiple queries. Comparing the non-asymptotic concentration bounds of both estimators, in this case, provides insights similar to those discussed in section 3.2. Extending this non-asymptotic analysis to the full estimation of the preference vector \(\theta^{*}\) is left for future work.

Given a query \(x\in\mathcal{X}\), the task is to estimate the utility difference \(u_{x}:=x^{\top}\theta^{*}\) using the fixed i.i.d. dataset \(\{(c_{x,s_{x},i},t_{x,s_{x},i})\}_{i\in[n_{x}]}\). Applying the choice-decision-time estimator from eq. (3), we get the following estimate (for details, see appendix C.3.1), which estimates \(u_{x}/a\) rather than \(u_{x}\):","\widehat{u}_{x,\text{CH,DT}}\coloneqq\frac{\sum_{i=1}^{n_{x}}c_{x,s_{x,i}}}{ \sum_{i=1}^{n_{x}}t_{x,s_{x,i}}}.",,"\widehat{u}_{\text{CH,DT},x,n_{x}}\coloneqq x^{\top}\widehat{\theta}_{\text{CH,DT},n_{x}}.",Estimate of the utility difference $u_{x}/a$ for a single query $x$ using the choice-decision-time estimator.
neurips_2024_oral_15,9,"In contrast, applying the choice-only estimator from eq. (4), we get the following estimate (for details, see appendix C.3.2), which estimates \(2au_{x}\) rather than \(u_{x}\):","\widehat{u}_{x,\text{CH}}\coloneqq\mu^{-1}(\frac{1}{n_{x}}\sum_{i=1}^{n _{x}}\frac{c_{x,s_{x,i}}+1}{2}),","where \((c_{x,s_{x,i}}+1)/2\) is the binary choice coded as 0 or 1, and \(\mu^{-1}(p)\coloneqq\log\left(p/(1-p)\right)\) is the logit function (inverse of \(\mu\) introduced in eq","\widehat{u}_{x,\text{CH}}\coloneqq\frac{1}{n_{x}}\sum_{i=1}^{n_{x}}c_{x,s_{x,i}}x^{\top}\widehat{\theta}_{\text{CH}}.","Estimate of the utility difference $u_{x}$ using the choice-only estimator, which relies solely on human choices to approximate $2au_{x}$."
neurips_2024_oral_15,10,"eq. (4)).

Notably, the choice-only estimator in eq. (6) aligns with the EZ-diffusion model's drift estimator (67, eq. (5)). Moreover, the estimators in Xiang Chiong et al. (73, eq. (6)) and Berlinghieri et al. (8, eq. (7)) combine elements of both estimators from eqs. (5) and (6). In section 5.2, we demonstrate that both estimators from Wagenmakers et al. (67, eq. (5)) and Xiang Chiong et al. (73, eq. (6)) are outperformed by our proposed estimator in eq. (3) for the full bandit problem.

Assuming the utility difference \(u_{x}\neq 0\), the choice-decision-time estimator in eq. (5) satisfies the following non-asymptotic concentration bound, proven in appendix C.3.1:

**Theorem 3.3** (Non-asymptotic concentration of \(\widehat{u}_{x,\text{CH,DT}}\)).: _For each query \(x\in\mathcal{X}\) with \(u_{x}\neq 0\), given a fixed i.i.d. dataset \(\left\{\left(c_{x,s_{x},i},t_{x,s_{x},i}\right)\right\}_{i\in[n_{x}]}\), for any \(\epsilon>0\) satisfying \(\epsilon\leq\min\left\{|u_{x}|/(\sqrt{2}a),\left(1+\sqrt{2}\right)a|u_{x}|/ \mathbb{E}\left[t_{x}\right]\right\}\), the following holds:_","\mathbb{P}(|\widehat{u}_{x,\text{CH,DT}}-\frac{u_{x}}{a}|> \epsilon)\leq 4\exp(-[m_{\text{CH,DT}}^{\text{non-axym}}(x^{ \top}\theta^{*})]^{2}\,n_{x}\,[\epsilon\cdot a]^{2} ),",,"\mathbb{P}\left[\left|\widehat{u}_{x,\text{CH,DT}}-\frac{u_{x}}{a}\right|\geq\epsilon\right]\leq 2\exp\left(-\frac{n_{x}a^{2}\epsilon^{2}}{8\mathbb{E}[t_{x}]}\right)+2\exp\left(-\frac{n_{x}\epsilon^{2}\mathbb{E}[t_{x}]}{8a^{2}}\right).","Non-asymptotic concentration bound for the choice-decision-time estimator of the utility difference, showing exponential decay of error probability with sample size and dependence on expected decision time and utility difference."
neurips_2024_oral_15,11,"_where \(m_{\text{CH,DT}}^{\text{non-axym}}\left(x^{\top}\theta^{*}\right)\coloneqq \mathbb{E}\left[t_{x}\right]/\,\left[\left(2+2\sqrt{2}\right)a\right]\)._

Figure 2: This figure illustrates key terms from our theoretical analyses, highlighting the different contributions of choices and decision times to utility estimation. These terms are functions of the utility difference \(x^{\top}\theta^{*}\) and are plotted for two barrier values, \(a\). (a) compares the weights \(\mathbb{E}\left[t_{x}\right]\) and \(a^{2}\,\mathbb{V}\left[c_{x}\right]\) in the asymptotic variances for the choice-decision-time estimator (orange, theorem 3.1) and the choice-only estimator (gray, theorem 3.2), respectively. This comparison shows that _decision times complement choices, particularly for queries with strong preferences_. (b) compares the weights in the non-asymptotic concentration bounds (theorems 3.3 and 3.4), showing similar trends, though these weights may not be optimal due to proof techniques.

In contrast, the choice-only estimator in eq. (6) has the following non-asymptotic concentration result, adapted from Jun et al. [31, theorem 5]2:

Footnote 2: In Jun et al. [31, theorem 5], we let \(x_{1}=\dots=x_{t}=1\) and \(t_{\text{eff}}=d=1\).

**Theorem 3.4** (Non-asymptotic concentration of \(\widehat{u}_{x,\text{CH}}\)).: _For each query \(x\in\mathcal{X}\), given a fixed i.i.d. dataset \(\left\{c_{x,s_{x,i}}\right\}_{i\in[n_{x}]}\), for any positive \(\epsilon<0.3\), if \(n_{x}\geq 1/\dot{\mu}(2au_{x})\cdot\max\{3^{2}\log(6e)/\epsilon^{2},64\log(3)/(1- \epsilon^{2}/0.3^{2})\}\), the following holds:_","\mathbb{P}(|\widehat{u}_{x,\text{CH}}-2au_{x}|>\epsilon) \leq 6\exp(-[m_{\text{CH}}^{\text{non-asym}}(x^{\top}\theta^{*} )]^{2}\,n_{x}\,[\epsilon/(2a)]^{2}),",,"\mathbb{P}(|\widehat{u}_{x,\text{CH}}-2au_{x}|>\epsilon)\leq 6\exp\left(-n_{x}\cdot\frac{\epsilon^{2}}{9}\cdot\dot{\mu}(2au_{x})\right),",where $\dot{\mu}(y)\coloneqq\mu(y)(1-\mu(y))$ is the derivative of the logistic function $\mu(y)$.
neurips_2024_oral_16,1,"### DGI-like methods

DGI-like methods generate negative samples through random permutation of nodes. They employ a mutual information discriminator \(\mathcal{D}(\cdot)\), which maximizes the mutual information between nodes and their source graphs to train the model [6; 7; 15]. Here, we show that the objective function of DGI is a special case of representation scattering. To facilitate the proof, we present the following assumption:

**Assumption 1**.: _(a) The normalized propagation matrix \(\hat{\mathbf{A}}\) is defined as \(\tilde{\mathbf{A}}=\mathbf{D}^{-1}\hat{\mathbf{A}}\), where \(\hat{\mathbf{A}}=\mathbf{A}+\mathbf{I}\). (b) DGI generates the corrupted graph by randomly shuffling the entities in the feature matrix \(\mathbf{X}\), while keeping the adjacency matrix \(\hat{\mathbf{A}}\) unchanged. (c) The original data are class-balanced, i.e., for any classes \(k\) and \(j\), \(\text{num}(k)=\text{num}(j)\)._

The following results do not strictly require Assumption 1 to be satisfied. Assumption 1 represents a common scenario and serves to simplify the proof. A discussion on the validity of Assumption 1, and proofs of the subsequent results without Assumption 1, can be found in Appendix C.

**Theorem 1**.: _At the node level, minimizing the DGI loss is equivalent to maximizing the Jensen-Shannon (JS) divergence between the local semantic distribution in the original graph and its average distribution._

Proof.: Let \(p_{\text{data}}\) denote the distribution of all nodes in the original graph, characterized by a mean \(\mu\) and variance \(\sigma^{2}\). To analyze the local distribution of node \(v_{i}\) after its embedding aggregation by GNN encoders, we define \(p_{i}\) as the distribution of node \(v_{i}\) and its first-order neighbors with mean \(\mu_{i}\) and variance \(\sigma_{i}^{2}\). We make use of a conclusion in [6] and introduce the following lemma:

**Lemma 2**.: _Minimizing the DGI loss, denoted as \(\mathcal{L}_{\text{DGI}}\), equals to maximizing the Jensen-Shannon (JS) divergence between the distribution of the original graph \(\mathcal{G}\) and the corrupted graph \(\tilde{\mathcal{G}}\), i.e., \(\text{Min}(\mathcal{L}_{\text{DGI}})\Leftrightarrow\text{Max}(\text{JS}( \mathcal{G}\parallel\tilde{\mathcal{G}}))\)._We discuss Lemma 2 in Appendix D.1. Lemma 2 establishes the relationship between the DGI loss and the distributions of the original and corrupted graphs. To investigate the distribution of representations in original and corrupted graphs, we focus on the case of a single-layer GNN [21] and have the following formulation:","\mathbf{H}=\text{GNN}(\mathbf{A},\mathbf{X})=\mathbf{\tilde{A}}\mathbf{X} \mathbf{W}^{\prime},\mathbf{h}_{i}=\sum_{j\in\mathcal{N}_{i}}\alpha_{ij} \mathbf{x}_{j}\cdot\mathbf{W}^{\prime}=\sum_{j\in\mathcal{N}_{i}}\frac{1}{d_{ i}}\mathbf{x}_{j}\cdot\mathbf{W}^{\prime},","where \(\mathbf{W}^{\prime}=\xi(\mathbf{W}),\mathbf{W}\in\mathbb{R}^{D\times K}\) with \(\xi\) being an activation function like ReLU [22] for ease of understanding, \(\mathcal{N}_{i}\) denotes the set of first-order neighbors of node \(v_{i}\), inclusive of \(v_{i}\) itself","\begin{equation}
    \mathcal{L}_{\text{DGI}} = -\mathbb{E}_{(v_i, \mathcal{G}) \sim p_{\text{data}}} \left[ \log \mathcal{D}(z_i, \mathcal{G}) \right] - \mathbb{E}_{(v_i, \tilde{\mathcal{G}}) \sim p_{\text{corrupt}}} \left[ \log (1 - \mathcal{D}(z_i, \tilde{\mathcal{G}})) \right],
\end{equation}","The DGI loss function, which minimizes the negative log-likelihood of correctly classifying node representations from the original and corrupted graphs using a discriminator $\mathcal{D}(\cdot)$."
neurips_2024_oral_16,2,"lf. In Eq. 1, \(\forall j\in\mathcal{N}_{i},\mathbf{x}_{j}\sim p_{i}(\mu_{i},\sigma_{i}^{2})\). For node \(v_{i}\), subsequent to GNN message passing, we compute the mean and variance of the aggregated representation \(\mathbf{h}_{i}\) as follows:","\begin{split}\mathbb{E}[\mathbf{h}_{i}]&=\mathbb{E} [\sum_{j\in\mathcal{N}(i)}\alpha_{ij}\mathbf{x}_{j}\cdot\mathbf{W}^{ \prime}]=\sum_{j\in\mathcal{N}(i)}\alpha_{ij}\mathbb{E}[\mathbf{x}_{j}] \cdot\mathbf{W}^{\prime}=\sum_{j\in\mathcal{N}(i)}\alpha_{ij}\mu_{i}\cdot \mathbf{W}^{\prime},\\ \text{Var}(\mathbf{h}_{i})&=\text{Var}(\sum_{ j\in\mathcal{N}(i)}\alpha_{ij}\mathbf{x}_{j}\cdot\mathbf{W}^{\prime})= \sum_{j\in\mathcal{N}(i)}\alpha_{ij}\text{Var}(\mathbf{x}_{j})\cdot\mathbf{W} ^{\prime 2}=\sigma_{i}^{2}\sum_{j\in\mathcal{N}(i)}\alpha_{ij}^{2}\cdot \mathbf{W}^{\prime 2}.\end{split}",,"\mathbb{E}[\mathbf{h}_{i}] = \sum_{j \in \mathcal{N}_{i}} \frac{1}{d_{i}} \cdot \mathbb{E}[\mathbf{x}_{j}] \cdot \mathbf{W}^{\prime} = \mu_{i} \cdot \mathbf{W}^{\prime}, \quad \text{Var}[\mathbf{h}_{i}] = \sum_{j \in \mathcal{N}_{i}} \left(\frac{1}{d_{i}}\right)^2 \cdot \text{Var}[\mathbf{x}_{j}] \cdot \mathbf{W}^{\prime \, 2} = \frac{\sigma_{i}^{2}}{d_{i}} \cdot \mathbf{W}^{\prime \, 2}.",Computing the mean and variance of the aggregated representation after GNN message passing for node $v_i$.
neurips_2024_oral_16,3,"Given that \(\mathbf{\tilde{A}}\) is a normalized propagation matrix with \(\sum_{j\in\mathcal{N}(i)}\alpha_{ij}=1\) for the aggregated representation \(\mathbf{h}_{i}\) in the original graph, the mean of representation \(\mathbf{h}_{i}\)'s distribution equals \(\mu_{i}\cdot\mathbf{W}^{\prime}\). We then analyze the distribution of the aggregated representation \(\hat{\mathbf{h}}_{i}\) in the corrupted graph. Based on Assumption 1, the adjacency matrix \(\mathbf{A}\) is unaltered and the feature vector \(\tilde{\mathbf{x}}_{i}\) is selected randomly from the feature matrix \(\mathbf{X}\) of the original graph, following the distribution \(p_{\text{data}}(\mu,\sigma^{2})\). According to Eq.2, the mean of the distribution of \(\hat{\mathbf{h}}_{i}\) is \(\mu\cdot\mathbf{W}^{\prime}\), and the variance is \(\sigma^{2}\sum_{j\in\mathcal{N}(i)}\alpha_{ij}^{2}\cdot\mathbf{W}^{\prime 2}\). Consequently, we have \(\mathbf{h}_{i}\sim p_{i}^{\prime}(\mu_{i}\cdot\mathbf{W}^{\prime},\sigma_{i}^{ 2}\cdot\mathbf{W}^{\prime 2})\) and \(\hat{\mathbf{h}}_{i}\sim p_{\text{data}}^{\prime}(\mu\cdot\mathbf{W}^{ \prime},\sigma^{2}\cdot\mathbf{W}^{\prime 2})\), which directly prove that maximizing the JS divergence between the original and corrupted graph distributions at the node level maximizes, in effect, the JS divergence between the local semantic distribution of node \(v_{i}\) and the mean distribution of the nodes in the original graph. 

**Corollary 3**.: _Taking the mean of the original graph as the center \(\mathbf{c}\), and the original representation space as a subspace \(\mathbb{S}^{k}\), the objective of DGI can be described as follows: within the subspace \(\mathbb{S}^{k}\), DGI increases the distance between the nodes of the original graph and its center \(\mathbf{c}\), achieving the objective of representation scattering._

Corollary 3 reveals, for the first time, that the primary goal of DGI-like methods is to position node representations away from a central point to encourage a uniform distribution of the nodes. To illustrate these theoretical insights, we performed a visualization experiment. As shown in Figure 1 (a) and (b), both a randomly initialized GNN and a trained single-layer GNN demonstrated that their representations are distanced from the mean of the original graph nodes. Additionally, Figure 1 (b) and (c) provide clearer evidence that minimizing the \(\mathcal{L}_{\text{DGI}}\) is equivalent to maximizing the Jensen-Shannon (JS) divergence between the positive and negative samples, i.e., maximizing the JS divergence between the local semantic distribution in the original graph and its mean distribution. However, the row-wise shuffling mechanism may introduce potential disturbances. In a graph with \(n\) nodes, each node \(v\) retains its local semantic distribution with a probability of \(1/k\) during the shuffling process, where \(k\) denotes the number of distinct node types, assuming class balance. Despite this, the non-discriminatory nature of the perturbation means that these unchanged nodes could still be mistakenly classified as negative samples. Consequently, genuine node representations may be incorrectly labeled as negatives, leading to bias in the learning process. The overlapping nodes depicted in Figure 1 intuitively support this perspective.

Figure 1: t-SNE embedding of DGI on Co.CS dataset. The blue points represent the embeddings of the perturbed negative samples, and the red points denote that of positive nodes. As can be seen in Figures (a) and (b) of model random initialization and the layer of the trained encoder, the DGI-like methods essentially maximize the JS divergence between node embedding and embedding mean.

### InfoNCE-based methods

We now show that the mechanism of negative sampling in the InfoNCE-based methods is equivalent to representation scattering.

**Theorem 4**.: _Let \(\bar{\mathbf{h}}=1/n\sum_{i=1}^{n}\mathbf{h}_{i}\), and \(\text{sim}(\cdot)\) be the cosine similarity function (here, \(\mathbf{h}_{i}\) represents the encoded representation of node \(v_{i}\)). For node \(v_{i}\), the lower bound of the InfoNCE loss \(\mathcal{L}_{\text{InfoNCE}}(\mathbf{h}_{i})\) exists: \(\mathcal{L}_{\text{InfoNCE}}(\mathbf{h}_{i})\geq\text{sim}(\mathbf{h}_{i}, \bar{\mathbf{h}})+\ln(2n)\)._

A detailed proof is given in Appendix D.3. Theorem 4 indicates that when minimizing \(\mathcal{L}_{\text{InfoNCE}}(\mathbf{h}_{i})\), the similarity between node \(v_{i}\) and mean node \(\bar{v}\), i.e., \(\text{sim}(\mathbf{h}_{i},\bar{\mathbf{h}})\) is also minimized. Taking the mean of all nodes as the scattered center \(\mathbf{c}\), and the hypersphere as the subspace \(\mathbb{S}^{k}\), the mechanism of negative sampling is equivalent to representation scattering. However, the InfoNCE loss function is inefficient for representation scattering as it needs to compute and reduce the similarity of each negative pair. Moreover, it indiscriminately treats all negative samples, ignoring the distinctions among them, which leads to inappropriate scattering of negative samples and potential bias from false negatives. Consequently, many recent methods have incurred additional computational overhead by manually and intuitively defining positive and negative samples [12; 23].

### BGRL-like methods

All BGRL-like methods incorporate a key component: Batch Normalization (BN). For a feature vector \(\mathbf{x}_{i}\) and its corresponding batch statistics, mean \(\mu\) and variance \(\sigma^{2}\), the BN is applied as follows: \(\text{BN}(\mathbf{x}_{i})=\gamma(\mathbf{x}_{i}-\mu)/\sqrt{\sigma^{2}+\epsilon }+\beta\), where \(\gamma\) and \(\beta\) are learnable parameters that scale and shift the normalized value, and \(\epsilon\) is a small constant added for numerical stability.

**Theorem 5**.: _The process of data normalization by batch normalization can be seen as a special case of representation scattering._

A detailed proof can be found in Appendix D.2. To empirically assess the impact of batch normalization on BGRL, we conducted experiments comparing both BGRL and BGRL w/o BN across four benchmark datasets. The results, shown in Figure 2, reveal a significant drop in accuracy for BGRL across all datasets without batch normalization, highlighting its critical role in the bootstrapping framework. While BGRL incorporates representation scattering through batch normalization, its training process lacks explicit guidance or a dedicated mechanism to efficiently manage this scattering. The absence of direct supervision during the representation scattering phase can lead to an unoptimized distribution of representation within the embedding space, resulting in suboptimal performance.

In summary, our analysis of the three mainstream GCL frameworks theoretically proved that they all inherently utilize representation scattering but also fail to fully utilize this effective mechanism. This analytic result has motivated us to design a more effective representation scattering method tailored for learning on graphs.

## 4 Methodology

The proceeding sections have revealed the importance of representation scattering in GCLs. Based on the findings, we have designed a novel method, namely Scattering Graph Representation Learning, short-handed as SGRL (Figure 3). We introduce the components of SGRL and provide discussion in the following sub-sections.

### Representation Scattering Mechanism (RSM)

To address the shortcomings in the application of representation scattering within the three mainstream graph contrastive learning frameworks, we design RSM to explicitly guide the target encoder in learning scattered representations. Following Definition 1, we introduce a subspace \(\mathbb{S}^{k}\) and a scattered

Figure 2: The impact of Batch Normalization in BGRL.

center \(\mathbf{c}\) to effectively perform representation scattering. For the subspace \(\mathbb{S}^{k}\), a transformation function \(\text{Trans}(\cdot)\) is introduced to transform representations from the original space \(\mathbb{R}^{d}\) into \(\mathbb{S}^{k}\). Specifically, we apply \(\ell_{2}\) normalization to each row vector \(\mathbf{h}_{i}\) in the matrix \(\mathbf{H}_{\text{target}}\):","\tilde{\mathbf{h}}_{i}=\text{Trans}_{\mathbb{R}^{d}\to\mathbb{S}^{k}}( \mathbf{h}_{i})=\frac{\mathbf{h}_{i}}{\text{Max}(\|\mathbf{h}_{i}\|_{2}, \varepsilon)},\quad\mathbb{S}^{k}=\{\tilde{\mathbf{h}}_{i}:\|\tilde{\mathbf{h} }_{i}\|_{2}=1\},","where \(\mathbf{h}_{i}\) is representation for node \(v_{i}\in\mathcal{V}\), generated by the target encoder, \(\|\tilde{\mathbf{h}}_{i}\|_{2}=(\sum_{j=1}^{k}\tilde{\mathbf{h}}_{ij}^{2})^{ \frac{1}{2}}\), and \(\epsilon\) is a small value to avoid division by zero",\mathbf{z}_{i}=\text{Trans}(\mathbf{h}_{i})=\frac{\mathbf{h}_{i}}{\|\mathbf{h}_{i}\|_{2}}.,"Equation 3 defines the transformation function $\text{Trans}(\cdot)$, which applies $\ell_2$ normalization to each row vector $\mathbf{h}_i$ in the matrix $\mathbf{H}_{\text{target}}$, mapping the representations into the subspace $\mathbb{S}^k$ for effective representation scattering."
neurips_2024_oral_16,4,"ro. As defined by Eq. 3, the representations of all nodes are distributed on a hypersphere \(\mathbb{S}^{k}\). This mapping prevents arbitrary scattering of representations in the space, avoiding instability and optimization difficulties during training.

Next, we define the scattered center \(\mathbf{c}\) and introduce a representation scattering loss function \(\mathcal{L}_{\text{scattering}}\) in \(\mathbb{S}^{k}\) to push node representations away from the center \(\mathbf{c}\), as formulated as follows:","\mathcal{L}_{\text{scattering}}=-\frac{1}{n}\sum\nolimits_{i=1}^{n}\|\tilde{ \mathbf{h}}_{i}-\mathbf{c}\|_{2}^{2},\quad\mathbf{c}=\frac{1}{n}\sum\nolimits _{i=1}^{n}\tilde{\mathbf{h}}_{i}.",,"$$\mathcal{L}_{\text{scattering}} = -\frac{1}{n} \sum_{i=1}^{n} \log\left(1 - \text{sim}(\tilde{\mathbf{h}}_{i}, \mathbf{c})\right),$$","where $\mathbf{c}$ is the center of the hypersphere $\mathbb{S}^{k}$, and the loss function $\mathcal{L}_{\text{scattering}}$ encourages node representations to be scattered away from this center"
neurips_2024_oral_16,5,"Through Eq. 4, SGRL achieves uniformity of representations globally across the entire dataset, without emphasizing local uniformity. Specifically, RSM enables representations of different semantics to be globally scattered across the hypersphere while accommodating representations of the same semantics to aggregate locally.

**Discussion of RSM.** We now provide a theoretical analysis demonstrating that the proposed representation scattering mechanism outperforms the three graph contrastive learning frameworks. RSM achieves representation scattering more effectively by encouraging distances between node representations and the scattered center, eliminating the dependence on manually designed negative samples. Most traditional methods [7; 8; 9; 15] rely on negative samples to indirectly promote representation scattering, which is inefficient and introduces biases. DGI-like methods, as discussed in Section 3.1, aim to maximize the Jensen-Shannon (JS) divergence between the distribution of the original graph and its mean distribution. Based on this, generating additional noise graph is inefficient. Moreover, some negative samples generated through random shuffling may align with the distribution of positive samples, resulting in false negative samples that hinder model training. InfoNCE-based methods consider all nodes as negative samples, except the matching ones in two augmented views. While pushing nodes away from each other ensures the discriminability of each node, it also results in significant computational overhead. Moreover, due to the potential conflict between the encoder's message-passing mechanism and the InfoNCE loss function, many negative samples can not effectively distance themselves from each other [24]. Therefore, by employing a center-away strategy,

Figure 3: The overview of SGRL. Consider a graph \(\mathcal{G}\) processed using two distinct encoders (online encoder and target encoder): \(f_{\theta}(\cdot)\) with parameters \(\theta\) and \(f_{\phi}(\cdot)\) with \(\phi\), aimed at generating node representations \(\mathbf{H}_{\text{online}}\) and \(\mathbf{H}_{\text{target}}\), respectively. For \(\mathbf{H}_{\text{target}}\), the mean representation of all nodes is calculated to serve as the scattered center \(\mathbf{c}\). The parameters of \(f_{\phi}(\cdot)\) are updated via RSM that encourages node representations to diverge from \(\mathbf{c}\). \(\mathbf{H}_{\text{online}}\) is processed through TCM to incorporate topology information, resulting in \(\mathbf{H}_{\text{online}}^{\text{topology}}\). Subsequently, \(\mathbf{H}_{\text{online}}^{\text{topology}}\) is embedded through a predictor \(q_{\theta}\) to predict \(\mathbf{H}_{\text{target}}\), and the parameters in \(f_{\theta}(\cdot)\) is updated through back-propagation while stopping the gradient of \(f_{\phi}(\cdot)\). Both channels are trained simultaneously. At the end of each epoch, we employ an Exponential Moving Average (EMA) to update parameters \(\phi\). Finally, the representations generated by \(f_{\theta}(\cdot)\) are employed across various downstream tasks.

RSM effectively reduces the additional computational overhead and mitigates the biases caused by manually designed negative samples.

### Topology-based Constraint Mechanism(TCM)

After obtaining the scattered representations \(\mathbf{H}_{\text{target}}=f_{\phi}(\mathbf{A},\mathbf{X})\) through the target encoder, it is necessary to consider the differences in the degree of scattering of different node representations.

Consider the interconnected nature of graphs, the representations of two nodes connected topologically should be closer in space \(\mathbb{S}^{k}\). Specifically, \(\forall v_{i},v_{j}\in V\), \(\mathbf{h}_{i},\mathbf{h}_{j}\in\mathbb{S}^{k}\), given a threshold \(d\): \(\|\mathbf{h}_{i}-\mathbf{h}_{j}\|_{2}^{2}<d\), if \((i,j)\in\mathcal{V}\) and \(\|\mathbf{h}_{i}-\mathbf{h}_{j}\|_{2}^{2}>d\), if \((i,j)\notin\mathcal{V}\). Intuitively, to this end, a simple way is to replace the individually scattered representations with the aggregated representations \(\mathbf{H}_{\text{target}}^{\text{topology}}\) from first-order neighbors: \(\mathbf{H}_{\text{target}}^{\text{topology}}=\mathbf{A}\mathbf{H}_{\text{ target}}\). However, attempting to consider the topology information and achieve representation scattering through the same encoder may lead to conflicts.

To address this issue, we propose a Topology-based Constraint Mechanism (TCM). Specifically, we separate the process of constraint from the process of scattering by letting the online encoder generate topologically aggregated representations instead of the target. The online encoder enhances its representations by summing the original representations \(\mathbf{H}_{\text{online}}\) with the topologically aggregated representations of its k-order neighbors \(\hat{\mathbf{A}}^{\text{k}}\mathbf{H}_{\text{online}}\), which can be described as:","\mathbf{H}_{\text{online}}^{\text{topology}}=\hat{\mathbf{A}}^{\text{k}} \mathbf{H}_{\text{online}}+\mathbf{H}_{\text{online}},",where k represents the order of neighbors and \(\hat{\mathbf{A}}=\mathbf{A}+\mathbf{I}\) is the adjacency matrix with self-loops,\mathbf{H}_{\text{online}}^{\text{topology}} = (1 - \lambda) \mathbf{H}_{\text{online}} + \lambda \hat{\mathbf{A}}^{\text{k}} \mathbf{H}_{\text{online}}.,"where $\mathbf{H}_{\text{online}}^{\text{topology}}$ denotes the topologically enhanced representations, $\hat{\mathbf{A}}^{\text{k}}$ represents the k-order adjacency matrix, and $\lambda$ is a hyperparameter balancing the contribution of the original and aggregated representations"
neurips_2024_oral_16,6,"ps. By separating scattering and constraints, SGRL can effectively achieve a balance between representation scattering and topology aggregation adaptively, rather than setting the scattering distance empirically. Next, the topology representations \(\mathbf{H}_{\text{online}}^{\text{topology}}\) are fed into a predictor \(q_{\theta}(\cdot)\) to generate the predicted representations \(\mathbf{Z}_{\text{online}}=q_{\theta}(\mathbf{H}_{\text{online}}^{\text{topology}})\). Our objective is to align the predicted topology representations \(\mathbf{Z}_{\text{online}}\) closely to the scattered representations \(\mathbf{H}_{\text{target}}\), enhancing the model's effectiveness in capturing the essential semantic details of the graph. Based on this, the alignment loss \(\mathcal{L}_{\text{alignment}}\) is defined as follows:","\mathcal{L}_{\text{alignment}}=-\frac{1}{N}\sum_{i=1}^{N}\frac{\mathbf{Z}_{( \text{online},i)}^{\top}\mathbf{H}_{(\text{target},i)}}{\|\mathbf{Z}_{(\text {online},i)}\|\|\mathbf{H}_{(\text{target},i)}\|},","where \(\mathbf{Z}_{\text{online}}\) and \(\mathbf{H}_{\text{target}}\) represent the predicted and scattered representations, respectively","\mathcal{L}_{\text{alignment}}=\frac{1}{n}\sum_{i=1}^{n}\|\mathbf{z}_{i} -\tilde{\mathbf{h}}_{i}\|_{2}^{2},",
neurips_2024_oral_16,7,"ly. During this process, the online encoder's parameters \(\theta\) are updated and the target encoder's parameters \(\phi\) stop gradient propagation. Compared to directly aligning constrained and scattered representations, this predictive objective can act as a buffer, allowing the online encoder to adaptively learn scattered representations and topology information. Furthermore, to make the target encoder consider topological semantic information into the process of representation scattering, instead of solely focusing on scattering, we employ an Exponential Moving Average at the end of each training epoch:","\phiarrow\tau\phi+(1-\tau)\theta,","where \(\tau\) is a target decay rate and \(\tau\in[0,1]\)","\phi_{t+1}=\eta\theta_{t}+(1-\eta)\phi_{t},","where $\phi_{t+1}$ is the updated parameter of the target encoder, $\theta_{t}$ is the parameter of the online encoder at training step $t$, and $\eta$ is the momentum coefficient that controls the update rate of the target encoder."
neurips_2024_oral_17,1,"### Integrating SSL Techniques into a Consistency Loss Framework

In semi-supervised learning (SSL), the goal is to enhance model performance by leveraging unlabeled data, traditionally drawn from the same class spectrum as the labeled data [90]. The goal of SSL can be formalized by integrating three fundamental techniques: _(1) Consistency regularization_ ensures that the model outputs consistent predictions for augmented versions of the same instance. This technique utilizes different transformations to test the robustness of the model's predictions, promoting stability across variations in the input data [8; 59; 36]. _(2) Pseudo-labeling_ utilizes the model to generate artificial labels for unlabeled data by adopting ""hard"" labels (that is, the argmax of the model output) and keeping only the labels where the highest class probability exceeds a predefined threshold [62; 47; 57; 37; 23]. _(3) Teacher-Student model_ incorporates a structured learning relationship where the **teacher** model, typically trained on weakly-augmented instances, generates high-quality pseudo labels. These pseudo labels are then used to guide the training of the **student** model, which processes strongly-augmented instances. This approach helps improve the generalization capabilities of the student model by learning from the refined knowledge and stable supervision signals provided by the teacher [66; 43; 77; 12; 53]. Several methods integrate some of these techniques and achieve advanced performance in SSL [63; 83; 78; 87]. We unify these SSL techniques into one consistency loss:","\mathcal{L}_{\text{cons}}=\frac{1}{|\mathbb{D}_{U}|}\sum_{\mathbf{x}\in \mathbb{D}_{U}}H\Big{(}p_{\text{m}}(y\mid f_{\theta}(\alpha(\mathbf{x}))),p_{ \text{m}}(y\mid f_{\theta}(\mathcal{A}(\mathbf{x})))\Big{)},",,"\mathcal{L}_{\text{consistency}} = \mathcal{L}_{\text{reg}} + \lambda_{\text{pl}} \mathcal{L}_{\text{pseudo}} + \lambda_{\text{ts}} \mathcal{L}_{\text{teacher-student}},","where $\mathcal{L}_{\text{reg}}$ denotes the consistency regularization loss, $\mathcal{L}_{\text{pseudo}}$ represents the pseudo-labeling loss, and $\mathcal{L}_{\text{teacher-student}}$ corresponds to the teacher-student model loss, with $\lambda_{\text{pl}}$ and $\lambda_{\text{ts}}$ being the respective trade-off hyperparameters."
neurips_2024_oral_17,2,"Figure 2: Exploring prior gaps between SSL and GCD on SCars and CUB datasets. Left: Accuracy of sorted pseudo labels for old and new classes. Middle: Consistency loss trends over epochs, illustrating challenges in optimization and slower convergence for new classes. Right: Categorize errors [75], where “True Old” refers to predicting an ‘Old’ class sample to another ‘Old’ class, while ‘False Old” indicates predicting an ‘Old’ class sample as some ‘New’ class.

where cross-entropy \(H(\cdot,\cdot)\) measures _consistency for regularization_, while the prediction \(p_{\mathsf{m}}(y\mid f_{\mathbf{\theta}}(\mathbf{x}))\) serves as a _pseudo label_. This setup captures a _teacher-student dynamic_, where \(\alpha(\mathbf{x})\) and \(\mathcal{A}(\mathbf{x})\) represent the **teacher** (weakly-augmented) and **student** (strongly-augmented) instances, respectively.

### Class Prior Gap between SSL and GCD

The GCD task pushes the boundaries of SSL by questioning the closed-world assumption that all classes in the unlabeled dataset \(\mathbb{D}_{U}\) are previously known [69]. Instead, GCD incorporates new classes \(\mathbb{C}_{N}\) into the unlabeled dataset, demanding that the model learn to recognize and then correctly classify them [4; 21; 80; 5]. In this open-world setting, SSL methods face obstacles with new classes due to the lack of supervision [24; 56], resulting in significantly lower quality of pseudo-labels for these new classes than for the old ones (Fig. 2 left). This gap exacerbates the complexity of optimizing the consistency loss Eq. 1 for new classes, leading to learning instability and slow convergence (Fig. 2, middle). Such optimization issues lead to severe prediction bias, resulting in new classes' performance lagging behind that of old classes (Fig. 2, right), underlining the limitations of existing SSL techniques in GCD scenarios. More empirical analysis can be found in Appendix B.2.

## 3 How Consistency Loss Goes Awry: Unraveling the Pitfalls

Acknowledging challenges presented by the absence of prior knowledge for new classes in traditional semi-supervised learning (SSL) methods is the first step toward addressing the complexities of open-world tasks. We further identify that the 'prior gap' manifests as issues in learning synchronization and representation discrepancy (Sec. 3.1). Our analysis targets the minimization of the energy function between teacher and student representations to bridge the 'prior gap'. We find that aligning their attention on similar patterns reduces energy, indicating effective alignment and learning (Sec. 3.2). This key understanding paves the way for the development of our proposed methods, aiming to synchronize teacher-student attentions for improved model learning dynamics (Sec. 4).

### What to Bridge the Class Prior

The challenge of optimizing consistency loss leads to a learning gap between the student and the teacher, particularly evident when dealing with new classes (Fig. 1 top left). This gap causes the student to plateau, as it cannot keep pace with the teacher's more advanced understanding, which in turn restricts the teacher's progress in new classes Moreover, this learning gap also manifests itself in the divergent representations between teacher and student (Fig. 1 middle), specifically for new classes. Based on these observations, we revisit consistency loss (Eq. 1) in the closed-world setting.

**Insight 3.1**.: _The large discrepancy between \(f_{\mathbf{\theta}}(\alpha(x))\) and \(f_{\mathbf{\theta}}(\mathcal{A}(x))\) complicates maintaining consistency across the model predictions. To narrow this divide, an intuitive idea is to align \(f_{\mathbf{\theta}}(\alpha(\mathbf{x}))\) more closely with \(f_{\mathbf{\theta}}(\mathcal{A}(\mathbf{x}))\), simplifying the optimization of \(\mathcal{L}_{cons}\):_","\mathcal{L}_{\text{cons}}=\frac{1}{|\mathbb{D}_{U}|}\sum_{\mathbf{x}\in \mathbb{D}_{U}}d\Big{(}p_{\mathsf{m}}(y\mid f_{\mathbf{\theta}}(\alpha(\mathbf{x }))-\Delta\Re),p_{\mathsf{m}}(y\mid f_{\mathbf{\theta}}(\mathcal{A}(\mathbf{x})) )\Big{)},",,\min_{\theta}\mathcal{L}_{\text{cons}} \propto \min_{\theta}\frac{1}{|\mathbb{D}_{U}|}\sum_{\mathbf{x}\in \mathbb{D}_{U}}\Big{\|}f_{\theta}(\alpha(\mathbf{x})) - f_{\theta}(\mathcal{A}(\mathbf{x}))\Big{\|}^{2}.,"This equation reformulates the consistency loss as a minimization of the squared difference between teacher and student representations, highlighting the importance of representation alignment in reducing prediction inconsistency."
neurips_2024_oral_17,3,"_where \(\Delta\Re\) aims to pull \(f_{\mathbf{\theta}}(\alpha(\mathbf{x}))\) closer to \(f_{\mathbf{\theta}}(\mathcal{A}(\mathbf{x}))\). Ideally, \(\Delta\Re\) would be adaptive, scaling with the discrepancy between \(f_{\mathbf{\theta}}(\mathcal{A}(x))\) and \(f_{\mathbf{\theta}}(\alpha(x))\), while avoiding make them too similar, which enables model to find a shortcut of \(\mathcal{L}_{cons}\)._

To design it, we delve into the vision transformer, a representation encoder that has significantly advanced the performance of the GCD task. We found that the self-attention mechanism excels at capturing critical image patterns: as depicted in Fig. 3 left, deeper features (after the 8th layer) reveal semantic, high-level commonalities (_e.g._, car shell) across all images; and the shallow features are more attuned to high-frequency, low-level details (_e.g._, color and texture).

### Inconsistent Patterns Spoil the Whole Barrel

Inconsistent patterns can disrupt learning, making it crucial to align stored and queried patterns effectively. To address this, we draw inspiration from the Hopfield Network [2] -- an associative memory model known for its energy-based mechanism that naturally pulls similar patterns together (see _Appendix_ A.1 for details). We follow Ramsauer et al. to define the energy function for a state pattern (query) \(\mathbf{\xi}\in\mathbb{R}^{d}\), parameterized by \(N\) stored (key) patterns \(\mathbf{X}=[\mathbf{x}_{1},\cdots,\mathbf{x}_{N}]\in\mathbb{R}^{d\times N}\):","E(\mathbf{\xi};\mathbf{X})=\frac{1}{2}\mathbf{\xi}^{\top}\mathbf{\xi}-\text{\rm{lse}}( \mathbf{X}^{\top}\mathbf{\xi},\mathbf{\beta})+c,\quad\text{with }\text{\rm{lse}}(\mathbf{v},\mathbf{\beta}):=\beta^{-1}\log(\sum_{i=1}^{N} \exp(\mathbf{v}_{i})).",,,
neurips_2024_oral_17,4,"Minimizing \(E(\mathbf{\xi};\mathbf{X})\) resembles retrieving stored pattern \(\mathbf{x}_{i}\) that is most similar to the query \(\mathbf{\xi}\), and _log-sum-exp_ (\(\text{\rm{lse}}\)) function is parameterized by \(\beta>0\) and \(c\) is a preset constant. Particularly, the first term ensures the finiteness of the query, while the second term measures the alignment of the query with each stored pattern. The update rule for a state pattern \(\mathbf{\xi}\) is equivalent to a gradient descent update of minimizing the energy \(E\) with step size \(\eta=1\)[54], ensures that the query moves closer to the most similar stored pattern:",\mathbf{\xi}arrow\mathbf{\xi}-\eta\nabla_{\mathbf{\xi}}E(\mathbf{\xi};\mathbf{X})=\mathbf{ \xi}-\text{\rm{sm}}(\beta\mathbf{\xi}^{\top}\mathbf{X})\mathbf{X}^{\top}.,,\mathbf{\xi}^{(t+1)} = \mathbf{\xi}^{(t)} - \nabla_{\mathbf{\xi}}E(\mathbf{\xi}^{(t)};\mathbf{X}) = \sum_{i=1}^{N} \frac{\exp(\beta \mathbf{x}_{i}^{\top}\mathbf{\xi}^{(t)})}{\sum_{j=1}^{N}\exp(\beta \mathbf{x}_{j}^{\top}\mathbf{\xi}^{(t)})}\mathbf{x}_{i}.,"This equation describes the update rule for a state pattern in the Hopfield Network, aligning the query with the most similar stored patterns through a gradient descent step on the energy function."
neurips_2024_oral_17,5,"Moreover, the energy function is closely related to the Transformer's self-attention mechanism [67] (_Appendix_ A.1.2). By extending the energy model from self-attention to cross-attention, we model the dynamics between student and teacher learning patterns. Taking the student representations \(\mathbf{f}_{s}=f(\mathcal{A}(\mathbf{x}))\) as examples, we have \(\mathbf{Q}_{s}=\mathbf{f}_{s}\mathbf{W}_{Q}\) and \(\mathbf{K}_{s}=\mathbf{f}_{s}\mathbf{W}_{K}\). By applying Eq. 3 to key and query matrices, we set energy functions to track the teacher-student relationship:","E(\mathbf{Q}_{s};\mathbf{K}_{t})=\frac{\alpha}{2}\text{diag}( \mathbf{K}_{t}\mathbf{K}_{t}^{T})-\sum_{i=1}^{N}\text{\rm{lse}}(\mathbf{Q}_{s} \mathbf{k}_{t,i}^{T},\mathbf{\beta})+c, || E(\mathbf{K}_{t})=\text{\rm{lse}}(\frac{1}{2}\text{diag}( \mathbf{K}_{t}\mathbf{K}_{t}^{T}),1)=\log\sum_{i=1}^{N}\exp(\frac{ 1}{2}\mathbf{k}_{t,i}\mathbf{k}_{t,i}^{T})+c,",,"E_{\text{teacher-student}} = \frac{1}{2}\mathbf{Q}_{s}^{\top}\mathbf{Q}_{s} - \text{\rm{lse}}(\mathbf{K}_{t}^{\top}\mathbf{Q}_{s}, \mathbf{\beta}) + c,",Energy function modeling the teacher-student relationship using cross-attention between student queries and teacher keys.
neurips_2024_oral_17,6,"\). Intuitively, \(\text{\rm{lse}}(\mathbf{Q}_{s}\mathbf{k}_{t,i}^{T},\mathbf{\beta})\) captures the smooth maximum alignment between student queries \(\mathbf{q}_{s,i}\) and teacher keys \(\mathbf{k}_{t,i}\). Specifically, it nudges each teacher key \(\mathbf{k}_{t,j}\) towards a semantic alignment with its most corresponding student query \(\mathbf{q}_{s,j}\). The regularization term \(\text{diag}(\mathbf{K}_{t}\mathbf{K}_{t}^{T})\) acts as a constraint on the energy levels of teacher's representation \(\mathbf{k}_{t,i}\), guarding against any disproportionate increase during the maximization of \(\text{\rm{lse}}(\mathbf{Q}_{s}\mathbf{k}_{t,i}^{T},\mathbf{\beta})\). This ensures that no individual teacher representation becomes too closely mirrored in the student's representation, maintaining a diverse learning trajectory.

**Insight 3.2**.: _When applying closed-world consistency regularization to the GCD task, it becomes difficult to gradually reduce the energy \(E(\mathbf{Q}_{s};\mathbf{K}_{t})\) as training progresses (Fig. 3 right). The sustained high energy demonstrated a flaw in the previous methods: teachers and students focused on identifying patterns that were inconsistent, leading to divergent learning paths. Specifically, when teachers and students focus on similar patterns (e.g., taillights), energy is reduced, indicating better prediction consistency and effective learning. In contrast, when their attention is distracted, the energy rises, leading to severe inconsistencies in predictions and making the optimization of \(\mathcal{L}_{cons}\) more difficult._

Figure 3: Left: Attention heatmaps for teacher and student across attention layers. Right: Energy trend over epochs, with lower energy indicating less discrepancy in pattern recognition between teacher and student.

FlipClass: Teacher-Student Attention Alignment

### Teacher Attention Update Rule

Based on Insights 3.1 and 3.2, our objective is to minimize the energy function \(E(\mathbf{Q}_{s};\mathbf{K}_{t})\) between teacher and student representations, thereby easing the optimization of \(\mathcal{L}_{\text{cons}}\).

**Theorem 4.1**.: _The minimization can be formulated as obtaining a maximum a posteriori probability (MAP) estimate of teacher keys \(\mathbf{K}_{t}\) given a set of observed student queries \(\mathbf{Q}_{s}\):_","p(\mathbf{K}_{t}|\mathbf{Q}_{s})=\frac{p(\mathbf{Q}_{s}|\mathbf{K}_{t})p( \mathbf{K}_{t})}{p(\mathbf{Q}_{s})},",,"\mathbf{K}_{t}^{\ast}=\arg\max_{\mathbf{K}_{t}}\log p(\mathbf{K}_{t}\mid\mathbf{Q}_{s})=\arg\max_{\mathbf{K}_{t}}\Big{(}\log p(\mathbf{Q}_{s}\mid\mathbf{K}_{t})+\log p(\mathbf{K}_{t})\Big{)},","The optimal teacher keys $\mathbf{K}_{t}^{\ast}$ are obtained by maximizing the posterior probability given the student queries, balancing the likelihood of the student queries given the teacher keys and the prior distribution of the teacher keys."
neurips_2024_oral_17,7,"_where \(p(\mathbf{Q}_{s}|\mathbf{K}_{t})\) and \(p(\mathbf{K}_{t})\) are modeled by energy functions Eq. 5a and 5b, respectively. We approximate the posterior inference by the gradient of the log posterior, estimated as:_","\begin{split}\nabla_{\mathbf{K}_{t}}\log p(\mathbf{K}_{t}| \mathbf{Q}_{s})&=-(\nabla_{\mathbf{K}_{t}}E(\mathbf{Q}_{s}; \mathbf{K}_{t})+\nabla_{\mathbf{K}_{t}}E(\mathbf{K}_{t}))\\ &=sm(\beta\mathbf{Q}_{s}\mathbf{K}_{t}^{T})\mathbf{Q}_ {s}-(\alpha\mathbf{I}+\mathcal{D}(sm(\frac{1}{2}\text{diag}( \mathbf{K}_{t}\mathbf{K}_{t}^{T}))))\mathbf{K}_{t},\end{split}",,"\nabla_{\mathbf{K}_{t}}\log p(\mathbf{K}_{t}|\mathbf{Q}_{s})=\nabla_{\mathbf{K}_{t}}\log p(\mathbf{Q}_{s}|\mathbf{K}_{t})+\nabla_{\mathbf{K}_{t}}\log p(\mathbf{K}_{t}),",The gradient of the log posterior is approximated by summing the gradients from the likelihood and prior terms modeled by energy functions.
neurips_2024_oral_17,8,"_where \(sm(\mathbf{v}):=\exp\left(\mathbf{v}-\text{lse}(\mathbf{v},1)\right)\) and \(\mathcal{D}(\cdot)\) is a vector-to-diagonal-matrix operator. Incorporating Eq. 4, the update rule of teacher keys \(\mathbf{K}_{t}\) is derived as follows:_","\mathbf{K}_{t}^{update}=\mathbf{K}_{t}+\gamma_{update}[\,(sm( \beta\mathbf{K}\mathbf{Q}^{T})\mathbf{Q}\mathbf{W}_{K}^{T})-\gamma_{\text{reg }}(\alpha\mathbf{I}+\mathcal{D}(sm(\frac{1}{2}\text{diag} (\mathbf{K}\mathbf{K}^{T})))\mathbf{K}\mathbf{W}_{K}^{T})\, ],",,\mathbf{K}_{t} \leftarrow \mathbf{K}_{t} + \eta\left( \text{sm}(\beta\mathbf{Q}_{s}\mathbf{K}_{t}^{T})\mathbf{Q}_{s} - (\alpha\mathbf{I} + \mathcal{D}(\text{sm}(\frac{1}{2}\text{diag}(\mathbf{K}_{t}\mathbf{K}_{t}^{T}))))\mathbf{K}_{t} \right).,"This update rule aligns teacher keys with student queries by minimizing the energy function, incorporating both alignment and regularization terms."
neurips_2024_oral_17,9,"_where \(\alpha\), \(\gamma_{update}\) and \(\gamma_{\text{reg}}\) are hyper-parameters._

For a proof, refer to _Appendix_ A.2. The teacher-attention update rule in Theorem 4.1 minimizes an implicit energy function determined by student queries and teacher keys. It serves as using the student queries to search for the most similar teacher patterns in the stored set. As illustrated in Fig. 4, the update rule adjusts the teacher's attention in the direction of student attention, facilitating the retrieval of related patterns and improving semantic alignment. This design establishes a bidirectional information flow: the teacher not only imparts advanced knowledge to the student, but also adjusts guidance based on the student's learning effects, achieving a more cohesive learning dynamic.

### Representation Learning and Parametric Classification

Contrastive learning plus consistency regularization under the parametric paradigm has been demonstrated effective in GCD task [75]. Formally, given two views (random augmentations \(\mathbf{x}_{i}\) and \(\mathbf{x}_{i}^{\prime}\)) of the same image in a mini-batch \(\mathbb{B}\), the supervised and self-supervised contrastive loss is written as:","\mathcal{L}_{\text{rep}}^{s}=\frac{1}{|\mathbb{B}^{\prime}|}\sum_{i \in\mathbb{B}^{\prime}}\frac{1}{|\mathbb{N}_{i}|}\sum_{q\in\mathbb{N}_{i}}- \log\frac{\exp(\mathbf{z}_{i}^{T}z_{q}^{\prime}/\tau_{c})}{\sum_{i^{\prime}\neq i }\exp(\mathbf{z}_{i}^{T}z_{i^{\prime}}^{\prime}/\tau_{c})}, || \mathcal{L}_{\text{rep}}^{u}=\frac{1}{|\mathbb{B}|}\sum_{i\in \mathbb{B}}-\log\frac{\exp(\mathbf{z}_{i}^{T}\mathbf{z}_{i}^{\prime}/\tau_{u} )}{\sum_{i^{\prime}\neq i}\exp(\mathbf{z}_{i}^{T}z_{i^{\prime}}^{\prime}/\tau_ {u})},",,"\mathcal{L}_{\text{contrast}} = \frac{1}{|\mathbb{B}|} \sum_{i=1}^{|\mathbb{B}|} \left[ \log \left(1 + \sum_{j \neq i} \exp(-\lambda (\text{sim}(\mathbf{z}_i, \mathbf{z}_j) - m)) \right) \right],","This contrastive loss encourages similar representations for different views of the same image while pushing apart representations of different images, enhancing discriminative feature learning for both old and new classes in the GCD task."
neurips_2024_oral_17,10,"es. For \(\mathcal{L}_{\text{rep}}^{s}\), \(\mathbb{N}_{i}\) indexes all other images in the same batch that hold the same label as \(\mathbf{x}_{i}\). The representation learning loss is balanced with \(\lambda\): \(L_{\text{rep}}=(1-\lambda)L_{\text{rep}}^{u}+\lambda L_{\text{rep}}^{s}\), where \(\mathbb{B}^{\prime}\) corresponds to the labeled subset of \(\mathbb{B}\).

Figure 4: **Framework of _FlipClass_ demonstrating teacher-student interaction, where teacher’s and student’s attention is aligned by teacher’s updating (Eq. 8). Then \(\mathcal{L}_{\text{rep}}\) and \(\mathcal{L}_{\text{cons}}\) are combined for optimization.

The consistency regularization objectives (Eq. 1) are then simply cross-entropy loss \(\ell(q^{\prime},p)=-\sum_{k}q^{\prime}(k)\log p(k)\) between the predictions and pseudo-labels or ground-truth labels:","\mathcal{L}_{\text{cons}}=\begin{cases}\frac{1}{|\mathbb{B}|}\sum_{i\in\mathbb{B }}\ell(q^{\prime}_{i},p_{i})-\varepsilon H(\bar{p})&\text{for unlabeled,}\\ \frac{1}{|\mathbb{B}|}\sum_{i\in\mathbb{B}}\ell(y_{i},p_{i})&\text{for labeled.} \end{cases}",,"\mathcal{L}_{\text{cons}}^{s}=\frac{1}{|\mathbb{D}_{L}|}\sum_{\mathbf{x}\in \mathbb{D}_{L}}\ell(p_{\text{m}}(y\mid f_{\theta}(\alpha(\mathbf{x}))),y),\quad \mathcal{L}_{\text{cons}}^{u}=\frac{1}{|\mathbb{D}_{U}|}\sum_{\mathbf{x}\in \mathbb{D}_{U}}\ell(p_{\text{m}}(y\mid f_{\theta}(\alpha(\mathbf{x}))),p_{\text{m}}(y\mid f_{\theta}(\mathcal{A}(\mathbf{x})))),","Equation 10 defines the supervised and unsupervised components of the consistency regularization loss using cross-entropy, where $\mathcal{L}_{\text{cons}}^{s}$ compares predictions with ground-truth labels on labeled data and $\mathcal{L}_{\text{cons}}^{u}$ compares pseudo-labels from the teacher model with predictions from the student model on unlabeled data."
neurips_2024_oral_18,1,"## 3 Preliminary

In this section, we briefly review the core idea of 3D Gaussian Splitting (3DGS) [15] and introduce its point-based alpha blending to render depth maps and foreground masks. Typically, 3DGS utilizes 3D Gaussians, each defined by a central point \(\mu_{0}\), a covariance matrix \(\Sigma_{0}\), a density value \(\sigma\), and a color attribute \(c\), to efficiently render images from specific viewpoints. Each point is denoted as","G(x)=\exp(-\frac{1}{2}(x-\mu_{0})^{T}\Sigma_{0}^{-1}(x-\mu_{0})),","where \(\Sigma_{0}\) can be factorized as \(\Sigma_{0}=R_{0}S_{0}S_{0}^{T}R_{0}^{T}\), in which \(R_{0}\) is a rotation matrix represented by a quaternion vector \(r_{0}\in\mathbb{R}^{4}\), and \(S_{0}\) is a a diagonal scaling matrix characterized by a 3D vector \(s_{0}\in\mathbb{R}^{3}\)","\begin{equation}
\mathcal{P} = \left\{ \mu_0, \Sigma_0, \sigma, c \right\}
\end{equation}","A 3D Gaussian point represented by its central position $\mu_0$, covariance matrix $\Sigma_0$, density $\sigma$, and color $c$."
neurips_2024_oral_18,2,"\). If we consider isotropic Gaussian representation, the scaling matrix can be written as \(s_{0}I\), where \(s_{0}\) is a scalar and \(I\) is the identity matrix. When performing splatting, the 3D Gaussians are projected into 2D with the covariance matrix defined as \(\Sigma_{0}^{\prime}=JW\Sigma_{0}W^{T}J^{T}\), where \(J\) is the Jacobian of affine approximation of the projective transformation [49], and \(W\) is the viewing transformation matrix. The rendered color \(I(u)\) with its foreground mask \(A(u)\) at pixel \(u\) are then evaluated by integrating \(N\) ordered slatted Gaussians via the point-based alpha blending. Since the depth of each Gaussian point at a specific view can be obtained according to its transformation matrix, we can further render the depth map \(D\) using the same blending method [16; 50], as","I(u)=\sum_{i\in N}T_{i}\alpha_{i}c_{i},\qquad A(u)=\sum_{i\in N}T_{i}\alpha_{i },\qquad D(u)=\sum_{i\in N}T_{i}\alpha_{i}d_{i},","where \(T_{i}=\prod_{j=1}^{i-1}(1-\alpha_{j})\) is the accumulated transmittance, \(\alpha_{i}\) is the probability of termination at point \(i\), and \(d_{i}\) is the depth of the Gaussian point at the specific view","D(u) = \frac{\sum_{i=1}^{N} G_i(x_i) \cdot d_i \cdot \prod_{j=1}^{i-1} (1 - A_j(u))}{\sum_{i=1}^{N} G_i(x_i) \cdot \prod_{j=1}^{i-1} (1 - A_j(u))},","where $d_i$ denotes the depth of the $i$-th Gaussian point, and the blending weights are determined by the transparency of preceding points in the rendering order."
neurips_2024_oral_18,3,"ew.

## 4 Method

### Problem Definition and Overview

In this work, we aim to reconstruct the geometries and the physical properties of various object types from multi-view videos. Formally, given a set of video sequences \(\{V_{i}|i=1...n\}\) with moving object and the corresponding camera extrinsic and intrinsic parameters \(\{(T_{i},K_{i})|i=1...n\}\), the goal of this task is to recover the explicit geometries of the object represented by continuum particles \(P(t)\) and its corresponding physical parameters \(\Theta\) (e.g., Young's modulus \(E\) and Poisson's ratio \(\nu\) for elastic objects). We follow the assumption in PAC-NeRF and PhysGaussian [12; 51] that the object types (e.g., elastic, granular, Newtonian/non-Newtonian, plastic) are known and the physical phenomenon follows continuum mechanics [52; 17].

The overview of the proposed pipeline is illustrated in Fig. 1, which consists of three modules: a motion-factorized dynamic 3D Gaussian network (Sec. 4.2) for 4D reconstruction of the object, a coarse-to-fine density field generation strategy (Sec. 4.3) for continuum generation, surface extraction, and Gaussian attribute assignment, and a procedure (Sec. 4.4) showing how we leverage Gaussian-informed continuum and extracted surfaces to estimate physical properties.

### Motion-factorized Dynamic 3D Gaussian Network

Our dynamic 3D Gaussian network follows existing frameworks [16; 29; 30] that simultaneously maintain a canonical 3D Gaussian set and a deformation field modeled by a neural network to warp the canonical shape into object states at specific times. The core idea of this pipeline, presented in Fig. 2, is that the motion of every point in the object can be decomposed into a small range of motion bases.

**Architecture**. We first factorize the entire motion into \(N_{m}\) bases that are modeled by a fully connected neural network, where every basis shares a common backbone except the final layer. The output of each basis consists of the deformations at position \(d\mu_{i}(t)\in\mathbb{R}^{3}\) and at scale \(ds_{i}(t)\in\mathbb{R}\). To model the exact deformation for each position, we next propose a lightweight coefficient network that maps the positions at canonical space with specific time to their corresponding motion coefficients \(w(\mu_{0},t)\in\mathbb{R}^{N_{m}}\). Therefore, the deformed position and the scale for each Gaussian point are evaluated by the linear combination of the motion basis according to the motion coefficients:","\mu(t)=\mu_{0}+\sum_{i=1}^{N_{m}}w_{i}(\mu_{0},t)d\mu_{i}(t),\qquad s(t)=s_{0} +\sum_{i=1}^{N_{m}}w_{i}(\mu_{0},t)ds_{i}(t).",,"\mu(t)=\mu_{0}+\sum_{i=1}^{N_{m}}w_{i}(\mu_{0},t)d\mu_{i}(t),\qquad s(t)=s_{0}+\sum_{i=1}^{N_{m}}w_{i}(\mu_{0},t)ds_{i}(t),","where $\mu(t)$ and $s(t)$ are the deformed position and scale of the Gaussian point at time $t$, computed as a linear combination of motion bases weighted by motion coefficients $w_i(\mu_0, t)$."
neurips_2024_oral_18,4,"In this work, we regard all the Gaussians as isotropic kernels, which has been demonstrated as an effective way to simplify the model and better reconstruct the scene [6; 53]. We should note that

Figure 1: Overview. (a) **Continuum Generation:** Given a series of multi-view images capturing a moving object, the motion-factorized dynamic 3D Gaussian network is trained to reconstruct the dynamic object as 3D Gaussian point sets across different time states. From the reconstructed results, we employ a coarse-to-fine strategy to generate density fields to recover the continuums and extract object surfaces. The continuum is endowed with Gaussian attributes to allow mask rendering. (b) **Identification:** The MPM simulates the trajectory with the initial continuum \(\mathbb{P}(0)\) and the physical parameters \(\Theta\). The simulated object surfaces and the rendered masks are then compared against the previously extracted surfaces (colored in blue) and the corresponding masks from the dataset. The differences are quantified to guide the parameter estimation process. (c) **Simulation:** Digital twin demonstrations are displayed. Simulated objects (colored by stress increasing from blue to red), characterized by the properties estimated from observation, exhibit behavior consistent with real-world objects.

although previous works [29; 54] also perform motion decomposition modeling, our pipeline shows two major differences: 1) instead of modeling each basis with an independent neural network, our module shares a common backbone. Our key observation is that for reconstructing a dynamic object, all points on the object should follow a similar moving tendency, and the final heads of the neural network are sufficient to model the details of different parts of the object; 2) to increase the ability to fit high rank of the dynamic scene [16], we model the motion coefficients as time-variant variables rather than constant Gaussian attributes [29].

**Optimization**. We employ the same setting in [16] to train our pipeline. Concretely, the canonical 3D Gaussians are initialized with points randomly sampled from the given bounding box of the scene. We start training the deformation network after 3,000 iterations of warm-up for the 3D Gaussians. Similar to previous works [16; 29], we optimize the pipeline by computing the L1 norm and Structural Similarity Index Measure (SSIM) between the rendered image \(I\) and the ground truth image \(\tilde{I}\). Moreover, since large scales may lead to inaccurate reconstructed shapes [55], we thus perform L1 norm on the scale attributes of all the points to recover more fine-grand shapes of the object. Therefore, the overall loss function is defined as:","\mathcal{L}_{gs}=\mathcal{L}_{1}(I,\tilde{I})+\lambda_{1}\mathcal{L}_{ssim}(I,\tilde{I})+\lambda_{2}\mathcal{L}_{1}(s(t)),",where \(\lambda_{1}\) and \(\lambda_{2}\) are balancing hyperparameters,"\mathcal{L} = \lambda_{1}\mathcal{L}_{\text{L1}}(I, \tilde{I}) + \lambda_{2}\mathcal{L}_{\text{SSIM}}(I, \tilde{I}) + \lambda_{3}\mathcal{L}_{\text{scale}},","where $\mathcal{L}_{\text{L1}}$ and $\mathcal{L}_{\text{SSIM}}$ are the image reconstruction losses, and $\mathcal{L}_{\text{scale}}$ regularizes the scale attributes of the Gaussians to improve shape accuracy."
neurips_2024_oral_18,5,"rs. More in-depth analysis of the proposed pipeline, including implementation details and effects of scale regularization, are presented in Appendix A.1.

### Gaussian-informed Continuum Generation

**Coarse-to-fine density field generation**. Since the reconstructed Gaussian particles are served for rendering only, meaning that they are not evenly distributed on the objects, they cannot be directly used for simulation [51]. Therefore, we propose a novel coarse-to-fine filling strategy to iteratively generate density fields of the object based on the reconstructed Gaussian particles from Eqn. 3 and the internal particles filtered by the rendered depth maps. The proposed strategy is presented in Alg. 1. The implementation details and visual results are illustrated in Appendix A.2.

Concretely, the internal particles, initialized by uniform sampling from the bounding box of Gaussian particles, are filtered by projecting the particles to various images to compare the projected depth with rendered depth values (lines 1-6 in Alg. 1). The resulting particles can roughly represent the shape of the object. However, as denoted in Eqn. 2, the rendered depth maps are evaluated in an accumulated manner, making them less precise in representing the object surface.

Therefore, We employ a coarse-to-fine filling strategy by iteratively upsampling the density field and reassigning the densities on the indices computed from both the Gaussian and internal particles (lines 8-16 in Alg. 1). Fig. 3 provides a sketch illustration of the proposed strategy. Specifically, due to the large grid size at the initial stage, the object is completely inside the voxels with high densities. Next, we sequentially perform upsampling (line 10), mean filtering (line 13), and reassigning the

Figure 2: The pipeline of the proposed dynamic 3D Gaussian network. The motion network backbone consists of 8 fully connected (FC) layers. The output of the motion block is fed to \(N_{m}\) heads to generate motion residuals. The coefficient network contains 4 FC layers.

field (line 14) at each iteration. The first two operations produce more fine-grained shapes, and the reassigning operation ensures high densities at the surface to avoid over-erosion caused by the first two steps. Finally, the continuum particles with the corresponding object surfaces can be extracted by thresholding the density field (lines 16-17 in Alg. 1).

```
0: Gaussian particles at time \(t\): \(\mathbb{P}_{G}(t)=\{(\mu(t),s(t),\sigma,c)\}\); \(n\) pairs of camera extrinsic and intrinsic parameters: \(\{(\tilde{T}_{i},K_{i})|i=1...n\}\); parameters: grid size \(\Delta x\); number of upsampling steps \(n_{u}\); thresholds \(th_{min}\), \(th_{min}\);
0: Continuum particles \(\tilde{P}(t)\) and the corresponding surface \(\tilde{S}(t)\);
1: Randomly sample an initial particle set \(P_{in}\) from the bounding box of \(\{\mu(t)\}\);
2:for\(i\gets 1,n\)do
3:\(\tilde{D}_{i}=GaussianSplitting(\mathbb{P}_{G}(t),T_{i},K_{i})\); \(\triangleright\) render depth map at view \(i\)
4:\((u_{in},v_{in}),d_{in}\gets Proj(P_{in},T_{i},K_{i})\); \(\triangleright\) obtain image indices and depths of \(P_{in}\) at view \(i\)
5:\(P_{in}\gets P_{in}[\tilde{D}_{i}(u_{in},v_{in})\leq d_{in}]\); \(\triangleright\) filter out particles that are outside the object
6:endfor
7: initialize the zero-value density field \(F(t)\) with \(\Delta x\) and the bounding box of \(\{\mu(t)\}\);
8:for\(j\gets 1,n_{u}\)do
9:if\(j\neq 1\)then
10:\(F(t)-TrilinearInterpolation(F(t),2)\)\(\triangleright\) upsample \(F(t)\) with scale factor 2
11:\(F(t)[p,q,r]=1\), where \(p,q,r\gets Discretize(P_{in}\cup\{\mu(t)\})\);
12:endif
13:\(F(t)\gets MeanFiltering(F(t))\);
14:\(F(t)[p,q,r]=1\), where \(p,q,r\gets Discretize(P_{in}\cup\{\mu(t)\})\);
15:endfor
16:\(\hat{P}(t)\gets GetPosition(th_{min}\leq F(t))\);
17:\(\tilde{S}(t)\gets GetPosition(th_{min}\leq F(t)\leq th_{max})\);
```

**Algorithm 1** Pseudo code for coarse-to-fine filling

**Gaussian-informed continuum**. In PAC-NeRF, the particles are equipped with appearance features to enable image rendering for the continuum at different states. We can also achieve this function by treating the particles as Gaussian kernels and re-train the particles using the visual data. However, this process is cumbersome and will also face the same issue in PAC-NeRF where distorted RGB images will be rendered when large deformation occurs. Therefore, instead of injecting appearance attributes, we opt to assign density and scale attributes to the particles where the densities originate from the density field, and the scale attributes can be directly obtained by the field grid size. The Gaussian-informed continuum is defined as a set of triplets:","\mathbb{P}_{\tilde{P}}=\{(\tilde{p},s_{\Delta x},\sigma_{F})\},","where \(\tilde{p}\in\tilde{P}\), \(s_{\Delta x}=\Delta x/2^{n_{u}}\), and \(\sigma_{F}=F[Discretize(\tilde{p})]\) (we neglect \(t\) in the notation for simplicity)","\mathbb{P}(t)=\{(\hat{p}_{i},\hat{\sigma}_{i},\hat{s}_{i})|\hat{\sigma}_{i}=F(t)[p,q,r],\hat{s}_{i}=\Delta x\},","where $\hat{p}_{i}$ denotes the position of continuum particle $i$, $\hat{\sigma}_{i}$ is its density derived from the density field $F(t)$, and $\hat{s}_{i}$ is its scale attribute determined by the grid size $\Delta x$"
neurips_2024_oral_18,6,"y). Therefore, we only render object masks as 2D shape surrogates for supervision.

Figure 3: Sketch illustration of the coarse-to-fine filling strategy. Gaussian and internal particles are depicted in green and blue, respectively. (a) Voxels containing particles are assigned high densities. (b) Following the upsampling and smoothing of the field, densities near boundaries become blurred (indicated in light yellow). (c) The particles are again used to correct the voxels that contain particles with high densities. (d) and (e) repeat the previous operations to achieve a more detailed shape.

### Geometry-aware Physical Property Estimation

With the Gaussian-informed continuum at initial state \(\mathbb{P}_{\hat{P}}(0)\) and the extracted surfaces \(\tilde{S}(t)\) in place, we can employ MPM to perform simulation on the continuum and evaluate the difference in terms of both the 3D and 2D shapes. Concretely, after a rollout by MPM given the current estimation of physical parameters, we obtain a trajectory \(P(t)\) with corresponding object surfaces \(S(t)\). We thus can render object masks over the trajectory. Then the loss of the current rollout can be computed as:","\mathcal{L}_{ppe}=\frac{1}{m}\sum_{i=1}^{m}[\mathcal{L}_{CD}(S(t_{i}),\tilde{S} (t_{i}))+\frac{1}{n}\sum_{j=1}^{n}\mathcal{L}_{1}(A_{j}(t_{i}),\bar{A}_{j}(t_{ i}))],","where \(\mathcal{L}_{CD}\) and \(\mathcal{L}_{1}\) are chamfer distance and L1 norm respectively, \(S(t_{i})\) denotes the simulated surface at time \(t_{i}\), \(A_{j}(t_{i})\) is the rendered mask at view \(j\), and \(\bar{A}_{j}(t_{i})\) represents the object mask of the image extracted from video \(V_{j}\) at time \(t_{i}\)","\mathcal{L}_{phy} = \sum_{t} \left( \| S(t) - \tilde{S}(t) \|_1 + \lambda_3 \| A(t) - \tilde{A}(t) \|_1 \right),","where $S(t)$ and $\tilde{S}(t)$ are the simulated and extracted surfaces at time $t$, $A(t)$ and $\tilde{A}(t)$ are the rendered and ground truth object masks, and $\lambda_3$ is a balancing hyperparameter."
neurips_2024_oral_2,1,"## 2 _Aligner_

Preliminary: Supervised Fine-Tuning (SFT)SFT aims to finetune the pretrained LLM to generate target answers using supervised learning -- specifically, maximum likelihood estimation -- on a curated high-quality dataset \(\mathcal{D}_{\text{SFT}}=\{\mathbf{x}^{(i)},\mathbf{y}^{(i)}\}_{i=1}^{N}\). The goal is to obtain a model \(\pi_{\mathbf{\theta}}^{\text{SFT}}\) with the following training objective:","\operatorname*{minimize}_{\mathbf{\theta}}\mathcal{L}(\mathbf{\theta};\mathcal{D}_{ \text{SFT}})=-\mathbb{E}_{(\mathbf{x},\mathbf{y})\sim\mathcal{D}_{\text{SFT}}}[\log \pi_{\mathbf{\theta}}(\mathbf{y}|\mathbf{x})].",,"\begin{equation}
\theta^{\text{SFT}} = \argmax_{\theta} \frac{1}{N} \sum_{i=1}^{N} \log \pi_{\theta}^{\text{SFT}}(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)})
\end{equation}","The maximum likelihood estimation objective for supervised fine-tuning, which maximizes the average log-probability of target answers given input prompts under the model parameters θ."
neurips_2024_oral_2,2,"Similarly, illustrated in Figure 1, _Aligner_ improves alignment between the model and human intentions by redistributing the model's answers through conditional generation. In practical implementation, _Aligner_ only needs to make a minor adjustment to the SFT training code (only need to change one line of code), as detailed in Appendix D.

Overall, the whole pipeline of _Aligner_ training can be summarized as follows: Based on a preference dataset, the model is fine-tuned to learn the correctional residuals between preferred and non-preferred responses. After a single training session, this model can be deployed on any model to achieve corrected alignment.

Model TrainingBased on the above procedures, we have constructed the dataset \(\mathcal{M}=\{\mathbf{x}^{(i)},\mathbf{y}_{o}^{(i)},\mathbf{y}_{c}^{(i)}\}_{i=1}^{N}\), which \(\mathbf{x}\) represents the user's query, \(\mathbf{y}_{o}\) is the original answer, and \(\mathbf{y}_{c}\) is the corrected answer according to established principles. The model training process is relatively straightforward. We train the _Aligner_, a conditional seq2seq model \(\mu_{\mathbf{\phi}}(\mathbf{y}_{c}|\mathbf{y}_{o},\mathbf{x})\) parameterized by \(\mathbf{\phi}\), to redistribute the preliminary answers \(\mathbf{y}_{o}\) to the aligned answer \(\mathbf{y}_{c}\). Demonstrated in Figure 1, the composed answer generation process for aligned answers based on the upstream LLM \(\pi_{\mathbf{\theta}}\) is:","\pi^{\prime}(\mathbf{y}_{c}|\mathbf{x})=\sum_{\mathbf{y}_{k}}\mu_{\mathbf{\phi}}(\mathbf{y}_{c}| \mathbf{y}_{k},\mathbf{x})\pi_{\mathbf{\theta}}(\mathbf{y}_{k}|\mathbf{x})\geqslant\mu_{\mathbf{\phi}} (\mathbf{y}_{c}|\mathbf{y}_{o},\mathbf{x})\pi_{\mathbf{\theta}}(\mathbf{y}_{o}|\mathbf{x}),",where \(\mathbf{y}_{k}\) is a possible answer generated by upstream LLM \(\pi_{\mathbf{\theta}}\),"\operatorname*{minimize}_{\mathbf{\phi}}\mathcal{L}(\mathbf{\phi};\mathcal{M})=-\mathbb{E}_{(\mathbf{x},\mathbf{y}_o,\mathbf{y}_c)\sim\mathcal{M}}[\log \mu_{\mathbf{\phi}}(\mathbf{y}_c|\mathbf{y}_o,\mathbf{x})].",Aligner's training objective minimizes the negative log-likelihood of generating corrected answers $\mathbf{y}_c$ conditioned on original answers $\mathbf{y}_o$ and queries $\mathbf{x}$ over dataset $\mathcal{M}$.
neurips_2024_oral_2,3,"\). By calculating empirical loss on the whole dataset \(\mathcal{M}\), we can get equation (3) from equation (2):","-\mathbb{E}_{\mathcal{M}}[\log\pi^{\prime}(\mathbf{y}_{c}|\mathbf{x})]\leqslant- \mathbb{E}_{\mathcal{M}}[\log\mu_{\mathbf{\phi}}(\mathbf{y}_{c}|\mathbf{y}_{o},\mathbf{x})]- \mathbb{E}_{\mathcal{M}}[\log\pi_{\mathbf{\theta}}(\mathbf{y}_{o}|\mathbf{x})].",,"\mathcal{L}(\mathbf{\phi};\mathcal{M})=-\mathbb{E}_{(\mathbf{x},\mathbf{y}_{o},\mathbf{y}_{c})\sim\mathcal{M}}[\log\mu_{\mathbf{\phi}}(\mathbf{y}_{c}|\mathbf{y}_{o},\mathbf{x})].","Equation (3) represents the empirical loss function for training the _Aligner_ model $\mu_{\mathbf{\phi}}$ on the dataset $\mathcal{M}$, derived from the conditional generation process in Equation (2)."
neurips_2024_oral_2,4,The second term in equation (3) is not related to the _Aligner_ parameter and the training objective for _Aligner_ can be derived as equation (4):,"\operatorname*{minimize}_{\mathbf{\phi}}\mathcal{L}_{\text{Aligner}}(\mathbf{\phi}, \mathcal{M})=-\mathbb{E}_{\mathcal{M}}[\log\mu_{\mathbf{\phi}}(\mathbf{y}_{c} |\mathbf{y}_{o},\mathbf{x})].",,"\operatorname*{minimize}_{\mathbf{\phi}}\mathcal{L}(\mathbf{\phi};\mathcal{M})=-\mathbb{E}_{\mathcal{M}}[\log\mu_{\mathbf{\phi}}(\mathbf{y}_{c}|\mathbf{y}_{o},\mathbf{x})].","Equation (4) represents the training objective for the _Aligner_ model, which minimizes the negative log-likelihood of generating the corrected answer $\mathbf{y}_{c}$ given the original answer $\mathbf{y}_{o}$ and the input $\mathbf{x}$."
neurips_2024_oral_21,1,"### Diffusion Model on Graph Data

Diffusion models consist of forward and reverse diffusion processes [17]. We refer to the forward diffusion process as the diffusion process following [17]. The diffusion process \(q(G^{1:T}\mid G^{0})=\prod_{t=1}^{T}q(G^{t}\mid G^{t-1})\) corrupts molecular graph data (\(G^{0}=G\)) into noisy states \(G^{t}\). As timesteps \(T\to\infty\), \(q(G^{T})\) converges a stationary distribution \(\pi(G)\). The reverse Markov process \(p_{\theta}(G^{0:T})=q(G^{T})\prod_{t=1}^{T}p_{\theta}(G^{t-1}\mid G^{t})\), parameterized by neural networks, gradually denoises the latent states toward the desired data distribution.

Diffusion ProcessOne may perturb \(G^{t}\) in a discrete state-space to capture the structural properties of molecules [43]. Two transition matrices \(\mathbf{Q}_{V}\in\mathbb{R}^{F_{V}\times F_{V}}\) and \(\mathbf{Q}_{E}\in\mathbb{R}^{F_{E}\times F_{E}}\) are defined for nodes \(\mathbf{X}_{V}\) and edges \(\mathbf{X}_{E}\), respectively [43]. Then, each step \(q(G^{t}\mid G^{t-1},G^{0})=q(G^{t}\mid G^{t-1})\) in the diffusion process is sampled as follows.","\begin{cases}q(\mathbf{X}_{V}^{t}\mid\mathbf{X}_{V}^{t-1})=\mathrm{Cat}( \mathbf{X}_{V}^{t};\mathbf{p}=\mathbf{X}_{V}^{t-1}\mathbf{Q}_{V}^{t}),\\ q(\mathbf{X}_{E}^{t}\mid\mathbf{X}_{E}^{t-1})=\mathrm{Cat}(\mathbf{X}_{E}^ {t};\mathbf{p}=\mathbf{X}_{E}^{t-1}\mathbf{Q}_{E}^{t}),\end{cases}",where \(\mathrm{Cat}(\mathbf{X};\mathbf{p})\) denotes sampling from a categorical distribution with probability \(\mathbf{p}\),"tags. Let me draft that.
</think>","The transition probability for graph diffusion is defined as the product of node-wise and edge-wise transitions parameterized by matrices $\mathbf{Q}_{V}$ and $\mathbf{Q}_{E}$, where $X_{V,i}^{t}$ denotes node features and $X_{E,kl}^{t}$ denotes edge features at timestep $t$."
neurips_2024_oral_21,2,"\). We remove the subscript (\({}_{V/E}\)) when the description applies to both nodes and edges. It is assumed that the noise \(\mathbf{Q}^{i}\) (\(i\leq t\)) is independently applied to \(\mathbf{X}\) in each step \(i\), allowing us to rewrite \(q(\mathbf{X}^{t}\mid\mathbf{X}^{t-1})\) as the probability of the initial state \(q(\mathbf{X}^{t}\mid\mathbf{X}^{0})=\mathrm{Cat}\left(\mathbf{X}^{t};\mathbf{ p}=\mathbf{X}^{0}\bar{\mathbf{Q}}^{t}\right)\), where \(\bar{\mathbf{Q}}^{t}=\prod_{i\leq t}\mathbf{Q}^{i}\).

Noise SchedulingTransition matrices \(\mathbf{Q}_{V}\) and \(\mathbf{Q}_{E}\) control the noise applied to atom features and bond features, respectively. Vignac et al. [43] defined \(\pi(G)=(\mathbf{m}_{X}\in\mathbb{R}^{F_{V}},\mathbf{m}_{E}\in\mathbb{R}^{F_{E}})\) as the marginal distributions of atom types and bond types. The transition matrix at timestep \(t\) is \(\mathbf{Q}^{t}=\alpha^{t}\mathbf{I}+(1-\alpha^{t})\mathbf{1}\mathbf{m}^{\prime}\) for atoms or bonds, where \(\mathbf{m}^{\prime}\) denotes the transposed row vector. Therefore, we have \(\bar{\mathbf{Q}}^{t}=\bar{\alpha}^{t}\mathbf{I}+(1-\bar{\alpha}^{t})\mathbf{1 }\mathbf{m}^{\prime}\), where \(\bar{\alpha}^{t}=\prod_{\tau=1}^{t}\alpha^{\tau}\). The cosine schedule [32] is often chosen for \(\bar{\alpha}^{t}=\cos(0.5\pi(t/T+s)/(1+s))^{2}\).

Reverse ProcessWith the initial sampling \(G^{T}\sim\pi(G)\), the reverse process generates \(G^{0}\) iteratively in reversed steps \(t=T,T-1,\ldots,0\). We use a neural network to predict the probability \(p_{\theta}(\tilde{G}^{0}\mid G^{t})\) as the product over nodes and edges [1, 43]:",p_{\theta}(\tilde{G}^{0}\mid G^{t})=\prod_{v\in V}p_{\theta}(v^{t-1}\mid G^{t}) \prod_{e\in E}p_{\theta}(e^{t-1}\mid G^{t}),,"\begin{cases}p_{\theta}(\mathbf{X}_{V}^{t-1}\mid\mathbf{X}_{V}^{t},\mathbf{X}_{V}^{0})=\mathrm{Cat}(\mathbf{X}_{V}^{t-1};\mathbf{p}=\mathbf{X}_{V}^{t}(\bar{\mathbf{Q}}^{t-1}/\bar{\mathbf{Q}}^{t})^{\top}+\mathbf{X}_{V}^{0}(1-\bar{\mathbf{Q}}^{t-1}/\bar{\mathbf{Q}}^{t})^{\top}),\\ p_{\theta}(\mathbf{X}_{E}^{t-1}\mid\mathbf{X}_{E}^{t},\mathbf{X}_{E}^{0})=\mathrm{Cat}(\mathbf{X}_{E}^{t-1};\mathbf{p}=\mathbf{X}_{E}^{t}(\bar{\mathbf{Q}}^{t-1}/\bar{\mathbf{Q}}^{t})^{\top}+\mathbf{X}_{E}^{0}(1-\bar{\mathbf{Q}}^{t-1}/\bar{\mathbf{Q}}^{t})^{\top}),\end{cases}",where the reverse transition probabilities are defined using the ratio of cumulative transition matrices and the initial state prediction $\tilde{G}^{0}$.
neurips_2024_oral_21,3,"\(p_{\theta}(\tilde{G}^{0}\mid G^{t})\) could be combined with \(q(G^{t-1}\mid G^{t},G^{0})\) to estimate the reverse distribution on the graph \(p_{\theta}(G^{t-1}\mid G^{t})\). For example, \(p_{\theta}(v^{t-1}\mid G^{t})\) is marginalized over predictions of node types \(\tilde{v}\in\tilde{\mathbf{x}}_{v}\), which applies similarly to edges:","p_{\theta}(v^{t-1}\mid G^{t})=\sum_{\tilde{v}\in\tilde{\mathbf{x}}_{v}}q(v^{t-1 }\mid\tilde{v},G^{t})p_{\theta}(\tilde{v}\mid G^{t}).",,"\begin{aligned} p_{\theta}(v^{t-1}\mid G^{t}) = \sum_{\tilde{v}\in\tilde{\mathbf{x}}_{v}} q(v^{t-1}\mid v^{t},\tilde{v}) p_{\theta}(\tilde{v}\mid G^{t}) \end{aligned}","Equation 3 defines the reverse process distribution $p_{\theta}(v^{t-1} \mid G^{t})$ by marginalizing over predicted node types $\tilde{v}$, combining the forward transition $q(v^{t-1} \mid v^{t}, \tilde{v})$ with the neural network prediction $p_{\theta}(\tilde{v} \mid G^{t})$."
neurips_2024_oral_21,4,The neural network could be trained to minimize the negative log-likelihood [43].,L=\mathbb{E}_{q(G^{0})}\mathbb{E}_{q(G^{t}|G^{0})}[-\mathbb{E}_{\mathbf{x} \in G^{0}}\log p_{\theta}(\mathbf{x}\mid G^{t})],,"\mathcal{L} = -\mathbb{E}_{G^{0},t}\left[\log p_{\theta}(G^{0}\mid G^{t})\right]","where the loss function $\mathcal{L}$ is defined as the expected negative log-likelihood of the true data $G^{0}$ given the noisy state $G^{t}$, with the expectation taken over the data distribution and timesteps $t$."
neurips_2024_oral_21,5,"## 3 Multi-Conditional Graph Diffusion Transformers

We present the denoising framework of Graph DiT in Figure 2. The condition encoder learns the representation of \(M\) conditions. The statistics of this representation like mean and variance are used to replace the ones from the molecular representations [19] (see Section 3.2). Besides, we introduce a new noise model in the diffusion process to better fit graph-structured molecules (see Section 3.1).

### Graph-Dependent Noise Models

The transition probability of a node or an edge should rely on the joint distribution of nodes and edges in the prior state. However, as an example shown in Eq. (1), current diffusion models [22; 43; 25] treat node and edge state transitions as independent, misaligning with the denoising process in Eq. (3). This difference between the sampling distributions of noise in the diffusion and reverse processes introduces unnecessary challenges to multi-conditional molecular generations.

To address this, we use a single matrix \(\mathbf{X}_{G}\in\mathbb{R}^{N\times F_{G}}\) to represent graph tokens for \(G\), with \(F_{G}=F_{V}+N\cdot F_{E}\). Token representations are created by concatenating the node feature matrix \(\mathbf{X}_{V}\) and the flattened edge connection matrix from \(\mathbf{X}_{E}\). Each row vector in \(\mathbf{X}_{G}\) contains features for both nodes and edges, representing all connections and non-connections. Hence, we could design a transition matrix \(\mathbf{Q}_{G}\) considering the joint distribution of nodes and edges. \(\mathbf{Q}_{G}\in\mathbb{R}^{F_{G}\times F_{G}}\) is constructed from four matrices \(\mathbf{Q}_{V},\mathbf{Q}_{EV}\in\mathbb{R}^{F_{E}\times F_{V}},\mathbf{Q}_{ E},\mathbf{Q}_{VE}\in\mathbb{R}^{F_{V}\times F_{E}}\), denoting the transition probability (""dependent old state"" \(\rightarrow\) ""target new state"") node \(\rightarrow\) node; edge \(\rightarrow\) edge; node \(\rightarrow\) edge, respectively.","\mathbf{Q}_{G}=\begin{bmatrix}\mathbf{Q}_{V}&\mathbf{1}_{N}^{\prime}\otimes \mathbf{Q}_{VE}\\ \mathbf{1}_{N}\otimes\mathbf{Q}_{EV}&\mathbf{1}_{N\times N}\otimes\mathbf{Q}_{ E}\end{bmatrix},","where \(\otimes\) denotes the Kronecker product, \(\mathbf{1}_{N}\), \(\mathbf{1}_{N}^{\prime}\), and \(\mathbf{1}_{N\times N}\) represent the column vector, row vector, and matrix with all 1 elements, respectively",\mathbf{Q}_{G}=\begin{bmatrix}\mathbf{Q}_{V} & \mathbf{Q}_{VE}^{\top} & \cdots & \mathbf{Q}_{VE}^{\top}\\ \mathbf{Q}_{EV} & \mathbf{Q}_{E} & \cdots & \mathbf{0}\\ \vdots & \vdots & \ddots & \vdots\\ \mathbf{Q}_{EV} & \mathbf{0} & \cdots & \mathbf{Q}_{E} \end{bmatrix},Transition matrix $\mathbf{Q}_{G}$ for graph tokens that captures joint node-edge dependencies in the diffusion process.
neurips_2024_oral_21,6,"ly. According to Eq. (5), the first \(F_{V}\) columns in \(\mathbf{Q}_{G}\) determine the node feature transitions based on both node features (first \(F_{V}\) rows) and edge features (remaining \(N\cdot F_{E}\) rows). Conversely, the remaining \(N\cdot F_{E}\) columns determine the edge feature transitions, depending on the entire graph. We introduce a new diffusion noise model:","q(\mathbf{X}_{G}^{t}\mid\mathbf{X}_{G}^{t-1})=\widetilde{\mathrm{Cat}}( \mathbf{X}_{G}^{t};\tilde{\mathbf{p}}=\mathbf{X}_{G}^{t-1}\mathbf{Q}_{G}^{t} ),","where \(\tilde{\mathbf{p}}\) is the unnormalized probability and \(\widetilde{\mathrm{Cat}}\) denotes categorical sampling: The first \(F_{V}\) columns of \(\tilde{\mathbf{p}}\) are normalized to sample \(\mathbf{X}_{V}^{t}\), while the remaining \(N\cdot E\) dimensions are reshaped and normalized to sample edges \(\mathbf{X}_{E}^{t}\)",q(\mathbf{X}_{G}^{t} \mid \mathbf{X}_{G}^{0}) = \mathrm{Cat}(\mathbf{X}_{G}^{t}; \mathbf{p} = \mathbf{X}_{G}^{0} \bar{\mathbf{Q}}_{G}^{t}),"where $\mathbf{Q}_{G}^{t}$ is the transition matrix at timestep $t$, and $\bar{\mathbf{Q}}_{G}^{t} = \prod_{i \leq t} \mathbf{Q}_{G}^{i}$ represents the cumulative transition matrix up to timestep $t$"
neurips_2024_oral_21,7,"\). These components are combined to form \(\mathbf{X}_{G}^{t}\), completing the \(\widetilde{\mathrm{Cat}}\) sampling.

Choice of \(\mathbf{Q}_{VE}\) and \(\mathbf{Q}_{EV}\)Similar to the definitions of \(\mathbf{m}_{V}\) and \(\mathbf{m}_{E}\)[43], we leverage the prior knowledge within the training data for the formulation of task-specific matrices, \(\mathbf{Q}_{EV}\) and \(\mathbf{Q}_{VE}\). We calculate co-occurrence frequencies of atom and bond types in training molecular graphs to obtain the marginal atom-bond co-occurrence probability distribution. For each bond type, each row in \(\mathbf{m}_{EV}\) represents the probability of co-occurring atom types. \(\mathbf{m}_{VE}\) is the transpose of \(\mathbf{m}_{EV}\) and has a similar meaning. Subsequently, we define \(\mathbf{Q}_{EV}=\bar{\alpha}^{t}\mathbf{I}+(1-\bar{\alpha}^{t})\mathbf{1} \mathbf{m}_{EV}^{\prime}\) and \(\mathbf{Q}_{VE}=\bar{\alpha}^{t}\mathbf{I}+(1-\bar{\alpha}^{t})\mathbf{1} \mathbf{m}_{VE}^{\prime}\).

Figure 2: Denoising framework and architectures for Graph DiT. Details are in Section 3.2.

### Denoising Models with Multi-Property Conditions

We present Graph DiT as the denoising model to generate molecules under multi-conditions \(\mathcal{C}=\{c_{1},c_{2},\ldots,c_{M}\}\) without extra predictors.

Predictor-Free GuidanceThe predictor-free reverse process \(\hat{p}_{\theta}(G^{t-1}\mid G^{t},\mathcal{C})\) aims to generate molecules with a high probability \(q(\mathcal{C}\mid G^{0})\). This could be achieved by a linear combination of the log probability for unconditional and conditional denoising [16]:","\hat{p}_{\theta}(G^{t-1}\mid G^{t},\mathcal{C})=\log p_{\theta}(G^{t-1}\mid G^{ t})+s(\log p_{\theta}(G^{t-1}\mid G^{t},\mathcal{C})-\log p_{\theta}(G^{t-1} \mid G^{t})),",where \(s\) denotes the scale of conditional guidance,"\log\hat{p}_{\theta}(G^{t-1}\mid G^{t},\mathcal{C})=\log p_{\theta}(G^{t-1}\mid G^{t})+\lambda\log q(\mathcal{C}\mid G^{0}),",where $\lambda$ controls the strength of the multi-condition guidance during the reverse diffusion process
neurips_2024_oral_21,8,"ce. Unlike classifier-free guidance [16], which typically predicts noise, we directly estimate \(p_{\theta}(\tilde{G}^{0}\mid G^{t},\mathcal{C})\). We one one denoising model \(f_{\theta}(G^{t},\mathcal{C})\) for both \(p_{\theta}(\tilde{G}^{0}\mid G^{t})\) and \(p_{\theta}(\tilde{G}^{0}\mid G^{t},\mathcal{C})\). Here, \(f_{\theta}(G^{t},\mathcal{C}=\emptyset)\) computes the unconditional probability by substituting the original conditional embeddings with the null value. During training, we randomly drop the condition with a ratio, i.e., \(\mathcal{C}=\emptyset\), to learn the embedding of the null value. \(f_{\theta}(G^{t}=\mathbf{X}_{G}^{t},\mathcal{C})\) comprises two components: the condition encoder and the graph denoiser. An overview of the architecture is presented in Figure 2.

Condition EncoderWe treat the timestep \(t\) as a special condition and follow [31] to obtain a \(D\)-dimensional representation \(\mathbf{t}\) with sinusoidal encoding. For property-related numerical or categorical condition \(c_{i}\in\mathcal{C}\), we apply distinct encoding operations to get \(D\)-dimensional representation. For a categorical condition, we use the one-hot encoding. For a numerical variable, we introduce a clustering encoding method. This defines learnable centroids, assigning \(c_{i}\) to clusters, and transforming the soft assignment vector of condition values into the representation. It could be implemented using two \(\mathrm{Linear}\) layers and a \(\mathrm{Softmax}\) layer in the middle as: \(\mathrm{Linear}\left(\mathrm{Softmax}\left(\mathrm{Linear}(c_{i})\right)\right)\). Finally, we could obtain the representation of the condition as \(\mathbf{c}=\sum_{i=1}^{M}\mathrm{encode}(c_{i})\), where \(\mathrm{encode}\) is the specific encoding method based on the condition type. For numerical conditions, we evaluate our proposed clustering-based approach against alternatives like direct or interval-based encodings [28]. As noted in Section 4.4, the clustering encoding outperforms the other methods.

Graph Denoiser: Transformer LayersGiven the noisy graph at timestep \(t\), the graph tokens are first encoded into the hidden space as \(\mathbf{H}=\mathrm{Linear}(\mathbf{X}_{G}^{t})\), where \(\mathbf{H}\in\mathbb{R}^{N\times D}\). We then adapt the standard Transformer layers [42] with self-attention and multi-layer perceptrons (MLP), but replace the normalization with the adaptive layer normalization (\(\mathrm{AdaLN}\)) controlled by the representations of the conditions [19; 34]: \(\mathbf{H}=\mathrm{AdaLN}(\mathbf{H},\mathbf{c})\). For each row \(\mathbf{h}\) in \(\mathbf{H}\):","\mathrm{AdaLN}(\mathbf{h},\mathbf{c})=\gamma_{\theta}(\mathbf{c}) \odot\frac{\mathbf{h}-\mu(\mathbf{h})}{\sigma(\mathbf{h} )}+\beta_{\theta}(\mathbf{c}),",where \(\mu(\cdot)\) and \(\sigma(\cdot)\) are mean and variance values,"\mathrm{AdaLN}(\mathbf{h},\mathbf{c})=\gamma(\mathbf{c})\cdot\mathrm{LayerNorm}(\mathbf{h})+\beta(\mathbf{c}),",where $\gamma$ and $\beta$ are learnable parameters modulated by the condition representation $\mathbf{c}$ through two linear transformations
neurips_2024_oral_21,9,"es. \(\odot\) indicates element-wise product. \(\gamma_{\theta}(\cdot)\) and \(\beta_{\theta}(\cdot)\) are neural network modules in \(f_{\theta}(\cdot)\), each of which consists of two linear layers with \(\mathrm{SiLU}\) activation [11] in the middle. We have a gated variant \(\mathrm{AdaLN}_{gate}\) for residuals:","\mathrm{AdaLN}_{gate}(\mathbf{h},\mathbf{c})=\alpha_{\theta}( \mathbf{c})\odot\mathrm{AdaLN}(\mathbf{h},\mathbf{c})",,"\mathrm{AdaLN}_{gate}(\mathbf{h},\mathbf{c})=\sigma(\gamma_{\theta}^{\prime}(\mathbf{c}))\odot\gamma_{\theta}(\mathbf{c})\odot\frac{\mathbf{h}-\mu(\mathbf{h})}{\sigma(\mathbf{h})}+\beta_{\theta}(\mathbf{c}),","where $\sigma(\cdot)$ denotes the sigmoid function, and $\gamma_{\theta}^{\prime}(\cdot)$ is an additional neural network module for gating the adaptive scaling factors."
neurips_2024_oral_21,10,"We apply the zero initialization for the first layer of \(\gamma_{\theta}(\cdot),\beta_{\theta}(\cdot)\), and \(\alpha_{\theta}(\cdot)\)[34]. There are other options to learn the structure representation from the condition [34]: \(\mathrm{In\text{-}Context}\) conditioning adds condition representation to the structure representation at the beginning of the structure encoder, and \(\mathrm{Cross\text{-}Attention}\) calculates cross-attention between the condition and structure representation. We observe in Section 4.4 that \(\mathrm{AdaLN}\) performs best among them.

Graph Denoiser: Final MLPWe have the hidden states \(\mathbf{H}\) after the final Transformer layers, the MLP is used to predict node probabilities \(\tilde{\mathbf{X}}_{V}^{0}\) and edge probabilities \(\tilde{\mathbf{X}}_{E}^{0}\) at \(t=0\):","\tilde{\mathbf{X}}_{G}^{0}=\mathrm{AdaLN}(\mathrm{MLP}(\mathbf{H}),\mathbf{c}).",,"\tilde{\mathbf{X}}_{V}^{0},\tilde{\mathbf{X}}_{E}^{0}= \mathrm{MLP}_{V}(\mathbf{H}),\mathrm{MLP}_{E}(\mathbf{H})","where $\tilde{\mathbf{X}}_{V}^{0}$ and $\tilde{\mathbf{X}}_{E}^{0}$ denote the predicted node and edge probabilities at timestep $t=0$, obtained through separate MLPs applied to the hidden states $\mathbf{H}$"
neurips_2024_oral_22,1,"## 2 Background

Denoising diffusion.Denoising diffusion generates samples from a distribution \(p_{\text{data}}(\mathbf{x})\) by iteratively denoising a sample of pure white noise, such that a noise-free random data sample is gradually revealed [15]. The idea is to consider heat diffusion of \(p_{\text{data}}(\mathbf{x})\) into a sequence of increasingly smoothed densities \(p(\mathbf{x};\sigma)=p_{\text{data}}(\mathbf{x})*\mathcal{N}(\mathbf{x}; \mathbf{0},\sigma^{2}\mathbf{I})\). For a large enough \(\sigma_{\text{max}}\), we have \(p(\mathbf{x};\sigma_{\text{max}})\approx\mathcal{N}(\mathbf{x};\mathbf{0}, \sigma_{\text{max}}^{2}\mathbf{I})\), from which we can trivially sample by drawing normally distributed white noise. The resulting sample is then evolved backward towards low noise levels by a probability flow ODE [23; 47; 49]","\mathrm{d}\mathbf{x}_{\sigma}\,=\,-\sigma\nabla_{\mathbf{x}_{\sigma}}\!\log p (\mathbf{x}_{\sigma};\sigma)\;\mathrm{d}\sigma",,"p(\mathbf{x}; \sigma) = p_{\text{data}}(\mathbf{x}) * \mathcal{N}(\mathbf{x}; \mathbf{0}, \sigma^2 \mathbf{I})","Equation defining the smoothed density $p(\mathbf{x};\sigma)$ as the convolution of the data distribution $p_{\text{data}}(\mathbf{x})$ with a Gaussian kernel $\mathcal{N}(\mathbf{x}; \mathbf{0}, \sigma^2 \mathbf{I})$."
neurips_2024_oral_22,2,"that maintains the property \(\mathbf{x}_{\sigma}\sim p(\mathbf{x}_{\sigma};\sigma)\) for every \(\sigma\in[0,\sigma_{\text{max}}]\). Upon reaching \(\sigma=0\), we obtain \(\mathbf{x}_{0}\sim p(\mathbf{x}_{0};0)=p_{\text{data}}(\mathbf{x}_{0})\) as desired.

In practice, the ODE is solved numerically by stepping along the trajectory defined by Equation 1. This requires evaluating the so-called score function [20]\(\nabla_{\mathbf{x}}\log p(\mathbf{x};\sigma)\) for a given sample \(\mathbf{x}\) and noise level \(\sigma\) at each step. Rather surprisingly, we can approximate this vector using a neural network \(D_{\theta}(\mathbf{x};\sigma)\) parameterized by weights \(\theta\) trained for the denoising task","\theta\,=\,\arg\min_{\theta}\mathbb{E}_{\mathbf{y}\sim p_{\text{data}}, \sigma\sim p_{\text{train}},\mathbf{n}\sim\mathcal{N}(\mathbf{0},\sigma^{2} \mathbf{I})}\|D_{\theta}(\mathbf{y}+\mathbf{n};\sigma)-\boldsymbol{y}\|_{2}^ {2},",where \(p_{\text{train}}\) controls the noise level distribution during training,,
neurips_2024_oral_22,3,"ng. Given \(D_{\theta}\), we can estimate \(\nabla_{\mathbf{x}}\log p(\mathbf{x};\sigma)\approx(D_{\theta}(\mathbf{x}; \sigma)-\mathbf{x})/\sigma^{2}\), up to approximation errors due to, e.g., finite capacity or training time [23; 53]. As such, we are free to interpret the network as predicting either a denoised sample or a score vector, whichever is more convenient for the analysis at hand. Many reparameterizations and practical ODE solvers are possible, as enumerated by Karras et al. [23]. We follow their recommendations, including the schedule \(\sigma(t)=t\) that lets us parameterize the ODE directly via noise level \(\sigma\) instead of a separate time variable \(t\).

In most applications, each data sample \(\mathbf{x}\) is associated with a label \(\mathbf{c}\), representing, e.g., a class index or a text prompt. At generation time, we control the outcome by choosing a label \(\mathbf{c}\) and seeking a sample from the conditional distribution \(p(\mathbf{x}|\mathbf{c};\sigma)\) with \(\sigma=0\). In practice, this is achieved by training a denoiser network \(D_{\theta}(\mathbf{x};\sigma,\mathbf{c})\) that accepts \(\mathbf{c}\) as an additional conditioning input.

Classifier-free guidance.For complex visual datasets, the generated images often fail to reproduce the clarity of the training images due to approximation errors made by finite-capacity networks. A broadly used trick called _classifier-free guidance_ (CFG) [16] pushes the samples towards higher likelihood of the class label, sacrificing variety for ""more canonical"" images that the network appears to be better capable of handling.

In a general setting, guidance in a diffusion model involves two denoiser networks \(D_{0}(\mathbf{x};\sigma,\mathbf{c})\) and \(D_{1}(\mathbf{x};\sigma,\mathbf{c})\). The guiding effect is achieved by _extrapolating_ between the two denoising results by a factor \(w\):","D_{w}(\mathbf{x};\sigma,\mathbf{c})\,=\,wD_{1}(\mathbf{x};\sigma,\mathbf{c})+( 1-w)D_{0}(\mathbf{x};\sigma,\mathbf{c}).",,"\mathbf{x}_{\text{guided}} = (1 - w) D_{0}(\mathbf{x}; \sigma, \mathbf{c}) + w D_{1}(\mathbf{x}; \sigma, \mathbf{c}),","where $w$ controls the strength of the guidance, interpolating between the unconditioned denoiser $D_0$ and the conditioned denoiser $D_1$."
neurips_2024_oral_22,4,"Trivially, setting \(w=0\) or \(w=1\) recovers the output of \(D_{0}\) and \(D_{1}\), respectively, while choosing \(w>1\) over-emphasizes the output of \(D_{1}\). Recalling the equivalence of denoisers and scores [53], we can write","D_{w}(\mathbf{x};\sigma,\mathbf{c}) \approx \mathbf{x}+\sigma^{2}\nabla_{\mathbf{x}}\log\underbrace{(p_ {0}(\mathbf{x}|\mathbf{c};\sigma)[\frac{p_{1}(\mathbf{x}|\mathbf{c}; \sigma)}{p_{0}(\mathbf{x}|\mathbf{c};\sigma)}]^{w})}_{\propto:\ p_{w}( \mathbf{x}|\mathbf{c};\sigma)}.",,"D_{w}(\mathbf{x};\sigma,\mathbf{c})\,=\,D_{0}(\mathbf{x};\sigma,\mathbf{c})+w\big(D_{1}(\mathbf{x};\sigma,\mathbf{c})-D_{0}(\mathbf{x};\sigma,\mathbf{c})\big),",which expresses the guided denoiser as a base denoiser $D_0$ plus a weighted residual correction derived from the difference between $D_1$ and $D_0$.
neurips_2024_oral_22,5,"Thus, guidance grants us access to the score of the density \(p_{w}(\mathbf{x}|\mathbf{c};\sigma)\) implied in the parentheses. This score can be further written as [10; 16]",\nabla_{\mathbf{x}}\log p_{w}(\mathbf{x}|\mathbf{c};\sigma) = \nabla_{\mathbf{x}}\log p_{1}(\mathbf{x}|\mathbf{c};\sigma)+(w-1) \nabla_{\mathbf{x}}\log\frac{p_{1}(\mathbf{x}|\mathbf{c};\sigma)}{p_{0}( \mathbf{x}|\mathbf{c};\sigma)}.,,"\nabla_{\mathbf{x}}\log p_{w}(\mathbf{x}|\mathbf{c};\sigma) \approx \nabla_{\mathbf{x}}\log p_{0}(\mathbf{x}|\mathbf{c};\sigma) + w\,\nabla_{\mathbf{x}}\log\frac{p_{1}(\mathbf{x}|\mathbf{c};\sigma)}{p_{0}(\mathbf{x}|\mathbf{c};\sigma)}.",This equation decomposes the guided score into the base score and a weighted correction term derived from the ratio of two conditional densities.
neurips_2024_oral_23,1,"## 3 Methodology and preliminaries

**Notation.** Let \(X\in\mathcal{X}\) be a random variable denoting the inputs (or ""features"") which are available for making algorithmic predictions about an outcome \(Y\in[0,1]\). Let \(\hat{Y}\in[0,1]\) be an expert's prediction of \(Y\), and let \(x,y,\hat{y}\) denote realizations of the corresponding random variables. Our approach is parameterized by a class of predictors \(\mathcal{F}\), which is some set of functions mapping \(\mathcal{X}\) to \([0,1]\). We interpret \(\mathcal{F}\) as the class of predictive models which are relevant (or feasible to implement) for a given prediction task; we discuss this choice further below. Broadly, we are interested in whether the expert prediction \(\hat{Y}\) provides a predictive signal which cannot be extracted from \(X\) by any \(f\in\mathcal{F}\).

**Choice of model class \(\mathcal{F}\).** For now we place no restrictions on \(\mathcal{F}\), but it's helpful to consider a concrete model class (e.g., a specific neural network architecture) from which, given some training data, one could derive a _particular_ model (e.g., via empirical risk minimization over \(\mathcal{F}\)). The choice of \(\mathcal{F}\) could be guided by practical considerations; some domains might require interpretable models (e.g., linear functions) or be subject to computational constraints. We might also simply believe that a certain architecture or functional form is well suited to the task of interest. In any case, we are interested in whether human judgment can provide information which is not conveyed by any model in this class, but are agnostic as to _how_ this is accomplished: an expert may have information which is not encoded in \(X\), or be deploying a decision rule which is not in \(\mathcal{F}\) -- or both!

Another choice is to take \(\mathcal{F}\) to model more abstract limitations on the expert's cognitive process. In particular, to model experts who are subject to ""bounded rationality"" [63; 40], \(\mathcal{F}\) might be the set of functions which can be efficiently computed (e.g., by a circuit of limited complexity). In this case, an expert who provides a prediction which cannot be modeled by any \(f\in\mathcal{F}\) must have access to _information_ which is not present in the training data. We take the choice of \(\mathcal{F}\) as given, but emphasize that these two approaches yield qualitatively different insight about human expertise.

**Indistinguishability with respect to \(\mathcal{F}\).** Our approach will be to use human input to distinguish observations which are _indistinguishable_ to any predictor \(f\in\mathcal{F}\). We formalize this notion of indistinguishability as follows:

**Definition 3.1** (\(\alpha\)-Indistinguishable subset).: For some \(\alpha\geq 0\), a set \(S\subseteq\mathcal{X}\) is \(\alpha\)-indistinguishable with respect to a function class \(\mathcal{F}\) and target \(Y\) if, for all \(f\in\mathcal{F}\),","|\mathrm{Cov}(f(X),Y\mid X\in S)|\leq\alpha",,\left| \mathbb{E}[(Y - f(X)) \cdot \mathbf{1}_{X \in S}] \right| \leq \alpha.,"This equation defines an $\alpha$-indistinguishable subset $S$ with respect to a function class $\mathcal{F}$ and target $Y$, capturing the idea that no predictor $f \in \mathcal{F}$ can significantly correlate with the residual $Y - f(X)$ over $S$."
neurips_2024_oral_23,2,"To interpret this definition, observe that the subset \(S\) can be viewed as generalizing the intuition given in Section 1 for grouping identical inputs. In particular, rather than requiring that all \(x\in S\) are exactly equal, Definition 3.1 requires that all members of \(S\) effectively ""look the same"" for the purposes of making algorithmic predictions about \(Y\), as every \(f\in\mathcal{F}\) is only weakly related to the outcome within \(S\). We now adopt the definition of a multicalibrated partition [28] as follows:

**Definition 3.2** (\(\alpha\)-Multicalibrated partition).: For \(K\geq 1\), \(S_{1}\ldots S_{K}\subseteq\mathcal{X}\) is an \(\alpha\)-multicalibrated partition with respect to \(\mathcal{F}\) and \(Y\) if (1) \(S_{1}\ldots S_{K}\) partitions \(\mathcal{X}\) and (2) each \(S_{k}\) is \(\alpha\)-indistinguishable with respect to \(\mathcal{F}\) and \(Y\).3

Footnote 3: This is closely related to \(\alpha\)-approximate multicalibration [28], which requires that Definition 3.1 merely holds in expectation over the partition. We work with a stronger pointwise definition for clarity, but our results can also be interpreted as holding for the ‘typical’ element of an \(\alpha\)-approximately multicalibrated partition.

Intuitively, the partition \(\{S_{k}\}_{k\in[K]}\) ""extract[s] all the predictive power"" from \(\mathcal{F}\)[28]; within each element of the partition, every \(f\in\mathcal{F}\) is only weakly related to the outcome \(Y\). Thus, while knowing that an input \(x\) lies in subset \(S_{k}\) may be highly informative for predicting \(Y\) -- for example, it may be that \(\mathbb{E}[Y\mid X=x]\approx\mathbb{E}[Y\mid X=x^{\prime}]\) for all \(x,x^{\prime}\in S_{k}\) -- no predictor \(f\in\mathcal{F}\) provides significant _additional_ signal within \(S_{k}\). We provide a stylized example of such partitions in Figure 1 below.

It's not obvious that such partitions are feasible to compute, or even that they should exist. We'll show in Appendix B however that a multicalibrated partition can be efficiently computed for many natural classes of functions. Where the relevant partition is clear from context, we use \(\mathbb{E}_{k}[\cdot],\mathrm{Var}_{k}(\cdot),\mathrm{Cov}_{k}(\cdot,\cdot)\) to denote expectation, variance and covariance conditional on the event that \(\{X\in S_{k}\}\). For a subset \(S\subseteq\mathcal{X}\), we use \(\mathbb{E}_{S}[\cdot],\mathrm{Var}_{S}(\cdot)\) and \(\mathrm{Cov}_{S}(\cdot,\cdot)\) analogously.

**Incorporating human judgment into predictions.** To incorporate human judgment into predictions, a natural heuristic is to first test whether the conditional covariance \(\mathrm{Cov}_{k}(Y,\hat{Y})\) is nonzero within some indistinguishable subset. Intuitively, this indicates that the expert prediction is informative even though every model \(f\in\mathcal{F}\) is not. This suggests a simple method for incorporating human expertise: first, learn a partition which is multicalibrated with respect to \(\mathcal{F}\), and then use \(\hat{Y}\) to predict \(Y\) within each indistinguishable subset. We describe this procedure in Algorithm 1 below, where we define a univariate learning algorithm \(\mathcal{A}\) as a procedure which takes one or more \((\hat{y}_{i},y_{i})\in[0,1]^{2}\) training observations and outputs a function which predicts \(Y\) using \(\hat{Y}\). For example, \(\mathcal{A}\) might be an algorithm which fits a univariate linear or logistic regression which predicts \(Y\) as a function of \(\hat{Y}\).

Algorithm 1 simply learns a different predictor of \(Y\) as a function of \(\hat{Y}\) within each indistinguishable subset. As we show below, even simple instantiations of this approach can outperform the squared error achieved by _any_\(f\in\mathcal{F}\). This approach can also be readily extended to more complicated forms of human input (e.g., freeform text, which can be represented as a high-dimensional vector rather than a point prediction \(\hat{Y}\)), and can be used to _test_ whether human judgment provides information that an algorithm cannot learn from the available training data. We turn to these results below.

Figure 1: Partitions which are approximately multicalibrated with respect to the class of hyperplane classifiers (we consider the empirical distribution placing equal probability on each observation). In both panels, no hyperplane classifier has significant discriminatory power within each subset.

## 4 Technical results

In this section we present our main technical results. For clarity, all results in this section are presented in terms of population quantities, and assume oracle access to a multicalibrated partition. We present corresponding generalization arguments and background on learning multicalibrated partitions in Appendices A and B, respectively. All proofs are deferred to Appendix C.

**Theorem 4.1**.: _Let \(\{S_{k}\}_{k\in[K]}\) be an \(\alpha\)-multicalibrated partition with respect to a model class \(\mathcal{F}\) and target \(Y\). Let the random variable \(J(X)\in[K]\) be such that \(J(X)=k\) iff \(X\in S_{k}\). Define \(\gamma^{*},\beta^{*}\in\mathbb{R}^{K}\) as_","\gamma^{*},\beta^{*}\in\operatorname*{arg\,min}_{\gamma\in\mathbb{R}^{K}, \beta\in\mathbb{R}^{K}}\;\mathbb{E}[(Y-\gamma_{J(X)}+\beta_{J(X)} \hat{Y})^{2}].",,"\gamma^{*}_{k} = \mathbb{E}[Y \mid X \in S_{k}], \quad \beta^{*}_{k} = \mathbb{E}[\hat{Y} \mid X \in S_{k}]","These represent the expected outcome and expert prediction within each subset of the partition, respectively."
neurips_2024_oral_23,3,"_Then, for any \(f\in\mathcal{F}\) and \(k\in[K]\),_","\mathbb{E}_{k}[(Y-\gamma_{k}^{*}-\beta_{k}^{*}\hat{Y})^{2} ]+4\mathrm{Cov}_{k}(Y,\hat{Y})^{2}\leq\mathbb{E}_{k}[(Y-f(X) )^{2}]+2\alpha.",,\mathbb{E}[(Y - \gamma^{*}_{k} + \beta^{*}_{k} \hat{Y})^{2} \mid X \in S_{k}] \leq \mathbb{E}[(Y - f(X))^{2} \mid X \in S_{k}] + 2\alpha \cdot \mathrm{Var}_{k}(Y).,This inequality shows that the expected squared error of the human-augmented predictor within each indistinguishable subset $S_k$ is bounded by the error of any algorithmic predictor $f \in \mathcal{F}$ plus a term proportional to $\alpha$ and the conditional variance of $Y$ within $S_k$.
neurips_2024_oral_23,4,"That is, the squared error incurred by the univariate linear regression of \(Y\) on \(\hat{Y}\) within each indistinguishable subset outperforms that of any \(f\in\mathcal{F}\). This improvement is at least \(4\mathrm{Cov}_{k}(Y,\hat{Y})^{2}\), up to an additive approximation error \(2\alpha\). We emphasize that \(\mathcal{F}\) is an arbitrary class, and may include complex, nonlinear predictors. Nonetheless, given a multicalibrated partition, a simple linear predictor can improve on the _best_\(f\in\mathcal{F}\). Furthermore, this approach allows us to _selectively_ incorporate human feedback: whenever \(\mathrm{Cov}_{k}(Y,\hat{Y})=0\), we recover a coefficient \(\beta_{k}^{*}\) of 0.4

Footnote 4: Recall that the population coefficient in a univariate linear regression of \(Y\) on \(\hat{Y}\) is \(\frac{\mathrm{Cov}(Y,\hat{Y})}{\mathrm{Var}(\hat{Y})}\).

**Nonlinear functions and high-dimensional feedback.** Theorem 4.1 corresponds to instantiating Algorithm 1 with a univariate linear regression, but the same insight generalizes readily to other functional forms. For example, if \(Y\) is binary, it might be desirable to instead fit a logistic regression. We provide a similar guarantee for generic nonlinear predictors via Corollary A.1 in Appendix A. Furthermore, while the results above assume that an expert provides a prediction \(\hat{Y}\in[0,1]\), the same insight extends to richer forms of feedback. For example, in a medical diagnosis task, a physician might produce free-form clinical notes which contain information that is not available in tabular electronic health records. Incorporating this kind of feedback requires a learning algorithm better suited to high-dimensional inputs (e.g., a deep neural network), which motivates our following result.

**Corollary 4.2**.: _Let \(S\) be an \(\alpha\)-indistinguishable subset with respect to a model class \(\mathcal{F}\) and target \(Y\). Let \(H\in\mathcal{H}\) denote expert feedback which takes values in some arbitrary domain (e.g., freeform text, which might be tokenized to take values in \(\mathbb{Z}^{d}\) for some \(d>0\)), and let \(g:\mathcal{H}\to[0,1]\) be a function which satisfies the following approximate calibration condition for some \(\eta\geq 0\) and for all \(\beta,\gamma\in\mathbb{R}\):_",\mathbb{E}_{S}[(Y-g(H))^{2}]\leq\mathbb{E}_{S}[(Y-\gamma-\beta g(H))^{2}]+\eta.,,\mathbb{E}[(Y - \gamma - \beta g(H))^2] \leq \mathbb{E}[(Y - \gamma - \beta \mathbb{E}[Y \mid H])^2] + \eta.,
neurips_2024_oral_23,5,"_Then, for any \(f\in\mathcal{F}\),_","\mathbb{E}_{S}[(Y-g(H))^{2}]+4\text{Cov}_{S}(Y,g(H))^{2}\leq\mathbb{E}_ {S}[(Y-f(X))^{2}]+2\alpha+\eta.",,\mathbb{E}_{S}[(Y-g(H))^{2}]\leq\mathbb{E}_{S}[(Y-f(X))^{2}]+2\alpha+\eta.,"This inequality shows that the squared error of a calibrated expert feedback function $g(H)$ is bounded by the error of any algorithmic predictor $f \in \mathcal{F}$, plus approximation errors due to the indistinguishability parameter $\alpha$ and the calibration parameter $\eta$."
neurips_2024_oral_23,6,"To interpret this result, notice that (4) requires only that the prediction \(g(H)\) cannot be significantly improved by any linear post-processing function. For example, this condition is satisfied by any calibrated predictor \(g(H)\).5 Furthermore, any \(g(H)\) which does not satisfy (4) can be transformed by letting \(\tilde{g}(H)=\min_{\gamma,\beta}\mathbb{E}[(Y-\gamma-\beta g(H))^{2}]\); i.e., by linearly regressing \(Y\) on \(g(H)\), in which case \(\tilde{g}(H)\) satisfies (4). This result mirrors Theorem 4.1: a predictor which depends only on human feedback \(H\) can improve on the best \(f\in\mathcal{F}\) within each element of a multicalibrated partition.

Footnote 5: A calibrated predictor is one where \(\mathbb{E}_{S}[Y\mid g(H)]=g(H)\). This is a fairly weak condition; for example, it is satisfied by the constant predictor \(g(H)\equiv\mathbb{E}_{S}[Y]\)[18; 25].

**Testing for informative experts.** While we have thus far focused on incorporating human judgment to improve predictions, we may also be interested in the related question of simply _testing_ whether human judgment provides information that cannot be conveyed by any algorithmic predictor. For example, such a test might be valuable in deciding whether to automate a given prediction task.

Theorem 4.1 suggests a heuristic for such a test: if the conditional covariance \(\text{Cov}_{k}(Y,\hat{Y})\) is large, then we might expect that \(\hat{Y}\) is somehow ""more informative"" than any \(f\in\mathcal{F}\) within \(S_{k}\). While covariance only measures a certain form of _linear_ dependence between random variables, we now show that, in the special case of binary-valued algorithmic predictors, computing the covariance of \(Y\) and \(\hat{Y}\) within an indistinguishable subset serves as a stronger test for whether \(\hat{Y}\) provides _any_ predictive information which cannot be expressed by the class \(\mathcal{F}\).

**Theorem 4.3**.: _Let \(\{S_{k}\}_{k\in[K]}\) be an \(\alpha\)-multicalibrated partition for a binary-valued model class \(\mathcal{F}^{binary}\) and target outcome \(Y\). For all \(k\in[K]\), let there be \(\tilde{f}_{k}\in\mathcal{F}\) such that \(Y\perp\!\!\!\perp\hat{Y}\mid\tilde{f}_{k}(X),X\in S_{k}\). Then, for all \(k\in[K]\),_","|\text{Cov}_{k}(Y,\hat{Y})|\leq\sqrt{\frac{\alpha}{2}}.",,"\text{Cov}_{k}(Y,\hat{Y})^{2}\leq\alpha.","This equation quantifies the squared conditional covariance between the outcome $Y$ and the expert prediction $\hat{Y}$ within an indistinguishable subset, showing that it is bounded by the indistinguishability parameter $\alpha$ under the assumption of conditional independence given a binary-valued model class."
neurips_2024_oral_24,1,"### LoRA Basics

LoRA [18] achieves comparable performances to fine-tuning on many benchmarks by freezing the pre-trained model weights \(W_{0}\) and inserting trainable rank decomposition matrices into each layer of the pre-trained model. In particular, for each layer, LoRA uses two sequential low-rank matrices \(A\) and \(B\) to fit the residual weights for adaptation. The forward computation is written as follows:",y^{\prime}=y+\Delta y=W_{0}x+BAx,where \(y\in R\)d is the output and the \(x\in R\)k denotes the input,W = W_0 + B A,The adapted weight matrix is obtained by adding the product of low-rank matrices $A$ and $B$ to the pre-trained weights $W_0$.
neurips_2024_oral_24,2,"ut. \(B\in R\)dpr, \(A\in R\)rok with \(r\ll min(d,k)\). Normally matrix \(B\) is initialized with zeroes and matrix \(A\) is initialized with Kaiming Uniform [15] to force \(\Delta\ y=0\) at the beginning.

Figure 1: Illustration of LoRA architecture changes in _HydraLoRA_. Only the tunable parameters are shown in this Figure. (a) LoRA architecture with matrix A to achieve low rank and matrix B to recover. (b) under the same parameter count, a monolithic LoRA is split into multiple smaller A and B matrices to avoid training interference. (c) based on (b), _HydraLoRA_ has an asymmetric structure that has a shared A matrix and multiple B matrices.

### LoRA's Practical Dilemma

Parameter count has a clear impact on the performance of neural models [22; 33]. Yet, Parameter-Efficient Fine-tuning (PEFT) methods, such as Adapter [17] and prefix-tuning [25], focus on fine-tuning a limited set of parameters. These approaches present a practical dilemma: while restricting the number of tuned parameters is essential for training efficiency, it hinders the model's ability to learn from diverse datasets. This trade-off becomes particularly evident when considering corpus heterogeneity [2]. Figure 2 reveals a notable performance disparity between PEFT techniques and full fine-tuning (FFT), with the gap widening in scenarios involving a more diverse or heterogeneous training corpus.

### Observations

In this work, we aim for a PEFT approach that strikes a better balance between maximizing the learning capability for heterogeneous data and minimizing the number of parameters involved. A key goal is to ensure that our enhanced technique exhibits robust generalization across unseen tasks, independent of any prior task-specific knowledge. To achieve our objectives, we focus on LoRA and conduct a series of experiments as Table 1 to gain a deeper understanding of its mechanisms. Our methodology involves leveraging data from diverse tasks within a domain, and training distinct LoRA heads for each domain, leading to our first observation:

_Observation I: With the same parameter count, rather than employing a single LoRA for the entire domain dataset, it proves more effective to deploy multiple, smaller LoRA heads, each dedicated to a specific downstream task._

This suggests that interference among tasks might harmfully impact the training process. Furthermore, we posit that this interference is NOT exclusive to this explicit multi-task training. This interference could happen in any training setting since all datasets inherently consist of multiple implicit _intrinsic components_, such as sub-domains or tasks within a domain that is even unknown to domain experts. To better understand how multiple LoRA heads mitigate the interference among intrinsic components, in Figure 3, we employ the t-SNE technique [49] to visualize the parameters of matrix A and B across all heads. This analysis yields another critical observation:

_Observation II: When multiple LoRA heads are trained individually on different data, the parameters of matrix A from different heads tend to converge, while those of matrix B are distinguishable._

In detail, the parameters of matrix A across all heads exhibit a high degree of similarity, leading to their overlaps in the figure. Conversely, the parameters of matrix B from different heads are distinct and easily distinguishable. We posit that this divergence is an artifact of the initialization schemes, with matrix A inclined toward capturing commonalities across domains, while matrix B adapts to domain-specific diversities. The distinction between matrix A and B offers valuable insights for enhancing both parameter efficiency and effectiveness. From an efficiency standpoint, our hypothesis suggests that the parameters of matrix A could potentially be shared across multiple heads, thereby reducing redundancy. Regarding effectiveness, since the parameters of matrix B of different heads

\begin{table}
\begin{tabular}{c|c|c|c} \hline \hline \multicolumn{1}{c|}{**Schemes**} & \(\mathrm{r\times n}\) & \multicolumn{1}{c|}{**MMLU**} & \(\mathrm{\char 37}\)**\% Parameter** \\ \hline LoRA & \(8\times 1\) & 43.22 & 0.062 \\ LoRA & \(16\times 1\) & 45.45 & 0.124 \\ LoRA & \(32\times 1\) & 46.59 & **0.248** \\ \hline LoRA (Split) & \(16\times 2\) & 46.82 & 0.248 \\ LoRA (Split) & \(8\times 4\) & **46.94** & 0.248 \\ LoRA (Split) & \(4\times 8\) & 46.83 & 0.248 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Performance on instruction tuning with Dolly-15K [8] and evaluated with MMLU [16] with different ranks. For LoRA (Split) decomposes high-rank LoRA modules into smaller, equivalent low-rank components (\(r\times n\)). \(n\) is the number of LoRAs, \(r\) denotes the rank of each LoRA.

Figure 2: Performance impact of corpus heterogeneity on full fine-tuning vs. parameter-efficient fine-tuning. Heterogeneity signifies the diversity within the dataset, often leading to interference due to its varied content and style [2]. Parameter-efficient approaches are particularly sensitive, suffering greater performance losses in heterogeneous cases.

are dispersed, suggesting that using a single head to adapt to multiple domains might be less effective than using individual heads for each domain, which minimizes the interference between domains.

Building upon our observations, we propose an optimized LoRA architecture designed to enhance cost-effectiveness. In this architecture, we share the parameters of A matrix across various sub-domains or tasks to improve parameter efficiency, while deploying multiple B matrices, each tailored to handle different intrinsic components. This design allows for a more effective adaptation to the specific characteristics of each component. While these intrinsic components can be manually identified using prior knowledge of the training data, we also introduce end-to-end methods using Mixture-of-Experts (MoEs) [21], which will be detailed in the methodology section. This automatic approach facilitates flexibility and applicability, particularly in scenarios where prior knowledge is limited or unavailable.

## 3 _HydraLoRA_

In this section, we introduce the proposed _HydraLoRA_, an asymmetric LoRA architecture for efficient fine-tuning, as illustrated in Figure 1. After that, we show the workflow of _HydraLoRA_ as Figure 4.

### Asymmetric LoRA architecture

The LoRA method updates two low-rank matrices \(A\) and \(B\), and uses \(AB\) as the change of a pretrained and frozen weight \(W_{0}\) of a linear layer as shown in Eq. 1. The integral parameters are fine-tuned for the whole corpus in the original LoRA, which causes difficulty in learning the various knowledge aspects. Drawing from a detailed breakdown analysis of LoRA, a potential solution is to segment the entire LoRA into ""Hydra"" structured LoRA variants, that is, characterized by a central shared matrix \(A\) and several distinct matrices \(B\), fostering a blend of shared knowledge and specialized functionalities. As Figure 1, _HydraLoRA_ is to fine-tune LoRAs to achieve robust performance without redundancy, thereby benefiting the entire heterogeneous corpus. The asymmetric LoRA architecture can be formulated as:","\begin{split} W&=\,W_{0}+\Delta\,W\\ &=\,W_{0}+\sum_{i=1}^{N}\omega_{i}\cdot B_{i}A\end{split}",,"\Delta W = \sum_{i=1}^{n} B_i A, \quad \text{where } A \in \mathbb{R}^{r \times k}, B_i \in \mathbb{R}^{d \times r}","_HydraLoRA_ introduces an asymmetric architecture where a shared matrix $A$ is combined with multiple distinct matrices $B_i$, enabling parameter sharing while allowing task-specific adaptation."
neurips_2024_oral_24,3,"The matrices \(B_{i}\in\mathbb{R}^{d\times r}\) and shared \(A\in\mathbb{R}^{r\times k}\). The hyper-parameter \(N\) denotes the number of \(B\) matrices. The term \(\omega_{i}\) modulates these contribution weights for head \(B_{i}\).

### Workflow of _HydraLoRA_

Figure 4 illustrates the workflow of _HydraLoRA_. Initially, _HydraLoRA_ delves into the adaptive identification and initialization of LoRA modules within a heterogeneous corpus, aligning them with

Figure 3: Breakdown analysis of LoRA modules. Compare fine-tuned LoRA modules of Dolly-15K [8] with three subtasks of Dolly-15K including “_summarization (Sum)_”, “_closed QA (QA)_” and “_information extraction (IE)_” using t-SNE. Consider LLaMA2-7B (random seed=42), which contains 32 decoder layers, corresponding to 32 adaptive modules. Each module consists of (**0**: q_proj of A, I: q_proj of B, **2**: v_proj of A, **3**: v_proj of B) submodules. This makes a total of \(32\times 4\) submodules. Left displays all submodules. Center shows all even submodules, i.e. the A matrix. Right represents all odd submodules, i.e. the B matrix. It can be seen that the differences in the fine-tuned LoRA modules for different tasks arise mainly from the B matrix.

task relevance through the application of \(k\)-means or developer-specified size. Subsequently, we propose a Mixture-of-Experts (MoE) framework that handles \(B\) matrices as expert adapters to ensure computational efficiency throughout the fine-tuning (Section 3.2.1) and inference (Section 3.2.2) stages by freezing the rest of the LLM parameters. During inference, it flexibly and dynamically merges multiple \(B\) matrices through the MoE router.

#### 3.2.1 Fine-tuning

Motivated by Mixture-of-Experts (MoEs; [20; 40]), where experts are selectively activated by a gating mechanism (Router) in response to different inputs. In _HydraLoRA_, we substitute each expert with a lightweight LoRA adapter. During fine-tuning, while weights of LLMs remain frozen, the experts and router layers are trained from scratch. In order to achieve a unified approach to the distinct forward processes of multiple \(B\) matrices, we define a set of experts, denoted as \((E_{1},\ldots,E_{N})\), to learn the updated matrix \(\Delta W\). As _HydraLoRA_ fine-tunes the experts using the heterogeneous corpus, the shared matrix \(A\) inherently captures collaborative knowledge to augment intra-gains, and different matrices \(B\) foster knowledge modularity to mitigate fine-tuning inter-offsets. Based on this structure, the forward process of _HydraLoRA_ is expressed as:",y=\mathit{W}_{0}x+\sum_{i=1}^{N}\omega_{i}E_{i}Ax\quad(MoE),"where \(N\) denotes the number of experts, i","\begin{split} \Delta\,W &= \text{MoE}(\cdot)\\ &= \sum_{i=1}^{N}\text{Router}(x)_{i}\cdot B_{i}A \end{split}","Equation 3 defines the forward process of _HydraLoRA_ during fine-tuning, where the MoE router dynamically selects and combines multiple B matrices with the shared A matrix based on input x."
neurips_2024_oral_24,4,"i.e., \(B\) matrices.To regulate these contributions, we introduce a gate function (router network) commonly consisting of a dense layer with trainable weights (transformation matrix) \(\mathit{W}_{g}\in\mathbb{R}^{r\times N}\) followed by a softmax function which takes an intermediate token representation \(x\) as input and combines the output of each expert based on the gating scores \((\omega_{1},\ldots,\omega_{N})\):",\omega_{i}=\text{softmax}(\mathit{W}_{g}^{T}x)\quad(Router),,"\omega_{i}=\frac{\exp\left(g_{i}\right)}{\sum_{j=1}^{N}\exp\left(g_{j}\right)},\quad\text{where }g_{i}=x^{T}\mathit{W}_{g}^{(i)}","where $\omega_{i}$ denotes the gating score for expert $i$, computed using a softmax over the outputs of a router network with trainable weights $\mathit{W}_{g}$."
neurips_2024_oral_25,1,"### Bi-Directional MR Causal Models

Suppose we have \(n\) independent samples \((X_{i},Y_{i},\mathbf{G}_{i})\) from the distribution of \((X,Y,\mathbf{G})\), where \(X\in\mathbb{R}\) and \(Y\in\mathbb{R}\) is a pair of phenotypes of interest, and \(\mathbf{G}=(G_{1},G_{2},\ldots,G_{g})^{\intercal}\in\mathbb{R}^{g}\) denotes

Figure 1: Graphical illustration of a valid IV model, where dashed lines indicate the absence of arrows. \(G\) is a valid IV relative to the causal relationship \(X\to Y\).

measured genetic variants, which may include invalid IVs. Analogous to Li and Ye (2022), for each sample, indexed by \(i\), the phenotype \(X_{i}\) is taken as a linear function of the phenotype \(Y_{i}\), genetic variants \(\mathbf{G}_{i}\), and an error term \(\varepsilon_{X_{i}}\), while phenotype \(Y_{i}\) is modeled as a linear function of the phenotype \(X_{i}\), genetic variants \(\mathbf{G}_{i}\), and an error term \(\varepsilon_{Y_{i}}\). Without loss of generality, we assume that all variables have a zero mean (otherwise can be centered). Specifically, the generating process of the data is given below:","\begin{array}{l}X=Y\beta_{Y\to X}+\mathbf{G}^{\intercal}\boldsymbol{ \gamma}_{X}+\varepsilon_{X},\\ Y=X\beta_{X\to Y}+\mathbf{G}^{\intercal}\boldsymbol{\gamma}_{Y}+ \varepsilon_{Y},\end{array}",where \(\beta_{Y\to X}\) is the causal effect of \(Y\) on \(X\) and \(\beta_{X\to Y}\) is that of \(X\) on \(Y\),"\begin{aligned}
X_i &= \alpha Y_i + \mathbf{G}_i^\intercal \boldsymbol{\gamma}_X + \varepsilon_{X_i}, \\
Y_i &= \beta X_i + \mathbf{G}_i^\intercal \boldsymbol{\gamma}_Y + \varepsilon_{Y_i},
\end{aligned}","Equation 1 defines the bi-directional MR causal models where $X$ and $Y$ are modeled as linear functions of each other, genetic variants $\mathbf{G}$, and error terms $\varepsilon_{X}$ and $\varepsilon_{Y}$, with parameters $\alpha$, $\beta$, $\boldsymbol{\gamma}_X$, and $\boldsymbol{\gamma}_Y$."
neurips_2024_oral_25,2,"\). \(\boldsymbol{\gamma}_{X}=(\gamma_{X,1},...,\gamma_{X,g})^{\intercal}\) and \(\boldsymbol{\gamma}_{Y}=(\gamma_{Y,1},...,\gamma_{Y,g})^{\intercal}\) are the direct effects of genetic variants \(\mathbf{G}\) on \(X\) and \(Y\), respectively. Note that \(\varepsilon_{X}\) and \(\varepsilon_{Y}\) are error terms that are correlated due to unmeasured confounders \(\mathbf{U}\) between \(X\) and \(Y\). Besides, there may exist a single genetic variant \(G_{i}\in\mathbf{G}\) with both \(\gamma_{X,i}\neq 0\) and \(\gamma_{Y,i}\neq 0\), which implies that \(G_{i}\) also has a direct pathway to the outcome, violating Assumption A2 (_Exclusion Restriction_). Like Li and Ye (2022), we here restrict the independence between any two genetic variants in \(\mathbf{G}\) (i.e., genetic variants are randomized). In Section 5, we discuss the situation where this restriction is violated. Unlike Li and Ye (2022), we allow genetic variants to cause unmeasured confounders (partially violating Assumption A3 (_Randomness_)).

Following Hausman (1983), we assume that \(\beta_{X\to Y}\beta_{Y\to X}\neq 1\). Then let \(\Delta=1/(1-\beta_{X\to Y}\beta_{Y\to X})\), Eq.(1) can be reorganized to avoid recursive formulations as follows:","\begin{array}{l}X=(\mathbf{G}^{\intercal}\boldsymbol{\gamma}_{X}+\mathbf{G }^{\intercal}\boldsymbol{\gamma}_{Y}\beta_{Y\to X}+\varepsilon_{X}+ \varepsilon_{Y}\beta_{Y\to X})\Delta,\\ Y=(\mathbf{G}^{\intercal}\boldsymbol{\gamma}_{X}\beta_{X\to Y}+\mathbf{G}^{ \intercal}\boldsymbol{\gamma}_{Y}+\varepsilon_{X}\beta_{X\to Y}+ \varepsilon_{Y})\Delta.\end{array}",,"\begin{array}{l}X=\Delta\mathbf{G}^{\intercal}(\boldsymbol{\gamma}_{X}+\beta_{Y\to X}\boldsymbol{\gamma}_{Y})+\Delta\varepsilon_{X}+\Delta\beta_{Y\to X}\varepsilon_{Y},\\ Y=\Delta\mathbf{G}^{\intercal}(\boldsymbol{\gamma}_{Y}+\beta_{X\to Y}\boldsymbol{\gamma}_{X})+\Delta\beta_{X\to Y}\varepsilon_{X}+\Delta\varepsilon_{Y}.\end{array}","Reorganized non-recursive form of the bi-directional MR causal model under the assumption $\beta_{X\to Y}\beta_{Y\to X}\neq 1$, expressing $X$ and $Y$ in terms of genetic variants $\mathbf{G}$ and error terms scaled by $\Delta$."
neurips_2024_oral_25,3,"If \(\beta_{X\to Y}\beta_{Y\to X}=1\), the causal effects \(\beta_{X\to Y}\) and \(\beta_{Y\to X}\) are not identifiable, even in the presence of a valid IV.

_Goal._ In this paper, we aim to address whether the bi-directional MR model in Eq.(1) is identifiable; that is, whether we can estimate the causal effects \(\beta_{X\to Y}\) and \(\beta_{Y\to X}\) given infinite data, even without prior knowledge about which candidate IVs are valid or invalid.

### IV Estimator

In this section, we briefly describe the classical IV estimator, the two-stage least squares (TSLS) estimator, which is capable of consistently estimating the causal effects \(\beta_{X\to Y}\) and \(\beta_{Y\to X}\) of interest in the above bi-directional MR causal model of Eq.(1) when the valid IVs are known (Woolridge, 2010). We denote by \(\mathbf{G}_{\mathcal{V}}^{X\to Y}\) the set consisting of all valid IVs in \(\mathbf{G}\) relative to \(X\to Y\), and \(\mathbf{G}_{\mathcal{I}}^{X\to Y}\) the set that consists of at least one invalid IV in \(\mathbf{G}\) relative to \(X\to Y\). Similar notations are applied to the inverse causal relationship \(Y\to X\), i.e., \(\mathbf{G}_{\mathcal{V}}^{Y\to X}\) and \(\mathbf{G}_{\mathcal{I}}^{Y\to X}\).

**Proposition 1** (Two Stage Least Square (TSLS) Estimator).: _Assume the system is a linear bi-directional causal model 1. For a given causal relationship \(X\to Y\) in the system, the causal effect of \(X\) on \(Y\) can be identified by_","\hat{\beta}_{X\to Y}=[X^{\intercal}\mathbf{P}X]^{-1}X^{ \intercal}\mathbf{P}Y=\beta_{X\to Y},",,\beta_{X\to Y}^{\text{TSLS}}=(\mathbb{E}[\mathbf{G}_{\mathcal{V}}^{X\to Y}X])^{-1}\mathbb{E}[\mathbf{G}_{\mathcal{V}}^{X\to Y}Y].,The TSLS estimator for the causal effect $\beta_{X\to Y}$ using valid IVs $\mathbf{G}_{\mathcal{V}}^{X\to Y}$ in the bi-directional MR model.
neurips_2024_oral_25,4,"_where \(\mathbf{P}=(\mathbf{G}_{\mathcal{V}}^{X\to Y})^{\intercal}\left[\mathbf{G}_{ \mathcal{V}}^{X\to Y}(\mathbf{G}_{\mathcal{V}}^{X\to Y})^{\intercal}\right]^{- 1}\mathbf{G}_{\mathcal{V}}^{X\to Y}\) is the projection matrix._

**Remark 1**.: _Note that, when utilizing the TSLS estimator, the causal effect is biased if the candidate set \(\mathbf{G}\) includes invalid IVs. For instance, consider the causal relationship \(X\to Y\), given an invalid IV set \(\mathbf{G}_{\mathcal{I}}^{X\to Y}\), the causal effect of \(X\) on \(Y\) is given by_","\hat{\beta}_{X\to Y}=[X^{\intercal}\tilde{\mathbf{P}}X]^{-1}X^{ \intercal}\tilde{\mathbf{P}}Y =\beta_{X\to Y}+\underbrace{[X^{\intercal}\tilde{\mathbf{P}}X ]^{-1}X^{\intercal}\tilde{\mathbf{P}}(\mathbf{G}^{\intercal}\boldsymbol{ \gamma}_{Y}+\varepsilon_{Y})}_{\beta_{bias}},",,\hat{\beta}_{X\to Y}=[X^{\intercal}\mathbf{P}X]^{-1}X^{\intercal}\mathbf{P}Y=\beta_{X\to Y}+\underbrace{[X^{\intercal}\mathbf{P}X]^{-1}X^{\intercal}\mathbf{P}\varepsilon_{Y}}_{\text{bias term}}.,"The TSLS estimator introduces bias in the presence of invalid IVs, as captured by the non-zero bias term involving the projection matrix and error component."
neurips_2024_oral_25,5,"_where \(\tilde{\mathbf{P}}=(\mathbf{G}_{\mathcal{I}}^{X\to Y})^{\intercal}\left[ \mathbf{G}_{\mathcal{I}}^{X\to Y}(\mathbf{G}_{\mathcal{I}}^{X\to Y})^{ \intercal}\right]^{-1}\mathbf{G}_{\mathcal{I}}^{X\to Y}\)._

In the remaining of the paper, we denote such a procedure by \(\hat{\beta}_{X\to Y}=\mathrm{TSLS}(X,Y,\mathbf{G}^{X\to Y})\). Similarly, for causal relationship \(Y\to X\), one can also have the corresponding causal effect of \(Y\) on \(X\) given the set of IVs, i.e., \(\hat{\beta}_{Y\to X}=\mathrm{TSLS}(Y,X,\mathbf{G}^{Y\to X})\). For details about the proof of Proposition 1 and Remark 1, please refer to Appendix G.1 and G.2.

Identifiability of Bi-Directional MR Model

In this section, we first show by a simple example that the valid and invalid IV sets can impose different constraints. Then, we formulate these constraints and present the necessary and sufficient conditions that render the model identifiable.

### A Motivating Example

Figure 2 gives an illustration of our basic idea. Suppose there are 2 valid IVs, i.e., \(G_{1}\) and \(G_{3}\), and 3 invalid IVs, i.e., \(G_{2}\), \(G_{4}\) and \(G_{5}\), relative to the causal relationship \(X\to Y\). For the inverse \(Y\to X\), identical conclusions can be derived. Assume the generating process of \(X\) and \(Y\) follows Eq.(1), where specifically we set all variables to have a zero mean and unit variance and \(X=Y\beta_{Y\to X}+G_{1}\gamma_{X,1}+G_{2}\gamma_{X,2}+G_{3}\gamma_{X,3}+G_{4} \gamma_{X,4}+G_{5}\gamma_{X,5}+\varepsilon_{X}\), and \(Y=X\beta_{X\to Y}+G_{2}\gamma_{Y,2}+G_{4}\gamma_{Y,4}+G_{5}\gamma_{Y,5}+ \varepsilon_{Y}\), where \(\beta_{X\to Y}=\beta_{Y\to X}=0.6,\gamma_{X,1}=\gamma_{X,2}=\gamma_{X,3}=1, \gamma_{X,4}=1.8,\gamma_{X,5}=1.2,\gamma_{Y,2}=0.5,\gamma_{Y,4}=0.6,\gamma_{Y,5 }=0.3\).

Interestingly, we have the following observations:

**Observation 1.** Given a valid IV set where there are at least two IVs, e.g., \(\mathbf{G}_{\mathcal{V}}^{X\to Y}=(G_{1},G_{3})^{\intercal}\), we have:","\mathrm{corr}(Y-X\omega_{\{G_{3}\}},G_{1})=0,\qquad\mathrm{corr}(Y-X\omega_{\{ G_{1}\}},G_{3})=0,","where \(\mathrm{corr}(\cdot)\) denotes the Pearson's correlation coefficient between two random variables, and \(\omega_{\{G_{i}\}}=\mathrm{TSLS}(X,Y,\{G_{i}\})\) with \(i\in\{1,3\}\)","\mathrm{Cov}(G_{1},X)=\beta_{Y\to X}\mathrm{Cov}(G_{1},Y)+\gamma_{X,1},\quad \mathrm{Cov}(G_{3},X)=\beta_{Y\to X}\mathrm{Cov}(G_{3},Y)+\gamma_{X,3}.",The covariance between valid IVs and phenotype X depends on the causal effect of Y on X and the direct effect of the IV on X.
neurips_2024_oral_25,6,"\).

**Observation 2.** However, given an invalid IV set, e.g., \(\mathbf{G}_{\mathcal{I}}^{X\to Y}=(G_{2},G_{4},G_{5})^{\intercal}\), we have:","\mathrm{corr}(Y-X\omega_{\{G_{4},G_{5}\}},G_{2})\neq 0,\mathrm{corr}(Y-X\omega_{\{ G_{2},G_{5}\}},G_{4})\neq 0,\mathrm{corr}(Y-X\omega_{\{G_{2},G_{4}\}},G_{5})\neq 0,","where \(\omega_{\{G_{i},G_{j}\}}=\mathrm{TSLS}(X,Y,\{G_{i},G_{j}\})\) with \(i\neq j\) and \(i,j\in\{2,4,5\}\)","\mathrm{corr}(Y-X\omega_{\{G_{4}\}},G_{2})\neq 0,\qquad\mathrm{corr}(Y-X\omega_{\{G_{5}\}},G_{4})\neq 0.","Equation 6 illustrates that correlations between residuals and excluded IVs are non-zero when using invalid IVs, indicating violation of the exclusion restriction assumption."
neurips_2024_oral_25,7,"\).

Based on these two observations, we can see that the valid and invalid IV sets entail clearly different constraints regarding the correlation coefficients3, which inspires us to further derive our identification analysis. For details about the observations, please refer to Appendix C.

Footnote 3: Please note that these constraints can be tested easily by observed data.

### Main Identification Results

We begin with the following definition and the main assumptions.

**Definition 1**.: _(Pseudo-Residual) Let \(\mathbb{G}\) be a subset of candidate genetic variants. A pseudo-residual of \(\{X,Y\}\) relative to \(\mathbb{G}\) is defined as_","\mathcal{PR}_{(X,Y\,|\,\mathbb{G})}:=Y-X\omega_{\mathbb{G}},",,"R(X,Y,\mathbb{G})=Y-X\cdot\mathrm{TSLS}(X,Y,\mathbb{G}).",A pseudo-residual quantifies the discrepancy between the outcome $Y$ and the predicted value of $Y$ based on the TSLS estimator using a subset of genetic variants $\mathbb{G}$.
neurips_2024_oral_25,8,"_where \(\omega_{\mathbb{G}}=\mathrm{TSLS}(X,Y,\mathbb{G})\)._

This ""pseudo-residual"" is applied to measure the uncorrelated relationship with some genetic variants. Note that concepts to ""pseudo-residual"" have been developed to address different tasks (Dtron and Richardson, 2004; Chen et al., 2017; Cai et al., 2019; Xie et al., 2020), but our formalization is different from theirs in terms of the parameter \(\omega_{\mathbb{G}}\). To the best of our knowledge, it has not been realized that the uncorrelated property involving such pseudo-residuals reflects the validity of the IVs in the bi-directional MR causal model (see Propositions 2 \(\sim\) 3).

As analyzed by Chu et al. (2001), within the framework of linear models, a variable being a valid IV imposes no constraints on the joint marginal distribution of the observed variables. In other words, there is no available test to determine whether a variable is a valid IV without making further assumptions. Therefore, we introduce the following assumption.

**Assumption 1** (Valid IV Set).: _For a given causal relationship (if the relationship exists), there exists a valid IV set that consists of at least two valid IVs. For example, for the causal relationship \(X\to Y\), \(|\mathbf{G}_{\mathcal{V}}^{X\to Y}|\geq 2\)._

Figure 2: An illustrative example where valid and invalid IV sets induce distinct constraints, where \(\mathbf{G}_{\mathcal{V}}^{X\to Y}=(G_{1},G_{3})^{\intercal}\) is a valid IV set, while \(\mathbf{G}_{\mathcal{I}}^{X\to Y}=(G_{2},G_{4},G_{5})^{\intercal}\) is invalid due to pathways \(G_{2}\to Y\), \(G_{4}\to Y\) and \(G_{5}\to Y\).

Assumption 1 is the same as in (Silva and Shimizu, 2017; Cheng et al., 2023) if the model is a one-directional MR model. Notice that this assumption is much milder than the _Majority rule_ assumption (Kang et al., 2016; Bowden et al., 2016; Windmeijer et al., 2019; Hartford et al., 2021) and _Plurality rule_ assumption (Guo et al., 2018; Windmeijer et al., 2021): Assumption 1 does not rely on the total number of genetic variants, while _Majority rule_ and _Plurality rule_ assumptions do.

We now show that under Assumption 1, one can identify the class of invalid IV sets.

**Proposition 2** (Identifying Invalid IV Sets).: _Let \(\mathbb{G}=\{G_{1},\ldots,G_{p}\},p\geq 2\) be a subset of candidate genetic variants \(\mathbf{G}\). Suppose that Assumption 1 and the size of participants \(n\rightarrow\infty\) hold. If there exists a \(G_{j}\in\mathbb{G}\) such that,_","\mathrm{corr}(\mathcal{PR}_{\langle X,Y\,|\,\{G_{j}\}\rangle},G_{j })\neq 0,",,"\mathrm{corr}(\mathcal{PR}_{(X,Y\,|\,\mathbb{G}\setminus\{G_{j}\})},G_{j})\neq 0,","where $\mathcal{PR}_{(X,Y\,|\,\mathbb{G}\setminus\{G_{j}\})}$ denotes the pseudo-residual computed by excluding $G_j$ from the set $\mathbb{G}$, and $\mathrm{corr}(\cdot)$ is the Pearson correlation coefficient."
neurips_2024_oral_25,9,"_then \(\mathbb{G}\) is an invalid IV set for any one of the causal relationships._

Proposition 2 is valuable when given a set of genetic variants but lack prior knowledge of the validity of IV sets. By testing specific correlations in Eq.(5), we can conclude that the current IV set is invalid.

**Example 1**.: _Consider the example in Figure 2. Suppose \(\mathbb{G}=\{G_{1},G_{2}\}\), according to Proposition 2 we have \(\mathrm{corr}(\mathcal{PR}_{\left\langle X,Y\,|\,\{G_{2}\}\right\rangle},G_{1} )\neq 0\). Thus \(\mathbb{G}\) is an invalid IV set._

One may naturally raise the following question: Can all the invalid IV sets be identified by Proposition 2? In other words, can we identify all the valid IV sets? Unfortunately, Assumption 1 is an insufficient condition for identifying the sets of valid IVs. See an illustrative example below.

**Example 2** (Counterexample).: _Consider the example in Figure 3. Suppose \(\mathbb{G}=\{G_{1},G_{2}\}\). In subgraph (a), for \(G_{1}\in\mathbb{G}\), according to Proposition 2 we obtain_","\mathrm{corr}(\mathcal{PR}_{\langle X,Y\,|\,\{G_{2}\}\rangle},G_{1} )=0",,"\mathrm{corr}(\mathcal{PR}_{\langle X,Y\,|\,\{G_{1}\}\rangle},G_{1})=0,","where $ \mathcal{PR}_{\langle X,Y\,|\,\{G_{1}\}\rangle} $ denotes the pseudo-residual of $ \{X,Y\} $ relative to $ \{G_{1}\} $."
neurips_2024_oral_25,10,"_Similarly, for \(G_{2}\in\mathbb{G}\), we have_","\mathrm{corr}(\mathcal{PR}_{\langle X,Y\,|\,\{G_{1}\}\rangle}),G_{ 2})=0",,"\mathrm{corr}(\mathcal{PR}_{\langle X,Y\,|\,\{G_{1}\}\rangle},G_{2} )=0",
neurips_2024_oral_25,11,"_In subgraph (b), for \(G_{1}\in\mathbb{G}\), we check the condition of Proposition 2 and obtain_","\mathrm{corr}(\mathcal{PR}_{\langle X,Y\,|\,\{G_{2}\}\rangle},G_{1} )=\frac{\gamma_{Y,1}\gamma_{X,2}-\gamma_{Y,2}\gamma_{X,1}}{\beta_{Y\to X }\gamma_{Y,2}+\gamma_{X,2}}.",,"\mathrm{corr}(\mathcal{PR}_{\langle X,Y\,|\,\{G_{2}\}\rangle},G_{1})\neq 0",Correlation condition for identifying an invalid IV set in subgraph (b) of Example 2.
neurips_2024_oral_25,12,"_Similarly, for \(G_{2}\in\mathbb{G}\), we have_","\mathrm{corr}(\mathcal{PR}_{\langle X,Y\,|\,\{G_{1}\}\rangle},G_{2} )=\frac{\gamma_{Y,2}\gamma_{X,1}-\gamma_{Y,1}\gamma_{X,2}}{\beta_{Y\to X }\gamma_{Y,1}+\gamma_{X,1}}.",,"\mathrm{corr}(\mathcal{PR}_{\langle X,Y\,|\,\{G_{1}\}\rangle},G_{ 2})=\frac{\gamma_{Y,2}\gamma_{X,1}-\gamma_{Y,1}\gamma_{X,2}}{\beta_{Y\to X }\gamma_{Y,1}+\gamma_{X,1}}.","These correlations depend on the direct effects of genetic variants on phenotypes and the causal effect of $Y$ on $X$, revealing conditions under which a candidate IV set may appear valid even when it is not."
neurips_2024_oral_25,13,"_We know that \(\{G_{1},G_{2}\}\) is an invalid IV set in this case. However, if the proportion of effects of \(G_{1}\) on \(X\) and \(Y\) is equal to the proportion of the effects of \(G_{2}\) on \(X\) and \(Y\), i.e., \(\gamma_{X,1}\gamma_{Y,2}=\gamma_{Y,1}\gamma_{X,2}\), we have_","\mathrm{corr}(\mathcal{PR}_{\langle X,Y\,|\,\{G_{2}\}\rangle},G_{1} )=0,\qquad\mathrm{corr}(\mathcal{PR}_{\langle X,Y\,|\,\{G_{1}\}\rangle },G_{2})=0,",,"\mathrm{corr}(\mathcal{PR}_{\langle X,Y\,|\,\{G_{2}\}\rangle},G_{1})=0,\qquad\mathrm{corr}(\mathcal{PR}_{\langle X,Y\,|\,\{G_{1}\}\rangle},G_{2})=0.","Equation 13 shows that under a specific proportionality condition between the effects of $G_1$ and $G_2$ on $X$ and $Y$, the correlations between the pseudo-residuals and the genetic variants vanish, even though $\{G_1, G_2\}$ is an invalid IV set."
neurips_2024_oral_25,14,"_which is consistent with the conclusion of subgraph (a). These observations imply that one can not identify valid IV sets under only Assumption 1._

To give sufficient conditions for identifying valid IV sets, we first introduce the following assumption.

**Assumption 2** (Generic Identifiability).: _For a given MR causal model, parameters \(\gamma\) and \(\beta\) live in a set of Lebesgue measure non-zero._

Figure 3: An illustrative example that valid and invalid IV sets may induce the same constraints in Eq.(5) of Proposition 2, where \(\{G_{1},G_{2}\}\) is the set of valid IVs in (a), while invalid in (b). It implies Assumption 1 is not sufficient to find valid IV sets.

In other words, Assumption 2 specifically implies that given a general subset of candidate genetic variants \(\mathbb{G}\), there exists a \(G_{j}\in\mathbb{G}\) that satisfies \(|\sum_{G_{i}\in\mathbb{G}/G_{j}}\tilde{\beta}_{i}cov(G_{i},G_{i})[(\gamma_{X,i} +\gamma_{X,U}\gamma_{U,i})(\gamma_{Y,j}+\gamma_{Y,U}\gamma_{U,j})-(\gamma_{Y,i} +\gamma_{Y,U}\gamma_{U,i})(\gamma_{X,j}+\gamma_{X,U}\gamma_{U,j})]|\neq 0\), where \(\tilde{\beta}_{i}\neq 0\) is the estimated coefficient between \(G_{i}\) and \(X\). For instance, in Example 2, such an assumption simplifies to \(\gamma_{X,1}\gamma_{Y,2}\neq\gamma_{Y,1}\gamma_{X,2}\). Whenever \(\gamma_{X,1}\gamma_{Y,2}=\gamma_{Y,1}\gamma_{X,2}\), parameters \(\gamma\) and \(\beta\) would fall into a set of Lebesgue measure zero, rendering non-identifiable of the model. It is worth noting that in real life, although we cannot obtain the ground truths of causal effects between variables, especially when latent confounders are involved, Assumption 2 is generally satisfied. This is because the set of conditions to violate it occupies a very small portion of the entire space.

With Assumption 2, we now derive sufficient conditions for generic identifiability in Proposition 3.

**Proposition 3** (Identifying Valid IV Sets).: _Let \(\mathbb{G}=\{G_{1},\ldots,G_{p}\},p\geq 2\) be a subset of candidate genetic variants \(\mathbf{G}\). Suppose that Assumptions 1 and 2, and the size of participants \(n\to\infty\) hold. If each \(G_{j}\in\mathbb{G}\) satisfies_","\mathrm{corr}(\mathcal{PR}_{(X,Y\mid\mathbb{G}\setminus G_{j})},G_{j})=0,",,"\mathrm{corr}(\mathcal{PR}_{\langle X,Y\,|\,\mathbb{G}/\{G_{j}\}\rangle},G_{j })=0,","where $\mathcal{PR}_{\langle X,Y\,|\,\mathbb{G}/\{G_{j}\}\rangle}$ denotes the pseudo-residual obtained by regressing $Y$ on $X$ using the subset of genetic variants $\mathbb{G}$ excluding $G_j$, and the condition ensures that $G_j$ is a valid IV given the rest of the set $\mathbb{G}$."
neurips_2024_oral_26,1,"## 3 Background: Diffusion and Distribution Matching Distillation

This section gives a brief overview of diffusion models and distribution matching distillation (DMD).

**Diffusion Models** generate images through iterative denoising. In the forward diffusion process, noise is progressively added to corrupt a sample \(x\sim p_{\text{real}}\) from the data distribution into pure Gaussian noise over a predetermined number of steps \(T\), so that, at each timestep \(t\), the diffused samples follow the distribution \(p_{\text{real},t}(x_{t})=\int p_{\text{real}}(x)q(x_{t}|x)dx\), with \(q_{t}(x_{t}|x)\sim\mathcal{N}(\alpha_{t}x,\sigma_{t}^{2}\mathbf{I})\), where \(\alpha_{t},\sigma_{t}>0\) are scalars determined by the noise schedule [47, 48]. The diffusion model learns to iteratively reverse the corruption process by predicting a denoised estimate \(\mu(x_{t},t)\), conditioned on the current noisy sample \(x_{t}\) and the timestep \(t\), ultimately leading to an image from the data distribution \(p_{\text{real}}\). After training, the denoised estimate relates to the gradient of the data likelihood function, or score function [48] of the diffused distribution:","s_{\text{real}}(x_{t},t)=\nabla_{x_{t}}\log p_{\text{real},t}(x_{t})=-\frac{x_ {t}-\alpha_{t}\mu_{\text{real}}(x_{t},t)}{\sigma_{t}^{2}}.",,"p_{\text{real},t}(x_{t}) = \int p_{\text{real}}(x) \, q(x_{t} \mid x) \, dx","The marginal distribution of the diffused sample at step $t$, derived from the forward diffusion process."
neurips_2024_oral_26,2,"Sampling an image typically requires dozens to hundreds of denoising steps [49, 50, 51, 52].

**Distribution Matching Distillation (DMD)** distills a many-step diffusion models into a one-step generator \(G\)[22] by minimizing the expectation over \(t\) of approximate Kullback-Liebler (KL) divergences between the diffused target distribution \(p_{\text{real},t}\) and the diffused generator output distribution \(p_{\text{fake},t}\). Since DMD trains \(G\) by gradient descent, it only requires the gradient of this loss, which can be computed as the difference of 2 score functions:","\nabla\mathcal{L}_{\text{BMD}}=\mathbb{E}_{t}(\nabla_{\theta}\text{KL}( p_{\text{fake},t}\|p_{\text{real},t}))=-\mathbb{E}_{t}(\int(s_{ \text{real}}(F(G_{\theta}(z),t),t)-s_{\text{fake}}(F(G_{\theta}(z),t),t) )\frac{dG_{\theta}(z)}{d\theta}\,dz),","where \(z\sim\mathcal{N}(0,\mathbf{I})\) is a random Gaussian noise input, \(\theta\) are the generator parameters, \(F\) is the forward diffusion process (i","\nabla_{G}\mathcal{L}_{\text{DMD}}=\mathbb{E}_{t}\left[\nabla_{G}\text{KL}(p_{\text{real},t}||p_{\text{fake},t})\right]=\mathbb{E}_{t}\left[\mathbb{E}_{x_{t}\sim p_{\text{fake},t}}\left[s_{\text{real}}(x_{t},t)-s_{\text{fake}}(x_{t},t)\right]\nabla_{G}x_{t}\right].",The DMD loss gradient with respect to the generator $G$ is expressed as the expectation over timesteps $t$ of the inner product between the score function difference and the generator's output sensitivity.
neurips_2024_oral_26,3,"(i.e., noise injection) with noise level corresponding to time step \(t\), and \(s_{\text{real}}\) and \(s_{\text{fake}}\) are scores approximated using diffusion models \(\mu_{\text{real}}\) and \(\mu_{\text{fake}}\) trained on their respective distributions (Eq. (1)). DMD uses a frozen pre-trained diffusion model as \(\mu_{\text{real}}\) (the teacher), and dynamically updates \(\mu_{\text{fake}}\) while training \(G\), using a denoising score-matching loss on samples from the one-step generator, i.e., fake data [22, 47].

Yin et al. [22] found that an additional regression term [16] was needed to regularize the distribution matching gradient (Eq. (2)) and achieve high-quality one-step models. For this, they collect a dataset of noise-image pairs \((z,y)\) where the image \(y\) is generated using the teacher diffusion model, and a _deterministic_ sampler [53, 49, 50], starting from the noise map \(z\). Given the same input noise \(z\), the regression loss compares the generator output with the teacher's prediction:","\mathcal{L}_{\text{reg}}=\mathbb{E}_{(z,y)}d(G_{\theta}(z),y),","where \(d\) is a distance function, such as LPIPS [54] in their implementation","\mathcal{L}_{\text{reg}}=\mathbb{E}_{z}[\|G_{\theta}(z)-\mu_{\text{real}}(z,t_{\text{reg}})\|^{2}],","where $t_{\text{reg}}$ is a fixed timestep chosen for the regression loss, typically early in the diffusion process to provide stable target estimates."
neurips_2024_oral_26,4,"on. While gathering this data incurs negligible cost for small datasets like CIFAR-10, it becomes a significant bottleneck with large-scale text-to-image synthesis tasks, or models with complex conditioning [55, 56, 57]. For instance, generating one noise-image pair for SDXL [58] takes around 5 seconds, amounting to about 700 A100 days to cover the 12 million prompts in the LAION 6.0 dataset [59], as utilized by Yin et al. [22]. This dataset construction cost alone is already more than \(4\times\) our total training compute (asdetailed in Appendix J). This regularization objective is also at odds with DMD's goal of matching the student and teacher in _distribution_, since it encourages adherence to the teacher's sampling paths.

## 4 Improved Distribution Matching Distillation

We revisit multiple design choices in the DMD algorithm [22] and identify significant improvements.

### Removing the regression loss: true distribution matching and easier large-scale training

The regression loss [16] used in DMD [22] ensures mode coverage and training stability, but as we discussed in Section 3, it makes large-scale distillation cumbersome, and is at odds with the distribution matching idea, thus inherently limiting the performance of the distilled generator to that of the teacher model. Our first improvement is to remove this loss.

### Stabilizing pure distribution matching with a Two Time-scale Update Rule

Naively omitting the regression objective, shown in Eq. (3), from DMD leads to training instabilities and significantly degrades quality (Tab. 3). For example, we observed that the average brightness, along with other statistics, of generated samples fluctuates significantly, without converging to a stable point (See Appendix G). We attribute this instability to approximation errors in the fake diffusion model \(\mu_{\text{fake}}\), which does not track the fake score accurately, since it is dynamically optimized on the non-stationary output distribution of the generator. This causes approximation errors and biased generator gradients (as also discussed in [30]).

We address this using the two time-scale update rule inspired by Heusel et al. [60]. Specifically, we train \(\mu_{\text{fake}}\) and the generator \(G\) at different frequencies to ensure that \(\mu_{\text{fake}}\) accurately tracks the generator's output distribution. We find that using 5 fake score updates per generator update, without the regression loss, provides good stability and matches the quality of the original DMD on ImageNet (Tab. 3) while achieving much faster convergence. Further analysis are included in Appendix G.

### Surpassing the teacher model using a GAN loss and real data

Our model so far achieves comparable training stability and performance to DMD [22] without the need for costly dataset construction (Tab. 3). However, a performance gap remains between the distilled generator and the teacher diffusion model. We hypothesize this gap could be attributed to approximation errors in the real score function \(\mu_{\text{real}}\) used in DMD, which would propagate to the generator and lead to suboptimal results. Since DMD's distilled model is never trained with real data, it cannot recover from these errors.

We address this issue by incorporating an additional GAN objective into our pipeline, where the discriminator is trained to distinguish between _real_ images and images produced by our generator.

Figure 3: Our method distills a costly diffusion model (gray, right) into a one- or multi-step generator (red, left). Our training alternates between 2 steps: 1. optimizing the generator using the gradient of an implicit distribution matching objective (red arrow) and a GAN loss (green), and 2. training a score function (blue) to model the distribution of “fake” samples produced by the generator, as well as a GAN discriminator (green) to discriminate between fake samples and real images. The student generator can be a one-step or a multi-step model, as shown here, with an intermediate step input.

Trained using real data, the GAN classifier does not suffer from the teacher's limitation, potentially allowing our student generator to surpass it in sample quality. Our integration of a GAN classifier into DMD follows a minimalist design: we add a classification branch on top of the bottleneck of the fake diffusion denoiser (see Fig. 3). The classification branch and upstream encoder features in the UNet are trained by maximizing the standard non-saturing GAN objective:","\mathcal{L}_{\text{GAN}}=\mathbb{E}_{x\sim p_{\text{noise}},t\sim[0,T]}[\log D (F(x,t))]+\mathbb{E}_{z\sim p_{\text{noise}},t\sim[0,T]}[-\log(D(F(G_{\theta}(z ),t)))],","where \(D\) is the discriminator, and \(F\) is the forward diffusion process (i",\mathcal{L}_{\text{GAN}} = \mathbb{E}_{z} \left[ - \log D(G_{\theta}(z)) \right],"where $D$ is the discriminator trained to distinguish real images from generator outputs $G_{\theta}(z)$, and $z\sim\mathcal{N}(0,\mathbf{I})$ is Gaussian noise input."
neurips_2024_oral_27,1,"## 2 Background

We briefly review the background of Unsupervised Environment Design (UED) in this section. UED problems are modeled as an Underspecified Partially Observable Markov Decision Process (UPOMDP) defined by the tuple:","\langle S,A,O,\mathcal{I},\mathcal{T},\mathcal{R},\gamma,\Theta\rangle","where \(S\), \(A\) and \(O\) are the sets of states, actions, and observations, respectively",,
neurips_2024_oral_27,2,"ly. \(\Theta\) represents a set of free parameters where each \(\theta\in\Theta\) defines a specific instantiation of an environment (also known as a _level_). We use the terms ""environments"" and ""levels"" interchangeably throughout this paper. The level-conditional observation and transition functions are defined as \(\mathcal{I}:S\times\Theta\to O\) and \(\mathcal{T}:S\times A\times\Theta\to\Delta(S)\), respectively. The student agent, with policy \(\pi\), receives a reward based on the reward function \(\mathcal{R}:S\times A\to\mathbb{R}\) with a discount factor \(\gamma\in[0,1]\). The student seeks to maximize its expected value for each \(\theta\) denoted by \(V^{\theta}(\pi)=\mathbb{E}_{\pi}[\sum_{t=0}^{T}R(s_{t},a_{t})\gamma^{t}]\). The teacher's goal is to select levels forming the curriculum by maximizing a utility function \(U(\pi,\theta)\), which depends on \(\pi\).

Different UED approaches vary primarily in the teacher's utility function. _Domain Randomization_ (DR; [53]) uniformly randomizes environment parameters, with a constant utility \(U^{\mathcal{U}}(\pi,\theta)=C\), making it agnostic to the student's policy. _Minimax training_[39] adversarially generates challenging levels, with utility \(U^{\mathcal{M}}(\pi,\theta)=-V^{\theta}(\pi)\), to minimize the student's return. However, this approach incentivizes the teacher to make the levels completely unsolvable, limiting room for learning. Recent UED methods address this by using a teacher that maximizes _regret_, defined as the difference between the return of the optimal policy and the current policy. Regret-based utility is defined as \(U^{\mathcal{R}}(\pi,\theta)=\textsc{Regret}^{\theta}(\pi,\pi^{*})=V^{\theta}( \pi^{*})-V^{\theta}(\pi)\) where \(\pi^{*}\) is the optimal policy on \(\theta\). Regret-based objectives have been shown to promote the simplest levels that the student cannot solve optimally, and benefit from the theoretical guarantee of a minimax regret robust policy upon convergence in the two-player zero-sum game. However, since \(\pi^{*}\) is generally unknown, regret must be approximated. Dennis et al. [23], the pioneer UED work, introduced a principled level generation based on the regret objective and proposed the _PAIRED_ algorithm, where regret is estimated by the difference between the returns attained by an antagonist agent and the protagonist (student) agent. Later on, Jiang et al. [27] introduced \(\textit{PLR}^{\perp}\) which combines DR with regret using _Positive Value Loss_ (PVL), an approximation of regret based on Generalized Advantage Estimation (GAE; [48]):","\text{PVL}^{\theta}(\pi)=\frac{1}{T}\sum_{t=0}^{T}\max(\sum_{k=t}^{T}( \gamma\lambda)^{k-t}\delta_{k}^{\theta},0),","where \(\lambda\) and \(T\) are the GAE discount factor and MDP horizon, respectively","U^{\mathcal{R}}(\pi,\theta)=\textsc{Regret}^{\theta}(\pi,\pi^{*})=V^{\theta}( \pi^{*})-V^{\theta}(\pi)","Regret-based utility function where $ \pi^{*} $ is the optimal policy on $ \theta $, measuring the difference in return between the optimal policy and the current policy."
neurips_2024_oral_27,3,"ly. \(\delta_{k}^{\theta}\) is the TD-error at time step \(k\) for \(\theta\). The state-of-the-art UED algorithm, _ACCEL_[37], improves \(\text{PLR}^{\perp}\)[27] by replacing its random level generation with an editor that mutates previously curated levels to gradually introduce complexity into the curriculum.

## 3 Related Work

It is important to note that regret-based UED approaches provide a minimax regret guarantee at Nash Equilibrium; however, they provide no explicit guarantee of convergence to such equilibrium. Beukman et al. [10] demonstrated that the minimax regret objective does not necessarily align with learnability: an agent may encounter UPOMDPs with high regret on certain levels where it already performs optimally (given the partial observability constraints), while there exist other levels with lower regret where it could still improve. Consequently, selecting levels solely based on regret can lead to _regret stagnation_, where learning halts prematurely. This suggests that focusing exclusively on minimax regret may inhibit the exploration of levels where overall regret is non-maximal, but opportunities for acquiring transferable skills for generalization are significant. Thus, there is a compelling need for a complementary objective, such as novelty, to explicitly guide level selection towards enhancing zero-shot generalization performance and mitigating regret stagnation.

The _Paired Open-Ended Trailbazer_ (POET; [56]) algorithm computes novelty based on environment encodings--a vector of parameters that define level configurations. POET maintains a record of the encodings from previously generated levels and computes the novelty of a new level by measuring the average distance between the k-nearest neighbors of its encoding. However, this method for computing novelty is domain-specific and relies on human expertise in designing environment encodings, posing challenges for scalability to complex domains. Moreover, due to UED's underspecified nature, where free parameters may yield a one-to-many mapping between parameters and environments instances, each inducing distinct agent behaviors, quantifying novelty based on parameters alone is futile.

_Enhanced POET_ (EPOET; [57]) improves upon its predecessor by introducing a domain-agnostic approach to quantify a level's novelty. EPOET is grounded in the insight that novel levels offer new insights into how the behaviors of agents within them differ. EPOET evaluates both active and archived agents' performance in each environment, converting their performance rankings into rank-normalized vectors. The level's novelty is then computed by measuring the Euclidean distance between these vectors. Despite addressing POET's domain-specific limitations, EPOET encounters its own challenges. The computation of rank-normalized vectors only works for population-based approaches as it requires evaluating multiple trained student agents and incurs substantial computational costs. Furthermore, EPOET remains curriculum-agnostic, as its novelty metric relies on the ordering of raw returns within the agent population, failing to directly assess whether the environment elicits rarely observed states and actions in the existing curriculum.

_Diversity Induced Prioritized Level Replay_ (DIPLR; [33]), calculates novelty using the Wasserstein distance between occupancy distributions of agent trajectories from different levels. DIPLR maintains a level buffer and determines a level's novelty as the minimum distance between the agent's trajectory on the candidate level and those in the buffer. While DIPLR incorporates the agent's experiences into its novelty calculation, it faces significant scalability and robustness issues. First, relying on the Wasserstein distance is notoriously costly. Additionally, DIPLR requires pairwise distance computations between all levels in the buffer, causing computational costs to grow exponentially with more levels. Finally, although DIPLR promotes diversity within the active buffer, it fails to evaluate whether state-action pairs in the current trajectory have already been adequately explored through past curriculum experiences, making it arguably still curriculum-agnostic. Further discussions on relevant literature can be found in Appendix B.

## 4 Approach: CENIE

The limitations of prior approaches to quantifying environment novelty underscore the need for a more robust framework, motivating the development of CENIE. CENIE quantifies environment novelty through state-action space coverage derived from the agent's accumulated experiences across previous environments in its curriculum. In single-environment online reinforcement learning, coverage within the training distribution is often linked to sample efficiency [59], providing inspiration for the CENIE framework. Given UED's objective to enhance a student's generalizability across a vast and often unseen (during training) environment space, quantifying novelty in terms of state-action space coverage has several benefits. By framing novelty in this way, CENIE enables a sample-efficient exploration of the environment search space by prioritizing levels that drive the agent towards unfamiliar state-action combinations. This provides a principled basis for directing the environment design towards enhancing the generalizability of the student agent. Additionally, a distinctive benefit of this approach is that it is not confined to any particular UED or DRL algorithms since it solely involves modeling the agent's state-action space coverage. This flexibility allows us to implement CENIE atop any UED algorithm.

CENIE's approach to novelty quantification through state-action coverage introduces three key attributes, effectively addressing the limitations of previous methods: (1) **domain-agnostic**, (2) **curriculum-aware**, and (3) **scalable**. CENIE is domain-agnostic, as it quantifies novelty solely based on the state-action pairs of the student, thus eliminating any dependency on the encoding of the environment. CENIE achieves curriculum-awareness by quantifying novelty using a model of the student's past aggregated experiences, i.e., state-action space coverage, ensuring that the selection of environments throughout the curriculum is sample-efficient with regards to expanding the student's state-action coverage. Lastly, CENIE demonstrates scalability by avoiding the computational burden associated with exhaustive pairwise comparisons or costly distance metrics.

### Measuring the Novelty of a Level

To evaluate the novelty of new environments using the agent's state-action pairs, the teacher needs to first model the student's past state-action space coverage distribution. We propose to use GMMs as they are particularly effective due to their robustness in representing high-dimensional continuous distributions [14; 5]. A GMM is a probabilistic clustering model that represents the underlying distribution of data points using a weighted combination of multivariate Gaussian components. Once the state-action distribution is modeled using a GMM, we can leverage it for density estimation. Specifically, the GMM allows us to evaluate the likelihood of state-action pairs induced by new environments, where lower likelihoods indicate experiences that are less represented in the student's current state-action space. This likelihood provides a quantitative measure of dissimilarity in state-action space coverage, enabling a direct comparison of novelty between levels. It is important to note that CENIE defines a general framework for quantifying novelty through state-action space coverage;

Figure 1: An overview of the CENIE framework. The teacher will utilise environment regret and novelty for curating student’s curriculum. \(\Gamma\) contains past experiences and \(\tau\) is the recent trajectory.

GMMs represent just one possible method for modeling this coverage. Future research may explore alternatives to model state-action space coverage within the CENIE framework (see Section C in the appendix for more discussions).

Before detailing our approach, we first define the notations used in this section. Let \(l_{\theta}\) be a particular level conditioned by an environment parameter \(\theta\). We refer to \(l_{\theta}\) as the candidate level, for which we aim to determine its novelty. The agent's trajectory on \(l_{\theta}\) is denoted as \(\tau_{\theta}\), and can be decomposed into a set of sample points, represented as \(X_{\theta}=\{x=(s,a)\sim\tau_{\theta}\}\). The set of past training levels is represented by \(L\) and \(\Gamma=\{x=(s,a)\sim\tau_{L}\}\) is a buffer containing the state-action pairs collected from levels across \(L\). We treat \(\Gamma\) as the ground truth of the agent's state-action space coverage, against which we evaluate the novelty of state-action pairs from the candidate level \(X_{\theta}\).

To fit a GMM on \(\Gamma\), we must find a set of Gaussian mixture parameters, denoted as \(\lambda_{\Gamma}=\{(\alpha_{1},\mu_{1},\Sigma_{1}),...,(\alpha_{K},\mu_{K}, \Sigma_{K})\}\), that best represents the underlying distribution. Here, \(K\) denotes the predefined number of Gaussians in the mixture, where each Gaussian component is characterized by its weight (\(\alpha_{k}\)), mean vector (\(\mu_{k}\)), and covariance matrix (\(\Sigma_{k}\)), with \(k\in\{1,...,K\}\). We employ the _kmeans++_ algorithm [12, 4] for a fast and efficient initialization of \(\lambda_{\Gamma}\). The likelihood of observing \(\Gamma\) given the initial GMM parameters \(\lambda_{\Gamma}\) is expressed as:","P(\Gamma\mid\lambda_{\Gamma})=\prod_{j=1}^{J}\sum_{k=1}^{K}\alpha_{k}\mathcal{N }(x_{j}\mid\mu_{k},\Sigma_{k})",where \(x_{j}\) is a state-action pair sample from \(\Gamma\),"p(\Gamma|\lambda_{\Gamma})=\prod_{x\in\Gamma}\sum_{k=1}^{K}\alpha_{k}\mathcal{N}(x|\mu_{k},\Sigma_{k}),","where each Gaussian component is characterized by its weight $\alpha_k$, mean vector $\mu_k$, and covariance matrix $\Sigma_k$, with $k \in \{1, \dots, K\}$"
neurips_2024_oral_27,4,"\). \(\mathcal{N}(x_{j}\mid\mu_{k},\Sigma_{k})\) represents the multi-dimensional Gaussian density function for the \(k\)-th component with mean vector \(\mu_{k}\) and covariance matrix \(\Sigma_{k}\). To optimise \(\lambda_{\Gamma}\), we use the Expectation Maximization (EM) algorithm [22, 43] because Eq. 2 is a non-linear function of \(\lambda_{\Gamma}\), making direct maximization infeasible. The EM algorithm iteratively refines the initial \(\lambda_{\Gamma}\) to estimate a new \(\lambda_{\Gamma}^{\prime}\) such that \(P(X\mid\lambda_{\Gamma}^{\prime})>P(X\mid\lambda_{\Gamma})\). This process is repeated iteratively until some convergence, i.e., \(\parallel\lambda_{\Gamma}^{\prime}-\lambda_{\Gamma}\|<\epsilon\), where \(\epsilon\) is a small threshold.

Once the GMM is fitted, we can use \(\lambda_{\Gamma}\) to perform density estimation and calculate the novelty of the candidate level \(l_{\theta}\). Specifically, we consider the set of state-action pairs from the agent's trajectory, \(X_{\theta}\), and compute their posterior likelihood under the GMM. This likelihood indicates how similar the new state-action pairs are to the learned distribution of past state-action coverage. Consequently, the novelty score of \(l_{\theta}\) is represented as follows:",\textsc{Novelty}_{l_{\theta}}=-\frac{1}{|X_{\theta}|}\log\mathcal{L}(X_{ \theta}\mid\lambda_{\Gamma})=-\frac{1}{|X_{\theta}|}\sum_{t=1}^{T}\log p(x_{t} \mid\lambda_{\Gamma}),where \(x_{t}\) is a sample state-action pair from \(X_{\theta}\),\text{Novelty}(l_{\theta}) = -\frac{1}{|X_{\theta}|}\sum_{x\in X_{\theta}}\log P(x\mid\lambda_{\Gamma}),where $P(x\mid\lambda_{\Gamma})$ is the likelihood of observing a state-action pair $x$ under the GMM parameters $\lambda_{\Gamma}$
neurips_2024_oral_27,5,"\). As shown in Eq. 3, we take the negative mean log-likelihood across all samples in \(X_{\theta}\) to attribute higher novelty scores to levels with state-action pairs that are less likely to originate from the aggregated past experiences, \(\Gamma\). This novelty metric promotes candidate levels that induce more novel experiences for the agent during training. More details on fitting GMMs are explained in Appendix D.1.

Design considerations for the GMMFirst, we specifically designate the state-action coverage buffer, i.e., \(\Gamma\), as a First-In-First-Out (FIFO) buffer with a fixed window length. By focusing on a fixed window rather than the entire history of state-action pairs, our novelty metric avoids bias toward experiences that are outdated and have not appeared in recent trajectories. This design choice helps reduce the effects of catastrophic forgetting prevalent in DRL. Next, it is known that by allowing the adaptation of the number of Gaussians in the mixture, i.e., \(K\) in Eq. 2, any smooth density distribution can be approximated arbitrarily close [24]. Therefore, to optimize the GMM's representation of the agent's state-action coverage distribution, we fit multiple GMMs with varying numbers of Gaussians within a predefined range at each time step and select the best one based on the silhouette score [45], an approach inspired by Portelas et al. [40]. The silhouette score evaluates clustering quality by measuring both intra-cluster cohesion and inter-cluster separation. This approach enables CENIE to construct a pseudo-online GMM model that dynamically adjusts its complexity to accommodate the agent's changing state-action coverage distribution.
With a scalable method to quantify the novelty of levels, we demonstrate its versatility and effectiveness by deploying it on top of the leading UED algorithms, PLR\({}^{\perp}\) and ACCEL. For convenience, in subsequent sections, we will refer to this CENIE-augmented methodology of PLR\({}^{\perp}\) and ACCEL using GMMs as PLR-CENIE and ACCEL-CENIE, respectively. Both PLR\({}^{\perp}\) and ACCEL utilize a replay mechanism to train their students on the highest-regret levels curated within the level buffer. To integrate CENIE within these algorithms, we use normalized outputs of a prioritization function to convert the level scores (novelty and regret) into level replay probabilities (\(P_{S}\)):",P_{S}=\frac{h(S_{i})^{\beta}}{\sum_{j}h(S_{j})^{\beta}},where \(h\) is a prioritization function (e,"P_{S}(l_{\theta})=\frac{\beta\cdot\textsc{Novelty}_{l_{\theta}}+(1-\beta)\cdot U^{\mathcal{R}}(\pi,\theta)}{\max(\textsc{Novelty}_{L})+\max(U^{\mathcal{R}}(\pi,L))}",where $ \beta $ is a hyperparameter balancing the contribution of novelty and regret in the prioritization function
neurips_2024_oral_27,6,"(e.g. rank) with a tunable temperature \(\beta\) that defines the prioritization of levels with regards to any arbitrary score \(S\). Following the implementations in PLR\({}^{\perp}\) and ACCEL, we employ \(h\) as the rank prioritization function, i.e., \(h(S_{i})=1/\text{rank}(S_{i})\), where \(\text{rank}(S_{i})\) is the rank of level score \(S_{i}\) among all scores sorted in descending order. In ACCEL-CENIE and PLR-CENIE, we use both the novelty and regret scores to determine the level replay probability:",P_{replay}=\alpha\cdot P_{N}+(1-\alpha)\cdot P_{R},"where \(P_{N}\) and \(P_{R}\) are the novelty-prioritized probability and regret-prioritized probability respectively, and \(\alpha\) allows us to adjust the weightage of each probability","P_{S}=\frac{[\text{rank}(U^{\mathcal{R}}(\pi,\theta))]^{-\beta_{1}}\cdot[\text{rank}(\textsc{Novelty}_{l_{\theta}})]^{-\beta_{2}}}{\sum_{j}[\text{rank}(U^{\mathcal{R}}(\pi,\theta_{j}))]^{-\beta_{1}}\cdot[\text{rank}(\textsc{Novelty}_{l_{\theta_{j}}})]^{-\beta_{2}}}}","where $\beta_{1}$ and $\beta_{2}$ are temperature parameters that control the influence of regret and novelty scores, respectively, in determining the level replay probability"
neurips_2024_oral_31,1,"## 2 Background

Mesh-based simulationA simulation can be represented as a time series of system states \(\mathcal{S}^{1},\ldots,\mathcal{S}^{T}\). The goal is to learn a neural simulator that predicts the next state \(\mathcal{S}^{t+1}\) given a history of previous states \(\{\mathcal{S}^{t-h+1},\ldots,\mathcal{S}^{t}\}\). Simulators based on graph networks (GNNs) [8, 29] encode the system state into a graph \(\mathcal{G}=(\mathcal{V},\mathcal{E})\) with nodes \(\mathcal{V}\) and edges \(\mathcal{E}\). For rigid body simulation, this graph can be constructed from the triangle mesh of the individual objects: mesh vertices become the graph nodes, and mesh edges act as graph edges. The object motion is computed by message passing across the nodes and edges in the graph. Within individual objects, the position, rotation and velocity of the object can be computed by message passing through the nodes and edges of that object.

Why learned simulators?Analytical simulators like MuJoCo, PyBullet or Drake, are most commonly used for modelling rigid bodies. However, the traditional simulators rely on hard-coded approximations of physical interactions that might not match the properties of real objects, even with careful hyperparameter tuning[1]. The predictions from these simulators inevitably diverge from observations of real objects - a so-called sim-to-real gap [17].

Learned simulators have unique advantages that analytical simulators don't provide. First, learned simulators can be trained directly on real-world observations. They can track the real object trajectory better than analytical simulators, solving a well-known sim-to-real gap[17]. Another common issue is precisely estimating the initial states, which analytical simulators rely on - learned simulators can compensate for these inaccuracies [3]. Finally, learned simulators are differentiable and can be used for optimization and design[2]. At the same time, learned simulators are not optimized for runtime and memory, are slower and more memory-constrained than analytical simulators. In this work, we address that limitation of memory constraint and unlock the ability to run learned simulators on larger scenes in comparison to previous learned simulators.

Figure 1: Overview of SDF-Sim pipeline. SDFs parameterized by MLPs are learned for each object to implicitly represent the object shape and the distance field. A GNN-based simulator uses learned SDFs to predict object dynamics for the next simulation step.

Modelling collisions in GNN simulatorsThe most challenging component of the simulation is computation of collision impulses between objects. To do so, GNN simulators introduce collision edges \(\mathcal{E}^{coll}\) between nodes [31] or triangles [4] on the mesh surface that belong to different objects and are within a predefined distance threshold. However, the amount of these edges is the main bottleneck of GNN-based simulators. Asymptotically, the number of potential collision edges grows quadratically with the number of simulated nodes, leading to prohibitive compute and memory costs. Another challenge is identifying which pairs of triangles/nodes to connect with collision edges in the first place, by computing the distance to the closest point on a mesh. Typically, this procedure is implemented with tree search methods over all mesh triangles in the scene, such as BVH [11], that are difficult to integrate into deep learning pipelines. In this work, we address both of these challenges by using SDFs.

Signed distance functionsSigned-distance functions (SDFs) are widely used in computer graphics, game engines, and robotics for fast collision detection and computation of distances to an object [33, 45]. SDF defines a field \(f(\mathbf{y}):\ \mathbb{R}^{3}\rightarrow\mathbb{R}\) that represents the signed distance from an arbitrary point \(\mathbf{y}\) to the closest point on the surface of the object. The sign of the SDF determines whether a point is outside (positive) or inside (negative) of the object. The zero level of the SDF \(\{\mathbf{y}\in\mathbb{R}^{3}|f(\mathbf{y})=0\}\) implicitly represents the object surface. SDFs permit constant-time queries of the distance to an object surface, irrespective of the number of nodes/faces in the object mesh, which is an essential component of collision detection.

Figure 3: Simulating assets extracted from vision. (a) We extract the SDF from the images of a real-world scene with a garden table [7]. (b) We simulate a virtual shoe object falling onto a vase and a table using SDF-Sim. SDF-Sim is able to predict realistic dynamics, even for the collision of the shoe with the intricate shape of the vase (frames 50-70). See section 4.4 for details and the video on the website.

Figure 2: Example of rollouts from SDF-Sim scaled to large simulations, all simulated for 200 steps. (Top) 300 shoes (object from Movi-C), with 851k nodes, falling onto the floor (Middle) 270 knots from Movi-B, 384k nodes (Bottom) 380 objects from Movi-C, 1.1M nodes. See simulation videos on [https://sites.google.com/view/sdf-sim](https://sites.google.com/view/sdf-sim).

Constructing an SDFIn the computer graphics and simulation literature, SDF is often pre-computed as a high-resolution 3D grid containing signed distances from the points on the grid to the object surface [26; 37]. The grid SDF allows to speed up distance queries by trading off memory: for example, 512x512x512 grid would take \(\approx\)134M voxels (0.5Gb of memory) for a single object.

Learned SDFs started to gain popularity for reconstruction of water-tight 3D shapes from images. They approximate the continuous distance field \(f(\mathbf{y};\theta)\) with an MLP parameterized by \(\theta\)[23; 28; 43]. Unlike 3D-grid-SDFs, learned SDFs are not tied to a fixed grid resolution and can represent detailed shapes using a small set of parameters \(\theta\). Despite these advantages, _learned_ SDFs have not been sufficiently explored to speed up physical simulations. Limited available works combine learned SDFs with classic physics solvers [5; 33]. They demonstrate that learned SDFs can massively reduce the distance query time thanks to parallelization on a GPU compared to mesh-based computation, while taking \(\sim\)32x less memory than traditional 3D-voxel-grid SDFs.

Computing closest pointsFor any point in 3D space \(\mathbf{y}\) an SDF \(f_{\theta}\) allows us to easily compute the closest point \(\mathbf{y}^{*}\) on the object surface that it represents, as:","\mathbf{y}^{*}=\mathbf{y}-f_{\theta}(\mathbf{y})\nabla f_{\theta}(\mathbf{y}),","where, by definition, \(f_{\theta}(\mathbf{y})\) is the distance between \(\mathbf{y}\) and \(\mathbf{y}^{*}\), and the gradient \(\nabla f_{\theta}(\mathbf{y})\) points in the opposite direction of the shortest path from \(\mathbf{y}\) to the surface of the object and is unit-norm","\begin{equation}
\mathbf{y}^{*} = \arg\min_{\mathbf{y}' \in \mathcal{M}} \|\mathbf{y} - \mathbf{y}'\|
\end{equation}",Computes the closest point $\mathbf{y}^{*}$ on the object surface $\mathcal{M}$ to a given 3D point $\mathbf{y}$ using the SDF.
neurips_2024_oral_31,2,"rm. If \(f_{\theta}(\mathbf{y})\) is parameterized as an MLP, this calculation requires only one forward and one backward pass of the network. This provides an efficient way to calculate the closest points for collision resolution in the simulation. In this work, for the first time in the literature, we use learned SDFs to accelerate distance computation in SOTA graph-network simulators.

## 3 SDF-Sim

We introduce SDF-Sim, a graph-network-based simulator for rigid objects that uses learned SDFs to represent object shapes and to perform fast and memory-efficient distance computation between the objects. By leveraging SDF properties, we propose a new way to construct the input graph for the graph network, allowing us to use a smaller graph size and to get an order-of-magnitude reduction in runtime and memory on large simulations.

### Training SDF functions per object

We represent SDF \(f_{\theta}(\mathbf{y})\) as an MLP that takes in a 3D point \(\mathbf{y}\in\mathbb{R}^{3}\) and outputs a scalar SDF value. Learned SDFs are pre-trained separately for each object and remain fixed throughout the simulation.

_Meshes_ To compare to the existing mesh-based baselines, we apply SDF-Sim on benchmarks where meshes are available and train the learned SDF \(f_{\theta}(\mathbf{y})\) from a mesh. To train an SDF, we sample points in 3D space and compute the ground-truth signed distances from these points to the mesh surfaces using a classic BVH [11] algorithm. Finally, we train an MLP \(f_{\theta}(\mathbf{y})\) to fit these distances with supervision. See more details in section C. We use the same architecture and the model size for each object shape of 8 MLP layers with 128 units each, unless otherwise stated.

_Vision_ We use VolSDF [43] to distill an SDF from a set of images representing a 360-degree view of the outdoor garden scene first described in [7] with camera poses estimated via COLMAP [32]. See visualisation in Figure 3 and section C.2 for details.

Figure 4: Construction of graph edges in SDF-Sim.

### Learned simulator

Object representationWe represent the shape of the object \(O_{i}\) in the reference pose (centered at zero) as a learned SDF function \(f_{\theta_{i}}\). At a simulation step \(t\), a rigid transformation \(\mathcal{T}^{t}_{i}=(\mathbf{p}^{t}_{i},\mathbf{R}^{t}_{i})\) transforms the object from reference pose to the current pose in the simulation. Here, \(\mathbf{p}^{t}_{i}\) is an object translation, corresponding to the object's center of mass at timestep \(t\); \(\mathbf{R}^{t}_{i}\) is a rotation. The task of the learned simulator is predicting the next-step transformation \(\mathcal{T}^{t+1}_{i}\) for each object. In the following text, we omit the time index \(t\) for brevity.

To represent \(I\) objects in the input graph, we introduce _object_ nodes \(\mathcal{V}_{O}=\{\mathbf{o}_{i}|i=1..I\}\), located at position \(\mathbf{p}_{i}\), and a set of _surface_ nodes \(\mathcal{V}_{S}=\{\mathbf{n}_{ik}\in O_{i}|i=1..I,k=1..K_{i}\}\) on the surface of the objects, where the number of nodes \(K_{i}\) may differ for each object. These surface nodes \(\{\mathbf{n}_{ik}\}\) move with their corresponding object according to the transformation \(\mathcal{T}^{t}_{i}\). With a slight abuse of notation, we will refer to \(\{\mathbf{n}_{ik}\}\) both as the node entities and their 3D position in the simulation space.

Nodes and edges within the objectTo construct the graph connectivity _within_ an object, we follow an established line of work on learned simulators [4, 19, 29]. We connect surface nodes \(\{\mathbf{n}_{ik}\}\) to their corresponding object node \(\mathbf{o}_{i}\) using edges \(\mathcal{E}^{\rm intra}_{i}=\{\mathbf{e}_{\mathbf{o}_{i}\leftrightarrow\mathbf{ n}_{ik}}|\mathbf{n}_{ik}\in O_{i}\}\). Thus, all the information about object motion, e.g., impulses from collision events, is propagated between the nodes via the object node \(\mathbf{o}_{i}\). As shown by [19] we can omit the surface edges between the nodes \(\{\mathbf{n}_{ik}\}\) without loss of accuracy.

In graph networks, nodes and edges are associated with feature vectors that can hold the information about the motion and the relation between the nodes. We follow the approach of [4, 19, 29] to construct a set of nodes and edge features that are necessary for simulation: To construct the node features for surface nodes \(\mathbf{n}_{ik}\), we compute the finite-difference velocity estimates in the simulation space using a history of the latest three timesteps \(\mathbf{v}_{ik}=(\mathbf{n}^{t}_{ik}-\mathbf{n}^{t-1}_{ik},\mathbf{n}^{t-1}_ {ik}-\mathbf{n}^{t-2}_{ik})\). We set node features to be \([\mathbf{v}_{ik},||\mathbf{v}_{ik}||,\phi_{i}]\) where \(\phi_{i}\) are the constant object parameters: mass, friction and restitution. We use the same procedure for the object nodes \(\{\mathbf{o}_{i}\}\) using their positions \(\mathbf{p}_{i}\). For intra-object edges \(\mathcal{E}^{\rm intra}_{i}\), we use displacement vector between the surface node position and the object center as the edge feature \(\mathbf{e}_{\mathbf{o}_{i}\leftrightarrow\mathbf{n}_{ik}}=[\mathbf{o}_{i}- \mathbf{n}_{ik},||\mathbf{o}_{i}-\mathbf{n}_{ik}||]\).

SDF-based inter-object edges.Here we introduce a new way to construct collision edges between the objects by leveraging their SDF representations (Figure 4). We design these edges such that they contain sufficient information to detect collisions, while their number remains linear in the number of nodes. This is unlike quadratic number of collision edges in mesh-based simulators.

Consider two objects \(O_{i}\) and \(O_{j}\). For a node \(\mathbf{n}_{ik}\) on \(O_{i}\), we directly query the SDF \(f_{\theta_{j}}\) of object \(O_{j}\) to get the distance \(d^{j}_{ik}\) from \(\mathbf{n}_{ik}\) to the closest point on \(O_{j}\). We note that unlike mesh-based approaches, this is a single test, and we do not need to calculate the distance from \(\mathbf{n}_{ik}\) to _each_ node/triangle on \(O_{j}\). Then, if this distance \(d^{j}_{ik}\) is within a predefined distance threshold \(\mathcal{D}\), we connect the surface node \(\mathbf{n}_{ik}\) directly to the opposing _object_ node \(\mathbf{o}_{j}\) and refer to this edge as \(\mathbf{e}_{\mathbf{o}_{j}\leftrightarrow\mathbf{n}_{ik}}\). Thus, we define the set of inter-object edges as \(\mathcal{E}^{\rm coll}_{ji}=\{\mathbf{e}_{\mathbf{o}_{j}\leftrightarrow\mathbf{ n}_{ik}}|\forall\mathbf{n}_{ik}\in O_{i}:f_{j}(\mathbf{n}_{ik})\leq \mathcal{D}\}\). This approach is different from mesh-based simulators [4, 19, 29] that connect pairs of nodes or triangles on the two surfaces.

We construct the features for collision edges \(\mathbf{e}_{\mathbf{o}_{j}\leftrightarrow\mathbf{n}_{ik}}\) such that they contain information about potential points of collision. First, we compute the closest point \(\mathbf{c}^{j}_{ik}\) from \(\mathbf{n}_{ik}\in O_{i}\) to the surface of object \(O_{j}\). To do so, we transform the position of \(\mathbf{n}_{ik}\) into the reference space of \(O_{j}\) using \(\mathcal{T}^{-1}_{j}(\mathbf{n}_{ik})\). We call an SDF function to get the distance from the node \(\mathbf{n}_{ik}\) to the closest point on \(O_{j}\) as \(d^{j}_{ik}=f_{\theta_{j}}(\mathcal{T}^{-1}_{j}(\mathbf{n}_{ik}))\). The closest point on the surface of \(O_{j}\) is then computed similarly to Eq. 1:",\mathbf{c}^{j}_{ik}=\mathbf{n}_{ik}-d^{j}_{ik}\mathcal{T}_{j}\Big{(}\nabla f_{ \theta_{j}}(\mathcal{T}^{-1}_{j}(\mathbf{n}_{ik}))\Big{)},,\mathbf{c}^{j}_{ik} = \mathcal{T}_{j}\left( \mathcal{T}^{-1}_{j}(\mathbf{n}_{ik}) - d^{j}_{ik} \nabla f_{\theta_{j}}\left( \mathcal{T}^{-1}_{j}(\mathbf{n}_{ik}) \right) \right).,"Computes the closest point on the surface of object $O_j$ from a surface node $\mathbf{n}_{ik}$ on object $O_i$, using the SDF of $O_j$ and the rigid transformation between the objects."
neurips_2024_oral_34,1,"## 2 Preliminaries

**Diffusion Models.** The diffusion model refers to a range of generative models trained by reversing the trajectory from the data distribution to the noise distribution. Among diffusion models, we focus on discrete-time stochastic samplers, such as DDPM [3], which synthesize a sample through the following iteration producing \(\mathbf{x}_{0},\mathbf{x}_{1},\dots,\mathbf{x}_{T}\in\mathbb{R}^{D}\):","\mathbf{x}_{0}\sim\mathcal{N}(0,I)\quad\text{and}\quad\mathbf{x}_{t+1}=a_{t} \mathbf{x}_{t}+\mu(\mathbf{x}_{t},t)+\sigma_{t}\epsilon_{t}\quad\text{for} \quad t=0,1,\dots,T-1,","where \(\epsilon_{t}\sim\mathcal{N}(0,I)\) and \(\mu(\mathbf{x},t)\) is the output of a neural network","\mathbf{x}_t = \sqrt{\alpha_t} \mathbf{x}_{t-1} + \sqrt{1 - \alpha_t} \, \boldsymbol{\epsilon}_t","The forward diffusion process in DDPM, where each step adds Gaussian noise scaled by parameters α_t to transition from x_{t-1} to x_t."
neurips_2024_oral_34,2,"rk. The coefficients \(a_{t}\in\mathbb{R}\) and \(\sigma_{t}\in\mathbb{R}_{>0}\) are constants. Note that we reverse the time direction in a diffusion model from the convention to be consistent with RL. The final state \(\mathbf{x}_{T}\) is the sample generated by the diffusion model, and its marginal distribution is \(\pi(\mathbf{x}_{T})\). We will often drop the subscript \(T\) and write the distribution as \(\pi(\mathbf{x})\). The conditional distribution of a transition in Eq. (1) is denoted as \(\pi(\mathbf{x}_{t+1}|\mathbf{x}_{t})\). For continuous-time diffusion models [23], Eq. (1) corresponds to using the Euler-Maruyama solver.

The generation process in Eq. (1) defines a \(T\)-horizon Markov Decision Process (MDP) except for the reward. State \(\mathbf{s}_{t}\) and action \(\mathbf{a}_{t}\) are defined as \(\mathbf{s}_{t}=(\mathbf{x}_{t},t)\) and \(\mathbf{a}_{t}=\mathbf{x}_{t+1}\). The transition dynamics is defined as \(p(\mathbf{s}_{t+1}|\mathbf{s}_{t},\mathbf{a}_{t})=\delta_{(\mathbf{a}_{t},t+1)}\), where \(\delta_{(\mathbf{x}_{t},t)}\) is a Dirac delta function at \((\mathbf{x}_{t},t)\). With a reward function defined, a diffusion model can be trained with RL [11; 24; 19; 25]. In this paper, we consider a case where the reward is the log data density \(\log p(\mathbf{x})\), which is unknown.

**Energy-Based Models.** An energy-based model (EBM) \(q(\mathbf{x})\) uses a scalar function called an energy \(E(\mathbf{x})\) to represent a probability distribution:","q(\mathbf{x})=\frac{1}{Z}\exp(-E(\mathbf{x})/\tau),\quad E:\mathcal{X}\to \mathbb{R},","where \(\tau>0\) is temperature, \(\mathcal{X}\) is the compact domain of data, and \(Z=\int_{\mathcal{X}}\exp(-E(\mathbf{x})/\tau)d\mathbf{x}<\infty\) is the normalization constant","q(\mathbf{x}) = \frac{\exp(-E(\mathbf{x}))}{Z},","where $Z = \int \exp(-E(\mathbf{x})) \, d\mathbf{x}$ is the normalization constant, also known as the partition function."
neurips_2024_oral_34,3,"nt.

The standard method for training an EBM is by minimizing KL divergence between data \(p(\mathbf{x})\) and EBM \(q(\mathbf{x})\), i.e., \(\min_{q}KL(p||q)\), where \(KL(p||q):=\int_{\mathcal{X}}p(\mathbf{x})\log(p(\mathbf{x})/q(\mathbf{x}))d \mathbf{x}\). Computing the gradient of \(KL(p||q)\) with respect to EBM parameters requires MCMC sampling, which is computationally demanding and sensitive to hyperparameters. The algorithm presented in this paper serves as an alternative method for training an EBM without MCMC.

EBMs have a profound connection to maximum entropy IRL, where Eq. (2) serves as a model for an expert's policy [15; 16; 2]. In maximum entropy IRL, x corresponds to an action (or a sequence of actions), and \(E(\mathbf{x})\) represents the expert's cost of the action. The expert is then assumed to generate actions following \(q(\mathbf{x})\). This assumption embodies the maximum entropy principle because Eq. (2) is a distribution that minimizes the cost while maximizing the entropy of the action. Here, \(\tau\) balances cost minimization and entropy maximization.

## 3 Diffusion by Maximum Entropy Inverse Reinforcement Learning

### Objective: Generalized Contrastive Divergence

We aim to minimize the (reverse) KL divergence between a diffusion model \(\pi(\mathbf{x})\) and the data density \(p(\mathbf{x})\). This minimization can improve the sample quality of \(\pi(\mathbf{x})\), particularly when \(T\) is small.","\min_{\pi\in\Pi}KL(\pi(\mathbf{x})||p(\mathbf{x}))=\max_{\pi\in\Pi}\mathbb{E} _{\pi}[\log p(\mathbf{x})]+\mathcal{H}(\pi(\mathbf{x})),","where \(\Pi\) is the set of feasible \(\pi(\mathbf{x})\)'s, and \(\mathcal{H}(\pi)=-\int\pi(\mathbf{x})\log\pi(\mathbf{x})d\mathbf{x}\) is the differential entropy","KL(\pi||p)=\int_{\mathcal{X}}\pi(\mathbf{x})\log\left(\frac{\pi(\mathbf{x})}{p(\mathbf{x})}\right)d\mathbf{x},","which measures the dissimilarity between the diffusion model's distribution and the true data distribution, guiding the optimization of the diffusion process to enhance sample quality."
neurips_2024_oral_34,4,". This minimization is a maximum entropy RL problem: The log data density \(\log p(\mathbf{x})\) is the reward, and \(\pi(\mathbf{x})\) is the stochastic policy. However, we cannot solve Eq. (3) directly since \(\log p(\mathbf{x})\) is unknown in a typical generative modeling setting. Instead, training data from \(p(\mathbf{x})\) are available, allowing us to employ an _Inverse_ RL approach.

In this paper, we present **Diffusion by Maximum Entropy IRL (DxMI)** as an IRL approach for solving Eq. (3). We employ an EBM \(q(\mathbf{x})\) (Eq. (2)) as a surrogate for \(p(\mathbf{x})\) and use \(\log q(\mathbf{x})\) as the reward for training the diffusion model instead of \(\log p(\mathbf{x})\). At the same time, we train \(q(\mathbf{x})\) to be close to \(p(\mathbf{x})\) by minimizing the divergence between \(p(\mathbf{x})\) and \(q(\mathbf{x})\):","\min_{\pi\in\Pi}KL(\pi(\mathbf{x})||q(\mathbf{x}))\quad\text{and}\quad\min_{ q\in\mathcal{Q}}KL(p(\mathbf{x})||q(\mathbf{x})),",where \(\mathcal{Q}\) is the feasible set of EBMs,\min_{q}KL(p(\mathbf{x})||q(\mathbf{x})).,This minimization trains the energy-based model $q(\mathbf{x})$ to approximate the data distribution $p(\mathbf{x})$ by minimizing the Kullback-Leibler divergence.
neurips_2024_oral_34,5,"Ms. When the two KL divergences become zero, \(p(\mathbf{x})=q(\mathbf{x})=\pi(\mathbf{x})\) is achieved. However, \(\min_{q\in\mathcal{Q}}KL(p(\mathbf{x})||q(\mathbf{x}))\) is difficult due to the normalization constant of \(q(\mathbf{x})\). Instead, we consider an alternative minimax formulation inspired by Contrastive Divergence (CD; [26]), a celebrated algorithm for training an EBM.

**Objective.** Let \(p(\mathbf{x})\) be the data distribution. Suppose that \(\mathcal{Q}\) and \(\Pi\) are the feasible sets of the EBM \(q(\mathbf{x})\) and the diffusion model \(\pi(\mathbf{x})\), respectively. The learning problem of DxMI is formulated as follows:",\min_{q\in\mathcal{Q}}\max_{\pi\in\Pi}KL(p(\mathbf{x})||q(\mathbf{x}))-KL( \pi(\mathbf{x})||q(\mathbf{x})).,,\min_{\pi\in\Pi}\max_{q\in\mathcal{Q}}\ \mathbb{E}_{\pi}[\log q(\mathbf{x})]+\mathcal{H}(\pi(\mathbf{x})).,"This minimax formulation combines maximum entropy reinforcement learning with energy-based modeling, where the diffusion model $\pi(\mathbf{x})$ acts as the policy and the energy-based model $q(\mathbf{x})$ serves as the reward function."
neurips_2024_oral_34,6,"We shall call Eq. (5) _Generalized_ CD (GCD), because Eq. (5) generalizes CD by incorporating a general class of samplers. CD [26] is originally given as \(\min_{q\in\mathcal{Q}}KL(p(\mathbf{x})||q(\mathbf{x}))-KL(\nu^{k}_{p,q}( \mathbf{x})||q(\mathbf{x}))\). Here, \(\nu^{k}_{p,q}(\mathbf{x})\) is a \(k\)-step MCMC sample distribution where Markov chains having \(q(\mathbf{x})\) as a stationary distribution are initialized from \(p(\mathbf{x})\). GCD replaces \(\nu^{k}_{p,q}(\mathbf{x})\) with a general sampler \(\pi(\mathbf{x})\) at the expense of introducing a max operator.

When the models are well-specified, i.e., \(p(\mathbf{x})\in\mathcal{Q}=\Pi\), Nash equilibrium of GCD is \(p(\mathbf{x})=q(\mathbf{x})=\pi(\mathbf{x})\), which is identical to the solution of Eq. (4). Meanwhile, there is no need to compute the normalization constant, as the two KL divergences cancel the normalization constant out. Note that the objective function (Eq. (5)) can be negative, allowing \(q(\mathbf{x})\) be closer to \(p(\mathbf{x})\) than \(\pi(\mathbf{x})\).

Our main contribution is exploring the application of discrete-time diffusion models (Eq. (1)) as \(\pi(\mathbf{x})\) in GCD and not discovering GCD for the first time. GCD is mathematically equivalent to a formulation called variational maximum likelihood or adversarial EBM, which has appeared several times in EBM literature [27; 28; 29; 30; 31; 32; 33]. However, none of them have investigated the use of a diffusion model as \(\pi(\mathbf{x})\), where optimization and entropy estimation are challenging. We discuss the challenges in Section 3.2 and provide a novel algorithm to address them in Section 4.

### Alternating Update of EBM and Diffusion Model

In DxMI, we update a diffusion model and an EBM in an alternative manner to find the Nash equilibrium. We write \(\theta\) and \(\phi\) as the parameters of the energy \(E_{\theta}(\mathbf{x})\) and a diffusion model \(\pi_{\phi}(\mathbf{x})\), respectively. While EBM update is straightforward, we require a subroutine described in Section 4 for updating the diffusion model. The entire procedure of DxMI is summarized in Algorithm 1.

**EBM Update.** The optimization with respect to EBM is written as \(\min_{\theta}\mathbb{E}_{p(\mathbf{x})}[E_{\theta}(\mathbf{x})]-\mathbb{E}_{ \pi_{\phi}(\mathbf{x})}[E_{\theta}(\mathbf{x})]\). During the update, we also regularize the energy by the square of energies \(\gamma(\mathbb{E}_{p(\mathbf{x})}[E_{\theta}(\mathbf{x})^{2}]+\mathbb{E}_{ \pi_{\phi}(\mathbf{x})}[E_{\theta}(\mathbf{x})^{2}])\) for \(\gamma>0\) to ensure the energy is bounded. We set \(\gamma=1\) unless stated otherwise. This regularizer is widely adopted in EBM [34; 35].

**Difficulty of Diffusion Model Update.** Ideally, diffusion model parameter \(\phi\) should be updated by minimizing \(KL(\pi_{\phi}||q_{\theta})=\mathbb{E}_{\pi_{\phi}(\mathbf{x})}[E_{\theta}( \mathbf{x})/\tau]-\mathcal{H}(\pi_{\phi}(\mathbf{x}))\). However, this update is difficult in practice for two reasons.

First, marginal entropy \(\mathcal{H}(\pi_{\phi}(\mathbf{x}))\) is difficult to estimate. Discrete-time diffusion models (Eq. (1)) do not provide an efficient way to evaluate \(\log\pi_{\phi}(\mathbf{x})\) required in the computation of \(\mathcal{H}(\pi_{\phi}(\mathbf{x}))\), unlike some continuous-time models, e.g., continuous normalizing flows [36; 23]. Other entropy estimators based on k-nearest neighbors [37] or variational methods [38; 39] do not scale well to high-dimensional spaces. Second, propagating the gradient through time in a diffusion model may require significant memory. Also, the gradient may explode or vanish during propagation, making the training unstable [19].

## 4 Diffusion by Dynamic Programming

In this section, we present a novel RL algorithm for a diffusion model, **Diffusion by Dynamic Programming** (DxDP), which addresses the difficulties in updating a diffusion model for the reward. DxDP leverages optimal control formulation and value functions to perform the diffusion model update in DxMI efficiently. Note that DxDP can be used separately from DxMI to train a diffusion model for an arbitrary reward.

### Optimal Control Formulation

Instead of solving \(\min_{\phi}KL(\pi_{\phi}(\mathbf{x}_{T})||q_{\theta}(\mathbf{x}_{T}))\) directly, we minimize its upper bound obtained from the data processing inequality:",KL(\pi_{\phi}(\mathbf{x}_{T})||q_{\theta}(\mathbf{x}_{T}))\leq KL(\pi_{\phi}( \mathbf{x}_{0:T})||q_{\theta}(\mathbf{x}_{T})\tilde{q}(\mathbf{x}_{0:T-1}| \mathbf{x}_{T})).,,"\min_{\phi}\sum_{t=0}^{T-1}\mathbb{E}_{\pi_{\phi}}\left[\frac{1}{2\tau} \left\|\mu_{\phi}(\mathbf{x}_{t},t)-\mathbb{E}_{\epsilon_{t}}[\mu_{\phi}(\mathbf{x}_{t},t)]\right\|^{2}-\log\sigma_{t}\right],",This upper bound decomposes the KL divergence into a sum of terms involving the variance of the neural network output and the logarithm of the diffusion model's noise scale parameters.
neurips_2024_oral_34,7,"Here, we introduce an auxiliary distribution \(\tilde{q}(\mathbf{x}_{0:T-1}|\mathbf{x}_{T})\), and the inequality holds for an arbitrary choice of \(\tilde{q}(\mathbf{x}_{0:T-1}|\mathbf{x}_{T})\). In this paper, we consider a particularly simple case where \(\tilde{q}(\mathbf{x}_{0:T-1}|\mathbf{x}_{T})\) is factorized into conditional Gaussians as follows:","\tilde{q}(\mathbf{x}_{0:T-1}|\mathbf{x}_{T})=\prod_{t=0}^{T-1}\tilde{q}( \mathbf{x}_{t}|\mathbf{x}_{t+1}),\text{ where }\tilde{q}(\mathbf{x}_{t}| \mathbf{x}_{t+1})=\mathcal{N}(\mathbf{x}_{t+1},s_{t}^{2}I),\quad s_{t}>0.",,"\tilde{q}(\mathbf{x}_{0:T-1}|\mathbf{x}_{T})=\prod_{t=0}^{T-1}\mathcal{N}( \mathbf{x}_{t}|\alpha_{t}\mathbf{x}_{t+1},\beta_{t}^{2}I),",where $\alpha_{t}\in\mathbb{R}$ and $\beta_{t}\in\mathbb{R}_{>0}$ are parameters of the auxiliary distribution
neurips_2024_oral_34,8,"Now we minimize the right-hand side of Eq. (6): \(\min_{\phi}KL(\pi_{\phi}(\mathbf{x}_{0:T})||q_{\theta}(\mathbf{x}_{T})\tilde{ q}(\mathbf{x}_{0:T-1}|\mathbf{x}_{T}))\). When we plug in the definitions of each distribution, multiply by \(\tau\), and discard all constants, we obtain the following problem:","\min_{\phi}\mathop{\mathbb{E}}_{\pi_{\phi}(\mathbf{x}_{0:T})}[E_{\theta}( \mathbf{x}_{T})+\tau\sum_{t=0}^{T-1}\log\pi_{\phi}(\mathbf{x}_{t+1}|\mathbf{x }_{t})+\tau\sum_{t=0}^{T-1}\frac{1}{2s_{t}^{2}}||\mathbf{x}_{t+1}-\mathbf{x}_ {t}||^{2}],",,\min_{\phi}\mathbb{E}_{\pi_{\phi}(\mathbf{x}_{0:T})}\left[E_{\theta}(\mathbf{x}_{T})+\sum_{t=0}^{T-1}\frac{1}{2s_{t}^{2}}\|\mathbf{x}_{t+1}-\mathbf{x}_{t}\|^{2}\right].,"This objective combines the energy term from the EBM with a sum of squared differences between consecutive states, regularized by the noise scales $s_t^2$."
neurips_2024_oral_34,9,"which is an optimal control problem. The controller \(\pi_{\phi}(\cdot)\) is optimized to minimize the terminal cost \(E_{\theta}(\mathbf{x}_{T})\) plus the running costs for each transition \((\mathbf{x}_{t},\mathbf{x}_{t+1})\). The first running cost \(\log\pi_{\phi}(\mathbf{x}_{t+1}|\mathbf{x}_{t})\) is responsible for conditional entropy maximization, because \(\mathbb{E}_{\pi}[\log\pi_{\phi}(\mathbf{x}_{t+1}|\mathbf{x}_{t})]=-\mathcal{H }(\pi_{\phi}(\mathbf{x}_{t+1}|\mathbf{x}_{t}))\). The second running cost regularizes the ""velocity"" \(||\mathbf{x}_{t+1}-\mathbf{x}_{t}||^{2}\). The temperature \(\tau\) balances between the terminal and running costs.

We have circumvented the marginal entropy computation in GCD, as all terms in Eq. (8) are easily computable. For the diffusion model considered in this paper (Eq. (1)), the conditional entropy has a particularly simple expression \(\mathcal{H}(\pi_{\phi}(\mathbf{x}_{t+1}|\mathbf{x}_{t}))=D\log\sigma_{t}+0.5D \log 2\pi\). Therefore, optimizing the entropy running cost amounts to learning \(\sigma_{t}\)'s in diffusion, and we treat \(\sigma_{t}\)'s as a part of the diffusion model parameter \(\phi\) in DxMI.

### Dynamic Programming

We propose a dynamic programming approach for solving Eq. (8). Dynamic programming introduces value functions to break down the problem into smaller problems at each timestep, removing the need for back-propagation in time. Then, a policy, a diffusion model in our case, is optimized through iterative alternating applications of policy evaluation and policy improvement steps.

Value Function. A value function, or cost-to-go function \(V_{\psi}^{t}(\mathbf{x}_{t})\), is defined as the expected sum of the future costs starting from \(\mathbf{x}_{t}\), following \(\pi\). We write the parameters of a value function as \(\psi\).","V_{\psi}^{t}(\mathbf{x}_{t})=\mathbb{E}_{\pi}[E_{\theta}(\mathbf{x}_{T})+ \tau\sum_{t^{\prime}=t}^{T-1}\log\pi_{\phi}(\mathbf{x}_{t^{\prime}+1}|\mathbf{ x}_{t^{\prime}})+\sum_{t^{\prime}=t}^{T-1}\frac{\tau}{2s_{t^{\prime}}^{2}}|| \mathbf{x}_{t^{\prime}+1}-\mathbf{x}_{t^{\prime}}||^{2}\bigg{|}\mathbf{x}_{t} ],",,V_{\psi}^{t}(\mathbf{x}_{t})=\mathbb{E}_{\pi_{\phi}(\mathbf{x}_{t+1:T})}\left[ E_{\theta}(\mathbf{x}_{T})+\tau\sum_{k=t}^{T-1}\log\pi_{\phi}(\mathbf{x}_{k+1}|\mathbf{x}_{k})+\tau\sum_{k=t}^{T-1}\frac{1}{2s_{k}^{2}}||\mathbf{x}_{k+1}-\mathbf{x}_{k}||^{2}\right].,"The value function $V_{\psi}^{t}(\mathbf{x}_{t})$ represents the expected cumulative cost from timestep $t$ to $T$, starting at state $\mathbf{x}_{t}$ and following the policy $\pi_{\phi}$ thereafter."
neurips_2024_oral_34,10,"for \(t=0,\dots,T-1\). Note that \(V^{T}(\mathbf{x}_{T})=E(\mathbf{x}_{T})\). A value function can be implemented with a neural network, but there are multiple design choices, such as whether to share the parameters with \(\pi(\mathbf{x})\) or \(E(\mathbf{x})\), and also whether the parameters should be shared across time. We explore the options in our experiments.

Policy Evaluation. During policy evaluation, we estimate the value function for the current diffusion model by minimizing the Bellman residual, resulting in the temporal difference update.","\min_{\psi}\mathbb{E}_{\mathbf{x}_{t},\mathbf{x}_{t+1}\sim\pi}[(\text{sg}[V_{ \psi}^{t+1}(\mathbf{x}_{t+1})]+\tau\log\pi_{\phi}(\mathbf{x}_{t+1}|\mathbf{x} _{t})+\frac{\tau}{2s_{t}^{2}}||\mathbf{x}_{t}-\mathbf{x}_{t+1}||^{2}-V_{\psi}^ {t}(\mathbf{x}_{t}))^{2}],",where \(\text{sg}[\cdot]\) denotes a stop-gradient operator indicating that gradient is not computed for the term,"\min_{\psi}\mathbb{E}_{\pi_{\phi}}\left[\left(V_{\psi}^{t}(\mathbf{x}_{t}) - \left(\tau\log\pi_{\phi}(\mathbf{x}_{t+1}|\mathbf{x}_{t}) + \frac{\tau}{2s_{t}^{2}}||\mathbf{x}_{t+1}-\mathbf{x}_{t}||^{2} + V_{\psi}^{t+1}(\mathbf{x}_{t+1})\right)\right)^{2}\right],",Policy evaluation minimizes the Bellman residual to update the value function parameters $\psi$ for each timestep $t$ using the current diffusion model $\pi_{\phi}$.
neurips_2024_oral_34,11,"rm.

Policy Improvement. The estimated value is used to improve the diffusion model. For each \(\mathbf{x}_{t}\) in a trajectory \(\mathbf{x}_{0:T}\) sampled from \(\pi_{\phi}(\mathbf{x}_{0:T})\), the diffusion model is optimized to minimize the next-state value and the running costs.",\min_{\phi}\mathbb{E}_{\pi_{\phi}(\mathbf{x}_{t+1}|\mathbf{x}_{t})}[V_{ \psi}^{t+1}(\mathbf{x}_{t+1})+\tau\log\pi_{\phi}(\mathbf{x}_{t+1}|\mathbf{x} _{t})+\frac{\tau}{2s_{t}^{2}}||\mathbf{x}_{t}-\mathbf{x}_{t+1}||^{2}\bigg{|} \mathbf{x}_{t}].,,"\min_{\phi}\mathbb{E}_{\mathbf{x}_{t},\mathbf{x}_{t+1}\sim\pi_{\phi}}\left[V_{\psi}^{t+1}(\mathbf{x}_{t+1})+\tau\log\pi_{\phi}(\mathbf{x}_{t+1}|\mathbf{x}_{t})+\frac{\tau}{2s_{t}^{2}}\|\mathbf{x}_{t+1}-\mathbf{x}_{t}\|^{2}\right].",where the policy improvement step minimizes the expected future costs using the updated value function and running costs
neurips_2024_oral_34,12,"In practice, each iteration of policy evaluation and improvement involves a single gradient step.

Adaptive Velocity Regularization (AVR). We additionally propose a method for systematically determining the hyperparameter \(s_{t}\)'s of the auxiliary distribution \(\tilde{q}(\mathbf{x}_{0:T-1}|\mathbf{x}_{T})\). We can optimize \(s_{t}\) such that the inequality Eq. (6) is as tight as possible by solving \(\min_{s_{0},\dots,s_{T-1}}KL(\pi_{\phi}(\mathbf{x}_{0:T})||q_{\theta}(\mathbf{ x}_{T})\tilde{q}(\mathbf{x}_{0:T-1}|\mathbf{x}_{T}))\). After calculation (details in Appendix A), the optimal \(s_{t}^{*}\) can be obtained analytically: \((s_{t}^{*})^{2}=\mathbb{E}_{\mathbf{x}_{t},\mathbf{x}_{t+1}\sim\pi}[||\mathbf{ x}_{t}-\mathbf{x}_{t+1}||^{2}]/D\). In practice, we can use exponential moving average to compute the expectation \(\mathbb{E}_{\mathbf{x}_{t},\mathbf{x}_{t+1}\sim\pi}[||\mathbf{x}_{t}-\mathbf{ x}_{t+1}||^{2}]\) during training: \(s_{t}^{2}\leftarrow\alpha s_{t}^{2}+(1-\alpha)||\mathbf{x}_{t}-\mathbf{x}_{t+1} ||^{2}/D\) where we set \(\alpha=0.99\) for all experiment.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline Method & T & Pretrain & \(\tau\) & \(SW\) (\(\downarrow\)) & AUC (\(\uparrow\)) \\ \hline DDPM & 5 & - & - & 0.967\(\pm\)0.005 & - \\ DDPM & 10 & - & - & 0.824\(\pm\)0.002 & - \\ DDPM & 100 & - & - & 0.241\(\pm\)0.003 & - \\ DDPM & 1000 & - & - & 0.123\(\pm\)0.014 & - \\ \hline DxMI & 5 & \(\bigcirc\) & 0 & 0.074\(\pm\)0.018 & 0.707 \\ DxMI & 5 & \(\bigcirc\) & 0.01 & 0.074\(\pm\)0.017 & 0.751 \\ DxMI & 5 & \(\bigcirc\) & 0.1 & **0.068\(\pm\)**0.004** & **0.898** \\ DxMI & 5 & \(\bigcirc\) & 1 & 1.030\(\pm\)0.004 & 0.842 \\ DxMI & 5 & \(\times\) & 0.1 & 0.076\(\pm\)0.011 & 0.883 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Quantitative results for 8 Gaussians experiment. \(SW\) denotes the sliced Wasserstein distance between samples and data. AUC is computed for classification between data and uniform noise using the energy. The standard deviation is computed from 5 independent samplings. The ideal maximum value of AUC is about 0.906.

Figure 2: 2D density estimation on 8 Gaussians. Red shades indicate the energy (white is low), and the dots are generated samples.

### Techniques for Image Generation Experiments

When using DxDP for image generation, one of the most common applications of diffusion models, we introduce several design choices to DxDP to enhance performance and training stability. The resulting algorithm is summarized in Algorithm 2.

Time-Independent Value Function. In image generation experiments (Section 5.2), we let the value function be independent of time, i.e., \(V_{\psi}^{t}(\mathbf{x}_{t})=V_{\psi}(\mathbf{x}_{t})\). Removing the time dependence reduces the number of parameters to be trained. More importantly, a time-independent value function can learn better representation because the value function is exposed to diverse inputs, including both noisy and clean images. On the contrary, a time-dependent value function \(V_{\psi}^{t}(\mathbf{x}_{t})\) never observes samples having different noise levels than the noise level of \(\mathbf{x}_{t}\).

Time Cost. Also, in the value update (Eq. (10)) step of image generation experiments, we introduce _time cost_ function \(R(t)>0\), which replaces the running cost terms \(\tau\log\pi_{\phi}(\mathbf{x}_{t+1}|\mathbf{x}_{t})+\tau||\mathbf{x}_{t}- \mathbf{x}_{t+1}||^{2}/(2s_{t}^{2})\). The time cost \(R(t)\) only depends on time \(t\). The modified value update equation is given as follows:","\min_{\psi}\mathbb{E}_{\mathbf{x}_{t},\mathbf{x}_{t+1}\sim\pi}[(\text{s} \text{g}[V_{\psi}(\mathbf{x}_{t+1})]+R(t)-V_{\psi}(\mathbf{x}_{t}))^{2}].",,"\min_{\psi}\mathbb{E}_{\mathbf{x}_{t},\mathbf{x}_{t+1}\sim\pi}[(\text{sg}[V_{ \psi}(\mathbf{x}_{t+1})]+R(t)-V_{\psi}(\mathbf{x}_{t}))^{2}],",modified value update equation with time cost function $R(t)$ replacing running cost terms in policy evaluation step
neurips_2024_oral_35,1,"### 3D Representation and Model Architecture

**Triplane vs. 3D Voxels** Open-world sparse-view reconstruction requires extensive priors, which can be learned through a large-scale transformer. Prior arts [27; 67; 77; 79; 85] typically utilize the triplane representation, which decomposes a 3D neural field into a set of 2D planes. While straightforward for processing by transformers, the triplane representation lacks explicit 3D spatial structures and makes it hard to enable precise interaction between each 3D location and its corresponding 2D projected pixels from multi-view images. For instance, these methods often simply apply self-attention across all triplane patch tokens and cross-attention between triplane tokens and all multi-view image tokens. This all-to-all attention is not only costly but also makes the methods cumbersome to train. Moreover, the triplane representation often shows results with notable artifacts at the boundaries of patches and may suffer from limited expressiveness for complex structures. Consequently, we choose the 3D voxel representation instead, which explicitly preserves the 3D spatial structures.

**Combining Transformer with 3D Convolution** To leverage the explicit 3D structure and the powerful expressiveness of a large-scale transformer model while avoiding an explosion of computational costs, we propose VoxelFormer and SparseVoxelFormer, which follow a 3D UNet architecture while integrating a transformer at the bottleneck. The overall idea is that we use local 3D convolution to encode and decode a high-resolution 3D feature volume, while the global transformer layer handles reasoning and memorizing priors for the compressed low-resolution feature volume. Specifically, as shown in Figure 2, a 3D feature volume begins with a learnable token shared by all 3D voxels. With the 3D voxel coordinates, we can leverage the projection matrix to enable each 3D voxel to aggregate 2D local features from multi-view images via a projection-aware cross-attention layer. By iteratively performing projection-aware cross-attention and 3D (sparse) convolution, we can compress the 3D volume to a lower-resolution one. After compression, each 3D voxel feature then serves as a latent token, and a deep transformer model is applied to a sequence of all 3D voxel features (position encoded) to enhance the model's expressiveness. Finally, we use the convolution-based inverse upper branch with skip connection to decode a 3D feature volume with the initial high resolution.

**Projection-Aware Cross Attention** Regarding 3D-2D interaction, the input multi-view RGB and normal images are initially processed by a 2D feature extractor, such as a trainable DINov2 [46], to generate multi-view patch features. While previous cost-volume-based methods [3; 38] typically use mean or max pooling to aggregate multi-view 2D features, these simple pooling operations might be suboptimal for addressing occlusion and visibility issues. Instead, we propose a projection-aware cross-attention mechanism to adaptively aggregate the multi-view features for each 3D voxel. Specifically, we project each 3D voxel onto the \(m\) views to interpolate \(m\) RGB and normal features. We then concatenate these local patch features with the projected RGB and normal values to form \(m\) 2D features. In the projection-aware cross-attention module, we use the 3D voxel feature to calculate a query and use both the 3D voxel feature and the \(m\) 2D features to calculate \(m+1\) keys and values. A cross-attention is then performed for each 3D voxel, enabling precise interaction between each 3D location and its corresponding 2D projected pixels, and allowing adaptive aggregation of 2D features, which can be formulated as:","varrow\mathrm{CrossAttention}(Q=\{v\},K=\{p_{i}^{v}\}_{i=1}^{m}+\{v\},V=\{ p_{i}^{v}\}_{i=1}^{m}+\{v\})","Where \(v\) denotes a 3D voxel feature, and \(p_{i}^{v}\) denotes its projected 2D pixel feature from view \(i\), which is a concatenation of the RGB feature \(f_{i}^{v}\), the normal feature \(g_{i}^{v}\), and the RGB and normal values \(c_{i}^{v}\) and \(n_{i}^{v}\), respectively","\begin{equation}
    \mathbf{F}^{3D}_{i} = \text{CrossAttention}\left( \mathbf{Q}_i, \left[ \mathbf{K}_{i,0}, \mathbf{K}_{i,1}, \dots, \mathbf{K}_{i,m} \right], \left[ \mathbf{V}_{i,0}, \mathbf{V}_{i,1}, \dots, \mathbf{V}_{i,m} \right] \right),
\end{equation}","Equation 1 formulates the projection-aware cross-attention mechanism, where each 3D voxel feature is updated through cross-attention over a set of 2D multi-view features and its own key and value."
neurips_2024_oral_35,2,"ly.

**Coarse-to-Fine Feature Generation** As shown in Fig. 2, to generate a high-resolution 3D feature volume that captures the fine-grained details of 3D shapes, we follow previous work [31; 95] by employing a coarse-to-fine strategy. Specifically, we first use VoxelFormer, which is equipped with full 3D convolution, to predict a low-resolution (e.g., \(64^{3}\)), coarse 3D occupancy volume. Each voxel in this volume stores a binary value indicating whether it is close to the surface. The predicted occupied voxels are then subdivided to create higher-resolution sparse voxels (e.g., \(256^{3}\)). Next, we utilize a second module, SparseVoxelFormer, which features 3D sparse convolution [63], to predict features for these sparse voxels. After this, we trilinearly interpolate the 3D feature of any near-surface 3D point, which encodes both geometric and color information, from the high-resolution sparse feature volume. The features are then fed into various MLPs to learn the corresponding fields.

### Unified Single-Stage Training: Surface Rendering with SDF Supervision

Existing works typically use NeRF [42] and volume rendering or 3D Gaussian splatting [22] since they come with a relatively easy and stable learning process. However, extracting high-quality meshes from their results is often non-trivial. For example, directly applying Marching Cubes [39] to density fields of learned NeRFs typically generates meshes with many artifacts. Recent methods [78; 79; 85] have designed complex, multi-stage ""NeRF-to-mesh"" training with differentiable surface rendering, but the generated meshes still leave room for improvement. On the other hand, skipping a good initialization and directly learning meshes from scratch using purely differentiable surface rendering losses is also infeasible, as it is highly unstable to train and typically results in distorted geometry.

In this work, we propose leveraging explicit 3D supervision in addition to 2D rendering losses. As shown in Figure 2, we task MeshFormer with learning a signed distance function (SDF) field supervised by a high-resolution (e.g., \(512^{3}\)) ground truth SDF volume. The SDF loss provides explicit guidance for the underlying 3D geometry and facilitates faster learning. It also allows us to use mesh representation and differentiable surface rendering from the beginning without worrying about good geometry initialization or unstable training, as the SDF loss serves as a strong regularization for the underlying geometry. By combining surface rendering with explicit 3D SDF supervision, we train MeshFormer in a unified, single-stage training process. As shown in Figure 2, we employ three tiny MLPs that take as input the 3D feature interpolated from the 3D sparse feature volume to learn an SDF field, a 3D color texture, and a 3D normal texture. We extract meshes from the SDF volume using dual Marching Cubes [39] and employ NVDiffRast [25] for differentiable surface rendering. We render both the multi-view RGB and normal images and compute the rendering losses, which consist of both the MSE and perceptual loss terms. As a result, our training loss can be expressed as:",\mathcal{L}=\lambda_{1}\mathcal{L}_{\mathrm{MSE}}^{\mathrm{color}}+\lambda_{2 }\mathcal{L}_{\mathrm{LPIPS}}^{\mathrm{color}}+\lambda_{3}\mathcal{L}_{ \mathrm{MSE}}^{\mathrm{normal}}+\lambda_{4}\mathcal{L}_{\mathrm{LPIPS}}^{ \mathrm{normal}}+\lambda_{5}\mathcal{L}_{\mathrm{occ}}+\lambda_{6}\mathcal{L }_{\mathrm{SDF}},"where \(L_{\mathrm{occ}}\) and \(L_{\mathrm{SDF}}\) are MSE losses for occupancy and SDF volumes, and \(\lambda_{i}\) denotes the weight of each loss term",\mathcal{L} = \lambda_{\text{rgb}} \mathcal{L}_{\text{rgb}} + \lambda_{\text{normal}} \mathcal{L}_{\text{normal}} + \lambda_{\text{reg}} \mathcal{L}_{\text{reg}} + \lambda_{\text{SDF}} \mathcal{L}_{\text{SDF}},"The total training loss combines RGB rendering loss, normal rendering loss, regularization loss, and SDF supervision loss, each weighted by corresponding coefficients."
neurips_2024_oral_40,1,"## 1 Introduction

Boosting is an extremely powerful and elegant idea that allows one to combine multiple inaccurate classifiers into a highly accurate _voting classifier_. Algorithms such as AdaBoost [Freund and Schapire, 1997] work by iteratively running a base learning algorithm on reweighted versions of the training data to produce a sequence of classifiers \(h_{1},\ldots,h_{p}\). After obtaining \(h_{i}\), the weighting of the training data is updated to put larger weights on samples misclassified by \(h_{i}\), and smaller weights on samples classified correctly. This effectively forces the next training iteration to focus on points with which the previous classifiers struggle. After sufficiently many rounds, the classifiers \(h_{1},\ldots,h_{p}\) are finally combined by taking a (weighted) majority vote among their predictions. Many Boosting algorithms have been developed over the years, for example Grove and Schuurmans [1998], Ratsch et al. [2005], Servedio [2003], Friedman [2001], with modern Gradient Boosting [Friedman, 2001] algorithms like XGBoost [Chen and Guestrin, 2016] and LightGBM [Ke et al., 2017] often achieving state-of-the-art performance on learning tasks while requiring little to no data cleaning. See e.g. the excellent survey by Natekin and Knoll [2013] for more background on Boosting.

While Boosting enjoys many advantages, it does have one severe drawback, also highlighted in Natekin and Knoll [2013]: Boosting is completely sequential as each of the consecutive training steps requires the output of previous steps to determine the reweighted learning problem. This property is shared by all Boosting algorithms and prohibits the use of computationally heavy training by the base learning algorithm in each iteration. For instance, Gradient Boosting algorithms often require hundreds to thousands of iterations to achieve the best accuracy. The crucial point is that even if you have access to thousands of machines for training, there is no way to parallelize the steps of Boosting and distribute the work among the machines (at least beyond the parallelization possiblefor the base learner). In effect, the training time of the base learning algorithm is directly multiplied by the number of steps of Boosting.

Multiple recent works (Long and Servedio, 2013; Karbasi and Larsen, 2024; Lyu et al., 2024) have studied parallelization of Boosting from a theoretical point of view, aiming for an understanding of the inherent tradeoffs between the number of training rounds \(p\) and the total parallel work per round \(t\). These works include both strong lower bounds on the cost of parallelization and highly non-trivial parallel Boosting algorithms with provable guarantees on accuracy. Previous studies however leave a significant gap between the performance of the parallel algorithms and the proven lower bounds.

The main contribution of this work is to close this gap by both developing a parallel algorithm with a better tradeoff between \(p\) and \(t\), as well as proving a stronger lower bound on this tradeoff. To formally state our improved results and compare them to previous works, we first introduce the theoretical framework under which parallel Boosting is studied.

Weak-to-Strong Learning.Following the previous works Karbasi and Larsen (2024), Lyu et al. (2024), we study parallel Boosting in the theoretical setup of weak-to-strong learning. Weak-to-strong learning was introduced by Kearns (1988); Kearns and Valiant (1994) and has inspired the development of the first Boosting algorithms (Schapire, 1990). In this framework, we consider binary classification over an input domain \(\mathcal{X}\) with an unknown target concept \(c\colon\mathcal{X}\to\{-1,1\}\) assigning labels to samples. A \(\gamma\)-weak learner for \(c\) is then a learning algorithm \(\mathcal{W}\) that for any distribution \(\mathcal{D}\) over \(\mathcal{X}\), when given at least some constant \(m_{0}\) i.i.d. samples from \(\mathcal{D}\), produces with constant probability a hypothesis \(h\) with \(\mathcal{L}_{\mathcal{D}}(h)\leq 1/2-\gamma\). Here \(\mathcal{L}_{\mathcal{D}}(h)=\Pr_{\mathbf{x}\sim\mathcal{D}}[h(\mathbf{x}) \neq c(\mathbf{x})]\). The goal in weak-to-strong learning is then to _boost_ the accuracy of \(\mathcal{W}\) by invoking it multiple times. Concretely, the aim is to produce a strong learner: A learning algorithm that for any distribution \(\mathcal{D}\) over \(\mathcal{X}\) and any \(0<\delta,\varepsilon<1\), when given \(m(\varepsilon,\delta)\) i.i.d. samples from \(\mathcal{D}\), produces with probability at least \(1-\delta\) a hypothesis \(h\colon\mathcal{X}\to\{-1,1\}\) such that \(\mathcal{L}_{\mathcal{D}}(h)\leq\varepsilon\). We refer to \(m(\varepsilon,\delta)\) as the _sample complexity_ of the weak-to-strong learner.

Weak-to-strong learning has been extensively studied over the years, with many proposed algorithms, among which AdaBoost (Freund and Schapire, 1997) is perhaps the most famous. If \(\mathcal{H}\) denotes a hypothesis set such that \(\mathcal{W}\) always produces hypotheses from \(\mathcal{H}\), and if \(d\) denotes the VC-dimension of \(\mathcal{H}\), then in terms of sample complexity, AdaBoost is known to produce a strong learner with sample complexity \(m_{\mathrm{Ada}}(\varepsilon,\delta)\) satisfying","m_{\mathrm{Ada}}(\varepsilon,\delta)=\mathrm{O}\Bigg{(}\frac{d\ln(\frac{d}{ \varepsilon\gamma})\ln(\frac{1}{\varepsilon\gamma})}{\gamma^{2}\varepsilon}+ \frac{\ln(1/\delta)}{\varepsilon}\Bigg{)}.",,"m_{\mathrm{Ada}}(\varepsilon, \delta) = \tilde{O}\left( \frac{d}{\gamma^2} \left( \log\left(\frac{1}{\delta}\right) + \log\left(\frac{1}{\varepsilon}\right) \right) \right)","Equation 1 defines the sample complexity of AdaBoost as a function of VC-dimension $d$, weak learner edge $\gamma$, accuracy $\varepsilon$, and confidence $\delta$, with $\tilde{O}$ suppressing logarithmic factors."
neurips_2024_oral_40,2,"This can be proved by observing that after \(t=\mathrm{O}(\gamma^{-2}\ln m)\) iterations, AdaBoost produces a voting classifier \(f(x)=\mathrm{sign}(\sum_{i=1}^{t}\alpha_{i}h_{i}(x))\) with all _margins_ on the training data being \(\Omega(\gamma)\). The sample complexity bound then follows by invoking the best known generalization bounds for large margin voting classifiers (Breiman, 1999; Gao and Zhou, 2013). Here the margin of the voting classifier \(f\) on a training sample \((x,c(x))\) is defined as \(c(x)\sum_{i=1}^{t}\alpha_{i}h_{i}(x)/\sum_{i=1}^{t}\lvert\alpha_{i}\rvert\). This sample complexity comes within logarithmic factors of the optimal sample complexity \(m_{\mathrm{OPT}}(\varepsilon,\delta)=\Theta(d/(\gamma^{2}\varepsilon)+\ln(1/ \delta)/\varepsilon)\) obtained e.g. in Larsen and Ritzert (2022).

Parallel Weak-to-Strong Learning.The recent work by Karbasi and Larsen (2024) formalized parallel Boosting in the above weak-to-strong learning setup. Observing that all training happens in the weak learner, they proposed the following definition of parallel Boosting: A weak-to-strong learning algorithm has parallel complexity \((p,t)\) if for \(p\) consecutive rounds it queries the weak learner with \(t\) distributions. In each round \(i\), if \(D_{i}^{i},\ldots,D_{i}^{t}\) denotes the distributions queried, the weak learner returns \(t\) hypotheses \(h_{1}^{i},\ldots,h_{t}^{i}\in\mathcal{H}\) such that \(\mathcal{L}_{D_{j}^{i}}(h_{j}^{i})\leq 1/2-\gamma\) for all \(j\). At the end of the \(p\) rounds, the weak-to-strong learner outputs a hypothesis \(f\colon\mathcal{X}\to\{-1,1\}\). The queries made in each round and the final hypothesis \(f\) must be computable from the training data as well as all hypotheses \(h_{j}^{i}\) seen in previous rounds. The motivation for the above definition is that we could let one machine/thread handle each of the \(t\) parallel query distributions in a round.

Since parallel weak-to-strong learning is trivial if we make no requirements on \(\mathcal{L}_{\mathcal{D}}(f)\) for the output \(f\colon\mathcal{X}\to\{-1,1\}\) (simply output \(f(x)=1\) for all \(x\in\mathcal{X}\)), we from hereon focus on parallel weak-to-strong learners that are near-optimal in terms of the sample complexity and accuracy tradeoff.

More formally, from the upper bound side, our goal is to obtain a sample complexity matching at least that of AdaBoost, stated in 1. That is, rewriting the loss \(\varepsilon\) as a function of the number of samples \(m\), we aim for output classifiers \(f\) satisfying",\mathcal{L}_{\mathcal{D}}(f)=\mathrm{O}\bigg{(}\frac{d\ln(m)\ln(m/d)+\ln(1/ \delta)}{\gamma^{2}m}\bigg{)}.,,"\mathcal{L}_{\mathcal{D}}(f) \leq \tilde{\mathrm{O}}\left(\frac{d \ln^2(1/\gamma)}{\gamma^2 m}\right),","This equation specifies the target generalization error bound for the output classifier $f$ in terms of the sample size $m$, the VC-dimension $d$, and the weak learning advantage $\gamma$, aiming to match the near-optimal sample complexity of AdaBoost up to logarithmic factors."
neurips_2024_oral_40,3,"When stating lower bounds in the following, we have simplified the expressions by requiring that the _expected_ loss satisfies \(\mathcal{L}_{\mathcal{D}}(f)=\mathrm{O}(m^{-0.01})\). Note that this is far larger than the upper bounds, except for values of \(m\) very close to \(\gamma^{-2}d\). This only makes the lower bounds stronger. We remark that all the lower bounds are more general than this, but focusing on \(m^{-0.01}\) in this introduction yields the cleanest bounds.

With these definitions, classic AdaBoost and other weak-to-strong learners producing voting classifiers with margins \(\Omega(\gamma)\) all have a parallel complexity of \((\Theta(\gamma^{-2}\ln m),1)\): They all need \(\gamma^{-2}\ln m\) rounds to obtain \(\Omega(\gamma)\) margins. Karbasi and Larsen (2024) presented the first alternative tradeoff by giving an algorithm with parallel complexity \((1,\exp(\mathrm{O}(d\ln(m)/\gamma^{2})))\). Subsequent work by Lyu et al. (2024) gave a general tradeoff between \(p\) and \(t\). When requiring near-optimal accuracy, their tradeoff gives, for any \(1\leq R\leq 1/(2\gamma)\), a parallel complexity of \((O(\gamma^{-2}\ln(m)/R),\exp(\mathrm{O}(dR^{2}))\ln(1/\gamma))\). The accuracy of both of these algorithms was proved by arguing that they produce a voting classifier with all margins \(\Omega(\gamma)\).

On the lower bound side, Karbasi and Larsen (2024) showed that one of three things must hold: Either \(p\geq\min\{\Omega(\gamma^{-1}\ln m),\exp(\Omega(d))\}\), or \(t\geq\min\{\exp(\Omega(d\gamma^{-2})),\exp(\exp(\Omega(d)))\}\) or \(p\ln(tp)=\Omega(d\ln(m)\gamma^{-2})\).

Lyu et al. (2024) also presented a lower bound that for some parameters is stronger than that of Karbasi and Larsen (2024), and for some is weaker. Concretely, they show that one of the following two must hold: Either \(p\geq\min\{\Omega(\gamma^{-2}d),\Omega(\gamma^{-2}\ln m),\exp(\Omega(d))\}\), or \(t\geq\exp(\Omega(d))\). Observe that the constraint on \(t\) is only single-exponential \(d\), whereas the previous lower bound is double-exponential. On the other hand, the lower bound on \(p\) is essentially stronger by a \(\gamma^{-1}\) factor. Finally, they also give an alternative lower bound for \(p=\mathrm{O}(\gamma^{-2})\), essentially yielding \(p\ln t=\Omega(\gamma^{-2}d)\).

Even in light of the previous works, it is still unclear what the true complexity of parallel Boosting is. In fact, the upper and lower bounds only match in the single case where \(p=\Omega(\gamma^{-2}\ln m)\) and \(t=1\), i.e. when standard AdaBoost is optimal.

Our Contributions.In this work, we essentially close the gap between the upper and lower bounds for parallel Boosting. From the upper bound side, we show the following general result.

**Theorem 1.1**.: _Let \(c\colon\mathcal{X}\to\{-1,1\}\) be an unknown concept, \(\mathcal{W}\) be a \(\gamma\)-weak learner for \(c\) using a hypothesis set of VC-dimension \(d\), \(\mathcal{D}\) be an arbitrary distribution, and \(\mathbf{S}\sim\mathcal{D}^{m}\) be a training set of size \(m\). For all \(R\in\mathbb{N}\), Algorithm 1 yields a weak-to-strong learner \(\mathcal{A}_{R}\) with parallel complexity \((p,t)\) for_","p=\mathrm{O}\bigg{(}\frac{\ln m}{\gamma^{2}R}\bigg{)}\qquad\text{and}\qquad t =e^{\mathrm{O}(dR)}\cdot\ln\frac{\ln m}{\delta\gamma^{2}},",,\mathcal{L}_{\mathcal{D}}(f)=\mathrm{O}\left(\frac{d\ln(m)\ln(m/d)+\ln(1/\delta)}{\gamma^{2}m}\right).,"This equation specifies the desired bound on the expected loss of the output classifier $f$ in terms of the sample size $m$, the VC-dimension $d$, the weak learning parameter $\gamma$, and the confidence parameter $\delta$, which the proposed algorithm aims to achieve with improved parallel complexity."
neurips_2024_oral_40,4,"_such that, with probability at least \(1-\delta\) over \(\mathbf{S}\) and the randomness of \(\mathcal{A}_{R}\), it holds that_",\mathcal{L}_{\mathcal{D}}(\mathcal{A}_{R}(\mathbf{S}))=\mathrm{O}\bigg{(}\frac {d\ln(m)\ln(m/d)+\ln(1/\delta)}{\gamma^{2}m}\bigg{)}.,,\mathcal{L}_{\mathcal{D}}(f)=\mathrm{O}\bigg{(}\frac{dR\ln(m)\ln\big{(}\frac{\ln m}{\delta\gamma^{2}}\big{)}+\ln(1/\delta)}{\gamma^{2}m}\bigg{)}.,"This equation characterizes the expected loss of the output classifier $f$ produced by the parallel weak-to-strong learning algorithm $\mathcal{A}_R$, showing how the loss depends on the number of rounds $p$, the number of queries per round $t$, the VC-dimension $d$, the weak learning advantage $\gamma$, the number of samples $m$, and the confidence parameter $\delta$."
neurips_2024_oral_40,5,"Observe that this is a factor \(R\) better than the bound by Lyu et al. (2024) in the exponent of \(t\). Furthermore, if we ignore the \(\ln(\ln(m)/(\delta\gamma^{2}))\) factor, it gives the clean tradeoff","p\ln t=\mathrm{O}\bigg{(}\frac{d\ln m}{\gamma^{2}}\bigg{)},",,t=e^{\mathrm{O}(dR)}\qquad\text{and}\qquad p=\mathrm{O}\bigg{(}\frac{\ln m}{\gamma^{2}R}\bigg{)}.,"This equation presents the clean tradeoff between the number of parallel rounds $p$ and the total parallel work per round $t$ achieved by the proposed algorithm, improving upon previous bounds by a factor of $R$ in the exponent of $t$."
neurips_2024_oral_40,6,"for any \(p\) from \(1\) to \(\mathrm{O}(\gamma^{-2}\ln m)\).

We complement our new upper bound by an essentially matching lower bound. Here we show that 

**Theorem 1.2**.: _There is a universal constant \(C\geq 1\) for which the following holds. For any \(0<\gamma<1/C\), any \(d\geq C\), any sample size \(m\geq C\), and any weak-to-strong learner \(\mathcal{A}\) with parallel complexity \((p,t)\), there exists an input domain \(\mathcal{X}\), a distribution \(\mathcal{D}\), a concept \(c\colon\mathcal{X}\to\{-1,1\}\), and a \(\gamma\)-weak learner \(\mathcal{W}\) for \(c\) using a hypothesis set \(\mathcal{H}\) of VC-dimension \(d\) such that if the expected loss of \(\mathcal{A}\) over the sample is no more than \(m^{-0.01}\), then either \(p\geq\min\{\exp(\Omega(d)),\Omega(\gamma^{-2}\ln m)\}\), or \(t\geq\exp(\exp(\Omega(d)))\), or \(p\ln t=\Omega(\gamma^{-2}d\ln m)\)._

Comparing Theorem1.2 to known upper bounds, we first observe that \(p=\Omega(\gamma^{-2}\ln m)\) corresponds to standard AdaBoost and is thus tight. The term \(p=\exp(\Omega(d))\) is also near-tight. In particular, given \(m\) samples, by Sauer-Shelah, there are only \(\mathrm{O}((m/d)^{d})=\exp(\mathrm{O}(d\ln(m/d)))\) distinct labellings by \(\mathcal{H}\) on the training set. If we run AdaBoost, and in every iteration, we check whether a previously obtained hypothesis has advantage \(\gamma\) under the current weighing, then we make no more than \(\exp(\mathrm{O}(d\ln(m/d)))\) queries to the weak learner (since every returned hypothesis must be distinct). The \(p\ln t=\Omega(\gamma^{-2}d\ln m)\) matches our new upper bound in Theorem1.1. Thus, only the \(t\geq\exp(\exp(\Omega(d)))\) term does not match any known upper bound.

Other Related Work.Finally, we mention the work by Long and Servedio (2013), which initiated the study of the parallel complexity of Boosting. In their work, they proved that the parallel complexity \((p,t)\) must satisfy \(p=\Omega(\gamma^{-2}\ln m)\), regardless of \(t\) (they state it as \(p=\Omega(\gamma^{-2})\), but it is not hard to improve by a \(\ln m\) factor for loss \(m^{-0.01}\)). This seems to contradict the upper bounds above. The reason is that their lower bound has restrictions on which query distributions the weak-to-strong learner makes to the weak learner. The upper bounds above thus all circumvent these restrictions. As a second restriction, their lower bound instance has a VC-dimension that grows with \(m\).

## 2 Upper Bound

In this section, we discuss our proposed method, Algorithm1. Here, \(C_{\mathrm{n}}\) refers a universal constant shared among results.

We provide a theoretical analysis of the algorithm, showing that it realizes the claims in Theorem1.1. Our proof goes via the following intermediate theorem:

**Theorem 2.1**.: _There exists universal constant \(C_{\mathrm{n}}\geq 1\) such that for all \(0<\gamma<1/2\), \(R\in\mathbb{N}\), concept \(c\colon\mathcal{X}\to\{-1,1\}\), and hypothesis set \(\mathcal{H}\subseteq\{-1,1\}^{\mathcal{X}}\) of VC-dimension \(d\), Algorithm1 given an input training set \(S\in\mathcal{X}^{m}\), a \(\gamma\)-weak learner \(\mathcal{W}\),_","p\geq\frac{4\ln m}{\gamma^{2}R},\qquad\text{and}\qquad t\geq e^{16C_{\mathrm{ a}}dR}\cdot R\ln\frac{pR}{\delta},",,p\ln t=\Omega\left(\frac{d\ln m}{\gamma^{2}}\right).,"This lower bound matches the upper bound tradeoff from Theorem 1.1, showing the tightness of the parallel complexity in terms of the product of rounds and logarithm of parallel queries."
neurips_2024_oral_40,7,"_produces a linear classifier \(\mathbf{g}\) at Line21 such that with probability at least \(1-\delta\) over the randomness of Algorithm1, \(\mathbf{g}(x)c(x)\geq\gamma/8\) for all \(x\in S\)._

In Theorem2.1 and throughout the paper, we define a _linear classifier_\(g\) as linear combination of hypotheses \(g(x)=\sum_{i=1}^{k}\alpha_{i}h_{i}(x)\) with \(\sum_{i}\lvert\alpha_{i}\rvert=1\). A linear classifier thus corresponds to a voting classifier with coefficients normalized and no \(\operatorname{sign}\) operation. Observe that the voting classifier \(f(x)=\operatorname{sign}(g(x))\) is correct if and only if \(c(x)g(x)>0\), where \(c(x)\) is the correct label of \(x\). Furthermore, \(c(x)g(x)\) is the margin of the voting classifier \(f\) on input \(x\).

Theorem1.1 follows from Theorem2.1 via generalization bounds for linear classifiers with large margins. Namely, we apply Breiman's min-margin bound:

**Theorem 2.2** (Breiman (1999)).: _Let \(c\colon\mathcal{X}\to\{-1,1\}\) be an unknown concept, \(\mathcal{H}\subseteq\{-1,1\}^{\mathcal{X}}\) a hypothesis set of VC-dimension \(d\) and \(\mathcal{D}\) an arbitrary distribution over \(\mathcal{X}\). There is a universal constant \(C>0\) such that with probability at least \(1-\delta\) over a set of \(m\) samples \(\mathbf{S}\sim\mathcal{D}^{m}\), it holds for every linear classifier \(g\) satisfying \(c(x)g(x)\geq\gamma\) for all \((x,c(x))\in\mathbf{S}\) that_",\mathcal{L}_{\mathcal{D}}(\operatorname{sign}(g))\leq C\cdot\frac{d\ln(m)\ln(m /d)+\ln(1/\delta)}{\gamma^{2}m}.,,\mathcal{L}_{\mathcal{D}}(\operatorname{sign}(g))\leq\frac{C}{m}\Bigg{(}d\ln\Big{(}\frac{m}{d}\Big{)}\ln(m)+\ln\Big{(}\frac{1}{\delta}\Big{)}\Bigg{)}.,"This bound relates the generalization error of a voting classifier to the margins achieved on the training data, ensuring that large margins lead to better generalization."
neurips_2024_oral_40,8,"Thus far, our general strategy mirrors that of previous works: We seek to show that given suitable parameters Algorithm1 produces a linear classifier with margins of order \(\gamma\) with good probability.

Therefore, this section focuses on the lemmas that describe how, with suitable parameters, Algorithm 1 produces a classifier with large margins. With these results in hand, the proof of Theorem 2.1 becomes quite straightforward, so we defer it to Appendix B.3.

```
Input : Training set \(S=\{(x_{1},c(x_{1})),\ldots,(x_{m},c(x_{m}))\}\), \(\gamma\)-weak learner \(\mathcal{W}\), number of calls to weak learner per round \(t\), number of rounds \(p\) Output : Voting classifier \(f\)
1\(\alpha\leftarrow\frac{1}{2}\ln\frac{1/2+\gamma/2}{1/2-\gamma/2}\)
2\(n\gets C_{n}d/\gamma^{2}\)
3\(D_{1}\leftarrow(\frac{1}{m},\frac{1}{m},\ldots,\frac{1}{m})\)
4for\(k\gets 0\)to\(p-1\)do
5 parallel for\(r\gets 1\)to \(R\)do
6 parallel for\(j\gets 1\)to\(t/R\)do
7 Sample \(\mathbf{T}_{kR+r,j}\sim\mathbf{D}_{kR+1}^{n}\)
8\(\mathbf{h}_{kR+r,j}\leftarrow\mathcal{W}(\mathbf{T}_{kR+r,j},\mathrm{Uniform}( \mathbf{T}_{kR+r,j}))\)
9\(\mathbf{\mathcal{H}}_{kR+r}\leftarrow\{\mathbf{h}_{kR+r,1},\ldots,\mathbf{h}_{kR+ r,t/R}\}\cup\{-\mathbf{h}_{kR+r,1},\ldots,-\mathbf{h}_{kR+r,t/R}\}\)
10for\(r\gets 1\)to\(R\)do
11ifthere exists \(\mathbf{h}^{*}\in\mathbf{\mathcal{H}}_{kR+r}\) s.t. \(\mathcal{L}_{\mathbf{D}_{kR+r}}(\mathbf{h}^{*})\leq 1/2-\gamma/2\)then
12\(\mathbf{h}_{kR+r}\leftarrow\mathbf{h}^{*}\)
13\(\alpha_{kR+r}\leftarrow\alpha\)
14
15else
16\(h_{kR+r}\leftarrow\) arbitrary hypothesis from \(\mathbf{\mathcal{H}}_{kR+r}\)
17\(\alpha_{kR+r}\gets 0\)
18for\(i\gets 1\)to\(m\)do
19\(\mathbf{D}_{kR+r+1}(i)\leftarrow\mathbf{D}_{kR+r}(i)\exp(-\mathbf{\alpha}_{kR+r}c(x_{i}) \mathbf{h}_{kR+r}(x_{i}))\)
20\(\mathbf{Z}_{kR+r}\leftarrow\sum_{i=1}^{m}\mathbf{D}_{kR+r}(i)\exp(-\mathbf{\alpha}_{kR+ r}c(x_{i})\mathbf{h}_{kR+r}(x_{i}))\)
21\(\mathbf{D}_{kR+r+1}\leftarrow\mathbf{D}_{kR+r+1}/\mathbf{Z}_{kR+r}\)
22\(\mathbf{g}\gets x\mapsto\frac{1}{\sum_{j=1}^{pR}\mathbf{\alpha}_{j}}\sum_{j= 1}^{pR}\mathbf{\alpha}_{j}\mathbf{h}_{j}(x)\) returnf : \(x\mapsto\mathrm{sign}(\mathbf{g}(x))\)
```

**Algorithm 1**Proposed parallel Boosting algorithm

Algorithm 1 is a variant of Lyu et al. (2024, Algorithm 2). The core idea is to use bagging to produce (in parallel) a set of hypotheses and use it to simulate a weak learner. To be more precise, we reason in terms of the following definition.

**Definition 1** (\(\varepsilon\)-approximation).: Given a concept \(c\colon\mathcal{X}\to\{-1,1\}\), a hypothesis set \(\mathcal{H}\subseteq\{-1,1\}^{\mathcal{X}}\), and a distribution \(\mathcal{D}\) over \(\mathcal{X}\), a multiset \(T\) is an \(\varepsilon\)_-approximation_ for \(\mathcal{D}\), \(c\), and \(\mathcal{H}\) if for all \(h\in\mathcal{H}\), it holds that","|\mathcal{L}_{\mathcal{D}}(h)-\mathcal{L}_{T}(h)|\leq\varepsilon,",where \(\mathcal{L}_{T}(h)\coloneqq\mathcal{L}_{\mathrm{Uniform}(T)}(h)\) is the empirical loss of \(h\) on \(T\),\left|\Pr_{\mathbf{x}\sim\mathcal{D}}[h(\mathbf{x})\neq c(\mathbf{x})]-\Pr_{\mathbf{x}\sim T}[h(\mathbf{x})\neq c(\mathbf{x})]\right|\leq\varepsilon.,
neurips_2024_oral_40,9,"\). Moreover, we omit the reference to \(c\) and \(\mathcal{H}\) when no confusion seems possible.

Consider a reference distribution \(D_{0}\) over a training dataset \(S\). The bagging part of the method leverages the fact that if a subsample \(\mathbf{T}\sim D_{0}^{n}\) is a \(\gamma/2\)-approximation for \(D_{0}\), then inputting \(\mathbf{T}\) (with the uniform distribution over it) to a \(\gamma\)-weak learner produces a hypothesis \(h\) that, besides having advantage \(\gamma\) on \(\mathbf{T}\), also has advantage \(\gamma/2\) on the entire dataset \(S\) (relative to \(D_{0}\)). Indeed, in this setting, we have that \(\mathcal{L}_{D_{0}}(h)\leq\mathcal{L}_{\mathbf{T}}(h)+\gamma/2\leq 1/2-\gamma+ \gamma/2=1/2-\gamma/2\). We can then take \(h\) as if produced by a \(\gamma/2\)-weak learner queried with \((S,D_{0})\), and compute a new distribution \(D_{1}\) via a standard Boosting step1. That is, we can simulate a \(\gamma/2\)-weak learner as long as we can provide a \(\gamma/2\)-approximation for the target distribution. The strategy is to have a parallel bagging step in which we sample \(\mathbf{T}_{1},\mathbf{T}_{2},\ldots,\mathbf{T}_{t}\stackrel{{ \text{\tiny{$\textcircled{$\textcircled{$\textcircled{$\textcircled{$ \textcircled{$\text{\textcircled{$\textcircledcled{$\text{\textfind at least one \(\gamma/2\)-approximation for each \(D_{r}\) among \(\mathbf{h}_{1},\mathbf{h}_{2},\ldots,\mathbf{h}_{t}\). A single parallel round of querying the weak learner is thus sufficient for performing \(R\) steps of Boosting, effectively reducing \(p\) by a factor \(R\). Crucially, testing the performance of the returned hypotheses \(\mathbf{h}_{1},\ldots,\mathbf{h}_{t}\) uses only inference/predictions and no calls to the weak learner.

The challenge is that the distributions \(D_{r}\) diverge (exponentially fast) from \(D_{0}\) as we progress in the Boosting steps. For the first Boosting step, the following classic result ensures a good probability of obtaining an approximation for \(D_{0}\) when sampling from \(D_{0}\) itself.

**Theorem 2.3** (Li et al. (2001); Talagrand (1994); Vapnik and Chervonenkis (1971)).: _There is a universal constant \(C>0\) such that for any \(0<\varepsilon,\delta<1\), \(\mathcal{H}\subseteq\{-1,1\}^{\mathcal{X}}\) of VC-dimension \(d\), and distribution \(\mathcal{D}\) over \(\mathcal{X}\), it holds with probability at least \(1-\delta\) over a set \(\mathbf{T}\sim\mathcal{D}^{n}\) that \(\mathbf{T}\) is an \(\varepsilon\)-approximation for \(\mathcal{D}\), \(c\), and \(\mathcal{H}\) provided that \(n\geq C((d+\ln(1/\delta))/\varepsilon^{2})\)._

However, we are interested in approximations for \(D_{r}\) when we only have access to samples from \(D_{0}\). Lyu et al. (2024) approaches this problem by tracking the ""distance"" between the distributions in terms of their _max-divergence_","\mathrm{D}_{\infty}(D_{r},D_{0})\coloneqq\ln\bigl{(}\sup_{x\in\mathcal{X}}D_{ r}(x)/D_{0}(x)\bigr{)}.",,\mathcal{L}_{D_{0}}(h)\leq\mathcal{L}_{\mathbf{T}}(h)+\gamma/2\leq 1/2-\gamma+\gamma/2=1/2-\gamma/2.,"This inequality demonstrates that a hypothesis $h$ with advantage $\gamma$ on a $\gamma/2$-approximation $\mathbf{T}$ of $D_0$ also has advantage $\gamma/2$ on the full dataset $S$ under $D_0$, enabling the simulation of a $\gamma/2$-weak learner using bagging."
neurips_2024_oral_40,10,"By bounding both \(\mathrm{D}_{\infty}(D_{r},D_{0})\) and \(\mathrm{D}_{\infty}(D_{0},D_{r})\), the authors can leverage the _advanced composition theorem_(Dwork et al., 2010)2 from the differential privacy literature to bound the probability of obtaining an approximation for \(D_{r}\) when sampling from \(D_{0}\). In turn, this allows them to relate the number of samples \(t\) and the (sufficiently small) number of Boosting steps \(R\) in a way that ensures a good probability of success at each step.

Footnote 2: Lemma 4.6 of Lyu et al. (2024).

Besides setting up the application of advanced composition, the use of the max-divergence also simplifies the analysis since its ""locality"" allows one to bound the divergence between the two distributions via a worst-case study of a single entry. However, this approach sacrifices global information, limiting how much we can leverage our understanding of the distributions generated by Boosting algorithms. With that in mind, we instead track the distance between \(D_{r}\) and \(D_{0}\) in terms of the _Kullback-Leibler divergence_ (KL divergence)(Kullback and Leibler, 1951) between them:",\mathrm{KL}(D_{r}\parallel D_{0})\coloneqq\sum_{x\in\mathcal{X}}D_{r}(x)\ln \frac{D_{r}(x)}{D_{0}(x)}.,,\mathrm{D}_{\mathrm{KL}}(D_{r}\parallel D_{0})\coloneqq\sum_{x\in\mathcal{X}}D_{ r}(x)\ln\frac{D_{r}(x)}{D_{0}(x)}.,
neurips_2024_oral_40,11,"Comparing this expression to Eq.2 reveals that the max-divergence is indeed a worst-case estimation of the KL divergence.

The KL divergence --also known as _relative entropy_-- between two distributions \(P\) and \(Q\) is always non-negative and equal to zero if and only if \(P=Q\). Moreover, in our setting, it is always finite due to the following remark.3

Footnote 3: We only need \(P\) to be absolutely continuous with respect to \(Q\); i.e., that for any event \(A\), we have \(P(A)=0\) whenever \(Q(A)=0\). We express our results in terms of identical supports for the sake of simplicity as they can be readily generalized to only require absolute continuity.

**Remark 1**.: In the execution Algorithm1, every distribution \(D_{\ell}\), for \(\ell\in[pR]\), has the same support. This must be the case since Line20 always preserves the support of \(D_{1}\).

On the other hand, the KL divergence is not a proper metric as it is not symmetric and it does not satisfy the triangle inequality, unlike the max-divergence. This introduces a number of difficulties in bounding the divergence between \(\mathcal{D}_{0}\) and \(\mathcal{D}_{r}\). Overcoming these challenges requires a deeper and highly novel analysis. Our results reveal that the KL divergence captures particularly well the behavior of our Boosting algorithm. We remark that we are not the first to relate KL divergence and Boosting, see e.g. Schapire and Freund (2012, Chapter 8 and the references therein), yet we make several new contributions to this connection.

To study the probability of obtaining a \(\gamma/2\)-approximation for \(D_{r}\) when sampling from \(D_{0}\), rather than using advanced composition, we employ the _duality formula for variational inference_(Donsker and Varadhan, 1975) --also known as _Gibbs variational principle_, or _Donsker-Varadhan formula_--to estimate such a probability in terms of \(\mathrm{KL}(D_{r}\parallel D_{0})\).

**Lemma 2.4** (Duality formula4).: _Given finite probability spaces \((\Omega,\mathcal{F},P)\) and \((\Omega,\mathcal{F},Q)\), if \(P\) and \(Q\) have the same support, then for any real-valued random variable \(\mathbf{X}\) on \((\Omega,\mathcal{F},P)\) we have that_

Footnote 4: Corollary of, e.g., Dembo and Zeitouni (1998, Lemma 6.2.13) or Lee (2022, Theorem 2.1). Presented here in a weaker form for the sake of simplicity.",\ln\mathbb{E}_{P}\big{[}e^{\mathbf{X}}\big{]}\geq\mathbb{E}_{Q}[\mathbf{X}]- \mathrm{KL}(Q\parallel P).,,\mathbb{E}_{P}[\mathbf{X}]\leq\mathrm{KL}(P\parallel Q)+\ln\mathbb{E}_{Q}[e^{ \mathbf{X}}].,Lemma 2.4 provides a duality formula that upper bounds the expectation of a random variable under distribution $P$ in terms of the Kullback-Leibler divergence between $P$ and $Q$ and the log moment generating function under $Q$.
neurips_2024_oral_40,12,"Lemma 2.4 allows us to prove that if \(\mathrm{KL}(D_{r}\parallel D_{0})\) is sufficiently small, then the probability of obtaining a \(\gamma/2\)-approximation for \(D_{r}\) when sampling from \(D_{0}\) is sufficiently large. Namely, we prove the following.

**Lemma 2.5**.: _There exists universal constant \(C_{\mathrm{n}}\geq 1\) for which the following holds. Given \(0<\gamma<1/2\), \(R,m\in\mathbb{N}\), concept \(c\colon\mathcal{X}\to\{-1,1\}\), and hypothesis set \(\mathcal{H}\subseteq\{-1,1\}^{\mathcal{X}}\) of VC-dimension \(d\), let \(\tilde{D}\) and \(D\) be distributions over \([m]\) and \(\mathcal{G}\in[m]^{*}\) be the family of \(\gamma/2\)-approximations for \(D\), \(c\), and \(\mathcal{H}\). If \(\tilde{D}\) and \(D\) have the same support and_","\mathrm{KL}(D\parallel\tilde{D})\leq 4\gamma^{2}R,",,"\mathrm{KL}(\tilde{D}\parallel D)\leq\frac{\gamma^{2}n}{C_{\mathrm{n}}dR},",Upper bound on the KL divergence between distributions ensuring a sufficiently high probability of sampling a γ/2-approximation for D when sampling from D̃
neurips_2024_oral_40,13,_then for all \(n\geq C_{\mathrm{n}}\cdot d/\gamma^{2}\) it holds that_,\Pr_{\mathbf{T}\sim\tilde{D}^{n}}[\mathbf{T}\in\mathcal{G}]\geq\exp(-16C_{ \mathrm{n}}dR).,,\Pr_{\mathbf{T}\sim\tilde{D}^{n}}[\mathbf{T}\text{ contains at least one }\gamma/2\text{-approximation for }D]\geq 1-e^{-R}.,"Probability that a subsample $\mathbf{T}$ drawn from $\tilde{D}^n$ contains at least one $\gamma/2$-approximation for $D$, given that the KL divergence between $D$ and $\tilde{D}$ is bounded by $4\gamma^2 R$, and $n$ is sufficiently large in terms of $d$ and $\gamma$."
neurips_2024_oral_40,14,"Proof sketch.: Our argument resembles a proof of the Chernoff bound: After taking exponentials on both sides of Eq.3, we exploit the generality of Lemma2.4 by defining the random variable \(\mathbf{X}\colon T\mapsto\lambda\mathbf{1}_{\{T\in\mathcal{G}\}}\) and later carefully choosing \(\lambda\). We then note that Theorem2.3 ensures that \(\mathbf{X}\) has high expectation for \(\mathbf{T}\sim D^{n}\). Setting \(\lambda\) to leverage this fact, we obtain a lower bound on the expectation of \(\mathbf{X}\) relative to \(\mathbf{T}\sim\tilde{D}^{n}\), yielding the thesis. 

We defer the detailed proof to AppendixB.1.

With Lemma2.5 in hand, recall that our general goal is to show that, with high probability, the linear classifier \(g\) produced by Algorithm1 satisfies that \(c(x)g(x)=\Omega(\gamma)\) for all \(x\in S\). Standard techniques allow us to further reduce this goal to that of showing that the product of the normalization factors, \(\prod_{\ell=1}^{pR}Z_{\ell}\), is sufficiently small. Accordingly, in our next lemma, we bound the number of samples needed in the bagging step to obtain a small product of the normalization factors produced by the Boosting steps.

Here, the analysis in terms of the KL divergence delivers a clear insight into the problem, revealing an interesting trichotomy: if \(\mathrm{KL}(D_{r}\parallel D_{0})\) is small, Lemma2.5 yields the result; on the other hand, if \(D_{r}\) has diverged too far from \(D_{0}\), then either the algorithm has already made enough progress for us to skip a step, or the negation of some hypothesis used in a previous step has sufficient advantage relative to the distribution at hand. Formally, we prove the following.

**Lemma 2.6**.: _There exists universal constant \(C_{\mathrm{n}}\geq 1\) such that for all \(R\in\mathbb{N}\), \(0<\delta<1\), \(0<\gamma<1/2\), and \(\gamma\)-weak learner \(\mathcal{W}\) using a hypothesis set \(\mathcal{H}\subseteq\{-1,1\}^{\mathcal{X}}\) with VC-dimension \(d\), if \(t\geq R\cdot\exp(16C_{\mathrm{n}}dR)\cdot\ln(R/\delta),\) then with probability at least \(1-\delta\) the hypotheses \(\mathbf{h}_{kR+1},\ldots,\mathbf{h}_{kR+R}\) obtained by Algorithm1 induce normalization factors \(\mathbf{Z}_{kR+1},\ldots,\mathbf{Z}_{kR+R}\) such that_",\prod_{r=1}^{R}\mathbf{Z}_{kR+r}<\exp(-\gamma^{2}R/2).,,,
neurips_2024_oral_41,1,"### Problem description

Consider the problem of minimizing objective (loss) \(\phi\),","\min_{x\in\mathbb{R}^{d}_{c}}\phi(x),",where \(\phi:\mathbb{R}^{d}\rightarrow\mathbb{R}\) is a differentiable function bounded from below (e,"\begin{equation}
    \min_{\mathbf{x}} \phi(\mathbf{x})
\end{equation}",Equation 1: Objective function minimization problem
neurips_2024_oral_41,2,"(e.g., by zero), and \(\mathbb{R}^{d}_{c}\subset\mathbb{R}^{d}\) is a set of all possible quantized weights that can be represented with a given quantization method. Without loss of generality4, we first analyze the case of scalar nonlinear quantization. In this scenario, \(c\in[d]:=\{1,2,\dots,d\}\) (typically \(c\ll d\)), and \(\mathbb{R}^{d}_{c}\subset\mathbb{R}^{d}\) is the set of all vectors in \(\mathbb{R}^{d}\) whose \(d\) entries take exactly \(c\) distinct values. In other words, the cardinality of the set \(V(x):=\{x_{1},\dots,x_{d}\}\) is equal to \(c\), and we can therefore write \(\mathbb{R}^{d}_{c}:=\{x\in\mathbb{R}^{d}\ :\ |V(x)|=c\}\).

Footnote 4: We explain how this generalizes to other quantized representations in Appendix C 

**Useful notation.** A vector \(x\in\mathbb{R}^{d}_{c}\) naturally induces a partition, which we shall call \(P(x)\), of the set \(\{1,\ldots,d\}\) into \(c\) nonempty subsets \(P_{1}(x),\ldots,P_{c}(x)\) characterized by",x_{i}=x_{j}\quad\Leftrightarrow\quad\exists k\;:\;i\in P_{k}\text{ and }j\in P_{k}.,,"\mathbb{R}^{d}_{c} := \{x \in \mathbb{R}^{d} : |V(x)| = c\}, \quad \text{where } V(x) := \{x_1, \dots, x_d\},",defines the set of all vectors in $\mathbb{R}^d$ whose entries take exactly $c$ distinct values.
neurips_2024_oral_41,3,"Let's denote \(P(x):=\{P_{1}(x),\ldots,P_{c}(x)\}\). Moreover, we shall write \(P(y)\sqsupseteq P(x)\) if each element of \(P(x)\) is a subset of some element of \(P(y)\). For distinct \(i,j\in[d]\), let us introduce the notation \(\delta_{ij}(x)=1\) if there exists \(k\) such that \(i,j\in P_{k}(x)\), and \(\delta_{ij}(x)=0\) otherwise. Given this notation, notice that \(P(y)\sqsupseteq P(x)\) if and only if for all \(i\neq j\) we have \(\delta_{ij}(x)=1\Rightarrow y_{i}=y_{j}.\) Finally, we define \(\mathbb{R}^{d}_{\leq c}:=\mathbb{R}^{d}_{1}\cup\cdots\cup\mathbb{R}^{d}_{c}\) as the set of all vectors in \(\mathbb{R}^{d}\) whose \(d\) entries take at most \(c\) distinct values. So, if \(x\in\mathbb{R}^{d}_{c}\) and \(P(y)\sqsupseteq P(x)\), then \(y\in\mathbb{R}^{d}_{\leq c}\).

**PV method.** Following this notation, we define an optimization algorithm that alternates between optimizing \(\phi\) with fixed \(P\) or fixed \(V\). From a practitioner's point of view, these represent optimizing continuous parameters (scales, codebooks, zeros) and discrete codes (assignments), respectively.

\(\diamond\)**The P step (fixing \(P\)).** Given \(x\in\mathbb{R}^{d}_{c}\), consider the mapping","M_{P}(x)=M_{P,\phi}(x):=\arg\min_{y\in\mathbb{R}^{d}}\{\phi(y)\,:\,P(y)\sqsupseteq P (x)\}.",,"\mathcal{M}_{P}(x):=\{y\in\mathbb{R}^{d}\ :\ \forall i,j,\ \delta_{ij}(x)=1\Rightarrow y_{i}=y_{j}\},",defines the set of vectors $y$ that preserve the partition structure of $x$ by enforcing equality among components within each partition subset.
neurips_2024_oral_41,4,"Notice that, necessarily, \(M_{P}(x)\in\mathbb{R}^{d}_{\leq c}\) and \(\phi(M_{P}(x))\leq\phi(M_{P}(x))\leq\phi(x).\) Evaluating \(M_{P}\) amounts to solving an unconstrained optimization problem in a \(c\)-dimensional space.

\(\diamond\)**The V step (fixing \(V\)).** Similarly, given \(y\in\mathbb{R}^{d}_{c}\), we define the mapping","M_{V}(y)=M_{V,\phi}(y):=\arg\min_{x\in\mathbb{R}^{d}}\{\phi(x)\,:\,V(x)\subseteq V (y)\}.",,"M_{V}(y)=M_{V,\phi}(y):=\arg\min_{x\in\mathbb{R}^{d}}\{\phi(x)\,:\,V(x)=V(y)\},",which finds the minimizer of $\phi(x)$ subject to the constraint that the set of distinct values in $x$ matches that of $y$.
neurips_2024_oral_41,5,"Likewise, \(M_{V}(y)\in\mathbb{R}^{d}_{\leq c}\) and \(\phi(M_{V}(y))\leq\phi(M_{V}(y))\leq\phi(y).\) Evaluating \(M_{V}\) amounts to solving difficult discrete optimization problems with a search space of size \(|V(x)|^{d}\leq c^{d}\) (exponential in \(d\)).

```
1:Initialization: starting point \(x^{0}\in\mathbb{R}^{d}_{\leq c}\)
2:for\(k=0,1,\ldots\)do
3:\(y^{k}=M_{P}(x^{k}):=\arg\min_{y\in\mathbb{R}^{d}}\left\{\phi(y):P(y)\sqsupseteq P (x^{k})\right\}\) (P step: continuous)
4:\(x^{k+1}=M_{V}(y^{k}):=\arg\min_{x\in\mathbb{R}^{d}}\left\{\phi(x):V(x)\subseteq V (y^{k})\right\}\) (V step: discrete)
5:endfor
```

**Algorithm 1** PV algorithm

Our key algorithmic idea, in its simplest form, is to optimize \(\phi\) by alternating the P and V steps, i.e., iteratively applying the \(M_{P}\) and \(M_{V}\) operators. (We will propose several more practically-useful approximations and variations later; see Sections 3.2-3.3 and also Appendix B.) This resulting method, which we call the PV method, is formalized as Algorithm 1. Our key guarantee for the PV method is formalized in the next result.

**Theorem 3.1** (Convergence of the PV method).: _Assume \(\phi\) is bounded below, and let \(x^{0}\in\mathbb{R}^{d}_{c}\). Then (i) \(y^{k}\in\mathbb{R}^{d}_{\leq c}\) and \(x^{k}\in\mathbb{R}^{d}_{\leq c}\) for all \(k\geq 0\); (ii) \(\phi(x^{k+1})\leq\phi(y^{k})\leq\phi(x^{k})\) for all \(k\geq 0\); and (iii) the sequence \(\{\phi(x^{k})\}_{k\geq 0}\) converges._

The proof can be found in Appendix A.1. Note that we do not claim that the method converges to a minimizer of \(\phi\); the optimization problem is too difficult for us to be able to guarantee this. However, as we shall see in the numerical results, we nevertheless obtain great empirical performance, especially when coupling the PV approach with some additional algorithmic tricks.

This general approach is popular in ""shallow"" machine learning problems; for instance, if \(\phi(x)=\|x-z\|^{2}\) is the squared error with respect to some user-specified vector \(z\), then the above algorithm recovers \(1\)-dimensional \(K\)-means on the data vector \(z\). Likewise, if \(\phi(\cdot)\) is the log-likelihood, then, depending on the choice of the set \(\mathbb{R}^{d}_{c}\), the approach is related to the EM algorithm [16].

In turn, we apply the PV method to obtaining highly-accurate quantized LLMs. Applying the PV method ""as is"", would be infeasible in practice: computing the P and V mappings requires solving difficult optimization problems especially due to LLM parameter scales. However, both mappings can be approximated. The P step can be reparameterized as an unconstrained optimization problem on theunique values in the weight matrix. Practically it means that the ""codebooks"" can be optimized using an automated differentiation engine (i.e. PyTorch). However, for many quantized representations, \(M_{P}(x)\) can be approximated by one or more steps of GD, directly optimizing \(\phi\) over the set \(V(x)\) of its \(c\) unique values. The \(c\)-dimensional gradient can be computed efficiently by backprop, as described in prior works [63, 71]. On the other hand, the V step \((M_{V}(\cdot))\) is more difficult to approximate as it involves searching a discrete space of size \(c^{d}\). We dedicate the next two sections to this task.

### Linearized V step & gradient-based discrete updates

The V mapping (3) can be approximated by solving a discrete least squares problem using an approximation of \(\phi(x)\) around \(y\):","\phi(x)\approx\widetilde{\phi}_{y}(x):=\phi(y)+\langle\nabla\phi(y),x-y \rangle+\tfrac{L}{2}\norm{x-y}^{2},",where \(L>0\) is a sufficiently large constant,"M_{V}(y)\approx\arg\min_{x\in\mathbb{R}^{d}_{c},\,V(x)\subseteq V(y)}\left\{ \phi(y)+\nabla\phi(y)^{\top}(x-y)\right\}.",Approximation of the V step by solving a discrete least squares problem using a linearization of $\phi$ around $y$.
neurips_2024_oral_41,6,"nt. Subsequently, we perform the V step using the simpler convex quadratic function \(\widetilde{\phi}_{y}\) instead of the typically more complicated function \(\phi\):","M_{V,\phi}(y)\overset{\eqref{eq:v_def}}{\approx}M_{V,\widetilde{\phi}_{y}}(y )\overset{\eqref{eq:v_def}}{=}\arg\min_{x\in\mathbb{R}^{d}}\{\widetilde{ \phi}_{y}(x)\ :\ V(x)\subseteq V(y)\}.",,"\widetilde{M}_{V}(y):=\arg\min_{x\in\mathbb{R}^{d}}\left\{\widetilde{\phi}_{y}(x)\,:\,V(x)\subseteq V(y)\right\}.","This defines the linearized V step, which approximates the original discrete optimization problem using a convex quadratic surrogate function derived from the gradient and Hessian of $\phi$ at $y$."
neurips_2024_oral_41,7,"Our first lemma shows that we can replace \(\widetilde{\phi}_{y}\) by a more convenient function \(\widehat{\phi}_{y}\) measuring the squared distance between \(x\) and \(y^{+}:=y-\frac{1}{L}\nabla\phi(y)\), the latter being the point obtained after taking a single GD step from \(y\) with learning rate \(\frac{1}{L}\), disregarding the constraint:

**Lemma 3.2**.: _For any \(y\in\mathbb{R}^{d}_{\leq c}\) we have \(M_{V,\widetilde{\phi}_{y}}(y)=M_{V,\widehat{\phi}_{y}}(y),\) where_",\widehat{\phi}_{y}(x):=\norm{x-(y-\tfrac{1}{L}\nabla\phi(y))}^{2}= \norm{x-y^{+}}^{2}=\sum\limits_{i=1}^{d}(x_{i}-y_{i}^{+})^{2}.,,\widehat{\phi}_{y}(x):=\tfrac{L}{2}\norm{x - \left(y - \tfrac{1}{L}\nabla\phi(y)\right)}^{2}.,defines a simplified quadratic surrogate function based on the gradient descent update
neurips_2024_oral_41,8,"The proof can be found in Appendix A.2. To summarize, the V step of the PV method (Algorithm 1), i.e., \(x=M_{V,\phi}(y),\) can be approximated via the ""linearized V step""","x:=M_{V,\phi}(y)\approx M_{V,\widehat{\phi}_{y}}(y):=\hat{x}.",,"x=M_{V,\phi}(y)\approx M_{V,\widehat{\phi}_{y}}(y):=\arg\min_{x\in\mathbb{R}^{d}}\left\{\sum\limits_{i=1}^{d}(x_{i}-y_{i}^{+})^{2}\ :\ V(x)\subseteq V(y)\right\},","where $y^{+} = y - \frac{1}{L}\nabla\phi(y)$ is the gradient descent update of $y$ with learning rate $\frac{1}{L}$, and the minimization enforces that the values in $x$ are a subset of those in $y$."
neurips_2024_oral_41,9,"Our next lemma says that the above approximation is in a certain sense natural reasonable provided that \(\phi\) is \(L\)-smooth5 on \(\mathbb{R}^{d}_{\leq c}\), i.e., provided that

Footnote 5: It is possible to consider different class of functions instead; e.g., Lipschitz functions. In such a case, we would us a different approximation. For simplicity of exposition, we work with \(L\)-smooth functions.","\phi(x)\leq\phi(y)+\langle\nabla\phi(y),x-y\rangle+\tfrac{L}{2}\norm{x-y}^{2},\qquad\forall x,y\in\mathbb{R}^{d}_{\leq c}.",,"\|\nabla\phi(x)-\nabla\phi(y)\|\leq L\|x-y\|\quad\forall x,y\in\mathbb{R}^{d}_{\leq c}.","Equation 9 defines the $L$-smoothness condition on the gradient of $\phi$ over the set $\mathbb{R}^{d}_{\leq c}$, ensuring that the linearized V step approximation is well-justified and stable under small perturbations."
neurips_2024_oral_41,10,"**Lemma 3.3** (Monotonicity).: _Let \(y\in\mathbb{R}^{d}_{\leq c}\). If \(\phi\) is \(L\)-smooth on \(\mathbb{R}^{d}_{\leq c}\), then \(\phi\left(M_{V,\phi}(y)\right)\leq\phi(\hat{x})\leq\phi(y)\), where \(\hat{x}\) is the point obtained from \(y\) by the linearized V step (6)._

Indeed, the point \(\hat{x}\) obtained via the linearized V step can not have a worse loss than the previous point \(y\). Of course, one hopes that the loss will strictly decrease so that the method makes progress. From a practical perspective, the key advantage of linearized V step is that it can be performed much faster compared to the vanilla V step. The proof of Lemma 3.3 can be found in Appendix A.3.

Note that since \(\widehat{\phi}_{y}(x)\) is separable (see (8)), each entry/weight of \(x\) can be optimized independently of others. For scalar quantization, each individual problem can be solved in \(\mathcal{O}(\log_{2}(c))\) time using binary search in sorted version of \(V(y)\). For vector quantization, there are specialized optimization procedures for efficiently minimizing the \(L_{2}\) error (see Appendix D)

**Key challenge.** The main caveat with linearized V step is that it may be impossible to make small gradient-based updates to low-bitwidth discrete weights. More specifically, in (6), one must update the discrete assignments to approximate \(y^{k}-\frac{1}{L}\nabla\phi(y^{k})\). However, for low-bit weights, the desired update \(\frac{1}{L}\nabla\phi(y^{k})\) can be smaller than the lowest possible increment to obtain a quantized vector. As a result, the optimal solution to (6) is often \(y^{k}\) itself. In such a situation, the algorithm will get stuck on \(y^{k}\), which is undesirable. This problem is especially pronounced in deep LLMs, where \(L\) can be very large, or, from a practitioner's point of view, where one needs a small learning rate. In practice, as we explore in Section 4.2, the lowest learning rate where the algorithm makes _any_ updates at all is already too large for optimization, leading to divergence.

```
0: initial parameters \(x^{0}\in\mathbb{R}^{d}_{c}\), objective function \(\phi:\mathbb{R}^{d}\rightarrow\mathbb{R}\), subspace size \(\tau\in[d]\)
1:for\(k=0,\ldots,K-1\)do
2:\(\triangleright\)P step: update \(V(x)\) by backprop
3:\(y^{k}=\operatorname*{arg\,min}_{y\in\mathbb{R}^{d}_{c}}\{\phi(y)\ :\ P(y)\supseteq P(x^{k})\}\)
4:\(\triangleright\)V step: choose a subspace \(\mathcal{S}^{k}\) & update \(P(x)\)
5:\(\mathcal{S}^{k}=\operatorname*{arg\,top}_{1\leq i\leq d}\ \left|\nabla_{i}\phi(y^{k})\right|\ \rhd\) find \(\tau\) largest
6:\(\widehat{\phi}_{y,s^{k}}(x):=\left\|x-\left(y-\frac{1}{x_{\mathcal{S}^{k}}}Z^ {k}\left(\nabla\phi(y)\right)\right)\right\|^{2}\)
7:\(x^{k+\underline{\bot}}\operatorname*{arg\,min}_{x}\left\{\widehat{\phi}_{y^{k} },s^{k}(x):\!V(x)\!\subseteq\!V(y^{k})\right\}\)
8:endfor
```

**Algorithm 2** PV-Tuning: Optimization

Many popular strategies for discrete fine-tuning can be seen as attempts to reconcile coarse low-precision weights with the need to make small updates. These include straight-through estimation, stochastic rounding, or adding regularizers that push the solution to (6) away from \(y^{k}\). We review straight-through estimation in Appendix E.1 and stochastic rounding in Appendix E.2.

### Linearized subspace V step

Here we ask the following question: **Can we modify the PV method so as to force the V step to make a larger update?** In other words, we need an optimization algorithm that updates quantized weights either by a sufficiently large increment, or not at all.

A natural example of such an algorithm is coordinate descent (CD) [43; 58], or more generally, subspace descent [26; 38]. Instead of updating all parameters by a small margin, CD in each iteration chooses a single parameter, and makes a large update instead. This strategy can be generalized to updating more parameters at the same time, which leads to subspace descent methods.6 The parameters to be updated can be chosen either greedily, (e.g., several \(i\in[d]\) with the largest magnitude of the partial derivative \(|\nabla_{i}\phi(\cdot)|\)), or at random, or through a variety of other means.

Footnote 6: Very closely related methods include block coordinate descent and compressed gradient descent with sparsification operators such as RandK or TopK [4; 7].

Let \(\mathcal{S}^{k}\subset[d]\) be the set of parameters/weights/coordinates we wish to update at iteration \(k\). We choose \(|\mathcal{S}^{k}|=\tau\ll d\). Let \(Z^{k}:\mathbb{R}^{d}\rightarrow\mathbb{R}^{d}\) be the linear mapping defined as follows: \(\left(Z^{k}(x)\right)_{i}=x_{i}\) if \(i\in\mathcal{S}^{k}\) and \(\left(Z^{k}(x)\right)_{i}=0\) if \(i\notin\mathcal{S}^{k}\). We now formulate the linearized _subspace_ V step:","x^{+}:=M_{V,\widehat{\phi}_{y,\mathcal{S}^{k}}}(y):=\operatorname*{arg\,min} _{x\in\mathbb{R}^{d}}\{\widehat{\phi}_{y,\mathcal{S}^{k}}(x)\ :\ V(x)\subseteq V(y)\}, || \text{where}\quad\widehat{\phi}_{y,\mathcal{S}^{k}}(x):=\|x-(y-\frac {1}{L_{\mathcal{S}^{k}}}Z^{k}(\nabla\phi(y)))\|^{2},",,,
neurips_2024_oral_42,1,"### Introspecting based on the Imaginary Dataset

The direct output LLM in the rehearsing stage can be sub-optimal or incorrect. We would like to re-examine the collected data and try to distill a policy that can avoid the side effects. We can regard the data \(\mathcal{D}_{\mathrm{img}}\) collected during the rehearsing stage as an offline dataset and apply offline RL algorithms to train an improved policy from the dataset. However, directly applying existing offline RL algorithms over-simplifies the problem. Compared with the standard offline setting where only the behavior policy is sub-optimal, there is an additional misalignment in the data generatedduring the rehearsal: the transition and the reward function estimated by the LLM are also inaccurate. Overlooking such inaccuracy would result in a policy exploiting the sub-optimal transition and reward function and cause performance degradation or even risky behaviors in the final deployment.

To solve the problem, in this paper, we adopt the Conservative Q-learning [5] as the base offline RL algorithm, whose learning objective is as follows:","\min_{Q}\max_{\pi} \alpha(\mathbb{E}_{\hat{s}\sim\mathcal{D}_{\mathrm{img}}, \alpha\sim\pi(a|\hat{s})}[Q(\hat{s},a)]-\mathbb{E}_{\hat{s},\hat{a} \sim\mathcal{D}_{\mathrm{img}}}[Q(\hat{s},\hat{a})]+\mathcal{R}(\pi)) || +\mathbb{E}_{\hat{s},\hat{a}\sim\mathcal{D}_{\mathrm{img}}}[(Q( \hat{s},\hat{a})-\hat{\mathcal{B}}^{\pi}\hat{Q}(\hat{s},\hat{a}))^{2}],",,,
neurips_2024_oral_42,2,"5]. As a solution of introspecting from imaginary data, as shown in Fig. 2(C), we add the uncertainty of the reward and transition estimation as the regularization terms, \(\mathcal{R}_{R}\) and \(\mathcal{R}_{T}\) over the original reward \(\hat{r}\) output by the LLM \(\mathbf{M}_{R}\). In practice, we adopt these regularization terms by applying them when we backup the \(\hat{Q}^{k}\):","\hat{\mathcal{B}}_{\mathrm{T}}^{\pi}\hat{Q}(\hat{s},\hat{a}):=\hat{r}-\eta_{R }\mathcal{R}_{R}(\hat{s},\hat{a})-\eta_{T}\mathcal{R}_{T}(\hat{s},\hat{a})+ \gamma\mathbb{E}_{\hat{s}^{\prime}\sim\mathcal{D}_{\mathrm{img}},a^{\prime} \sim\pi_{k}(a^{\prime}|\hat{s}^{\prime})}[Q(\hat{s}^{\prime},a^{\prime})],","where \(\hat{s}^{\prime}\sim\mathcal{D}_{\mathrm{img}}\) is to sample the next state given \(\hat{s},\hat{a}\), \(\eta_{R}\) and \(\eta_{T}\) are two hyper-parameters to control the weighting of the uncertainty terms","\hat{Q}^{k}(\hat{s},\hat{a}) = \hat{r}(\hat{s},\hat{a}) + \gamma\mathbb{E}_{\hat{s}'\sim\mathbf{M}_{T}}[\hat{V}^{k}(\hat{s}')] - \beta_{R}\mathcal{R}_{R}(\hat{s},\hat{a}) - \beta_{T}\mathcal{R}_{T}(\hat{s},\hat{a}),","The updated Q-value estimate incorporates reward and transition uncertainty regularization terms scaled by coefficients $\beta_{R}$ and $\beta_{T}$, respectively."
neurips_2024_oral_48,1,"## 3 Notation and Preliminaries

**Notation.** We denote vectors, matrices, and tensors with bold-face letters, e.g., \(\mathbf{x},\mathbf{X},\mathbf{X}\), respectively and sets with calligraphic letters \(\mathcal{X}\). A normal font notation will be used for miscellaneous purposes (mostly indices and functions). Datapoint (input) functions/signals will be denoted with \(u\), while higher-order functions (functions of functions) will be denoted with fraktur font \(\mathfrak{F}\).

**Functions of Neural Networks.** Consider functions of the form \(u_{G,\boldsymbol{\theta}}:\mathcal{X}\rightarrow\hat{\mathcal{X}}\). Each function is parameterised (1) by a **computational graph**\(G\), which determines all the mathematical operationsthat should be performed to evaluate the function \(u\) at a datapoint \(\mathbf{x}\in\mathcal{X}\). When \(u\) is a neural network, \(G\) is determined by the NN _architecture_. Additionally, \(u\) is parameterised by (2), by **a tuple of numerical parameters \(\boldsymbol{\theta}\)**, on which the aforementioned mathematical operations are applied (along with the input \(\mathbf{x}\)) - these are the _learnable parameters_, which can be arranged into a vector. We are interested in learning unknown higher-order functions \(\mathfrak{F}:\hat{\mathcal{X}}^{\mathcal{X}}\to\mathcal{Y}\) of the form \(\mathfrak{F}\left(u_{G,\boldsymbol{\theta}}\right)\). In our case, the goal is to _learn \(\mathfrak{F}\) by accessing solely the parameters \((G,\boldsymbol{\theta})\) of each \(u\)_, i.e. via functions \(\hat{f}:\mathcal{G}\times\Theta\to\mathcal{Y}\) of the form \(\hat{f}(G,\boldsymbol{\theta})\), where \(\mathcal{G}\) is a space of architectures/computational graphs and \(\Theta\) a space of parameters.3 We are typically interested in learning _functionals_ (\(\mathcal{Y}\subseteq\mathbb{R}^{d}\)) or _operators_ (\(\mathcal{Y}=\mathcal{G}\times\Theta\) or \(\mathcal{Y}=\Theta\)). To approximate the desired higher-order function, we assume access to a dataset of parameters sampled i.i.d. from an unknown distribution \(\boldsymbol{\rho}\) on \(\mathcal{G}\times\Theta\). For example, in a supervised setup, we aim to optimise the following: \(\operatorname*{argmin}_{f\in\mathcal{F}}\mathbb{E}_{(G,\boldsymbol{\theta}) \sim\boldsymbol{\rho}}L\Big{(}\mathfrak{F}\!\left(u_{G,\boldsymbol{\theta}} \right),\hat{f}(G,\boldsymbol{\theta})\Big{)}\), where \(L(\cdot,\cdot)\) is a loss function and \(\mathcal{F}\) is an NN processing hypothesis class (e.g. metanetworks).

Footnote 3: Note the difference with architectures that access \(u\) via input-output pairs \((\mathbf{x}_{i},u_{G,\boldsymbol{\theta}}\left(\mathbf{x}_{i}\right))\)[46, 34].

**Feedforward Neural Networks (FFNNs).** In this paper, we focus our analysis on Feedforward Neural Networks (FFNNs), i.e. linear layers interleaved with non-linearities. Consider NN of the form \(u_{G,\boldsymbol{\theta}}:\mathbb{R}^{d_{\boldsymbol{\theta}}}\to\mathbb{R}^{d _{\boldsymbol{\theta}}}\) of the following form:","\mathbf{x}_{0}=\mathbf{x},\quad\mathbf{x}_{\ell}=\sigma_{\ell}(\mathbf{W }_{\ell}\mathbf{x}_{\ell-1}+\mathbf{b}_{\ell}),\quad u_{G,\boldsymbol{ \theta}}(\mathbf{x})=\mathbf{x}_{L}","where \(L\): the number of layers, \(\mathbf{W}_{i}\in\mathbb{R}^{d_{\ell}\times d_{\ell-1}}\): the weights of the NN, \(\mathbf{b}_{i}\in\mathbb{R}^{d_{\ell}}\): the biases of the NN, \(d_{0}=d_{\text{in}}\), \(d_{L}=d_{\text{out}}\), \(\sigma_{\ell}:\mathbb{R}\to\mathbb{R}\) activation functions applied element-wise","u_{G,\boldsymbol{\theta}}(\mathbf{x}) = \mathbf{W}_L \sigma(\mathbf{W}_{L-1} \sigma( \cdots \sigma(\mathbf{W}_1 \mathbf{x} + \mathbf{b}_1) \cdots ) + \mathbf{b}_{L-1}) + \mathbf{b}_L,","Feedforward Neural Network (FFNN) expressed as a composition of affine transformations and non-linear activation functions, where $\sigma$ denotes the non-linearity, $\mathbf{W}_l$ and $\mathbf{b}_l$ are the weight matrices and bias vectors at layer $l$, and $L$ is the total number of layers."
neurips_2024_oral_48,2,"se. Here, the learnable parameters are \(\boldsymbol{\theta}=(\mathbf{W}_{1},\ldots,\mathbf{W}_{L},\mathbf{b}_{1}, \ldots,\mathbf{b}_{L})\) and the computational graph encodes the connections between vertices, _but also the type of activations used in each layer_.

**Neural Network symmetries.** One of the major difficulties with working with function parameters directly is that the same function can be represented with more than one parameter, i.e. there exists transformations that _when applied to any parameter \((G,\boldsymbol{\theta})\), keep the represented function intact_. Formally, an NN symmetry is induced by a set \(\Psi\) of transformations \(\psi:\mathcal{G}\times\Theta\to\mathcal{G}\times\Theta\), such that \(u_{G,\boldsymbol{\theta}}(\mathbf{x})=u_{\psi(G,\boldsymbol{\theta})}(\mathbf{ x}),\forall\mathbf{x}\in\mathcal{X},\forall(G,\boldsymbol{\theta})\in\mathcal{G} \times\Theta\). If for a pair parameters \((G,\boldsymbol{\theta})\), \((G^{\prime},\boldsymbol{\theta}^{\prime})\), \(\exists\psi\) such that \((G,\boldsymbol{\theta})=\psi(G^{\prime},\boldsymbol{\theta}^{\prime})\), we will call the two parameters _equivalent_ and write \((G,\boldsymbol{\theta})\simeq(G^{\prime},\boldsymbol{\theta}^{\prime})\). To appropriately represent a _functional_\(\mathfrak{F}\), a hypothesis (metanetwork) \(\hat{f}\) should be _invariant_ to transformations in \(\Psi\): \(\hat{f}\left(\psi\left(G,\boldsymbol{\theta}\right)\right)=\hat{f}(G, \boldsymbol{\theta})\). For _operators_, \(\hat{f}\) should be _equivariant_ to transformations: \(f\left(\psi\left(G,\boldsymbol{\theta}\right)\right)=\psi\left(f(G,\boldsymbol{ \theta})\right)\), such that identical functions map to identical functions.

**Permutation symmetries (connectionist structure).** For a fixed computational graph \(G\), perhaps the most well-known symmetry of FFNNs are those induced by hidden neuron permutations [28]. _As far as metanetworks are concerned it is to date the only NN symmetry that has been accounted for_ - see Section 2. This symmetry implies that permuting hidden neurons (along with their biases and incoming and outgoing weights) within each layer preserves the NN function (regardless of the activation function). This reads:","\mathbf{W}_{\ell}^{\prime}=\mathbf{P}_{\ell}\mathbf{W}_{\ell}\mathbf{P}_{ \ell-1}^{-1},\,\mathbf{b}_{\ell}^{\prime}=\mathbf{P}_{\ell}\mathbf{b}_{ \ell}\Longrightarrow(\mathbf{W}_{\ell}^{\prime},\mathbf{b}_{\ell}^{\prime})_ {\ell=1}^{L}=\boldsymbol{\theta}^{\prime}\simeq\boldsymbol{\theta}=(\mathbf{W}_ {\ell},\mathbf{b}_{\ell})_{\ell=1}^{L},","where \(\ell\in\{1,\ldots,L\}\), \(\mathbf{P}_{0}=\mathbf{P}_{L}=\mathbf{I}\) and \(\mathbf{P}_{\ell}\in\mathbb{R}^{d_{\ell}\times d_{\ell}}\) are arbitrary permutation matrices","\forall\boldsymbol{\pi}=\left(\pi_{1},\ldots,\pi_{L-1}\right)\in\prod_{\ell=1}^{L-1}\mathfrak{S}_{d_{\ell}},\quad u_{G,\boldsymbol{\theta}}\left(\mathbf{x}\right)=u_{G,\boldsymbol{\theta}^{\boldsymbol{\pi}}}\left(\mathbf{x}\right),","where $\boldsymbol{\pi}$ denotes a tuple of permutations applied to hidden neurons in each layer, and $\boldsymbol{\theta}^{\boldsymbol{\pi}}$ represents the permuted parameters preserving the network function."
neurips_2024_oral_48,3,"es. Observe that they are different for each layer, with the input and output neurons held fixed.

**Graph Metanetworks (GMNs).** A recently introduced weight space architecture [33, 44], takes advantage of the permutation symmetries and treats FFNNs (among others, e.g. CNNs) as graphs, processing them with conventional GNNs. In particular, let \(G=(\mathcal{V},\mathcal{E})\) be the computational graph, \(i\in\mathcal{V}\) an arbitrary vertex in the graph (neuron) and \((i,j)\in\mathcal{E}\) an arbitrary edge from vertex \(j\) to vertex \(i\).4 Additionally, let \(\mathbf{x}_{V}\in\mathbb{R}^{|\mathcal{V}|\times d_{\ell}}\) be the vertex features and \(\mathbf{x}_{E}\in\mathbb{R}^{|\mathcal{E}|\times d_{\ell}}\) the edge features (i.e. biases and weights resp. in a FFNN). The general form of a \(T\) iteration (layer) GMN reads:

Footnote 4: We use this convention to align with the indexing of the weights \(\mathbf{W}(i,j)\).","\mathbf{h}_{V}^{0}(i)=\mathrm{INIT}_{V}(\mathbf{x}_{V}(i )),\quad\mathbf{h}_{E}^{0}(i,j)=\mathrm{INIT}_{E}(\mathbf{x}_{ E}(i,j))",,,
neurips_2024_oral_48,4,(Init),"\mathbf{m}_{V}^{t}(i)=\bigoplus_{j\in\mathcal{N}(i)}\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!where \(\mathbf{h}_{V}^{i},\mathbf{h}_{E}^{i}\) are vertex and edge representations at iteration \(t\) and \(\mathbf{h}_{G}\) is the overall graph (NN) representation. INIT, MSG, UPD are general function approximators (e.g. MLPs), while READ is a permutation invariant aggregator (e.g. DeepSets [81]). The above equations have appeared with several variations in the literature, e.g. in some cases the edge representations are not updated or the readout input involve edge representations as well. Another frequent strategy is to use _positional encodings_\(\mathbf{p}_{V},\mathbf{p}_{E}\) to break undesired symmetries. In FFNNs, Eq. (2) reveals that input and output vertices are not permutable, while vertices cannot be permuted across layers. Therefore, vertices (or edges) that are permutable share the same positional encoding (see Appendix A.1.2 for more details).

**Remark:** Although, typically, the neighbourhood \(\mathcal{N}(i)\) contains both incoming and outgoing edges, in Section 5 we will illustrate our method using only incoming edges: _forward neighbourhood_\(\mathcal{N}_{\text{FW}}(i)=\{j\in\mathcal{V}\mid\text{layer}\,(i)-\text{ layer}\,(j)=1\}\) and _backward_ where layer \((i)\) gives the layer neuron \(i\) belongs. Backward neighbourhoods \(\mathcal{N}_{\text{BW}}(i)\) are defined defined similarly. In Appendix A.2, we show a more elaborate _bidirectional version_ of our method, with both neighbourhoods considered.

## 4 Scaling symmetries in Feedforward Neural Networks

**Scaling symmetries (activation functions).** Intuitively, permutation symmetries stem from the _graph structure_ of neural networks, or put differently, from the fact that hidden neurons do not possess any inherent ordering. Apart from the affine layers \(\mathbf{W}_{\ell}\) that give rise to the graph structure, it is frequently the case that **activation functions**\(\sigma_{\ell}\) have inherent symmetries that are bestowed to the NN.

Let us dive into certain illustrative examples: for the ReLU activation \(\sigma(x)=\max(x,0)\) it holds that \(\sigma(ax)=\max(ax,0)=a\max(x,0),\ \forall a>0\). For the tanh and sine activations \(\sigma(x)=\tanh(x)\), \(\sigma(x)=\sin(x)\) respectively, it holds that \(\sigma(ax)=a\sigma(x),\ \forall a\in\{-1,1\}\). In a slightly more complex example, polynomial activations \(\sigma(x)=x^{k}\), we have \(\sigma(ax)=a^{d}\sigma(x)\), i.e. the multiplier differs between input and output. In general, we will be talking about _scaling symmetries_ whenever there exist pairs \((a,b)\) for which it holds that \(\sigma(ax)=b\sigma(x)\). To see how such properties affect NN symmetries, let us focus on FFNNs (see Appendix A.3 for CNNs): for a neuron \(i\) (we omit layer subscripts) we have \(\sigma\big{(}a\mathbf{W}(i,:)\mathbf{x}+a\mathbf{b}(i)\big{)}=\textit{b} \big{(}\mathbf{W}(i,:)\mathbf{x}+\mathbf{b}(i)\big{)}\), i.e. _multiplying its bias and all incoming weights with a constant a results in scaling its output with a corresponding constant \(b\)_. Generalising this to linear transformations, we may ask the following: which are the pairs of matrices \((\mathbf{A},\mathbf{B})\) for which we have \(\sigma\big{(}\mathbf{A}\mathbf{W}\mathbf{x}+\mathbf{A}\mathbf{b}\big{)}= \mathbf{B}\sigma\big{(}\mathbf{W}\mathbf{x}+\mathbf{b}\big{)}\)? Godfrey et al. [25] provide an answer for _any activation that respects certain conditions_. We restate here their most important results:

**Proposition 4.1** (Lemma 3.1. and Theorem E.14 from [25]).: _Consider an activation function \(\sigma:\mathbb{R}\to\mathbb{R}\). Under mild conditions,5 the following hold:_

Footnote 5: See Appendix A.7.1 for the precise statement and more details about \(\phi_{\sigma,d}\).

* _For any_ \(d\in\mathbb{N}^{+}\)_, there exists a (non-empty) group of invertible matrices defined as:_ \(I_{\sigma,d}=\{\mathbf{A}\in\mathbb{R}^{d\times d}:\text{ invertible}\mid\exists\ \mathbf{B}\in\mathbb{R}^{d\times d}\text{ invertible}\text{, such that: }\sigma(\mathbf{A}\mathbf{x})=\mathbf{B}\sigma(\mathbf{x})\}\) _(_intertwiner group_)_, and a mapping function_ \(\phi_{\sigma,d}\) _such that_ \(\mathbf{B}=\phi_{\sigma,d}(\mathbf{A})\)_._
* _Every_ \(\mathbf{A}\in I_{\sigma,d}\) _is of the form_ \(\mathbf{PQ}\)_, where_ \(\mathbf{P}\)_: permutation matrix and_ \(\mathbf{Q}=\textit{diag}\big{(}q_{1},\ldots q_{d}\big{)}\) _diagonal, with_ \(q_{i}\in D_{\sigma}=\{a\in\mathbb{R}\setminus\{0\}\mid\sigma(ax)=\phi_{\sigma,1}(a)\sigma(x)\}\)_: the 1-dimensional group, and_ \(\phi_{\sigma,d}(\mathbf{A})=\mathbf{P}\textit{diag}\big{(}\phi_{\sigma,1}(q_{1} ),\ldots\phi_{\sigma,1}(q_{d})\big{)}\)_._

This is a powerful result that completely answers the question above for most practical activation functions. Importantly, not only does it recover permutation symmetries, but also reveals symmetries to diagonal matrix groups, which can be identified by solely examining \(\phi_{\sigma,1}\), i.e. the one-dimensional case and the set \(D_{\sigma}\) (easily proved to be a group) we have already discussed in our examples above.

Using this statement, Godfrey et al. [25] characterised various activation functions (or recovered existing results), e.g. ReLU: \(I_{\sigma,d}\) contains **generalised permutation matrices with positive entries** of the form \(\mathbf{PQ}\), \(\mathbf{Q}=\text{diag}(q_{1},\ldots,q_{d})\), \(q_{i}>0\) and \(\phi_{\sigma,d}(\mathbf{PQ})=\mathbf{PQ}\)[56]. Additionally, here we characterise the intertwiner group of sine (used in the popular SIREN architecture [70] for INRs). Not surprisingly, it has the same intertwiner group with tanh [11, 21] (we also recover this here using Proposition 4.1). Formally, (proof in Appendix A.7.1):

**Corollary 4.2**.: _Hyperbolic tangent \(\sigma(x)=\tanh(x)\) and sine activation \(\sigma(x)=\sin(\omega x)\), satisfy the conditions of Proposition 4.1, when (for the latter) \(\omega\neq k\pi,k\in\mathbb{Z}\). Additionally, \(I_{\sigma,d}\) contains **signed permutation matrices** of the form \(\mathbf{PQ}\), with \(\mathbf{Q}=\text{diag}(q_{1},\ldots,q_{d})\), \(q_{i}=\pm 1\) and \(\phi_{\sigma_{d}}(\mathbf{PQ})=\mathbf{PQ}\)._It is straightforward to see that the symmetries of Proposition 4.1, induce equivalent parameterisations for FNNs. In particular, it follows directly from Proposition 3.4. in [25], that for activation functions \(\sigma_{\ell}\) satisfying the conditions of Proposition 4.1 and when \(\phi_{\sigma,\ell}(\mathbf{Q})=\mathbf{Q}\), we have that:

\[\mathbf{W}^{\prime}_{\ell}=\mathbf{P}_{\ell}\mathbf{Q}_{\ell}\mathbf{W}_{\ell} \mathbf{Q}_{\ell-1}^{-1}\mathbf{P}_{\ell-1}^{-1},\;\mathbf{b}^{\prime}_{\ell}= \mathbf{P}_{\ell}\mathbf{Q}_{\ell}\mathbf{b}_{\ell}\implies(\mathbf{W}^{\prime }_{\ell},\mathbf{b}^{\prime}_{\ell})_{\ell=1}^{L}=\boldsymbol{\theta}^{\prime} \simeq\boldsymbol{\theta}=(\mathbf{W}_{\ell},\mathbf{b}_{\ell})_{\ell=1}^{L},","where again \(\ell\in\{1,\ldots,L\}\), \(\mathbf{P}_{0}=\mathbf{Q}_{0}=\mathbf{P}_{L}=\mathbf{Q}_{L}=\mathbf{I}\)","\mathbf{h}_{V}^{t+1}(i) = \mathrm{UPDATE}_{V}\left(\mathbf{h}_{V}^{t}(i), \bigoplus_{j:(i,j)\in\mathcal{E}}\mathbf{h}_{E}^{t}(i,j)\right),\quad \mathbf{h}_{E}^{t+1}(i,j) = \mathrm{UPDATE}_{E}\left(\mathbf{h}_{E}^{t}(i,j), \mathbf{h}_{V}^{t}(i), \mathbf{h}_{V}^{t}(j)\right)","where $t = 0, \dots, T-1$, and $\bigoplus$ denotes a permutation-invariant aggregation (e.g., sum, mean, or max) over incoming edge features"
neurips_2024_oral_48,5,"\).

## 5 Scale Equivariant Graph MetaNetworks

As previously mentioned, the metanetworks operating on weight spaces that have been proposed so far, either do not take any symmetries into account or are invariant/equivariant to permutations alone as dictated by Eq. (2). In the following section, we introduce an architecture invariant/equivariant to **permutations and scalings**, adhering to Eq. (3). An important motivation for this is that in various setups, _these are the only function-preserving symmetries_, i.e. for a fixed graph \(u_{G,\boldsymbol{\theta}}=u_{G,\boldsymbol{\theta}^{\prime}}\Rightarrow( \boldsymbol{\theta},\boldsymbol{\theta}^{\prime})\) satisfy Eq. (3) - e.g. see [21] for the conditions for \(\mathsf{tanh}\) and [61, 26] for ReLU.

**Main idea.** Our framework is similar in spirit to most works on equivariant and invariant NNs [10]. In particular, we build equivariant GMNs that will preserve both symmetries at vertex- and edge-level, i.e. _vertex representations will have the same symmetries with the biases and edge representations with the weights._ To see this, suppose two parameter vectors are equivalent according to Eq. (3). Then, the _hidden_ neurons representations - the discussion on _input/output_ neurons is postponed until Appendix A.1.4 - should respect the following (the GMN iteration \(t\) is omitted to simplify notation):","\mathbf{h}^{\prime}_{V}(i) =q_{\ell}(\pi_{\ell}(i))\mathbf{h}_{V} (\pi_{\ell}(i)),\quad\ell=\text{layer}(i) \in\{1,\ldots,L-1\} || \mathbf{h}^{\prime}_{E}(i,j) =q_{\ell}(\pi_{\ell}(i))\mathbf{h}_{E} (\pi_{\ell}(i),\pi_{\ell-1}(j))q_{\ell-1}^{- 1}(\pi_{\ell-1}(j)),\;\ell=\text{layer}(i) \in\{2,\ldots,L-1\},",,"\mathbf{h}_{V}(i)\mapsto\mathbf{P}_{\ell}\mathbf{Q}_{\ell}\mathbf{h}_{V}(i ),\quad\mathbf{h}_{E}(i,j)\mapsto\mathbf{P}_{\ell}\mathbf{Q}_{\ell}\mathbf{h }_{E}(i,j)\mathbf{Q}_{\ell-1}^{-1}\mathbf{P}_{\ell-1}^{-1}","Transformation rules for vertex and edge representations under permutation and scaling symmetries, ensuring equivariance in the proposed Graph Metanetwork architecture."
neurips_2024_oral_48,6,"\). We will refer to the latter as _forward scaling_ in Eq. (4) and _bidirectonal scaling_ in Eq. (5). To approximate _operators_ (equivariance), we compose multiple equivariant GMN layers/iterations and in the end, project vertex/edge representations to the original NN weight space, while to approximate _functionals_ (invariance), we compose a final invariant one in the end summarising the input to a scalar/vector.

To ease exposition, we will first discuss our approach w.r.t. vertex representations. Assume that vertex representation symmetries are preserved by the initialisation of the MPNN - Eq. (Init) - and so are edge representation symmetries for all MPNN layers. Therefore, we can only focus on the message passing and vertex update steps - Eq. (Msg) and Eq. (Upd). Additionally, let us first focus on hidden neurons and assume only forward neighbourhoods. The following challenges arise:

**Challenge 1 - Scale Invariance / Equivariance.** First off, the message and the update function \(\mathrm{MSG}_{V},\mathrm{UPD}_{V}\) should be _equivariant to scaling_ - in this case to the forward scaling using the multiplier of the central vertex \(q_{\ell}(i)\). Additionally, the readout \(\mathtt{READ}\), apart from being permutation invariant should also be _invariant to the different scalar multipliers of each vertex_. Dealing with this requires devising functions of the following form:","g_{i}\big{(}q_{1}\mathbf{x}_{1},\ldots,q_{n}\mathbf{x}_{n}\big{)}=q_{i}g_{i} \mathbf{(x}_{1},\ldots,\mathbf{x}_{n}\big{)},\forall q_{i}\in D_{i},i\in\{1, \ldots,n\}",where \(D_{i}\) a 1-dimensional scaling group as defined in Proposition 4,,
neurips_2024_oral_48,7,"4.1. Common examples are those discussed in Section 4, e.g. \(D_{i}=\{1,-1\}\)**or**\(D_{i}=\mathbb{R}^{+}\). The first case, i.e. _sign symmetries_, has been discussed in recent work [43, 40]. Here we generalise their architecture into arbitrary scaling groups. In specific, _Scale Equivariant_ networks follow the methodology of [40], i.e. they are compositions of multiple linear transformations multiplied elementwise with the output of _Scale Invariant_ functions:","\mathsf{ScaleInv}^{k}(\mathbf{X})=\rho^{k}(\tilde{\mathbf{x} }_{1},\ldots,\tilde{\mathbf{x}}_{n}),",,"g_{i}\big{(}q_{1}\mathbf{x}_{1},\ldots,q_{n}\mathbf{x}_{n}\big{)}=q_{i}g_{i}\big{(}\mathbf{x}_{1},\ldots,\mathbf{x}_{n}\big{)},\quad\forall q_{j}\in D_{j},\,j=1,\ldots,n","where $D_{i}$ is a 1-dimensional scaling group associated with the $i$-th input, e.g., $\mathbb{R}^+$ for ReLU or $\{\pm1\}$ for tanh/sine"
neurips_2024_oral_48,8,(Scale Inv. Net),"\mathsf{ScaleEq}=\mathsf{f}^{K}\circ\cdots\circ\mathsf{f}^{1},\; \mathsf{f}^{k}(\mathbf{X})=\big{(}\boldsymbol{\Gamma}^{k}_{1}\mathbf{x}_{1}, \ldots,\boldsymbol{\Gamma}^{k}_{n}\mathbf{x}_{n}\big{)}\odot\mathsf{ScalelInv}^{k }(\mathbf{X}),",,"\mathsf{ScaleInv}^{k}(\mathbf{X})=\rho^{k}\left(\frac{\mathbf{x}_{1}}{\|\mathbf{x}_{1}\|},\ldots,\frac{\mathbf{x}_{n}}{\|\mathbf{x}_{n}\|}\right)","where $\rho^{k}$ is a scale-invariant function applied to normalised inputs $\tilde{\mathbf{x}}_{i}$, ensuring invariance under scaling transformations"
neurips_2024_oral_48,9,"(Scale Equiv. Net)

where \(\rho^{k}:\prod_{i=1}^{n}\mathcal{X}_{i}\rightarrow\mathbb{R}\Sigma_{i=1}^{n} \,d_{i}^{k}\) universal approximators (e.g. MLPs), \(\boldsymbol{\Gamma}^{k}_{i}:\mathcal{X}_{i}\rightarrow\mathbb{R}^{d_{i}^{k}}\) linear transforms (for each of the \(k\) invariant/equivariant layers resp - in practice, we observed experimentally that a single layer \(K=1\) was sufficient), and \(\tilde{\mathbf{x}}_{i}\) are explained in detail in the following paragraph.

Central to our method is defining a way to achieve invariance. One option is _canonicalisation_, i.e. by defining a function \(\mathsf{canon}:\mathcal{X}\rightarrow\mathcal{X}\)_that maps all equivalent vectors_\(\mathbf{x}\)_to a representative_ (obviouslynon-equivalent vectors have different representatives): If \(\mathbf{x}\simeq\mathbf{y}\in\mathcal{X}\), then \(\mathbf{x}\simeq\mathsf{canon}(\mathbf{x})=\mathsf{canon}(\mathbf{y})\). In certain cases, these are easy to define and _differentiable almost everywhere_, for example for positive scaling: \(\mathsf{canon}(\mathbf{x})=\frac{\mathbf{x}}{\|\mathbf{x}\|}\).6 For sign symmetries, this is not as straightforward: in 1-dimension one can use \(|x|\), but for arbitrary dimensions, a more complex procedure is required, as recently discussed in [49, 48]. Since the group is small - two elements - one can use _symmetrisation_ instead [79], as done by Lim et al. [41]: \(\mathsf{symm}(\mathbf{x})=\sum_{\mathbf{y}:\mathbf{y}\simeq\mathbf{x}} \mathrm{MLP}(\mathbf{y})\), i.e. for sign: \(\mathsf{symm}(\mathbf{x})=\mathrm{MLP}(\mathbf{x})+\mathrm{MLP}(-\mathbf{x})\). Therefore, we define: \(\mathsf{ScalelInv}^{k}(\mathbf{X})=\rho^{k}\left(\tilde{\mathbf{x}}_{1}, \ldots,\tilde{\mathbf{x}}_{n}\right)\), with \(\tilde{\mathbf{x}}_{i}=\mathsf{canon}(\mathbf{x}_{i})\) or \(\tilde{\mathbf{x}}_{i}=\mathsf{symm}(\mathbf{x}_{i})\). Importantly, it is known that both cases _allow for universality_, see [8, 29] and [79, 62] respectively.

Footnote 6: Fwd: \(\mathbf{x}\simeq\mathbf{y}\Rightarrow\mathbf{x}=q\mathbf{y}\Rightarrow\frac {\mathbf{y}}{\|\mathbf{y}\|}=\frac{q\mathbf{x}}{\|q\|\|\mathbf{x}\|}=\frac{ \mathbf{x}}{\|\mathbf{x}\|}\) (\(q>0\)). Reverse: \(\frac{\mathbf{x}}{\|\mathbf{x}\|}=\frac{\mathbf{y}}{\|\mathbf{y}\|}\Rightarrow \mathbf{x}=\frac{\|\mathbf{x}\|}{\|\mathbf{y}\|}\mathbf{y}\Rightarrow\mathbf{ x}\simeq\mathbf{y}\)

**Challenge 2 - Rescaling: Different multipliers.** Scale equivariance alone is not sufficient, since the input vectors of the message function \(\mathrm{MSG}_{V}\) are _scaled by different multipliers_ - central vertex: \(q_{\ell}(i)\), neighbour: \(q_{\ell-1}(j)\), edge: \(q_{\ell}(i)q_{\ell-1}^{-1}(j)\), _while its output should be scaled differently as well_ - \(q_{\ell}(i)\). We refer to this problem as _rescaling_. Dealing with Challenge 2 requires functions of the form:","g\big{(}q_{1}\mathbf{x}_{1},\ldots q_{n}\mathbf{x}_{n}\big{)}=g(\mathbf{x}_{1},\ldots\mathbf{x}_{n})\prod_{i=1}^{n}q_{i},\forall q_{i}\in D_{i}.",,"\mathsf{ScaleEq}=\mathsf{f}^{K}\circ\cdots\circ\mathsf{f}^{1},\; \mathsf{f}^{k}(\mathbf{X})=\big{(}\boldsymbol{\Gamma}^{k}_{1}\mathbf{x}_{1}, \ldots,\boldsymbol{\Gamma}^{k}_{n}\mathbf{x}_{n}\big{)}\odot\mathsf{ScaleInv}^{k }(\mathbf{X})","where $\rho^{k}:\prod_{i=1}^{n}\mathcal{X}_{i}\rightarrow\mathbb{R}^{\sum_{i=1}^{n} d_{i}^{k}}$ are universal approximators (e.g., MLPs), $\boldsymbol{\Gamma}^{k}_{i}:\mathcal{X}_{i}\rightarrow\mathbb{R}^{d_{i}^{k}}$ are linear transforms, and $\tilde{\mathbf{x}}_{i}$ are canonicalized/symmetrized inputs ensuring scale invariance"
neurips_2024_oral_48,10,"We call these functions _rescale equivariant_. Note, that this is an unusual symmetry in equivariant NN design. Our approach is based on the observation that _any n-other monomial containing variables from all vectors \(\mathbf{x}_{i}\) is rescale-equivariant_. Collecting all these monomials into a single representation is precisely the _outer product_\(\textbf{X}_{n}=\mathbf{x}_{1}\otimes\cdots\otimes\mathbf{x}_{n}\), where \(\textbf{X}_{n}(j_{1},\ldots,j_{n})=\prod_{i=1}^{n}\mathbf{x}_{i}(j_{i})\). Therefore, the general form of our proposed Rescale Equivariant Network is as follows:","\mathsf{ReScaleEq}(\mathbf{x}_{1},\ldots\mathbf{x}_{n})=\mathsf{ScaleEq} \big{(}\mathrm{vec}(\textbf{X}_{n})\big{)}.",,"g(\mathbf{x}_{1},\ldots,\mathbf{x}_{n})=\langle\textbf{W},\mathbf{x}_{1} \otimes\cdots\otimes\mathbf{x}_{n}\rangle=\sum_{j_{1},\ldots,j_{n}}\textbf{W }(j_{1},\ldots,j_{n})\prod_{i=1}^{n}\mathbf{x}_{i}(j_{i}),","where \(\textbf{W}\) is a learnable tensor and the inner product is taken over all indices, enabling rescale equivariance through the outer product structure."
neurips_2024_oral_48,11,"(ReScale Equiv. Net)

In practice, given that the size of \(\textbf{X}_{n}\) grows polynomially with \(n\), we resort to a more computationally friendly subcase, i.e. _hadamard products_, i.e. \(\mathsf{ReScaleEq}(\mathbf{x}_{1},\ldots\mathbf{x}_{n})=\odot_{i=1}^{n} \mathbf{\Gamma}_{i}\mathbf{x}_{i}\), \(\mathbf{\Gamma}_{i}:\mathcal{X}_{i}\rightarrow\mathbb{R}^{d}\). Contrary to the original formulation, the latter is linear (lack of multiplication with an invariant layer).

**Scale Equivariant Message Passing.** We are now ready to define our message-passing scheme. Starting with the message function, we require each message vector \(\mathbf{m}_{V}(i)\) to have the same symmetries as the central vertex \(i\). Given the scaling symmetries of the neighbour and the edge, for forward neighbourhoods, this reads: \(\mathrm{MSG}_{V}\left(q_{x}\mathbf{x},q_{y}\mathbf{y},q_{x}q_{y}^{-1}\mathbf{ e}\right)=q_{x}\mathrm{MSG}_{V}\left(\mathbf{x},\mathbf{y},\mathbf{e}\right)\). In this case, we opt to eliminate \(q_{y}\) by multiplication as follows:","\mathrm{MSG}_{V}(\mathbf{x},\mathbf{y},\mathbf{e})=\mathsf{ScaleEq} ([\mathbf{x},\mathsf{ReScaleEq}(\mathbf{y},\mathbf{e}) ]),","where \([\cdot,\cdot]\) denotes concatenation, \(\mathsf{ReScaleEq}(q_{y}\mathbf{y},q_{x}q_{y}^{-1}\mathbf{e})=q_{x}\mathsf{ ReScaleEq}(\mathbf{y},\mathbf{e})\)","\mathrm{MSG}_{V}(\mathbf{x},\mathbf{y},\mathbf{e})=\mathsf{ReScaleEq} \left(\mathbf{x},\mathbf{e}\odot\mathbf{y}\right)",Message function ensuring scale equivariance by combining vertex and edge representations through rescale-equivariant operations.
neurips_2024_oral_48,12,"\). In our experiments, we used only \(\mathbf{y}\) and \(\mathbf{e}\), since we did not observe significant performance gains by including the central vertex \(\mathbf{x}\). Now, the update function is straightforward to implement since it receives vectors with the same symmetry, i.e. it should hold that: \(\mathrm{UPD}_{V}(q_{x}\mathbf{x},q_{x}\mathbf{m})=q_{x}\mathrm{UPD}_{V}( \mathbf{x},\mathbf{m})\) which is straightforward to implement with a scale equivariant network, after concatenating \(\mathbf{x}\) and \(\mathbf{m}\):","\mathrm{UPD}_{V}(\mathbf{x},\mathbf{m})=\mathsf{ScaleEq}( [\mathbf{x},\mathbf{m}]).",,"\mathrm{UPD}_{V}(\mathbf{x},\mathbf{m})=\mathsf{ScaleEq}([\mathbf{x}, \mathbf{m}]).",Update function that ensures scale equivariance by applying a scale-equivariant network to the concatenation of the vertex representation and the aggregated message.
neurips_2024_oral_48,13,"Finally, to summarise our graph into a single scalar/vector we require a scale and permutation-invariant readout. The former is once more achieved using canonicalised/symmetrised versions of the vertex representations of hidden neurons, while the latter using a DeepSets architecture as usual:","\mathrm{READ}_{V}(\mathbf{X}):=\mathsf{DeepSets}(\tilde{ \mathbf{x}}_{1},\ldots,\tilde{\mathbf{x}}_{n}),\quad\tilde{\mathbf{x}}_{ i}=\mathsf{canon}_{i}(\mathbf{x}_{i})\text{ or }\tilde{\mathbf{x}}_{i}=\mathsf{symm}_{i}(\mathbf{x}_{i})",,"\mathtt{READ}(\{\mathbf{h}_{V}(i)\}_{i\in\mathcal{V}})=\rho\left( \sum_{i\in\mathcal{V}}\mathsf{canon}(\mathbf{h}_{V}(i))\right),","where $\rho$ is a universal approximator (e.g., an MLP) and $\mathsf{canon}$ is a canonicalisation function ensuring scale invariance."
neurips_2024_oral_54,1,"### Markov Decision Processes

In this work, we focus on an infinite-horizon Markov Decision Process (MDP), denoted by \(\mathcal{M}\), over a state space \(\mathcal{S}\) and an action space \(\mathcal{A}\) and with a discount factor \(\gamma\in(0,1)\). Both the state and action spaces are assumed to be finite sets. In an MDP, the state \(s\) evolves dynamically under the influence of actions based on a probability transition kernel, \(P:(\mathcal{S}\times\mathcal{A})\times\mathcal{S}\to[0,1]\). The entry \(P(s^{\prime}|s,a)\) denotes the probability of moving to state \(s^{\prime}\) when an action \(a\) is taken in the state \(s\). An MDP is also associated with a deterministic reward function \(r:\mathcal{S}\times\mathcal{A}\to[0,1]\), where \(r(s,a)\) denotes the immediate reward obtained for taking the action \(a\) in the state \(s\). Thus, the transition kernel \(P\) along with the reward function \(r\) completely characterize an MDP. In this work, we consider the synchronous setting, where each agent has access to an independent generative model or simulator from which they can draw independent samples from the unknown underlying distribution \(P(\cdot|s,a)\) for each state-action pair \((s,a)\)(Kearns and Singh, 1998).

A policy \(\pi:\mathcal{S}\to\Delta(\mathcal{A})\) is a rule for selecting actions across different states, where \(\Delta(\mathcal{A})\) denotes the simplex over \(\mathcal{A}\) and \(\pi(a|s)\) denotes the probability of choosing action \(a\) in a state \(s\). Each policy \(\pi\) is associated with a state value function and a state-action value function, or the Q-function, denoted by \(V^{\pi}\) and \(Q^{\pi}\) respectively. \(V^{\pi}\) and \(Q^{\pi}\) measure the expected discounted cumulative reward attained by \(\pi\) starting from a particular state \(s\) and state-action pair \((s,a)\) respectively. Mathematically, \(V^{\pi}\) and \(Q^{\pi}\) are given as","V^{\pi}(s):=\mathbb{E}[\sum_{t=0}^{\infty}\gamma^{t}r(s_{t},a_{t})| s_{0}=s];\quad Q^{\pi}(s,a):=\mathbb{E}[\sum_{t=0}^{\infty}\gamma^{t}r (s_{t},a_{t})|s_{0}=s,a_{0}=a],","where \(a_{t}\sim\pi(\cdot|s_{t})\) and \(s_{t+1}\sim P(\,\cdot\,|s_{t},a_{t})\) for all \(t\geq 0\)",,
neurips_2024_oral_54,2,"\). The expectation is taken w.r.t. the randomness in the trajectory \(\{s_{t},a_{t}\}_{t=1}^{\infty}\). Since the rewards lie in \([0,1]\), it follows immediately that both the value function and Q-function lie in the range \([0,\frac{1}{1-\gamma}]\).

An optimal policy \(\pi^{*}\) is a policy that maximizes the value function uniformly over all the states and it has been shown that such an optimal policy \(\pi^{*}\) always exists (Puterman, 2014). The optimal value and Q-functions are those corresponding to that of an optimal policy \(\pi^{*}\) are denoted as \(V^{\star}:=V^{\pi^{*}}\) and \(Q^{\star}:=Q^{\pi^{*}}\) respectively. The optimal Q-function, \(Q^{\star}\), is also the unique fixed point of the Bellman operator \(\mathcal{T}:\mathcal{S}\times\mathcal{A}\to\mathcal{S}\times\mathcal{A}\), given by","(\mathcal{T}Q)(s,a)=r(s,a)+\gamma\cdot\mathbb{E}_{s^{\prime}\sim P(\cdot|s,a) }[\max_{a^{\prime}\in\mathcal{A}}Q(s^{\prime},a^{\prime})].",,"Q^{\star}(s,a) = \mathcal{T}Q^{\star}(s,a) := r(s,a) + \gamma \mathbb{E}_{s' \sim P(\cdot|s,a)}\left[\max_{a'} Q^{\star}(s',a')\right],",where the Bellman operator $\mathcal{T}$ acts on the optimal Q-function $Q^{\star}$ by combining the immediate reward and the expected discounted future rewards under the optimal action selection.
neurips_2024_oral_54,3,"Q-learning (Watkins and Dayan, 1992) aims to learn the optimal policy by first learning \(Q^{*}\) as the solution to the fixed point equation \(\mathcal{T}Q=Q\) and then obtain a deterministic optimal policy via the maximization \(\pi^{*}(s)=\arg\max_{a}Q^{*}(s,a)\).

Let \(Z\in\mathcal{S}^{|\mathcal{S}||\mathcal{A}|}\) be a random vector whose \((s,a)^{\text{th}}\) coordinate is drawn from the distribution \(P(\cdot|s,a)\), independently of all other coordinates. We define the random operator \(\mathcal{T}_{Z}:(\mathcal{S}\times\mathcal{A})\to(\mathcal{S}\times\mathcal{A})\) as","(\mathcal{T}_{Z}Q)(s,a)=r(s,a)+\gamma V(Z(s,a)),","where \(V(s^{\prime})=\max_{a^{\prime}\in\mathcal{A}}Q(s^{\prime},a^{\prime})\)","(\mathcal{T}_{Z}Q)(s,a)=r(s,a)+\gamma\cdot\max_{a^{\prime}\in\mathcal{A}}Q(Z_{(s,a)},a^{\prime}),","where $Z_{(s,a)}$ denotes the $(s,a)^{\text{th}}$ coordinate of the random vector $Z$, representing a sampled next state from the transition distribution $P(\cdot|s,a)$."
neurips_2024_oral_54,4,". The operator \(\mathcal{T}_{Z}\) can be interpreted as the sample Bellman Operator, where we have the relation \(\mathcal{T}Q=\mathbb{E}_{Z}[\mathcal{T}_{Z}Q]\) for all Q-functions.

Lastly, the federated learning setup considered in this work consists of \(M\) agents, where all the agents face a common, unknown MDP, i.e., the transition kernel and the reward functions are the same across agents, which is popularly known as the homogeneous setting. For a given value of \(\varepsilon\in(0,\frac{1}{1-\gamma})\), the objective of agents is to collaboratively learn an \(\varepsilon\)-optimal estimate (in the \(\ell_{\infty}\) sense) of the optimal Q-function of the unknown MDP.

### Performance Measures

We measure the performance of a Federated Q-learning algorithm \(\mathscr{A}\) using two metrics -- sample complexity and communication complexity. For a given MDP \(\mathcal{M}\), let \(\widehat{Q}_{\mathcal{M}}(\mathscr{A},N,M)\) denote the estimate of \(Q^{\star}_{\mathcal{M}}\), the optimal Q-function of the MDP \(\mathcal{M}\), returned by an algorithm \(\mathscr{A}\), when given access to \(N\) i.i.d. samples from the generative model for each \((s,a)\) pair at all the \(M\) agents. The minimax error rate of the algorithm \(\mathscr{A}\), denoted by \(\mathsf{ER}(\mathscr{A};N,M)\), is defined as","\mathsf{ER}(\mathscr{A};N,M):=\sup_{\mathcal{M}=(P,r)}\mathbb{E}[\| \widehat{Q}_{\mathcal{M}}(\mathscr{A},N,M)-Q^{\star}_{\mathcal{M}}\|_{\infty }],",where the expectation is taken over the samples and any randomness in the algorithm,"\mathsf{ER}(\mathscr{A};N,M):=\sup_{\mathcal{M}}\mathbb{E}\left[\left\|\widehat{Q}_{\mathcal{M}}(\mathscr{A},N,M)-Q^{\star}_{\mathcal{M}}\right\|_{\infty}\right],",where the supremum is taken over all MDPs $\mathcal{M}$ and the expectation is over the randomness in the samples and the algorithm $\mathscr{A}$.
neurips_2024_oral_54,5,"hm. Given a value of \(\varepsilon>0\), the sample complexity of \(\mathscr{A}\), denoted by \(\mathsf{SC}(\mathscr{A};\varepsilon,M)\) is given as","\mathsf{SC}(\mathscr{A};\varepsilon,M):=|\mathcal{S}||\mathcal{A}|\cdot\min\{N \in\mathbb{N}:\mathsf{ER}(\mathscr{A};N,M)\leq\varepsilon\}.",,"\mathsf{SC}(\mathscr{A};\varepsilon,M):=\inf\{N\geq 0:\mathsf{ER}(\mathscr{A};N,M)\leq \varepsilon\}.",the smallest number of samples per agent required to achieve an $\varepsilon$-optimal estimate of the optimal Q-function in the $\ell_{\infty}$ sense
neurips_2024_oral_54,6,"Similarly, we can also define a high-probability version for any \(\delta\in(0,1)\) as follows:","\mathsf{SC}(\mathscr{A};\varepsilon,M,\delta):=|\mathcal{S}||\mathcal{A}|\cdot \min\{N\in\mathbb{N}:\Pr(\sup_{\mathcal{M}}\|\widehat{Q}_{\mathcal{M}}( \mathscr{A},N,M)-Q^{\star}_{\mathcal{M}}\|_{\infty}\leq\varepsilon)\geq 1-\delta\}.",,"\mathsf{ER}(\mathscr{A};N,M,\delta):=\sup_{\mathcal{M}=(P,r)}\inf\left\{ \varepsilon>0:\mathbb{P}\left(\|\widehat{Q}_{\mathcal{M}}(\mathscr{A},N,M)-Q^{\star }_{\mathcal{M}}\|_{\infty}>\varepsilon\right)\leq\delta\right\}.",where the probability is taken over the samples and any randomness in the algorithm
neurips_2024_oral_54,7,"We measure the communication complexity of any federated learning algorithm both in terms of frequency of information exchange and total number of bits uploaded by the agents. For each agent \(m\), let \(C^{m}_{\mathsf{round}}(\mathscr{A};N)\) and \(C^{m}_{\mathsf{bit}}(\mathscr{A};N)\) respectively denote the number of times agent \(m\) sends a message to the server and the total number of bits uploaded by agent \(m\) to the server when an algorithm \(\mathscr{A}\) is run with \(N\) i.i.d. samples from the generative model for each \((s,a)\) pair at each agent. The communication complexity of \(\mathscr{A}\), when measured in terms of frequency of communication and total number of bits exchanged, is given by","\mathsf{CC}_{\mathsf{round}}(\mathscr{A};N):=\frac{1}{M}\sum_{m=1}^{M}C^{m}_{ \mathsf{round}}(\mathscr{A};N);\quad\mathsf{CC}_{\mathsf{bit}}(\mathscr{A};N ):=\frac{1}{M}\sum_{m=1}^{M}C^{m}_{\mathsf{bit}}(\mathscr{A};N),",,"C_{\mathsf{round}}(\mathscr{A};N,M):=\max_{1\leq m\leq M}C^{m}_{\mathsf{round}}(\mathscr{A};N),\quad C_{\mathsf{bit}}(\mathscr{A};N,M):=\sum_{m=1}^{M}C^{m}_{\mathsf{bit}}(\mathscr{A};N).","where the first term captures the maximum number of communication rounds any agent participates in, and the second term captures the total number of bits uploaded across all agents"
neurips_2024_oral_54,8,"respectively. Similarly, for a given value of \(\varepsilon\in(0,\frac{1}{1-\gamma})\), we can also define \(\mathsf{CC}_{\mathsf{round}}(\mathscr{A};\varepsilon)\) and \(\mathsf{CC}_{\mathsf{bit}}(\mathscr{A};\varepsilon)\) based on when \(\mathscr{A}\) is run to guarantee a minimax error of at most \(\varepsilon\).

### Intermittent Communication Algorithms

In this work, we consider a popular class of federated learning algorithms referred to as algorithms with intermittent communication. The intermittent communication setting provides a natural framework to extend single agent Q-learning algorithms to the distributed setting. As the name suggests, under this setting, the agents intermittently communicate with each other, sharing their updated beliefs about \(Q^{\star}\). Between two communication rounds, each agent updates their belief about \(Q^{\star}\) using stochastic fixed point iteration based on the locally available data, similar to a single agent setup. Such intermittent communication algorithms have been extensively studied and used to establish lower bounds on communication complexity of distributed stochastic convex optimization (Woodworth et al., 2018, 2021).

A generic Federated Q-learning algorithm with intermittent communication is outlined in Algorithm 1. It is characterized by the following five parameters: (i) total number of updates \(T\); (ii) the number of communication rounds \(R\); (iii) a step size schedule \(\{\eta_{t}\}_{t=1}^{T}\); (iv) a communication schedule \(\{t_{r}\}_{r=1}^{R}\); (v) batch size \(B\). During the \(t^{\text{th}}\) iteration, each agent \(m\) computes \(\{\widehat{\mathcal{T}}_{\text{Zs}}(Q_{t-1}^{m})\}_{b=1}^{B}\), a minibatch of sample Bellman operators at the current estimate \(Q_{t-1}^{m}\), using \(B\) samples from the generative model for each \((s,a)\) pair, and obtains an intermediate local estimate using the Q-learning update as follows:",Q_{t-\frac{1}{2}}^{m}=(1-\eta_{t})Q_{t-1}^{m}+\frac{\eta_{t}}{B}\sum_{b=1}^{B} \mathcal{T}_{\text{Zs}}(Q_{t-1}^{m}).,,\overline{Q}_{t}^{m} = Q_{t-1}^{m} + \eta_{t} \cdot \left( \frac{1}{B} \sum_{b=1}^{B} \widehat{\mathcal{T}}_{Z_{b}^{m}}(Q_{t-1}^{m}) - Q_{t-1}^{m} \right).,Intermediate local Q-learning update at agent $m$ using a minibatch of $B$ samples and the current estimate $Q_{t-1}^{m}$.
neurips_2024_oral_54,9,"Here \(\eta_{t}\in(0,1]\) is the step-size chosen corresponding to the \(t^{\text{th}}\) time step. The intermediate estimates are averaged based on a communication schedule \(\mathcal{C}=\{t_{r}\}_{r=1}^{R}\) consisting of \(R\) rounds, i.e.,","Q_{t}^{m}=\begin{cases}\frac{1}{M}\sum_{j=1}^{M}Q_{t-\frac{1}{2}}^{j}&\text{ if }t\in\mathcal{C},\\ Q_{t-\frac{1}{2}}^{m}&\text{ otherwise.}\end{cases}",,"Q_{t}^{m}=\frac{1}{M}\sum_{m^{\prime}=1}^{M}Q_{t-\frac{1}{2}}^{m^{\prime}},\quad\text{if }t\in\mathcal{C},\quad\text{and}\quad Q_{t}^{m}=Q_{t-\frac{1}{2}}^{m},\quad\text{otherwise}.","Agents either update their local estimate by averaging the intermediate estimates across all agents if a communication round occurs at time $t$, or retain their intermediate estimate otherwise."
neurips_2024_oral_54,10,"In the above equation, the averaging step can also be replaced with any distributed mean estimation routine that includes compression to control the bit level costs. Without loss of generality, we assume that \(Q_{0}^{m}=0\) for all agents \(m\) and \(t_{R}=T\), i.e., the last iterates are always averaged. It is straightforward to note that the number of samples taken by an intermittent communication algorithm is \(BT\), i.e, \(N=BT\) and the communication complexity \(\textsf{CC}_{\textsf{round}}=R\).

## 3 Lower Bound

In this section, we investigate the first of the two questions regarding the lower bound on communication complexity. The following theorem establishes a lower bound on the communication complexity of a Federated Q-learning algorithm with intermittent communication.

**Theorem 1**.: _Assume that \(\gamma\in[5/6,1)\) and the state and action spaces satisfy \(|\mathcal{S}|\geq 4\) and \(|\mathcal{A}|\geq 2\). Let \(\mathscr{A}\) be a Federated Q-learning algorithm with intermittent communication that is run for \(T\geq\max\{16,\frac{1}{1-\gamma}\}\) steps with a step size schedule of either \(\eta_{t}:=\frac{1}{1+c_{u}(1-\gamma)t}\) or \(\eta_{t}:=\eta\) for all \(1\leq t\leq T\). If_",R=\textsf{CC}_{\textsf{round}}(\mathscr{A};N)\leq\frac{c_{0}}{(1-\gamma)\log ^{2}N};\text{ or }\textsf{CC}_{\textsf{left}}(\mathscr{A};N)\leq\frac{c_{1}|\mathcal{S}|| \mathcal{A}|}{(1-\gamma)\log^{2}N},,"\varepsilon\leq\frac{1}{4(1-\gamma)^{1/2}(T)^{1/2}},",upper bound on the target error level $\varepsilon$ in terms of the time horizon $T$ and the discount factor $\gamma$
neurips_2024_oral_54,11,"_for some universal constants \(c_{0},c_{1}>0\) then, for all choices of communication schedule, batch size \(B\), \(c_{\eta}>0\) and \(\eta\in(0,1)\), the minimax error of \(\mathscr{A}\) satisfies_","\textsf{ER}(\mathscr{A};N,M)\geq\frac{C_{\gamma}}{\log^{3}N\sqrt{N}},",,"\mathsf{ER}(\mathscr{A};N,M)\geq\frac{c_{2}}{(1-\gamma)^{2}M\sqrt{N}}}",where $c_{2}>0$ is a universal constant.
neurips_2024_oral_56,1,"### Reasoning Boundary

In order to quantify the capacity for complex reasoning in LLMs, we introduce an upper-bound concept termed reasoning boundary (RB), which formally defines the degree of ease that an LLM can handle within a specific reasoning process. In simpler terms, as shown in Figure 1 (a), RB reflects the limit beyond which a model's accuracy significantly degrades. Mathematically, RB is defined for a model \(m\) and a task \(t\) as the maximum of problem difficulty \(d\) at which the model's accuracy reaches a predefined threshold \(K_{1}\):","\mathcal{B}_{Acc=K_{1}}(t|m)=\sup_{d}\{d|Acc(t|d,m)\leq K_{1}\},","where \(Acc(t|d,m)\) represents the accuracy of the model's accuracy on task \(t\) with difficulty \(d\)","\begin{equation}
\text{RB}(m, t) = \max \{ d \mid \text{Accuracy}(m, t, d) \geq K_{1} \}
\end{equation}",Equation 1 defines the reasoning boundary (RB) for a model $m$ and task $t$ as the maximum problem difficulty $d$ at which the model's accuracy remains above a threshold $K_{1}$.
neurips_2024_oral_56,2,"\). Difficulty can be measured by factors like the number of reasoning steps or computational complexity. For brevity, we denote RB as \(\mathcal{B}(t|m)\) in subsequent sections.

**Conclusion:** The reasoning boundary for a model is defined by its ability to achieve a specific accuracy for a given task difficulty.

### Combination Law of Reasoning Boundary

In practical scenarios, models often require the integration of multiple capabilities to address a single task effectively. To quantify how a large language model can be boosted by the cooperation of multiple capabilities through the CoT mechanism, we introduce the ""_Combination Law of RB_"", giving a concrete formula of the upper-bound of the CoT. The law estimates the unified reasoning boundary \(\mathcal{B}_{\text{Acc}=K_{1}}(t_{1},t_{2},\dots,t_{n}|m)\) for \(n\) tasks within a model \(m\), which is formulated as:","\mathcal{B}_{\text{Acc}=K_{1}}(t_{1},t_{2},\dots,t_{n}|m)\approx\frac{1}{\sum _{i=1}^{n}\frac{N_{i}}{\mathcal{B}_{\text{Acc}=K_{1}}(t_{i}|m)-b_{i}}},",where \(\mathcal{B}_{\text{Acc}=K_{1}}(t_{i}|m)\) denotes the reasoning boundary of model \(m\) for task \(t_{i}\),"\mathcal{B}_{\text{Acc}=K_{1}}(t_{1},t_{2},\dots,t_{n}|m) = \prod_{i=1}^{n} \mathcal{B}(t_i|m)",the unified reasoning boundary for multiple tasks is the product of individual reasoning boundaries for each task under the same model
neurips_2024_oral_56,3,"\). \(N_{i}\), and \(b_{i}\) are scaling factors, which are only affected by the related task. As shown in Figure 1 (b), Equation (2) provides a mathematical formula to estimate the combined RBs from the independent ones, enabling deeper insights into model behavior for intricate tasks. See Appendix A for detailed mathematical analysis.

Furthermore, the combination law for reasoning boundary demonstrates favorable theoretical properties, with broad applicability across diverse scenarios and flexibility in accommodating various boundary segmentation methods. For detailed practical application, please refer to Appendix B.

**Conclusion:** The combination law of reasoning boundary satisfies the weighted harmonic average of each basic reasoning boundary.

### Categories of Reasoning Boundary

Furthermore, in order to guide the optimization of CoT and more convenient expression, as shown in Figure 1 (c), we define the following three categories of RBs based on their empirical accuracy:

**Completely Feasible Reasoning Boundary:** We define that the part with an accuracy greater than 90% is a completely feasible reasoning boundary (\(\text{CFRB}=\mathcal{B}_{\text{Acc}\geq 90\%}(t_{1},t_{2},\dots,t_{n}|m)\)), which means that LLMs can effectively grasp the performance of this part.

**Completely Infeasible Reasoning Boundary:** We believe that the part with an accuracy less than 10% is a completely infeasible reasoning boundary (\(\text{CIRB}=\mathcal{B}_{\text{Acc}\leq 10\%}(t_{1},t_{2},\dots,t_{n}|m)\)), which means that the model can never effectively grasp the performance of this part.

**Partially Feasible Reasoning Boundary:** We define the RB in the rest part except CFRB and CIRB as a partially feasible reasoning boundary (\(\text{PFRB}=\mathcal{B}_{10\%\text{Acc}<90\%}(t_{1},t_{2},\dots,t_{n}|m)\)), which requires the model to repeat thinking or more clear information to solve the problem.

Figure 1: Overview of the introduced concepts.

We analyze the nature of these three categories of RB in detail (in Section 4.3), and further utilize the combination law to optimize these three reasoning boundaries (in Section 5), so as to provide effective suggestions and guidance to support future CoT optimization.

## 3 Experimental Setup

Benchmark SettingsTo assess the reasoning boundaries of LLMs, we require a dataset rich in RB. This necessitates tasks with evenly distributed complexities and reasoning steps that challenge the models' upper-bounds. To meet these requirements, we introduce BigGSM, a new dataset offering greater calculation complexity and longer reasoning chains. The detailed construction process for BigGSM is provided in Appendix C.

Model SettingsExcept for model expansion experiments, all experiments are conducted on GPT-3.5-Turbo. Following the setting of Wei et al. (2022), in our CoT experiment, all multi-step reasoning tasks utilize three manually constructed demonstrations. In addition, for all the experiments, top-p is selected from \(\{0.95,1\}\). Temperature is selected from \([0,1]\) and serves as the main error variable.

## 4 Empirical Analysis of Reasoning Boundary

### Existence Verification for Reasoning Boundary

In this study, we investigate the hypothesis that an LLM exhibits varying levels of reasoning boundary across various tasks. To this end, we will verify whether the model has widespread reasoning boundary in various tasks in the following three tasks:

Basic Arithmetic CalculationFirst, to investigate the existence of RB, we first examine basic arithmetic operations (including addition, subtraction, multiplication, and division). As illustrated in Figure 2 (a), the results reveal significant performance variations across three distinct regions. For multiplication, accuracy surpasses 90% for results up to \(2.2e5\). Conversely, accuracy falls below 10% for products exceeding \(2e6\). Similar presences of varying RBs are observed for other operations, which verifies the existence of reasoning boundary in basic arithmetic calculation tasks. Further results and implementation details are provided in Appendix D.

Nature Language PlanningWe further investigate RB in natural language planning tasks for mathematical reasoning. We prompt the model to generate plans and assess their accuracy through manual evaluation. There is a strong correlation between the number of reasoning steps and LLMs' performance in Figure 2 (b). When the model meets the question with fewer than 2 reasoning steps, accuracy surpasses 90%. Conversely, when reasoning steps exceed 4, accuracy falls below 10%. This finding suggests that there are also three different RB categories in natural language planning tasks.

Code PlanningFor further extensive exploration, we further prompt LLMs by PAL (Gao et al., 2023) to generate code-format plans and evaluate them by manual annotation. As shown in Figure 2 (c), the code planning task is similar to natural language planning, which is also an obvious division

Figure 2: Existence Verification for Reasoning Boundary. Figures (b, c) present evaluations performed on BigGSM, where the reasoning paths are manually analyzed to identify the specific steps at which errors occur, without considering whether the final conclusions are correct.

and different categories of RBs. Notably, since code planning utilizes code for clearer logic and reduced expression complexity, its planning accuracy surpasses that of natural language planning.

### Combination Law Verification on Different Tasks

Combination Law in Complex Arithmetic CalculationBuilding on the proof of Equation (13), we hypothesize that the combination law for RB in the complex arithmetic calculation is the harmonic average of the arithmetic calculation RB and calculation planning RB. To verify this, we designed an experiment focusing on formulas containing addition, subtraction, and multiplication, like ""\((1+2)*3-4\)"". Since addition and subtraction complexities are assumed to be around \(1e15\) (as shown in Figure 13), the arithmetic calculation RB primarily depends on the multiplication RB and calculation planning RB. Therefore, as shown in Figure 3 (a), there are two obvious RB lines, namely \(\mathcal{B}_{Acc=90\%}\) and \(\mathcal{B}_{Acc=10\%}\), which are completely consistent with the combination law of these basic RB based on the Equation (2). Besides, these two lines also clearly divide the RBs into three categories.

Combination Law in Mathematical ReasoningInspired by Tan (2023); Xiao and Liu (2024), we posit that the natural language mathematical CoT task is determined by two sub-tasks: step planning task and step calculation task for global logic planning and local mathematical calculation. Furthermore, each model output step requires a single basic operation, resulting in a step calculation boundary close to the maximum number of multiplications, denoted by \(\mathcal{B}(c)\approx\mathcal{B}(m)\). Formally, with step planning RB denoted by (\(\mathcal{B}(p)\)) and the step calculation RB by (\(\mathcal{B}(c)\)), then the combined RB satisfies the following law:","\mathcal{B}^{\texttt{CGT}}(c,p)=\frac{1}{\frac{N_{1}}{(\mathcal{B}(c)-b_{1})} +\frac{N_{2}}{(\mathcal{B}(p)-b_{2})}}.",,"\mathcal{B}(t_{1},t_{2},\dots,t_{n}|m)\approx\frac{1}{\sum _{i=1}^{n}\frac{1}{\mathcal{B}(t_{i}|m)}}.","Equation 3 represents the simplified form of the combination law of reasoning boundary, where the combined reasoning boundary is approximated by the harmonic average of individual task reasoning boundaries."
neurips_2024_oral_56,4,"As illustrated in Figure 3 (b), the actual performance distribution of RB (including \(\mathcal{B}_{Acc=90\%}\) and \(\mathcal{B}_{Acc=10\%}\)) in natural language mathematical reasoning task fully aligns with the proposed combination law in Equation (3). Additionally, there are also obviously three RBs in Figure 3 (b).

Combination Law in Multi-hop ReasoningBeyond the realm of mathematics, we further extend our exploration of the combination law to the field of multi-hop question answering. Specifically, we validate our law on HotpotQA (Yang et al., 2018), where we define the reasoning boundary as the combination of global hop-planning RB and local knowledge entity reasoning RB. As shown in Figure 3 (c), \(\mathcal{B}_{Acc=90\%}\) and \(\mathcal{B}_{Acc=10\%}\) also satisfy the weighted harmonic mean of these two sub-reasoning boundaries. It is also proved that, in addition to math-related tasks, multi-hop question answering also satisfies our proposed combined law and also exhibits three distinct RBs. We will describe in detail how to calculate the combination law on multi-hop reasoning in Appendix E.

### Nature Analysis for different Reasoning Boundary

According to the definition of different RBs, we have divided the problem into three parts for LLMs. In this section, we will verify whether the defined RB adheres to the intrinsic nature of the model itself. We will discuss the natures of these RBs in detail:

CFRB **means complete mastery of the model even without demonstration.** According to the definition, we assume that a question within CFRB implies a comprehensive understanding of the

Figure 3: Combination law verification of RB on different tasks. More verification results on other tasks are shown in Figure 12.

associated issue for a certain LLM. To verify this, following Zhang et al. (2022) and Wei et al. (2022), we formulate a mathematical request and generate chain-of-thought rationale and answer through zero-shot prompting without any demonstration. As shown in Figure 4 (a), it still achieves 29.2% improvement in CFRB on generating the correct rationale compared to other RBs. This also proves that the model can indeed master tasks well on the questions in CFRB.

PFRB **means moderate confidence in its solution and needs consensus building process.** To gauge the level of performance and confidence, we draw parallels to human decision-making, where moderate confidence often necessitates multiple times of consensus building. Inspired by this, we investigate it on Self-Consistency (Wang et al., 2022), which integrates results from various reasoning answers to reach a conclusive answer. Figure 4 (b) demonstrates that as the integration of reasoning paths increases, the accuracy improves significantly within PFRB compared with other RBs. This suggests that within PFRB, the LLM exhibits moderate confidence in solving problems, which needs multiple consensus building.

CIRB **exhibits poor reasoning performance even with consensus building.** As illustrated in Figure 4 (a), questions in CIRB display extremely low accuracy (around 9.5%). And the model shows consistently poor performance and no improvement on Self-consistency in this boundary in Figure 4. It signifies that the model exhibits poor reasoning performance.

**LLM has self-awareness of its own RBs.** In parallel, a natural question arises: _Is the model capable of discerning its inherent RBs?_ To investigate this, we employ the Synthetic-CoT (Shao et al., 2023) to prompt LLM to generate CoT data. As depicted in Figure 4 (c), the results demonstrated that there are over 65% of generated samples within CFRB, which achieves a much higher percentage and performance than other RBs. This suggests that LLMs possess an intrinsic understanding of their RBs and constraints to generate the task they grasp, indicative of a potential for self-assessment.

**Takeaways:** (1) Reasoning boundary (RB) and the combination law of RB are both widespread across a series of tasks. (2) Different categories of RB can reflect the corresponding performance, and the model can also have a self-understanding of its own RB.

## 5 RB-based CoT Optimization

### How can we improve CoT by optimizing RB?

Based on our framework, the reasoning boundary limits the performance of the model. The simplest approach to improve CoT is to optimize the step calculation RB \(\mathcal{B}(c)\) to promote the value of RB. Specifically, Tool-Usage (Paranjape et al., 2023) and Program-of-Thought (PoT) (Chen et al., 2024) have shown significant success in CoT optimization. We explain the rationale behind their effectiveness, why PoT consistently outperforms direct Tool Usage (Yao et al., 2023; Chen et al., 2023), and take them as examples to demonstrate how to improve CoT by promoting RB.

Figure 4: Nature analysis at different reasoning boundaries. For Figure (c), all demosntrations in CIRB are special value points obtained by calculation methods similar to \(25000\times 1000\). In fact, no real CIRB demosntrations are obtained.

**Tool Usage can boost the value of RB for an LLM.** When the model uses tools [14], we can simply think that the model can perform calculations with infinite precision, so that the RB of mathematical calculations tends to infinity, viz \(\mathcal{B}(c)\rightarrow+\infty\). It is obvious that the combined RB of the model can be calculated as:","\mathcal{B}^{\mathsf{Tool}}(c,p)=\lim_{\mathcal{B}(c)arrow+\infty}\frac{ 1}{\frac{N_{1}}{(\mathcal{B}(c)-b_{1})}+\frac{N_{2}}{(\mathcal{B}(p)-b_{2})}}= \frac{\mathcal{B}(p)-b_{2}}{N_{2}}.",,"\mathcal{B}_{\text{Acc}=K_{1}}(t_{1},t_{2},\dots,t_{n}|m)\approx\frac{1}{\sum _{i=1}^{n}\frac{N_{i}}{\mathcal{B}_{\text{Acc}=K_{1}}(t_{i}|m)-b_{i}}},",where $ \mathcal{B}_{\text{Acc}=K_{1}}(t_{i}|m) $ denotes the reasoning boundary of model $ m $ for task $ t_{i} $
neurips_2024_oral_57,1,"### Holistic Facial Dynamics Generation with Diffusion Transformer

Given the constructed face latent space and trained encoders, we can extract the facial dynamics and head movements from real-life talking face videos and train a generative model. Crucially, we consider identity-agnostic holistic facial dynamics generation (HFDG), where our learned latent codes represent all facial movements such as lip motion, (non-lip) expression, and eye gaze and blinking. This is in contrast to existing methods that apply separate models for different factors with interleaved regression and generative formulations [62; 76; 71; 60; 74]. Furthermore, previous methods often train on a limited number of identities [74; 68; 21] and cannot model the wide range of motion patterns of different humans, especially given an expressive motion latent space.

In this work, we utilize diffusion models for audio-conditioned HFDG and train on massive talking face videos from a large number of identities. In particular, we apply a transformer architecture [58; 38; 52] for our sequence generation task. Figure 2 shows an overview of our HFDG framework.

Formally, a motion sequence extracted from a video clip is defined as \(\mathbf{X}=\{[\mathbf{z}_{i}^{pose},\mathbf{z}_{i}^{dyn}]\},i=1,\ldots,W\). Given its accompanying audio clip \(\mathbf{a}\), we extract the synchronized audio features \(\mathbf{A}=\{\mathbf{f}_{i}^{audio}\}\), for which we use a pretrained feature extractor Wav2Vec2 [3].

Diffusion formulation.Diffusion models define two Markov chains [27; 47; 48], the forward chain progressively adds Gaussian noise to the target data, while the reverse chain iteratively restores the raw signal from noise. Following the denoising score matching objective [48], we define the simplified loss function as","\mathbb{E}_{t\sim\mathcal{U}[1,T],\;\mathbf{X}^{0},\mathbf{C}\sim q(\mathbf{ X}^{0},\mathcal{C})}(\|\mathbf{X}^{0}-\mathcal{H}(\mathbf{X}^{t},t,\mathbf{C})\|^{2}),","where \(t\) denotes the time step, \(\mathbf{X}^{0}=\mathbf{X}\) is the raw motion latent sequence, and \(\mathbf{X}^{t}\) is the noisy inputs generated by the diffusion forward process \(q(\mathbf{X}^{t}|\mathbf{X}^{t-1})=\mathcal{N}(\mathbf{X}^{t};\sqrt{1-\beta_{ t}}\mathbf{X}^{t-1},\beta_{t}\text{I})\)","\begin{equation}
    \mathcal{L} = \mathbb{E}_{t, \mathbf{x}_t, \boldsymbol{\epsilon}} \left[ \| \boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t, \mathbf{A}) \|^2 \right],
\end{equation}","where $\boldsymbol{\epsilon}_\theta$ is the neural network predicting the noise residual conditioned on the audio features $\mathbf{A}$, and $\mathcal{L}$ represents the expected loss over time steps $t$, noisy samples $\mathbf{x}_t$, and noise terms $\boldsymbol{\epsilon}$."
neurips_2024_oral_57,2,"\). \(\mathcal{H}\) is our transformer network which predicts the raw signal itself instead of noise. \(\mathbf{C}\) is the condition signal, to be described next.

Conditioning signals.The primary condition signal for our audio-driven motion generation task is the audio feature sequence \(\mathbf{A}\). We also incorporate several additional signals, which not only make the generative modeling more tractable but also increase the generation controllability.

Specifically, we consider the main eye gaze direction \(\mathbf{g}\), head-to-camera distance \(d\), and emotion offset \(\mathbf{e}\). The main gaze direction, \(\mathbf{g}=(\theta,\phi)\), is defined by a vector in spherical coordinates. It specifies the focused direction of the generated talking face. We extract \(\mathbf{g}\) for the training video clips using [2] on each frame followed by a simple histogram-based clustering algorithm. The head distance \(d\) is a normalized scalar controlling the distance between the face and the virtual camera, which affects the face scale in the generated face video. We obtain this scale label for the training videos using [17]. The emotion offset \(\mathbf{e}\) modulates the depicted emotion on the talking face. Note that emotion is often intrinsically linked to and can be largely inferred from audio; hence, \(\mathbf{e}\) serves only as a _global offset_ added to enhance or moderately alter the emotion when required. It is _not_ designed to achieve a total emotion shift during inference or produce emotions incongruent with the input audio. In practice, we use the averaged emotion coefficients extracted by [43] as our emotion signal.

In order to achieve a seamless transition between adjacent windows, we incorporate the last \(K\) frames of the audio feature and generated motions from the previous window as the condition of the current one. To summarize, our input condition can be denoted as \(\mathbf{C}=[\mathbf{X}^{pre},\mathbf{A}^{pre};\mathbf{A},\mathbf{g},d, \mathbf{e}]\). All conditions are concatenated with noise along the temporal dimension as the input to the transformer.

Classifier-free guidance (CFG) [28].In the training stage, we randomly drop each of the input conditions. During inference, we apply","\hat{\mathbf{X}}^{0}=(1+\sum_{\mathbf{e}\in\mathbf{C}}\lambda_{\mathbf{e}}) \cdot\mathcal{H}(\mathbf{X}^{t},t,\mathbf{C})-\sum_{\mathbf{e}\in\mathbf{C}} \lambda_{c}\cdot\mathcal{H}(\mathbf{X}^{t},t,\mathbf{C}|_{\mathbf{e}=\emptyset})",,"\mathbf{C} = [\mathbf{X}^{pre}, \mathbf{A}^{pre}; \mathbf{A}, \mathbf{g}, d, \mathbf{e}]","Concatenated conditioning signal including previous motion and audio, current audio, gaze direction, head distance, and emotion offset."
neurips_2024_oral_58,1,"### Preliminary: autoregressive modeling via next-token prediction

**Formulation.** Consider a sequence of discrete tokens \(x=(x_{1},x_{2},\dots,x_{T})\), where \(x_{t}\in[V]\) is an integer from a vocabulary of size \(V\). The next-token autoregressive posits the probability of observing the current token \(x_{t}\) depends only on its prefix \((x_{1},x_{2},\dots,x_{t-1})\). This **unidirectional token dependency assumption** allows for the factorization of the sequence \(x\)'s likelihood:","p(x_{1},x_{2},\dots,x_{T})=\prod_{t=1}^{T}p(x_{t}\mid x_{1},x_{2},\dots,x_{t-1 }).",,P(x) = \prod_{t=1}^T P(x_t \mid x_{<t}),"The likelihood of a sequence $x$ factorized into the product of conditional probabilities of each token $x_t$ given its prefix, with $T$ tokens and vocabulary size $V$."
neurips_2024_oral_58,2,"Training an autoregressive model \(p_{\theta}\) involves optimizing \(p_{\theta}(x_{t}\mid x_{1},x_{2},\dots,x_{t-1})\) over a dataset. This is known as the ""next-token prediction"", and the trained \(p_{\theta}\) can generate new sequences.

**Tokenization.** Images are inherently 2D continuous signals. To apply autoregressive modeling to images via next-token prediction, we must: 1) tokenize an image into several _discrete_ tokens, and 2) define a 1D _order_ of tokens for unidirectional modeling. For 1), a quantized autoencoder such as [30] is often used to convert the image feature map \(f\in\mathbb{R}^{h\times w\times C}\) to discrete tokens \(q\in[V]^{h\times w}\):","f=\mathcal{E}(im),\ \ \ \ \ q=\mathcal{Q}(f),","where \(im\) denotes the raw image, \(\mathcal{E}(\cdot)\) a encoder, and \(\mathcal{Q}(\cdot)\) a quantizer",q = \text{Quantize}(f) \in [V]^{h \times w}.,The quantization function maps a continuous feature map $f$ to a grid of discrete tokens $q$ from a vocabulary of size $V$.
neurips_2024_oral_58,3,"er. The quantizer typically includes a learnable codebook \(Z\in\mathbb{R}^{V\times C}\) containing \(V\) vectors. The quantization process \(q=\mathcal{Q}(f)\) will map each feature vector \(f^{(i,j)}\) to the code index \(q^{(i,j)}\) of its nearest code in the Euclidean sense:","q^{(i,j)}=(\operatorname*{arg\,min}_{v\in[V]}\|\text{lookup}(Z,v)-f^{(i,j)} \|_{2})\in[V],","where \(\text{lookup}(Z,v)\) means taking the \(v\)-th vector in codebook \(Z\)","q^{(i,j)} = \arg\min_{k \in [V]} \|f^{(i,j)} - Z_k\|^2,","where $Z_k$ denotes the $k$-th codevector in the codebook $Z$, and $\|\cdot\|^2$ represents the squared Euclidean distance used for nearest-neighbor quantization."
neurips_2024_oral_58,4,"\). To train the quantized autoencoder, \(Z\) is looked up by every \(q^{(i,j)}\) to get \(\hat{f}\), the approximation of original \(f\). Then a new image \(\hat{im}\) is reconstructed using the decoder \(\mathcal{D}(\cdot)\) given \(\hat{f}\), and a compound loss \(\mathcal{L}\) is minimized:","\hat{f} =\text{lookup}(Z,q),\ \ \ \ \ \ \ \ \hat{im}=\mathcal{D}(\hat{f}), || \mathcal{L} =\|im-\hat{im}\|_{2}+\|f-\hat{f}\|_{2}+\lambda_{\text{P}} \mathcal{L}_{\text{P}}(\hat{im})+\lambda_{\text{G}}\mathcal{L}_{\text{G}}( \hat{im}),",,"\hat{f}=\text{lookup}(Z,q),\ \ \ \ \ \hat{im}=\mathcal{D}(\hat{f}),\ \ \ \ \ \mathcal{L}=\underbrace{\|\hat{im}-im\|_{2}^{2}}_{\text{reconstruction loss}}+\beta\underbrace{\|\text{sg}[f]-\hat{f}\|_{2}^{2}}_{\text{codebook loss}}+\gamma\underbrace{\|\text{sg}[\hat{f}]-f\|_{2}^{2}}_{\text{commitment loss}},","where $\text{sg}[\cdot]$ denotes the stop-gradient operation, and $\beta,\gamma$ are weighting coefficients for the codebook and commitment losses, respectively."
neurips_2024_oral_58,5,"ts. Once the autoencoder \(\{\mathcal{E},\mathcal{Q},\mathcal{D}\}\) is fully trained, it will be used to tokenize images for subsequent training of a unidirectional autoregressive model.

The image tokens in \(q\in[V]^{h\times w}\) are arranged in a 2D grid. Unlike natural language sentences with an inherent left-to-right ordering, the order of image tokens must be explicitly defined for unidirectional autoregressive learning. Previous AR methods [30, 92, 51] flatten the 2D grid of \(q\) into a 1D sequence \(x=(x_{1},\dots,x_{h\times w})\) using some strategy such as row-major raster scan, spiral, or z-curve order. Once flattened, they can extract a set of sequences \(x\) from the dataset, and then train an autoregressive model to maximize the likelihood in (1) via next-token prediction.

**Discussion on the weakness of vanilla autoregressive models.** The above approach of tokenizing and flattening enable next-token autoregressive learning on images, but introduces several issues:

1. **Mathematical premise violation.** In quantized autoencoders (VQVAEs), the encoder typically produces an image feature map \(f\) with inter-dependent feature vectors \(f^{(i,j)}\) for all \(i,j\). So after quantization and flattening, the token sequence \((x_{1},x_{2},\dots,x_{h\times w})\) retains _bidirectional_ correlations. This contradicts the _unidirectional_ dependency assumption of autoregressive models, which dictates that each token \(x_{t}\) should only depend on its prefix \((x_{1},x_{2},\dots,x_{t-1})\).
2. **Inability to perform some zero-shot generalization.** Similar to issue 1), The unidirectional nature of image autoregressive modeling restricts their generalizability in tasks requiring bidirectional reasoning. E.g., it cannot predict the top part of an image given the bottom part.
3. **Structural degradation.** The flattening disrupts the spatial locality inherent in image feature maps. For example, the token \(q^{(i,j)}\) and its 4 immediate neighbors \(q^{(i\pm 1,j)}\), \(q^{(i,j\pm 1)}\) are closely correlated due to their proximity. This spatial relationship is compromised in the linear sequence \(x\), where _uni_directional constraints diminish these _cor_relations.

4. **Inefficiency.** Generating an image token sequence \(x=(x_{1},x_{2},\dots,x_{n\times n})\) with a conventional self-attention transformer incurs \(\mathcal{O}(n^{2})\) autoregressive steps and \(\mathcal{O}(n^{6})\) computational cost.

Issues 2) and 3) are evident (see examples above). Regarding issue 1), we present empirical evidence in Appendix C. The proof of issue 3) is detailed in Appendix D. These theoretical and practical limitations call for a rethinking of autoregressive models in the context of image generation.

### Visual autoregressive modeling via next-scale prediction

**Reformulation.** We reconceptualize the autoregressive modeling on images by shifting from ""next-token prediction"" to ""next-scale prediction"" strategy. Here, the autoregressive unit is _an entire token map_, rather than _a single token_. We start by quantizing a feature map \(f\in\mathbb{R}^{h\times w\times C}\) into \(K\) multi-scale token maps \((r_{1},r_{2},\dots,r_{K})\), each at a increasingly higher resolution \(h_{k}\times w_{k}\), culminating in \(r_{K}\) matches the original feature map's resolution \(h\times w\). The autoregressive likelihood is formulated as:","p(r_{1},r_{2},\dots,r_{K})=\prod_{k=1}^{K}p(r_{k}\mid r_{1},r_{2},\dots,r_{k-1 }),","where each autoregressive unit \(r_{k}\in[V]^{h_{k}\times w_{k}}\) is the token map at scale \(k\) containing \(h_{k}\times w_{k}\) tokens, and the sequence \((r_{1},r_{2},\dots,r_{k-1})\) serves as the the ""prefix"" for \(r_{k}\)","p(r_{1},r_{2},\dots,r_{K})=\prod_{k=1}^{K}p(r_{k}\mid r_{1},r_{2},\dots,r_{k-1}),","The joint probability of multi-scale token maps is factorized into a product of conditional probabilities, where each scale $r_k$ is predicted autoregressively based on all previous coarser scales $r_1, \dots, r_{k-1}$."
neurips_2024_oral_58,6,"\). During the \(k\)-th autoregressive step, all distributions over the \(h_{k}\times w_{k}\) tokens in \(r_{k}\) will be generated in parallel, conditioned on \(r_{k}\)'s prefix and associated \(k\)-th position embedding map. This ""next-scale prediction"" methodology is what we define as visual autoregressive modeling (VAR), depicted on the right side of Fig. 4. Note that in the training of VAR, a block-wise causal attention mask is used to ensure that each \(r_{k}\) can only attend to its prefix \(r_{\leq k}\). During inference, kv-caching can be used and no mask is needed.

**Discussion.** VAR addresses the previously mentioned three issues as follows:

1. The mathematical premise is satisfied if we constrain each \(r_{k}\) to depend only on its prefix, that is, the process of getting \(r_{k}\) is solely related to \(r_{\leq k}\). This constraint is acceptable as it aligns with the natural, coarse-to-fine progression characteristics like human visual perception and artistic drawing (as we discussed in Sec. 1). Further details are provided in the _Tokenization_ below.
2. The spatial locality is preserved as (i) there is no flattening operation in VAR, and (ii) tokens in each \(r_{k}\) are fully correlated. The multi-scale design additionally reinforces the spatial structure.
3. The complexity for generating an image with \(n\times n\) latent is significantly reduced to \(\mathcal{O}(n^{4})\), see Appendix for proof. This efficiency gain arises from the _parallel_ token generation in each \(r_{k}\).

**Tokenization.** We develope a new multi-scale quantization autoencoder to encode an image to \(K\) multi-scale discrete token maps \(R=(r_{1},r_{2},\dots,r_{K})\) necessary for VAR learning (6). We employ the same architecture as VQGAN [30] but with a modified multi-scale quantization layer. The encoding and decoding procedures with residual design on \(f\) or \(\hat{f}\) are detailed in algorithms 1 and 2. We empirically find this residual-style design, akin to [51], can perform better than independent interpolation. Algorithm 1 shows that each \(r_{k}\) would depend only on its prefix \((r_{1},r_{2},\dots,r_{k-1})\)

Figure 4: **VAR involves two separated training stages. Stage 1: a multi-scale VO autoencoder encodes an image into \(K\) token maps \(R=(r_{1},r_{2},\dots,r_{K})\) and is trained by a compound loss (5). For details on “Multi-scale quantization” and “Embedding”, check Algorithm 1 and 2. Stage 2: a VAR transformer is trained via next-scale prediction (6): it takes \((\texttt{[s]},r_{1},r_{2},\dots,r_{K-1})\) as input to predict \((r_{1},r_{2},r_{3},\dots,r_{K})\). The attention mask is used in training to ensure each \(r_{k}\) can only attend to \(r_{\leq k}\). Standard cross-entropy loss is used.**

Note that a shared codebook \(Z\) is utilized across all scales, ensuring that each \(r_{k}\)'s tokens belong to the same vocabulary \([V]\). To address the information loss in upscaling \(z_{k}\) to \(h_{K}\times w_{K}\), we use \(K\) extra convolution layers \(\{\phi_{k}\}_{k=1}^{K}\). No convolution is used after downsampling \(f\) to \(h_{k}\times w_{k}\).

```
1Inputs: raw image \(im\);
2Hyperparameters: steps \(K\), resolutions \((h_{k},w_{k})_{k=1}^{K}\);
3\(f=\mathcal{E}(im)\), \(R=[]\);
4for\(k=1,\dots,K\)do
5\(r_{k}=\mathcal{Q}(\text{interpolate}(f,h_{k},w_{k}))\);
6\(R=\text{queue}_{\text{push}}(R,r_{k})\);
7\(z_{k}=\text{lookup}(Z,r_{k})\);
8\(z_{k}=\text{interpolate}(z_{k},h_{K},w_{K})\);
9\(f=f-\phi_{k}(z_{k})\);
10Return: multi-scale tokens \(R\);
```

**Algorithm 1** Multi-scale VQVAE Encoding

## 4 Implementation details

**VAR tokenizer.** As aforementioned, we use the vanilla VQVAE architecture [30] and a multi-scale quantization scheme with \(K\) extra convolutions (0.03M extra parameters). We use a shared codebook for all scales with \(V=4096\). Following the baseline [30], our tokenizer is also trained on OpenImages [50] with the compound loss (5) and a spatial downsample ratio of \(16\times\).

**VAR transformer.** Our main focus is on VAR algorithm so we keep a simple model architecture design. We adopt the architecture of standard decoder-only transformers akin to GPT-2 and VQGAN [67; 30] with adaptive normalization (AdaLN), which has widespread adoption and proven effectiveness in many visual generative models [47; 48; 46; 75; 74; 43; 64; 19]. For class-conditional synthesis, we use the class embedding as the start token [s] and also the condition of AdaLN. We found normalizing \(queries\) and \(keys\) to unit vectors before attention can stablize the training. We do not use advanced techniques in large language models, such as rotary position embedding (RoPE), SwiGLU MLP, or RMS Norm [83; 84]. Our model shape follows a simple rule like [44] that the width \(w\), head counts \(h\), and drop rate \(dr\) are linearly scaled with the depth \(d\) as follows:","w=64d,\qquad h=d,\qquad dr=0.1\cdot d/24.",,"R=(r_{1},r_{2},\dots,r_{K})\sim\text{Multi-scale VQVAE}(im),","where $R$ denotes the set of multi-scale token maps generated by the Multi-scale VQVAE from the input image $im$, with each $r_{k}$ representing a token map at resolution $h_{k}\times w_{k}$"
neurips_2024_oral_58,7,"Consequently, the main parameter count \(N\) of a VAR transformer with depth \(d\) is given by3:

Footnote 3: Due to resource limitation, we use a single shared adaptive layernorm (AdaLN) across all attention blocks in 512\(\times\)512 synthesis. In this case, the parameter count would be reduced to around \(12dw^{2}+6w^{2}\approx 49152\,d^{3}\).","N(d)=\underbrace{d\cdot 4w^{2}}_{\text{self-attention}}+\underbrace{d\cdot 8w^{2}} _{\text{feed-forward}}+\underbrace{d\cdot 6w^{2}}_{\text{adaptive layernorm}}=18\,dw^{2}=73728 \,d^{3}.",,N\approx 12dw^{2}+6w^{2}+6dw^{2}/d=18dw^{2}+6w^{2}.,"The main parameter count $N$ of a VAR transformer with depth $d$ is derived from the model's architecture components, including attention blocks and adaptive normalization layers."
neurips_2024_oral_59,1,"### Message Passing Neural Networks and Weisfeiler-Leman

Message passing is an iterative algorithm that updates the _colors_ of each node \(v\in V(G)\) as","c^{(t+1)}(v)\gets f^{(t+1)}(c^{(t)}(v),g^{(t+1)}(\{\{c ^{(t)}(u)\mid u\in\mathcal{N}(v)\}\})).",,"tags and the description in <description>. The example equation uses a function c_v^{(t)} that updates based on the previous color and the sum of colors from neighbors. 

I should follow that structure. Let me think about the notation. The node's color at iteration t would be c_v^{(t)}, and it's a function of the previous color c_v^{(t-1)} and the sum of the neighbors' colors. The function f is likely a neural network or some aggregation function. 

The equation should show the update rule. So, the left side would be c_v^{(t)}, and the right side would involve f applied to c_v^{(t-1)} and the sum over neighbors. The sum could be written as \sum_{u \in N(v)} c_u^{(t-1)}, where N(v) is the neighborhood of v. 

I need to make sure the LaTeX is correct. Use \mathcal{N}(v) for the neighborhood? Or maybe just N(v). Let me check standard notation. In graph theory, N(v) is common for neighbors. 

Also, the description should be a one-sentence variable description. The example used ""where $f$ is a learnable function..."", so I should mirror that. 

Putting it all together: The equation is c_v^{(t)} = f\left(c_v^{(t-1)}, \sum_{u \in N(v)} c_u^{(t-1)}\right). 

Check for proper LaTeX syntax. The subscript in the sum should be in braces if needed. Also, using \left and \right for parentheses. 

Yes, that looks right. The description would explain that f is learnable and how the node's color is updated based on its previous state and neighbors' colors.
</think>",". The example equation uses a function c_v^{(t)} that updates based on the previous color and the sum of colors from neighbors. 

I should follow that structure. Let me think about the notation. The node's color at iteration t would be c_v^{(t)}, and it's a function of the previous color c_v^{(t-1)} and the sum of the neighbors' colors. The function f is likely a neural network or some aggregation function. 

The equation should show the update rule. So, the left side would be c_v^{(t)}, and the right side would involve f applied to c_v^{(t-1)} and the sum over neighbors. The sum could be written as \sum_{u \in N(v)} c_u^{(t-1)}, where N(v) is the neighborhood of v. 

I need to make sure the LaTeX is correct. Use \mathcal{N}(v) for the neighborhood? Or maybe just N(v). Let me check standard notation. In graph theory, N(v) is common for neighbors. 

Also, the description should be a one-sentence variable description. The example used ""where $f$ is a learnable function..."", so I should mirror that. 

Putting it all together: The equation is c_v^{(t)} = f\left(c_v^{(t-1)}, \sum_{u \in N(v)} c_u^{(t-1)}\right). 

Check for proper LaTeX syntax. The subscript in the sum should be in braces if needed. Also, using \left and \right for parentheses. 

Yes, that looks right. The description would explain that f is learnable and how the node's color is updated based on its previous state and neighbors' colors.
</think>

<latex>
$$
c_v^{(t)} = f\left(c_v^{(t-1)}, \sum_{u \in N(v)} c_u^{(t-1)}\right)
$$
</latex>"
neurips_2024_oral_59,2,The graph output after \(t\) iterations is given by,c^{(t)}(G)\coloneqq h(\{\{c^{(t)}(v)\mid v\in V(G)\}\} ).,,h^{(t)}(G)\gets h^{(t)}(\{\{c^{(t)}(v)\mid v\in V(G)\}\}).,The graph-level representation at iteration $t$ is obtained by applying a readout function $h^{(t)}$ to the multiset of node colors $c^{(t)}(v)$ across all nodes $v$ in the graph $G$.
neurips_2024_oral_59,3,"Here, \(g^{(t)},h\) are functions on the domain of multisets and \(f^{(t)}\) is a function on the domain of tuples. For each \(t\), the colorings \(c^{(t)}\) are graph invariants. When the subsets of nodes with the same colors cannot be further split into different color groups, the algorithm terminates; the stable coloring after convergence is denoted by \(c(G)\).

Choosing injective functions for all \(f^{(t)}\) and setting \(g^{(t)}\) and \(h\) as the identity function results in 1-WL (Weisfeiler et al., 1968). If \(f^{(t)},g^{(t)},h\) are chosen as suitable neural networks, one obtains a Message Passing Neural Network (MPNN). Xu et al. (2019) proved that MPNNs are as powerful as \(1\)-WL if the functions \(f^{(t)},g^{(t)}\), and \(h\) are injective on their respective domains. The \(k\)-WL algorithms uplift the expressive power of \(1\)-WL by considering interactions between \(k\)-tuples of nodes. This results in a hierarchy of strictly more powerful graph invariants (see Appendix B.1 for a formal definition).

### Homomorphism and Subgraph Counting Expressivity

A more nuanced graph invariant can be built by considering the occurrences of a motif \(F\).

**Definition 4**.: _Let \(F\in\mathcal{G}\). A graph invariant \(\zeta\) can homomorphism-count \(F\) if for all pairs \(G,H\in\mathcal{G}\)\(\zeta(G)=\zeta(H)\) implies \(\hom(F,G)=\hom(F,H)\). By analogy, \(\zeta\) can subgraph-count \(F\) if for all pairs \(G,H\in\mathcal{G}\), \(\zeta(G)=\zeta(H)\) implies \(\sub(F,G)=\sub(F,H)\)._

If \(\mathcal{F}\) is a family of graphs, we say that \(\zeta\) can homomorphism-count \(\mathcal{F}\) if \(\zeta\) can homomorphism-count every \(F\in\mathcal{F}\); we denote the vector of homomorphism-count by \(\hom(\mathcal{F},G)\coloneqq(\hom(F,G))_{F\in\mathcal{F}}\). Interpreting \(\hom(\mathcal{F},\cdot)\) as a graph invariant, given by \(G\mapsto\hom(\mathcal{F},G)\), another graph invariant \(\zeta\) can homomorphism-count \(\mathcal{F}\) if and only if \(\zeta\sqsubseteq\hom(\mathcal{F},\cdot)\).

The ability of a graph invariant to count homomorphisms is highly relevant because \(\hom(\mathcal{G},\cdot)\) is a complete graph invariant. Conversely, if \(\zeta\) is a complete graph invariant, then \(\zeta\) can homomorphism-count all graphs (Lovasz, 1967). Additionally, homomorphism-counting serves as a quantitative expressivity measure to compare different WL variants and GNNs, such as \(k\)-WL, Subgraph GNNs, and other methods (Lanzinger et al., 2024; B. Zhang et al., 2024), and allows for relating them to our proposed \(r\)-\(\ell\)WL variant, as detailed in Corollary 2.

## 4 Loopy Weisfeiler-Leman Algorithm

In this section, we introduce a new graph invariant by enhancing the direct neighborhood of nodes with _simple paths_ between neighbors.

**Definition 5**.: _Let \(G\in\mathcal{G}\). A simple path of length \(r\) is a collection \(\mathbf{p}=\{p_{i}\}_{i=1}^{r+1}\) of \(r+1\) nodes such that \(\{p_{i},p_{i+1}\}\in E(G)\) and \(i\neq j\implies p_{i}\neq p_{j}\) for every \(i,j\in\{1,\ldots,r\}\),._

Simple paths are the building blocks of \(r\)-neighborhoods, which in turn are the backbone of our \(r\)-\(\ell\)WL algorithm. The following definition is inspired by (Cantwell et al., 2019; Kirkley et al., 2021).

**Definition 6**.: _Let \(G\in\mathcal{G}\) and \(r\in\mathbb{N}\setminus\{0\}\), we define the \(r\)-neighborhood \(\mathcal{N}_{r}(v)\) of \(v\in V(G)\) as_","\mathcal{N}_{r}(v)\coloneqq\{\mathbf{p}\mid\mathbf{p}\text{ simple path of length }r,\,p_{1},p_{r+1}\in\mathcal{N}(v),v\notin\mathbf{p}\}\,.",,"\mathcal{N}_r(v) \coloneqq \left\{ u \in V(G) \mid \text{there exists a simple path } \mathbf{p} = \{p_i\}_{i=1}^{r+1} \text{ with } p_1 = v, p_{r+1} = u \right\}.",The $r$-neighborhood $\mathcal{N}_r(v)$ of a node $v$ consists of all nodes $u$ reachable from $v$ via a simple path of length $r$.
neurips_2024_oral_59,4,"For consistency, we set \(\mathcal{N}_{0}(v)\coloneqq\mathcal{N}(v)\). An example of the construction of \(r\)-neighborhood is shown in Figure 2, where different \(r\)-neighborhoods of node \(v\) are represented with different colors.

We generalize \(1\)-WL in (1) as follows.

**Definition 7**.: _We define the \(r\)-loop Weisfeiler-Leman (\(r\)-\(\ell\)WL) test by the following color update:_","c_{r}^{(t+1)}(v)arrow\mathrm{HASH}_{r}(c_{r}^{(t)}(v),\{\{c _{r}^{(t)}(\mathbf{p})\mid\mathbf{p}\in\mathcal{N}_{0}(v)\}\}, \ldots,\{\{c_{r}^{(t)}(\mathbf{p})\mid\mathbf{p}\in\mathcal{N}_{r}(v )\}\}),",,"c^{(t+1)}(v)\gets f^{(t+1)}\left(c^{(t)}(v),g^{(t+1)}\left(\left\{\left\{c^{(t)}(u),\left(\mathcal{N}_{r}(u)\right)\right\}\mid u\in\mathcal{N}(v)\right\}\right)\right).",The $r$-$\ell$WL color update rule extends 1-WL by incorporating both node colors and $r$-neighborhoods of simple paths into the message passing framework.
neurips_2024_oral_59,5,"_where \(c_{r}^{(t)}(\mathbf{p})\coloneqq\left(c_{r}^{(t)}(p_{1}),c_{r}^{(t)}(p_{2}), \ldots,c_{r}^{(t)}(p_{r+1})\right)\) is the sequence of colors of nodes in the path._

Figure 2: Example of \(r\)-neighborhoods.

We denote by \(c_{r}^{(t)}(G)\) the final graph output after \(t\) iterations of \(r\)-\(\ell\)WL, i.e.,","c_{r}^{(t)}(G)=\mathrm{HASH}_{r}(\{\{c_{r}^{(t)}(v)\mid v\in V(G) \}\}\}),",,c_{r}^{(t)}(G)\coloneqq\{\{c_{r}^{(t)}(v)\mid v\in V(G)\}\}.,The final graph output after $t$ iterations of $r$-$\ell$WL is defined as the multiset of node colors at iteration $t$.
neurips_2024_oral_59,6,"and by \(c_{r}(G)\) the stable coloring after convergence. The stable coloring \(c_{r}\) serves as graph invariant and will be referred to as \(r\)-\(\ell\)WL.

## 5 Expressivity of \(r\)-\(\ell\)WL

We analyze the expressivity of \(r\)-\(\ell\)WL in terms of its ability to distinguish non-isomorphic graphs, subgraph-count, and homomorphism-count motifs. The proofs for all statements are in Appendix D.

### Isomorphism Expressivity

It is straightforward to check that \(0\)-\(\ell\)WL corresponds to \(1\)-WL, since \(\mathcal{N}_{0}(v)=\mathcal{N}(v)\) for all nodes \(v\). However, increasing \(r\) leads to a strict increase in expressivity.

**Proposition 1**.: _Let \(0\leq q<r\). Then, \(r\)-\(\ell\)WL is strictly more powerful than \(q\)-\(\ell\)WL. In particular, every \(r\)-\(\ell\)WL is strictly more powerful than \(1\)-WL._

This shows that the number of graphs we can distinguish monotonically increases with \(r\). We empirically verify this fact on several synthetic datasets in Section 7.

### Subgraph Expressivity

Recent studies highlight limitations in the ability of certain graph invariants to subgraph-count cycles. For instance, \(1\)-WL cannot subgraph-count cycles (Chen et al., 2020, Theorem 3.3), while \(3\)-WL can only subgraph-count cycles of length up to \(7\)(Arvind et al., 2020, Theorem 3.5). Similarly, Subgraph GNNs have limited cycle-counting ability (Huang et al., 2022, Proposition 3.1). In contrast, \(r\)-\(\ell\)WL can count cycles of arbitrary length, as shown in the following statement.

**Theorem 1**.: _For any \(r\geq 1\), \(r\)-\(\ell\)WL can subgraph-count all cycles with at most \(r+2\) nodes._

Since \(3\)-WL cannot subgraph-count any cycle with more than \(7\) nodes, Theorem 1 implies that \(6\)-\(\ell\)WL is not less powerful than \(3\)-WL. This observation generalizes to any \(k\)-WL, as shown next.

**Corollary 1**.: _Let \(k\in\mathbb{N}\). There exists \(r\in\mathbb{N}\), such that \(r\)-\(\ell\)WL is not less powerful than \(k\)-WL. Specifically, \(r\in\mathcal{O}(k^{2})\), with \(r\leq\frac{k(k+1)}{2}-2\) for even \(k\) and \(r\leq\frac{(k+1)^{2}}{2}-2\) for odd \(k\)._

The \(r\)-\(\ell\)WL color refinement algorithm surpasses the limits of the \(k\)-WL hierarchy while only using local computation. This is particularly important since already \(3\)-WL is computationally infeasible, whereas our method can scale efficiently to higher orders if the graphs are sparse, which is commonly the case in real-world applications.

### Homomorphism Expressivity

The following section unveils a close connection between the expressivity of \(r\)-\(\ell\)WL and cactus graphs (Harary et al., 1953), a significant class between trees and graphs with tree-width 2.

**Definition 8**.: _A cactus graph is a graph where every edge lies on at most one simple cycle. For \(r\geq 2\), an \(r\)-cactus graph is a cactus where every simple cycle has at most \(r\) vertices. We denote by \(\mathcal{M}\) the set of all cactus graphs, and by \(\mathcal{M}^{r}\) the set of all \(q\)-cactus graphs for \(q\leq r\)._

Figure 6 shows two examples of cactus graphs. From the expressivity perspective, the ability to homomorphism-count cactus graphs establishes a lower bound strictly between the homomorphism-counting capabilities of 1-WL and 3-WL (Neuen, 2024), as cactus graphs are a strict superset of all trees and a strict subset of all graphs of treewidth two. With this in mind, we are now ready to present our significant result on the homomorphism expressivity of our \(r\)-\(\ell\)WL algorithm.

**Theorem 2**.: _Let \(r\geq 0\). Then, \(r\)-\(\ell\)WL can homomorphism-count \(\mathcal{M}^{r+2}\)._We refer to Appendix G for a detailed proof of Theorem 2, which is fairly involved and requires defining canonical tree decompositions of cactus graphs and unfolding trees of \(r\)-\(\ell\)WL. Demonstrating their strong connection, we then follow the approach in (Dell et al., 2018; B. Zhang et al., 2024) to decompose homomorphism counts of cactus graphs. In fact, we prove a more general result, showing that \(r\)-\(\ell\)WL can count all _fan-cactus graphs_, see Appendix G for more details.

The class \(\mathcal{M}^{2}\) contains only forests; hence, Theorem 2 implies the standard results on the ability of \(1\)-WL to count forests. Since forests are the only class of graphs \(1\)-WL can count, Theorem 2 implies that \(r\)-\(\ell\)WL is always strictly more powerful than \(1\)-WL, corroborating the claim in Proposition 1.

The implications of Theorem 2 are profound: it establishes that \(r\)-\(\ell\)WL can homomorphism-count a large class of graphs. Specifically, Theorem 2 provides a quantitative expressivity measure that enables comparison of \(r\)-\(\ell\)WL's expressivity with other WL variants and GNNs. This comparison is achieved by examining the range of graphs that \(r\)-\(\ell\)WL can homomorphism-count against those countable by other models, as detailed in works by Barcelo et al. (2021) and B. Zhang et al. (2024). For instance, B. Zhang et al. (2024) showed that Subgraph GNNs (Bevilacqua et al., 2021; You et al., 2021; Frasca et al., 2022; Huang et al., 2022) are limited to homomorphism-count graphs with end-point shared NED. Hence, Subgraph GNNs can not homomorphism-count \(F=\left\{\begin{array}{c}\includegraphics[height=14.226378pt]{images/b.eps} \end{array}\right\}\), while \(1\)-\(\ell\)WL can. Based on this, we can identify pairs of graphs that \(1\)-\(\ell\)WL can distinguish but Subgraph GNNs cannot. We summarize these and other implications of Theorem 2 in the following corollary.

**Corollary 2**.: _Let \(r\in\mathbb{N}\setminus\{0\}\). Then,_

1. \(r\)_-_\(\ell\)WL _is more powerful than_ \(\mathcal{F}\)_-Hom-GNNs, where_ \(\mathcal{F}=\{C_{3},\ldots,C_{r+2}\}\)_._
2. \(1\)_-_\(\ell\)WL _is not less powerful than Subgraph GNNs. In particular, any_ \(r\)_-_\(\ell\)WL _can separate infinitely many graphs that Subgraph GNNs fail to distinguish._
3. _For any_ \(k>0\)_,_ \(1\)_-_\(\ell\)WL _is not less powerful than Subgraph_ \(k\)_-GNNs. In particular, any_ \(r\)_-_\(\ell\)WL _can separate infinitely many graphs that Subgraph_ \(k\)_-GNNs fail to distinguish._
4. \(r\)_-_\(\ell\)WL _can subgraph-count all graphs_ \(F\) _such that_ \(\operatorname{\mathrm{spasm}}(F)\subset\mathcal{M}^{r+2}\)_, where_ \(\operatorname{\mathrm{spasm}}(F)\coloneqq\left\{H\in\mathcal{G}\mid\exists \text{ surjective }h\in\operatorname{Hom}(F,H)\right\}\)_. In particular, if_ \(1\leq r\leq 4\)_, then_ \(r\)_-_\(\ell\)WL _can subgraph-count all paths up to length_ \(r+3\)_._

A detailed explanation of Subgraph (\(k\)-)GNNs, \(\mathcal{F}\)-Hom-GNNs, along with the proofs of Corollary 2, can be found in Appendix H. Finally, we note that Theorem 2 states a loose lower bound on the homomorphism expressivity of \(r\)-\(\ell\)WL. This observation opens the avenue for future research to explore tight lower bounds, or upper bounds, on the homomorphism expressivity of \(r\)-\(\ell\)WL.

## 6 Loopy Message Passing

In this section, we build a GNN emulating \(r\)-\(\ell\)WL.

**Definition 9**.: _For \(t\in\{0,\ldots,T-1\}\) and \(k\in\{0,\ldots,r\}\), \(r\)-\(\ell\)MPNN applies the following message, update and readout functions:_","\begin{split} m_{k}^{(t+1)}(v)&=f_{k}^{(t+1)}( \{\{c_{k}^{(t)}(\mathbf{p})\mid\mathbf{p}\in\mathcal{N}_{k}(v) \}\}),\\ c_{r}^{(t+1)}(v)&=g^{(t+1)}(c_{r}^{(t)}(v),\,m_{0 }^{(t+1)}(v),\ldots,m_{r}^{(t+1)}(v)),\end{split}",,"c_{r}^{(t+1)}(v)\gets\mathrm{ReLU}\left(\sum_{k=0}^{r}W_{k}^{(t)}\cdot\{\{c_{r}^{(t)}(\mathbf{p})\mid\mathbf{p}\in\mathcal{N}_{k}(v)\}\}\right),",The color update for $r$-$\ell$MPNN aggregates multisets of path colors from $k$-neighborhoods using learnable weights and ReLU activation.
neurips_2024_oral_59,7,"_and final readout layer \(c_{r}^{(T)}(G)=h\left(\left\{\left\{c_{r}^{(T)}(v)\mid v\in V(G)\right\} \right\}\right)\)._

In the following statement, we link the expressive power of \(r\)-\(\ell\)MPNN and \(r\)-\(\ell\)WL.

**Theorem 3**.: _For fixed \(t,r\geq 0\), \(t\) iterations of \(r\)-\(\ell\)WL are more powerful than \(r\)-\(\ell\)MPNN with \(t\) layers. Conversely, \(r\)-\(\ell\)MPNN is more powerful than \(r\)-\(\ell\)WL if the functions \(f^{(t)},g^{(t)}\) in (3) are injective._

The previous result derives conditions under which \(r\)-\(\ell\)MPNN is as expressive as \(r\)-\(\ell\)WL. To implement \(r\)-\(\ell\)MPNN in practice, we choose suitable neural layers for \(f_{k}^{(t)},g^{(t)}\), and \(h\) in Definition 9. As a consequence of (Xu et al., 2019, Lemma 5), the aggregation function in (3) can be written as","f_{k}^{(t+1)}(\{\{c_{k}^{(t)}(\mathbf{p})\mid\mathbf{p}\in \mathcal{N}_{k}(v)\}\}):=f(\sum_{\mathbf{p}\in \mathcal{N}_{k}(v)}g(\mathbf{p})),",,c_{r}^{(T)}(G)=h\left(\left\{\left\{c_{r}^{(T)}(v)\mid v\in V(G)\right\} \right\}\right),"Final graph representation after T iterations of r-loop Message Passing Neural Network, obtained by applying a readout function h to the multiset of node colors."
neurips_2024_oral_59,8,"for suitable functions \(f,g\). Since 1-WL is injective on forests (Arvind et al., 2015), hence on paths, and since GIN can approximate 1-WL (Xu et al., 2019), we choose \(f=\mathrm{MLP}\) and \(g=\mathrm{GIN}\). Hence, \(r\)-\(\ell\)GIN is defined as an \(r\)-\(\ell\)MPNN that updates node features via",x_{r}^{(t+1)}(v):=\mathrm{MLP}(x_{r}^{(t)}(v)+(1+\varepsilon_{0})\sum_{u \in\mathcal{N}_{0}(v)}x_{r}^{(t)}(u)+\sum_{k=1}^{r}(1+\varepsilon_{k})\sum_{ \mathbf{p}\in\mathcal{N}_{k}(v)}\mathrm{GIN}_{k}(\mathbf{p})).,,"c_{r}^{(t+1)}(v)=\mathrm{MLP}^{(t+1)}\left(c_{r}^{(t)}(v)\oplus\bigoplus_{k=0}^{r}\sum_{\mathbf{p}\in\mathcal{N}_{k}(v)}\mathrm{GIN}^{(t+1)}(c_{r}^{(t)}(\mathbf{p}))\right),",Node feature update in $r$-$\ell$GIN using MLP and GIN layers to aggregate path information from $k$-neighborhoods.
neurips_2024_oral_6,1,"## 2 Background

Our work builds on and extends differentiable logic gate networks [7]. To recap, logic gate networks (LGNs) are networks of nodes that are binary logic gates like AND, NAND, or XOR. LGNs are also known as binary circuits or logical circuits, and are the format in which any digital hardware is implemented on the lowest pre-transistor abstraction level. The function that an LGN computes depends on the choices of logic gates that form its nodes and how these nodes are connected. Optimizing an LGN requires choosing the connections and deciding on a gate for each node. A primary challenge when optimizing LGNs is that they are, by default, non-differentiable, preventing gradient descent-based training, making this problem conventionally a combinatorial problem. However, when applied to machine learning problems, solving the combinatorial problem conventionally becomes infeasible as we require millions of parameters or gates. Thus, a differentiable relaxation of randomly connected LGNs has been proposed, which allows training LGNs with gradient descent [7], overcoming the exponential difficulty of optimizing LGNs. In the remainder of this section, we cover the structure, relaxation, and training of differentiable LGNs, which we also illustrate in Figure 2.

StructureLGNs follow a layered structure with each layer comprising a number of nodes, each comprising one logic gate (3 layers with 4 logic gates each in Fig. 2). As logic gates are inherently non-linear, LGNs do not require any activation functions. Further, LGNs do not have any weights nor any biases as they do not rely on matrix multiplications. Due to the binary (i.e., two-input) nature of the nodes, LGNs are necessarily sparse and cannot form fully-connected networks. The connectivity between nodes has so far been (fully) randomly selected, which works well for easier tasks but can become problematic if there is inherent structure in the data as, e.g., in images. During training, the connections remain fixed and the learning task comprises the choice of logic gate at each node.

Differentiable RelaxationTo learn the choices of logic gates for each node with gradient descent requires the network to be differentiable; however, the LGN is by default not differentiable for two reasons: (i) Because a logic gate computes a discrete function of its (Boolean) inputs, it is not differentiable. (ii) Because the choice of a logic gate is not a continuous parameter, but a discrete

Figure 2: Architecture of a randomly connected LGN. Each node corresponds to one logic gate. During training, the distribution over choices of logic gates (bottom, 16 options) is learned for each node.

decision, it is not differentiable. Petersen _et al._[7] propose to differentiably relax each logic gate to real-valued logic via probabilistic logic [10], [11]. For example, a logical _and_ (\(a_{1}\wedge a_{2}\)) is relaxed to \(a_{1}\cdot a_{2}\) and a logical _exclusive or_ (\(a_{1}\oplus a_{2}\)) is relaxed to \(a_{1}+a_{2}-2\cdot a_{1}\cdot a_{2}\), which corresponds to the output probability when considering two independent Bernoulli variables with coefficients \(a_{1},a_{2}\). To make the choice of logic gate learnable, Petersen _et al._[7] introduce a probability distribution over the 16 possible logic gates (\(\mathcal{S}\)), which is encoded as the softmax of 16 trainable parameters. For a trainable parameter vector \(\mathbf{z}\in\mathbb{R}^{16}\) and all 16 possible logic gate operations as \(g_{0},...,g_{15}\), the differentiable logic gate as the expectation over its outputs can be computed in closed-form as","f_{\mathbf{z}}(a_{1},a_{2})=\mathbb{E}_{i\sim\mathcal{S}(\mathbf{z})\,,\,A_{1} \sim\mathcal{B}(a_{1})\,,\,A_{2}\sim\mathcal{B}(a_{2})}\Big{[}g_{i}(A_{1},A_{2 })\Big{]}=\sum_{i=0}^{15}\frac{\exp(z_{i})}{\sum_{j}\exp(z_{j})}\cdot g_{i}(a_ {1},a_{2})\,.",,"\begin{equation}
    \bar{g}(\mathbf{a}) = \sum_{i=0}^{15} \sigma(\mathbf{z})_i \cdot g_i(\mathbf{a}),
\end{equation}","Equation 1 defines the differentiable logic gate as a convex combination of all possible logic gate operations, where the weights are given by the softmax of a trainable parameter vector z."
neurips_2024_oral_6,2,"With these two ingredients, logic gate networks become end-to-end differentiable.

Initialization, Training, and DiscretizationTraining differentiable logic gate networks corresponds to learning the parameters inducing the probability distributions over possible gates. The parameter vector \(\mathbf{z}\) for each node has so far been initialized with a standard Gaussian distribution. The connections are randomly initialized and remain fixed during training. For classification tasks, each class is associated with a set of neurons in the output layer and active neurons in each set are counted composing a class score (group sum, right part of Fig. 2). After dividing them by a temperature \(\tau\), the class scores are used as logits in a softmax cross-entropy loss. Differentiable LGNs perform best when trained with the Adam optimizer [12]. Empirical evidence showed that the softmax distributions typically converge to concrete choices of logic gates. Thus, differentiable LGNs can be discretized to hard LGNs for deployment on hardware by selecting the logic gate with the largest probability. This discretization process incurs only a minimal loss in accuracy compared to the differentiable LGN [7].

LimitationsDifferentiable LGNs have shown significant limitations wrt. the available architectural components. Previously, they did not provide the option to capture local spatial patterns as they were randomly connected and only operated on flattened inputs [7]. Further, they previously performed well only up to a depth of \(6\) layers [7]. Thus, more complex relationships between inputs cannot be modeled. Finally, while they provide SOTA performance, differentiable LGNs are very computationally expensive to train, e.g., a vanilla 5 million gate network required 90 hours on an A6000 GPU [7]. In the following, we address these limitations by introducing convolutional logic tree layers, logical or pooling, residual initializations, as well as computational considerations for scaling.

## 3 Convolutional Logic Gate Networks

Convolutional neural networks (CNNs) have experienced tremendous success, being a core contributor to the current machine learning ascendancy starting with their progress on the ImageNet classification challenge in 2012 [13]. Underlying CNNs is the discrete convolution of an input tensor \(\mathbf{A}\) (e.g., an input image or hidden activations) and a linear function / kernel \(\mathbf{W}\), denoted as \(\mathbf{A}*\mathbf{W}\). CNNs are especially effective in vision tasks due to the equivariance of the convolution, which allows the network to generalize edge, texture, and shapes in different locations by sharing the parameters at all placements. However, existing differentiable LGN methods do not support convolutions.

In this work, we propose to convolve activations \(\mathbf{A}\) with differentiable binary logic gate trees. While we could convolve \(\mathbf{A}\) with an individual logic gate, we observe that actually convolving \(\mathbf{A}\) with a (deep) logic gate network or tree leads to substantially better performance as it allows for greater expressivity of the model. Similar to how the inputs to each logic gate are randomly initialized and remain fixed in conventional differentiable LGNs, we randomly construct the connections in our

Figure 3: Conventional convolutional neural networks (a) compared to convolutional logic gate networks (b). The images illustrate the first and second to last kernel placements. The nodes correspond to weighted sums (a), and binary logic gates \(f_{1},f_{2},f_{3}\) (b), respectively. The weights / choices of logic gates are shared between kernel placements. For visual simplicity, only a single input channel and kernel (output channel) is displayed.

logic gate tree kernel function. However, we need to put additional restrictions on the connections for logic gate network kernels. Specifically, we construct each logic gate network kernel as a complete binary tree of depth \(d\) with logic gates as nodes and binary input activations as leaves. The output of the logic gate operation is then the input to the next higher node, etc. To capture spatial patterns, we select the inputs / leaves of the tree from the predefined receptive field of the kernel of size \(s_{h}\times s_{w}\). Based on the depth of the tree, we randomly select as many inputs as necessary. For example, we could construct a binary tree of depth \(d=2\), which means that we need to randomly select \(2^{d}=4\) inputs from our receptive field, e.g., of size \(64\times 3\times 3\), which corresponds to \(64\) input channels with a kernel size of \(3\times 3\). This tree structure allows to capture fixed spatial patterns and correlations beyond pair-wise binary inputs. Further, it extends the concept of spatial equivariance to LGNs as such trees can be used as kernel filters, capturing general patterns in different locations. Using trees of logic gates instead of individual logic gates also has the advantage of reducing memory accesses and improving training and inference efficiency. We remark that, as we apply convolution, the parameterization of each node is shared between all placements of the kernel (which contrasts convolution from mere local connectivity.) In Figure 3, we illustrate the difference between conventional CNN models and convolutional logic gate networks.

During training, the network learns which logic gate operation to choose at each node. Thus, each logic tree kernel is parameterized via the choices of each of the \(2^{d}-1\) logic gates, which are learnable. For a logic kernel of depth \(2\), we call these logic gates \(f_{1},f_{2},f_{3}\) (or more formally \(f_{\mathbf{z}_{1}},f_{\mathbf{z}_{2}},f_{\mathbf{z}_{3}}\) for parameter vectors \(\mathbf{z}_{1},\mathbf{z}_{2},\mathbf{z}_{3}\) corresponding to Equation 1). Given input activations \(a_{1},a_{2},a_{3},a_{4}\), the kernel is expressed as a binary tree of these logic gates:","f_{3}(\,f_{1}(a_{1},a_{2}),f_{2}(a_{3},a_{4})\,).",,"f_{\mathbf{z}_{1},\mathbf{z}_{2},\mathbf{z}_{3}}(a_{1},a_{2},a_{3},a_{4}) = f_{\mathbf{z}_{1}}\left(f_{\mathbf{z}_{2}}(a_{1},a_{2}), f_{\mathbf{z}_{3}}(a_{3},a_{4})\right).","This equation defines the output of a depth-2 logic gate tree kernel, composed of three differentiable logic gates $f_{\mathbf{z}_1}, f_{\mathbf{z}_2}, f_{\mathbf{z}_3}$, applied hierarchically to four input activations $a_1, a_2, a_3, a_4$."
neurips_2024_oral_6,3,"For an input \(\mathbf{A}\) of shape \(m\times h\times w\) (\(m\) input channels; height; width) and connection index tensors \(\mathbf{C}_{M},\mathbf{C}_{H},\mathbf{C}_{W}\)1, each of shape \(n\times 4\) (\(n\) tree kernels / channels; \(4\) inputs per tree), the output is

Footnote 1: \(\mathbf{C}_{M}\in\{1,...,m\}^{n\times 4}\) indicates which out of \(m\) input channels is selected; \(\mathbf{C}_{H}\in\{1,...,s_{h}\}^{n\times 4}\) and \(\mathbf{C}_{W}\in\{1,...,s_{w}\}^{n\times 4}\) indicate the selected position inside of the receptive field of size \(s_{h}\times s_{w}\).","\mathbf{A}^{\prime}[k,i,j]=f_{3}^{k}\big{(}f_{1}^{k}\big{(}\mathbf{A}\big{[} \mathbf{C}_{M}[k,\!1]\!,\mathbf{C}_{H}[k,\!1]\!+\!i,\mathbf{C}_{W}[k,\!1]\!+\! j\big{]},\mathbf{A}\big{[}\mathbf{C}_{M}[k,\!2]\!,\mathbf{C}_{H}[k,\!2]\!+\!i, \mathbf{C}_{W}[k,\!2]\!+\!j\big{]}\big{)}, || \qquad\qquad\qquad f_{2}^{k}\big{(}\mathbf{A}\big{[}\mathbf{C}_{M} [k,\!3]\!,\mathbf{C}_{H}[k,\!3]\!+\!i,\mathbf{C}_{W}[k,\!3]\!+\!j\big{]}, \mathbf{A}\big{[}\mathbf{C}_{M}[k,\!4]\!,\mathbf{C}_{H}[k,\!4]\!+\!i,\mathbf{C} _{W}[k,\!4]\!+\!j\big{]}\big{)}\big{)}",,"\mathbf{Y}_{k} = \sum_{i=0}^{15} \frac{\exp(z_{k,i})}{\sum_{j=0}^{15} \exp(z_{k,j})} \cdot g_{i}(\mathbf{A}_{\mathbf{C}_{M}[k,1], \mathbf{C}_{H}[k,1], \mathbf{C}_{W}[k,1]}, \mathbf{A}_{\mathbf{C}_{M}[k,2], \mathbf{C}_{H}[k,2], \mathbf{C}_{W}[k,2]}),",Equation 3 defines the output of a logic gate tree kernel as a softmax-weighted expectation over all 16 possible binary logic operations applied to selected input activations from the receptive field.
neurips_2024_oral_60,1,"### Self-Decoder

Self-decoder takes embeddings \(X^{0}\) as input and compute intermediate vector representation \(X^{\nicefrac{{L}}{{2}}}\):",Y^{l} =\mathrm{ESA}(\mathrm{LN}(X^{l}))+X^{l} || X^{l+1} =\mathrm{SwiGLU}(\mathrm{LN}(Y^{l}))+Y^{l},,"\begin{equation}
    X^{\nicefrac{L}{2}} = \text{Self-Decoder}(X^{0})
\end{equation}",Intermediate vector representation computed by the Self-Decoder from initial embeddings $X^{0}$
neurips_2024_oral_60,2,"\). Causal masking is used for efficient self-attention.

The key property of the efficient self-attention module is \(\mathcal{O}(1)\) inference memory, i.e., constant number of KV caches. For example, the cache size of sliding-window attention [4] depends on the window size instead of the input length. More design choices (e.g., gated retention) of the efficient self-attention module are detailed in Section 4.

### Cross-Decoder

First, the output of the self-decoder \(X^{\nicefrac{{L}}{{2}}}\) generates global KV caches \(\hat{K},\hat{V}\) for cross-decoder:","\hat{K}=\mathrm{LN}(X^{\nicefrac{{L}}{{2}}})W_{K},\quad\hat{V}=\mathrm{LN}(X^ {\nicefrac{{L}}{{2}}})W_{V}","where \(W_{K},W_{V}\in\mathbb{R}^{d\times d}\) are learnable",|\mathcal{C}| = \mathcal{O}(1),"The efficient self-attention module maintains constant KV cache size independent of input length, with sliding-window attention [4] as a specific example where cache size scales with window size rather than sequence length."
neurips_2024_oral_60,3,"le. Then, cross-decoder layers are stacked after self-decoder to obtain the final output \(X^{L}\). The KV caches \(\hat{K},\hat{V}\) are reused by all the \(\frac{L}{2}\) cross-decoder modules:","Q^{l} =\mathrm{LN}(X^{l})W_{Q}^{l} || Y^{l} =\mathrm{Attention}(Q^{l},\hat{K},\hat{V})+X^{l} || X^{l+1} =\mathrm{SwiGLU}(\mathrm{LN}(Y^{l}))+Y^{l}",,"Y^{l} =\mathrm{ECA}(\mathrm{LN}(X^{l}),\hat{K},\hat{V})+X^{l}","where ECA denotes efficient cross-attention that reuses the global KV caches $\hat{K}, \hat{V}$"
neurips_2024_oral_60,4,"ix. Causal masking is also used for cross-attention. Because cross-attention is compatible with group query attention [1], we can further save the memory consumption of KV caches.

Figure 1: Overview of the decoder-decoder architecture. Self-decoder generates the global KV cache. Then cross-decoder employs cross-attention to reuse the shared KV caches. Both self-decoder and cross-decoder use causal masking. The overall architecture behaves like a decoder-only Transformer, autoregressively generating tokens.

### Inference Advantages

In addition to competitive language modeling results, YOCO significantly reduces serving costs and improves inference performance. We report detailed inference comparisons in Section 5.4.

Saving GPU Memory and Serving More Tokens.Table 1 compares the memory complexity between Transformers and YOCO. Specifically, because global KV caches are reused and efficient self-attention needs constant caches, the number of caches is \(\mathcal{O}(N+CL)\), where \(N\) is the input length, \(C\) is a constant (e.g., sliding window size), and \(L\) is the number of layers. For long sequences, \(CL\) is much smaller than \(N\), so about \(\mathcal{O}(N)\) caches are required, i.e., you only cache once. In comparison, Transformer decoders have to store \(N\times L\) keys and values during inference. So YOCO roughly saves \(L\) times GPU memory for caches compared to Transformer. Because the bottleneck of inference capacity becomes KV caches, our method enables us to serve many more tokens without being out of GPU memory. The increased batch size is also beneficial to inference throughput.

Reducing Prefilling Time and Improving Throughput.As shown in Figure 2, because the cross-decoder reuses the outputs of self-decoder, we can exit early before entering the cross-decoder during the prefill stage. The intriguing property of computation dependency greatly accelerates the prefilling speed. First, only half the layers are needed for forward computation, i.e., at least half prefilling latency reduction. Second, the efficient attention modules of the self-decoder are usually fast. For the example of 512K context length, we can decrease the prefilling latency from 180 seconds (Transformer with optimized inference, such as Flash-Decoding) to less than 6 seconds (Figure 9). Even for 32K length, YOCO has about three times speedup in terms of prefilling time. Table 2 compares prefilling time complexity of attention modules between Transformer and YOCO.

## 4 Design Choices of Self-Decoder

We can choose various efficient self-attention methods for self-decoder. As long as the module only requires constant inference memory, the self-decoder's cache memory complexity depends on the number of layers. Moreover, a good module choice improves both training and deployment costs. In this work, we use sliding-window attention (Section 4.1) or gated retention (Section 4.2).

### Sliding-Window Attention

Sliding-window attention [4] restricts the attention range into a fixed window size \(C\). In contrast, vanilla Transformer decoders attend to all previous tokens. During inference, the KV cache memory complexity can be reduced from \(\mathcal{O}(N)\) to \(\mathcal{O}(C)\), i.e., the memory usage is constant rather than increasing with sequence length. Similar to multi-head self-attention [40], we compute the output of

\begin{table}
\begin{tabular}{l c} \hline \hline \multicolumn{2}{c}{**KV Cache Memory**} \\ \hline Transformer & \(\mathcal{O}(LND)\) \\ YOCO & \(\mathcal{O}((N+L)D)\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Inference memory complexity of KV caches. \(N,L,D\) are the sequence length, number of layers, and hidden dimension.

\begin{table}
\begin{tabular}{l c} \hline \hline \multicolumn{2}{c}{**Prefilling Time**} \\ \hline Transformer & \(\mathcal{O}(LN^{2}D)\) \\ YOCO & \(\mathcal{O}(LND)\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Prefilling time complexity of attention modules. \(N,L,D\) are the same as above.

Figure 2: YOCO Inference. **Prefill**: encode input tokens in parallel. **Generation**: decode output tokens one by one. The computation flow enables prefilling to early exit without changing the final output, thereby significantly speeding up the prefill stage.

sliding-window attention via:","\begin{split} Q=XW_{Q},\quad K=XW_{K},\quad V=XW_{V}\\ \mathrm{head}_{i}=\mathrm{softmax}(Q_{[i]}K_{[i]}^{\intercal}+B)V, \quad B_{ij}=\begin{cases}0,&i-C<j\leq i\\ -\infty,&\text{otherwise}\end{cases}\\ \mathrm{SWA}(X)=\mathrm{Concat}(\mathrm{head}_{1},\cdots,\mathrm{head}_{h})W_ {O}\end{split}","where \(W_{Q},W_{K},W_{V},W_{O}\in\mathbb{R}^{d\times d}\) are learnable matrices, and the window causal mask \(B\) controls each query only attends to the previous keys whose distances are less than \(C\)",Y^{l} =\mathrm{ESA}(\mathrm{LN}(X^{l}))+X^{l},"Sliding-window attention computes output using a fixed window size, reducing KV cache memory complexity to $\mathcal{O}(C)$ during inference."
neurips_2024_oral_60,5,"\). The pre-normalization and residual connection are also applied to the module.

### Gated Retention

Gated retention (gRet, aka gRetNet) augments retention [35] with a data-dependent gating mechanism. We use gRet as the default efficient self-attention module. The method unifies the parallel, recurrent, and chunkwise recurrent computation paradigms, which are equivalent and can obtain the same computation results. The training process usually uses the parallel or chunkwise recurrent paradigms, while the inference stage can employ the recurrent paradigm for constant KV memory.

The Parallel RepresentationThe gated retention is defined as:","\begin{split} Q=(XW_{Q})\odot\Theta,\quad K=(XW_{K})\odot \overline{\Theta},\quad V=XW_{V},\quad\Theta_{n}=e^{in\theta}\\ \gamma=\mathrm{sigmoid}(XW_{\gamma})^{1/\tau},\quad D_{nm}= \begin{cases}\prod\nolimits_{i=m+1}^{n}\gamma_{i},&n\geq m\\ 0,&n<m\end{cases}\\ \mathrm{gRet}(X)=(QK^{\intercal}\odot D)V\end{split}","where \(W_{Q},W_{K},W_{V}\in\mathbb{R}^{d\times d}\) and \(W_{\gamma}\in\mathbb{R}^{d\times 1}\) are learnable weights, and the temperature term \(\tau\) encourages \(\gamma\) to 1 for better memorization [46]","\begin{split} Q = XW_Q, \quad K = XW_K, \quad V = XW_V \\ \mathrm{GatedRet}(X) = \mathrm{Concat}_{h=1}^{H} \left( \sigma(Q_h \odot K_h^{\top}) V_h \right) W_O \end{split}","Parallel computation of gated retention, where $\sigma$ denotes a learnable activation function, $\odot$ represents element-wise multiplication, and $W_O$ is the output projection matrix."
neurips_2024_oral_60,6,"6]. The data-controlled decay is head-wise [19] rather than element-wise so that the computation can fully utilize NVIDIA tensor cores. Refer to [35] for more details about the other designs.

The Recurrent RepresentationBeing equivalent to Equation (5), the output of gated retention can be computed recurrently. For the \(n\)-th timestep, the output is obtained via:","\begin{split} S_{n}=\gamma_{n}S_{n-1}+K_{n}^{\intercal}V_{n}\\ g\mathrm{Ret}(X_{n})=Q_{n}S_{n},\quad n=1,\cdots,|x|\end{split}","where \(Q,K,V,\gamma\) are the same as in Equation (5)",\begin{split} h_{n}=\gamma_{n}h_{n-1}+K_{n-1}V_{n-1}^{\intercal}\\ \mathrm{gRet}(X)_{n}=Q_{n}h_{n}\end{split},"where $h_n$ is the recurrent state at timestep $n$, updated using the decay factor $\gamma_n$ and the previous key-value outer product."
neurips_2024_oral_60,7,"5). During auto-regressive inference, the self-decoder maintains \(S_{n}\) as the intermediate state for an efficient generation.

The Chunkwise Recurrent RepresentationThe chunk-wise representation is a unified formulation of recurrent and parallel representations. Given chunk size \(B\), the outputs are computed chunk by chunk. The computation is divided into inner-chunk and cross-chunk parts. Denote \([i]\) as the \(i\)-th chunk, i.e., \(x_{[i]}=x_{(i-1)B+1},\cdots,x_{iB}\), we compute the \(i\)-th chunk as:","\begin{split}\beta_{(i-1)B+j}&=\prod\limits_{k=(i- 1)B+1}^{(i-1)B+j}\gamma_{k},\quad D_{[i]}(j,k)=\frac{\beta_{(i-1)B+k}}{\beta_{ (i-1)B+j}}\ \ \mathrm{if}\ \ j\leq k\ \ \mathrm{else}\ \ 0\\ R_{i}&=K_{[i]}^{\intercal}(V_{[i]}\odot\frac{ \beta_{iB}}{\beta_{[i]}})+\beta_{iB}R_{i-1},\ \ \beta_{[i]}(j,k)=\beta_{(i-1)B+j}\\ \mathrm{gRet}(X)&=\underbrace{(Q_{[i]}K_{[i]}^{ \intercal}\odot D_{[i]})V_{[i]}}_{\text{Inner-Chunk}}+\underbrace{(Q_{[i]}R_{i -1})\odot\beta_{[i]}}_{\text{Cross-Chunk}}\end{split}","where \(R_{i}\) is the intermediate state of the \(i\)-th chunk, and \(\beta\) summarizes the data-controlled decay \(\gamma\)","\begin{split} Y_{[i]} = \mathrm{SWA}(X_{[i]}) + \mathrm{gRet}_{\text{cross}}(X_{[i]}, S_{(i-1)B})\\ S_{iB} = \gamma_{iB}S_{(i-1)B} + \sum_{j=(i-1)B+1}^{iB} K_j^\intercal V_j \end{split}","Chunk-wise recurrent computation splits sequence into chunks of size B, where inner-chunk outputs use sliding-window attention (SWA) and cross-chunk dependencies update state S through data-controlled decay γ."
neurips_2024_oral_60,8,"\). Appendix B proves the equivalence between the computation paradigms. The chunkwise paradigm combines the best of parallelism and recurrence, i.e., saving FLOPs compared to fully parallel computation and reducing iterations compared to recurrent computation. During the training and prefill stages, the chunk-wise representation increases throughput and reduces GPU memory consumption.

Multi-Head Gated RetentionSimilar to multi-head attention [40] and multi-scale retention [35], we apply gated retention to each head and combine the outputs together:","\mathrm{head}_{i} =\mathrm{gRet}(X) || Y =\mathrm{GroupNorm}_{h}(\mathrm{Concat}(\mathrm{head}_{1},\cdots, \mathrm{head}_{n})) || \mathrm{MHGR}(X) =(\mathrm{swish}(XW_{G})\odot Y)W_{O}",,"\mathrm{MgRet}(X)=\mathrm{Concat}(\mathrm{gRet}_{1}(X),\cdots,\mathrm{gRet}_{h}(X))W_{O}","where $W_{O}\in\mathbb{R}^{hd\times d}$ is a learnable projection matrix, and each head uses different learnable parameters"
neurips_2024_oral_8,1,"### Model Fusion Pruning on Edge Devices

In this subsection, we present the design of the MFP module employed by DapperFL. The goal of the MFP module is to tailor the footprint of the local model for edge devices in the presence of domain shifts in the local data, thereby addressing the system heterogeneity problem. Inspired by the spirit of the transfer learning [42; 43; 44], MFP fuses the global model \(\mathcal{W}^{t-1}\) into the fine-tuned local model \(\hat{\mathbf{w}}_{i}^{t}\) to learn the cross-domain knowledge. This avoids over-fitting the model to the local domain while enhancing the generality of the model. After that, MFP calculates a binary mask matrix \(\mathbf{M}_{i}^{t}\) to generate the pruned local model. The detailed pruning process is described in Algorithm 1.

Specifically, in the initial epoch of each communication round \(t\in[1,T]\), the MFP module first fine-tunes the global model \(\mathcal{W}^{t-1}\) on local data \(\mathcal{D}_{i}\) to produce local model \(\hat{\mathbf{w}}_{i}^{t}\) (in line 1). We utilize one epoch of local training to determine the pruned models for the following reasons: 1) Additional local epochs do not significantly enhance the model's performance, which justifies the use of a single epoch for efficiency. As noted in [45], experiments have demonstrated that extending local training beyond one epoch yields results comparable to those achieved with just one epoch. 2) In previous domain generalization-related FL research, such as [45], one epoch is also employed to collect local domain information. This method has proven adequate for capturing essential features and domain characteristics. 3) Pioneering research in model design and neural architecture search, such as [46],

Figure 2: Overview of DapperFL framework with two clients for each communication round.

has demonstrated that a few epochs are sufficient to obtain a coarse estimate of a sub-model. This approach is effective in quickly assessing model configurations without extensive computation.

Next, the MFP module fuses the global model \(\mathcal{W}^{t-1}\) into the local model \(\hat{\mathbf{w}}_{i}^{t}\) to embed the cross-domain information into the local model (in line 2). The fusion operation is formulated as follows:","\mathbf{w}_{i}^{t}=\alpha^{t}\mathcal{W}^{t-1}+(1-\alpha^{t})\hat{\mathbf{w}}_{i}^{t},",where \(\alpha^{t}\) is the fusion factor used to control the quality of the fused global model \(\mathcal{W}^{t-1}\),\tilde{\mathbf{w}}_{i}^{t} = \lambda \mathcal{W}^{t-1} + (1 - \lambda) \hat{\mathbf{w}}_{i}^{t},Equation 1 represents the fusion of the global
neurips_2024_oral_8,2,"\). Considering that the local data on the client is typically limited, resulting to updating at the beginning of FL requires more guidance from the global model by exploring the commonalities across different domains. As the FL process proceeds, the local model is capable of learning more domain-dependent information from itself. Thus, we design a dynamic adjusting mechanism to modify the \(\alpha^{t}\) during the FL training. The \(\alpha^{t}\) is initialized with a relatively large value \(\alpha_{0}\) and decreases as the FL process proceeds. The decreased speed is controlled by a sensitivity factor \(\epsilon\). We assign a minimum value \(\alpha_{min}\) for \(\alpha^{t}\) to ensure the local model will consistently learn the knowledge from other domains. Formally, the dynamic adjusting mechanism for \(\alpha^{t}\) is described as follows:","\alpha^{t}=\max\{(1-\epsilon)^{t-1}\alpha_{0},\alpha_{min}\}.",,"\alpha^{t} = \max\left(\alpha_{0} \cdot e^{-\epsilon \cdot t}, \alpha_{\min}\right)","Dynamic adjustment of the fusion factor $\alpha^{t}$ based on the FL round $t$, ensuring a balance between global and local knowledge."
neurips_2024_oral_8,3,"Subsequently, the MFP module calculates a binary mask matrix \(\mathbf{M}_{i}^{t}\in\{0,1\}^{|\mathbf{w}_{i}^{t}|}\) for pruning the fused local model \(\mathbf{w}_{i}^{t}\) (in line 3). The matrix \(\mathbf{M}_{i}^{t}\) is derived through the channel-wise \(\ell_{1}\) norm, which has proven to be effective and efficient in assessing the importance of parameters [47; 48]. The elements in \(\mathbf{M}_{i}^{t}\) with a value of ""0"" indicate the corresponding parameters that need to be pruned, while those with a value of ""1"" indicate the parameters that will be retained. The pruning ratio \(\rho_{i}\) determines the proportion of ""0"" in \(\mathbf{M}_{i}^{t}\).3 Finally, the MFP module prunes the local model \(\mathbf{w}_{i}^{t}\) with the binary mask matrix \(\mathbf{M}_{i}^{t}\) (in line 4). The pruned model can be represented as \(\mathbf{w}_{i}^{t}\odot\mathbf{M}_{i}^{t}\). It is noteworthy that, the pruning strategy used in our DapperFL is structural pruning strategy (channel pruning), which is a hardware-friendly approach that can be easily implemented with popular machine learning libraries such as PyTorch, making it particularly suitable for deployment on edge devices.

Footnote 3: Following the conventional setting in heterogeneous FL [21; 11; 5; 12], we make the fundamental assumption that the system capabilities of the devices are available to the server and the pruning ratios for all devices are appropriately determined according to the system information.

### Domain Adaptive Regularization

In FL, each client \(i\in\mathcal{C}\) possess private local data \(\mathcal{D}_{i}=\{x_{i},y_{i}\}^{N_{i}}\), where \(x\in\mathcal{X}\) denotes the input, \(y\in\mathcal{Y}\) denotes the corresponding label, and \(N_{i}\) represents the local data sample size. The data distribution \(p_{i}(x,y)\) of client \(i\) typically varies from that of other clients, i.e., \(p_{i}(x)\neq p_{j}(x),p_{i}(x|y)\neq p_{j}(x|y)\), leading to the domain shifts problem. Due to the existence of domain shifts, representation \(z_{i}\) generated by the local encoder varies among different clients, resulting in degraded prediction results of the local predictor. To address the domain shifts problem, we design a DAR module to enhance the performance of DapperFL across multiple domains while maintaining compatibility with the MFP module.

Specifically, the DAR module introduces a regularization term to the local objective to alleviate the bias of representations \(z_{i}\) on different clients adaptively. To achieve this goal, we first segment each pruned local model \(\mathbf{w}\odot\mathbf{M}\) into two parts, i.e., an encoder \(\mathbf{w}_{e}\odot\mathbf{M}_{e}\) and a predictor \(\mathbf{w}_{p}\odot\mathbf{M}_{p}\), where \(\mathbf{w}=\{\mathbf{w}_{e},\mathbf{w}_{p}\}\) and \(\mathbf{M}=\{\mathbf{M}_{e},\mathbf{M}_{p}\}\).4 The encoder is responsible for learning a representation \(z_{i}\) given an input \(x_{i}\), denoted as \(z_{i}=g_{e}(\mathbf{w}_{e}\odot\mathbf{M}_{e};x_{i})\). While the predictor is responsible for predicting label \(\hat{y}_{i}\) given the representation \(z_{i}\), denoted as \(\hat{y}_{i}=g_{p}(\mathbf{w}_{p}\odot\mathbf{M}_{p};z_{i})\). Subsequently, we construct a regularization term \(\mathcal{L}^{DAR}\) on the local objective as follows:

Footnote 4: We omit the client index \(i\) and the communication round index \(t\) for notation simplicity. In this work, all layers except the final linear layer act as the encoder, while the last linear layer of the model acts as the predictor.",\mathcal{L}^{DAR}_{i}=||g_{e}(\mathbf{w}_{e}\odot\mathbf{M}_{e};x_{i})||_{2}^{2}.,,"\mathcal{L}^{DAR}(\mathbf{w},\mathbf{M};\mathcal{D}_{i})=\mathbb{E}_{(x_{i},y_{i})\sim\mathcal{D}_{i}}\left[\Omega\left(g_{e}(\mathbf{w}_{e}\odot\mathbf{M}_{e};x_{i}),y_{i}\right)\right],","where $\Omega(\cdot)$ is a domain adaptive regularization function that penalizes biased representations $z_i$ based on the corresponding labels $y_i$, encouraging domain-invariant feature learning across clients."
neurips_2024_oral_8,4,"Here, \(||\cdot||_{2}^{2}\) represents the squared \(\ell_{2}\) norm of the local representation. The \(\ell_{2}\) norm implicitly encourages different local encoders to generate aligned robust representations adaptively, thereby mitigating the impact of domain shifts. We adopt the squared \(\ell_{2}\) norm to construct the regularization term for the following reasons: a) The \(\ell_{2}\) norm encourages each element of the representation to converge to 0 but not equal to 0. This property is advantageous compared to the \(\ell_{1}\) norm, which tends to make smaller representation elements exactly equal to 0. Thus, the squared \(\ell_{2}\) norm ensures that the pruned model retains more information in the representations. b) Higher order regularization introduces significant computational overhead compared to the \(\ell_{2}\) norm, without significantly improving the regularization effectiveness.

Next, the cross-entropy loss used in the DAR module is constructed as:","\mathcal{L}^{CE}_{i}=-\frac{1}{|\mathcal{K}_{i}|}\sum_{k\in\mathcal{K}_{i}}y_{ i,k}\log(\hat{y}_{i,k}),","where \(\mathcal{K}_{i}\) denotes the set of possible labels on the client \(i\), \(\hat{y}_{i,k}\) denotes predicting label, and \(y_{i,k}\) denotes ground-truth label","\mathcal{L}_{i}^{CE} = -\frac{1}{N_i} \sum_{n=1}^{N_i} y_{i,n} \log(\hat{y}_{i,n}),","where $\mathcal{L}_{i}^{CE}$ denotes the cross-entropy loss for client $i$, $y_{i,n}$ is the true label, and $\hat{y}_{i,n}$ is the predicted probability for sample $n$ in local dataset $\mathcal{D}_i$."
neurips_2024_oral_8,5,"el. Finally, the training objective of each client is calculated as follows:","\mathcal{L}_{i}=\mathcal{L}^{CE}_{i}+\gamma\mathcal{L}^{DAR}_{i},",where the \(\gamma\) is a pre-defined coefficient controlling the importance of \(\mathcal{L}^{DAR}_{i}\) relative to \(\mathcal{L}^{CE}_{i}\),"\mathcal{L}_{i}=\mathcal{L}^{CE}_{i}+\lambda\mathcal{L}^{DAR}_{i},",where $\lambda$ is a hyperparameter that balances the cross-entropy loss $\mathcal{L}^{CE}_{i}$ and the domain adaptive regularization term $\mathcal{L}^{DAR}_{i}$ in the client's training objective.
neurips_2024_oral_8,6,"\).

### Heterogeneous Model Aggregation

Despite the MFP module and the DAR module being capable of alleviating domain shift issues, the heterogeneous local models generated by the MFP module cannot be aggregated directly using popular aggregation algorithms. Therefore, we propose a specific FL aggregation algorithm for DapperFL to effectively aggregate these heterogeneous local models.

To preserve specific domain knowledge while transferring global knowledge to the local model, the central server first recovers the structure of local models before aggregating. Specifically, in each communication round \(t\), the pruned local model is recovered as follows:","\mathbf{w}^{t}_{i}:=\underbrace{\mathbf{w}^{t}_{i}\odot\mathbf{M}^{t}_{i}}_{\text{local knowledge}}+\underbrace{\mathcal{W}^{t-1}\odot\overline{\mathbf{M}}^{t}_{i}}_{ \text{global knowledge}},","where \(\mathcal{W}^{t-1}\) is the global model aggregated at the \((t-1)\)-th round, and \(\overline{\mathbf{M}}^{t}_{i}\) denotes the logical NOT operation applied to \(\mathbf{M}^{t}_{i}\)","\tilde{\mathbf{w}}_{i}^{t} = \mathbf{w}_{i}^{t} \oslash (\mathbf{M}_{i}^{t} + \delta),",where the recovery process involves expanding the pruned local model using the binary mask matrix to restore its original structure
neurips_2024_oral_8,7,"\). The first term \(\mathbf{w}^{t}_{i}\odot\mathbf{M}^{t}_{i}\) contains local knowledge5 specific to client \(i\), while the second term \(\mathbf{w}^{t-1}\odot\overline{\mathbf{M}}^{t}_{i}\) contains the global knowledge6 from all clients. Additionally, the structure of \(\mathbf{w}^{t}_{i}\odot\mathbf{M}^{t}_{i}\), which includes local knowledge, complements \(\mathbf{w}^{t}_{i}\odot\mathbf{M}^{t}_{i}\). Consequently, the structure recovery operation not only reinstates the pruned model's architecture but also transfers global knowledge to the pruned model, which is essential for the subsequent aggregation process. By combining these two forms of knowledge, we aim to leverage both the specialized insights of local models and the generalized capabilities of the global model, thereby enhancing the performance and adaptability of DapperFL.

Footnote 5: In this work, “local knowledge” refers to the feature extraction capabilities of the local model, which are learned from the specific data available in its local domain. This knowledge encapsulates the nuances and characteristics of the data that the local model has been trained on.

Finally, the global model is calculated by aggregating the recovered local models as follows:","\mathcal{W}^{t}=\sum_{i\in\mathcal{C}}\frac{|\mathcal{D}_{i}|}{|\mathcal{D}|} \boldsymbol{w}_{i}^{t},","where \(|\mathcal{D}_{i}|\) is the sample number in the local dataset on client \(i\), and \(|\mathcal{D}|\) is the total number of samples in the entire FL system",\mathcal{W}^{t} = \frac{1}{|\mathcal{S}^{t}|}\sum_{i\in\mathcal{S}^{t}}\mathbf{w}_{i}^{t}.,"where $ \mathcal{W}^{t} $ is the updated global model at round $ t $, and $ \mathcal{S}^{t} $ denotes the set of selected clients in round $ t $"
neurips_2024_oral_9,1,"### _Decompose_ Stage

According to the Analogical Reasoning theory [2], when humans conduct reasoning, they often analogize the logical processes of new questions to those of similar questions. Therefore, to make the decomposition logic of sub-questions \(q_{t}\) at each level \(t\) more closely resemble that of humans, we first use human-annotated question decomposition examples (_Appendix_ A.1) as a demonstration pool \(P\). Then we calculate the cosine similarity of the representations between \(Q\) and each \(Q_{i}^{d}\) in \(P\) and select top-\(K\) nearest neighbors in the vector space. After that, we concatenate each \(Q_{i}^{d}\) with its human-annotated sub-questions \(subqs^{i}=(subq_{1}^{i},subq_{2}^{i},...,subq_{n}^{i})\) to form \(K\) question-decomposition examples (_Appendix_ A.1)","lh_{Q}=(Q_{i}^{d},subqs^{i})(i=1,2,...,K).",,"\text{Similarity}(Q, Q_i^d) = \frac{Q \cdot Q_i^d}{\|Q\| \cdot \|Q_i^d\|}",Computes cosine similarity between input question $Q$ and each demonstration example $Q_i^d$ in pool $P$ to identify top-$K$ nearest neighbors.
neurips_2024_oral_9,2,"These examples are regarded as ""logic heuristics"" that inspire the model to decompose questions in a manner closely aligned with human reasoning.

After obtaining \(lh_{Q}\), we utilize them to decompose the sub-question \(q_{t}\) at level \(t\) into multiple sub-questions at level \(t+1\). Specifically, given question \(q_{t}\), if its coherence score \(s_{t}\) (Eq. (5)) is higher than a threshold \(\epsilon_{1}\), We ask the LLM whether it needs to be further decomposed. If \(q_{t}\) requires decomposition, we then prompt the LLM to autonomously break it down into several sub-questions \(\{q_{t+1}^{j},j=1,...,J\}\). It is worth noting that in our decomposition approach, we do not pre-specify the number \(J\) of sub-questions; instead, we allow LLMs to adapt tively determine it based on the logic of each question. However, the number of sub-questions is capped at a predefined maximum branch limit to ensure computational efficiency and manageability 5.1.2. This enhances adaptability and more closely aligns with human logical characteristics when compared to existing methods like ToT [49] and GoT [3], etc. To facilitate this process, we design a heuristic-enhanced prompt that consists of a prompt head \(h_{1}\) and ""logic heuristics"" \(lh_{Q}\). The prompt head describes the question decomposition task in natural language. This process is formulated in Eq. (2). Additionally, we validate the effectiveness of using logic heuristics, and provide detailed explanations and templates in _Appendix_A.1.","\{q_{t+1}^{j},j=1,...,J\}\gets Decompose(p_{\theta},\;h_{1},\;lh_{Q},\;q_ {t}).",,"\begin{equation}
\{q_{t+1}^{j}\}_{j=1}^{J} = \text{LLM}\left( h_1 \parallel \text{lh}_{Q} \parallel q_t \right)
\end{equation}",Equation (2) formulates the decomposition of sub-question $q_t$ into $J$ sub-questions at level $t+1$ using a logic-heuristic-enhanced prompt composed of prompt head $h_1$ and retrieved logic heuristics $\text{lh}_{Q}$.
neurips_2024_oral_9,3,"After decomposition, each \(q_{t+1}^{j}\) is added as a new node \(n_{t+1}^{j}\) at level \(t+1\), with a directed edge from \(n_{t}\) to \(n_{t+1}^{j}\) (denoted as \(e^{j}=(n_{t},n_{t+1}^{j})\)). If the LLM determines that \(q_{t}\) does not require further decomposition, we create a leaf node \(n_{end}\) as a child of \(n_{t}\).

### _Analyze_ Stage

In _Analyze_ stage, we reason the answers for all the sub-questions \(\{q_{t+1}^{j}\}\) at level \(t+1\). To be specific, we first prompt the LLM to generate the essential rationale \(r_{t+1}^{j}\) for each sub-question \(q_{t+1}^{j}\):","r_{t+1}^{j}\gets Solve(p_{\theta},\;h_{2},\;q_{t+1}^{j}).",,"\{r_{t+1}^{j}\}\gets Analyze(p_{\theta},\;h_{2},\;\{q_{t+1}^{j}\}),",where $h_{2}$ is the prompt head describing the rationale generation task for sub-questions at level $t+1$.
neurips_2024_oral_9,4,"Here, \(h_{2}\) denotes the prompt head, which is a natural language sentence that asks the model to generate detailed solutions (see _Appendix_A.2).

After obtaining the rationales for the sub-questions, we evaluate and correct them, as large language models (LLMs) often tend to hallucinate during problem-solving [54]. Using generated rationales without verification can propagate errors, leading to incorrect outcomes. To address this issue, we develop a self-check method that promptly identifies and corrects these errors while providing a coherence score (Eq. (5)) for each node.

Specifically, we first instruct the LLM to perform a self-check on the rationale \(r_{t+1}^{j}\) generated for the sub-question \(q_{t+1}^{j}\) (see _Appendix_A.2 for the prompt head \(h_{3}\)) to identify any potential errors. If the LLM detects errors in the original rationale \(r_{t+1}^{j}\), it modifies the rationale to \(\hat{r}_{t+1}^{j}\); otherwise, the rationale is output unchanged. Take the case in Figure 2 as an example, we expect the LLM to identify the error ""Each egg is sold for $3"" in \(r_{1}^{1}\), and correct it to ""Each egg is sold for $2"". This process is denoted as:","\hat{r}_{t+1}^{j}\gets Self\_Check(p_{\theta},\;h_{3},\;q_{t+1}^{j},\;r_{t+ 1}^{j}).",,"\hat{r}_{t+1}^{j}\gets SelfCheck(p_{\theta},\;h_{3},\;q_{t+1}^{j},\;r_{t+1}^{j}).",This equation represents the self-check process where the LLM verifies and corrects the generated rationale for a sub-question based on the provided prompt head and the original rationale.
neurips_2024_oral_9,5,"Then, we prompt the LLM to evaluate the logical coherence between the refined rationale \(\hat{r}_{t+1}^{j}\) and the question \(q_{t+1}^{j}\), by generating a coherence score \(s_{t+1}^{j}\) (see _Appendix_A.2 for prompt head \(h_{4}\)):","s_{t+1}^{j}\gets Score(p_{\theta},\;h_{4},\;q_{t+1}^{j},\;\hat{r}_{t+1}^{j }).",,"s_{t+1}^{j} \gets Evaluate(p_{\theta},\;h_{4},\;q_{t+1}^{j},\;\hat{r}_{t+1}^{j}).",Coherence score $s_{t+1}^{j}$ is generated by evaluating the logical alignment between the refined rationale $\hat{r}_{t+1}^{j}$ and the corresponding sub-question $q_{t+1}^{j}$.
neurips_2024_oral_9,6,"The score \(s_{t+1}^{j}\) can also be obtained through voting or classification methods. Here, we specifically investigate the effectiveness of directly prompting LLMs to generate numerical values as scores.

At the end of the _Analyze_ stage, we fill the obtained rationales and scores into nodes \(n_{t+1}^{j}(j\geq 1)\):","n_{t+1}^{j}=(q_{t+1}^{j},\hat{r}_{t+1}^{j},s_{t+1}^{j}).",where \(s_{t+1}^{j}\) can support the current or subsequent cycles in _Rethink_ (4,"\forall j,\;n_{t+1}^{j}.\text{rationale}=\hat{r}_{t+1}^{j},\;n_{t+1}^{j}.\text{score}=s_{t+1}^{j}.",This equation assigns the refined rationale $\hat{r}_{t+1}^{j}$ and the coherence score $s_{t+1}^{j}$ to the corresponding node $n_{t+1}^{j}$ at level $t+1$ after the self-check and scoring processes are completed.
neurips_2024_oral_9,7,"(4.3) and _Decompose_ (4.1).

### _Rethink_ Stage

According to self-reflection theories [11; 13; 6] in cognitive science, humans constantly update and reflect on their previous reasoning results based on the current information. This allows us to correct past mistakes and ultimately achieve a consistent and stable answer. For example in Figure 2, a person might initially answer question \(Q\) (""Janet's ducks... How much... market?"") with the rationale \(r_{0}\) ""She makes \(9\times 3=\$27\) per day"". However,after considering responses to sub-questions \(q_{1}^{1}\) (""Whatis the selling price of one egg?"") and \(q_{1}^{2}\) (""How many eggs does Janet have per day?""), he/she realizes an error in \(r_{0}\). The correct calculation, using the values ""\(2\)"" for the price per egg and ""\(9\)"" for the daily number of eggs, should be ""\(2\times 9=\$18\)"".

Nevertheless, existing methods like ToT [48] search reasoning paths based solely on preceding steps, lacking the ability to retrospectively update earlier content based on the influence of later steps. To address this, we introduce a _Rethink_ stage that mirrors the human reflective process.

Specifically, during the rethinking process, humans first identify which existing reasoning steps may require revision. We aim to automate this by using LLMs to detect logical connections between ancestral and newly generated nodes, updating ancestral nodes based on insights from the rationales of new nodes. In our proposed ""Reasoning Tree"", we essentially use information from lower-level nodes to ""rethink"" higher-level nodes, closely mirroring the human cognitive simplification process in problem-solving [33].

To achieve this, after obtaining node \(n_{t+1}^{j}\) in _Analyze_ Stage, we first check its coherence score \(s_{t+1}^{j}\) (Eq. (5)). If \(s_{t+1}^{j}\) exceeds the threshold \(\epsilon_{2}\), we then examine the correlation between \(q_{t+1}^{j}\) and all sub-questions above level \(t\), specifically, \(\{q_{l},\ l\leq t\}\). Next, we extract a subset of \(k\) most related nodes \(L_{k}\) from \(L\triangleq\{n_{l},\ l\leq t\}\) (the specific nodes to be extracted are determined by the LLM):","L_{k}\gets Extract(p_{\theta},\ h_{5},\ L,q_{t+1}^{j}),\ L_{k}\subseteq L.",where \(h_{5}\) is a prompt head (_Appendix_ A,L_{k} \gets \text{Top-}k\ \text{nodes in } L \text{ most related to } q_{t+1}^{j} \text{ based on cosine similarity}.,We extract the top-$k$ nodes from the set $L$ that are most related to the sub-question $q_{t+1}^{j}$ using cosine similarity.
neurips_2024_oral_9,8,"A.3). Next, we use the rationale \(\hat{r}_{t+1}^{j}\) of sub-question \(q_{t+1}^{j}\) to update the rationale \(r\) of each extracted node \(n_{e}\) in \(L_{k}\):","r^{\prime}\gets Update(p_{\theta},\ h_{6},\ n_{e}(q,r,s),\ \hat{r}_{t+1}^{j}).",,"\hat{r}_{e}\gets Update(p_{\theta},\ h_{6},\ n_{e},\ \hat{r}_{t+1}^{j}),\ \forall n_{e}\in L_{k}.","This equation represents the process of updating the rationale of previously extracted nodes using the refined rationale of the current sub-question, simulating human reflective reasoning by incorporating new insights into earlier reasoning steps."
neurips_2024_oral_9,9,"Finally, we replace \(r\) with the updated rationale \(r^{\prime}\):","n_{e}(q,r^{\prime},s)\gets n_{e}(q,r,s).",,"n_{e}(q,r^{\prime},s)\gets Replace(n_{e}(q,r,s),\ r^{\prime}).",This operation updates the rationale of the selected ancestral node $n_{e}$ with the refined rationale $r^{\prime}$ derived from the newly generated sub-question $q_{t+1}^{j}$.
