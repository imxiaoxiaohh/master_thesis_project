paper_id,equation_id,context,ground_truth_eq,ground_truth_description,generated_equation,generated_description
2024.acl-short.14,1,"### Forward Sentence-Level Generation

In this part, we decompose the generation of the text to describe the full KG into a sequential decoding problem: the model sequentially generate a semantically complete sentence with the sentence-specific subset of KG triples. Then the model generates the full text of KG based on the triples and the sentence-level generation. The generation process can be formulated as follows:","P(\mathbf{cot},T|\mathbf{KG}) || =P((s_{1},t_{1}),\cdots,(s_{n},t_{n}),T|\mathbf{KG}) || =\prod_{i=1}^{n}P((s_{i},t_{i})|(s_{1},t_{1}),\cdots,(s_{i-1},t_ {i-1}),\mathbf{KG})\cdot || P(T|(s_{1},t_{1}),\cdots,(s_{n},t_{n}),\mathbf{KG}) || =\prod_{i=1}^{n}P(t_{i}|(s_{1},t_{1}),\cdots,(s_{i-1},t_{i-1}), \mathbf{KG})\cdot || \prod_{i=1}^{n}P(s_{i})|t_{i},(s_{1},t_{1}),\cdots,(s_{i-1},t_{i- 1}),\mathbf{KG})\cdot || P(T|(s_{1},t_{1}),\cdots,(s_{n},t_{n}),\mathbf{KG})",,"y_t = f(x_t, h_{t-1})","y_t: the generated word at time t, x_t: the input at time t, h_{t-1}: the hidden state from the previous time step"
2024.acl-short.16,3,"Here, for each projection tensor at iteration \(t\), we compute a smoothed gradient (\(\overline{T}^{(t)}_{A^{l}}\)) and uncertainly tensor (\(\overline{U}^{(t)}_{A^{l}}\)), as shown in Eq. 2 and 3, respectively. We then evaluate the freezing score \(s^{(t)}_{A^{l}}\), as the mean of the tensor generated via Hadamard product (\(\circ\)) between \(\overline{T}^{(t)}_{A^{l}}\) and \(\overline{U}^{(t)}_{A^{l}}\).

To apply thresholding on the LoRA freezing scores, we use the cubic schedule as (Zhang et al., 2022). In specific, we keep the projection matrices trainable for the initial \(t_{i}\) training steps, and then progressively freeze them by calculating the freezing fraction \(r(t)\) as shown in Eq. 5. Finally, all the projection matrices freeze beyond \(T-t_{f}\) steps. Note, at step \(t\), for a computed freezing fraction \(k\), we freeze the lowest \(k\%\) projection matrices.",r(t)=\{\begin{array}{ll}0&0\leq t<t_{i}\\ 1-(1-\frac{t-t_{i}}{T-t_{i}-t_{f}})^{3}&t_{i}\leq t<T-t_{f}\\ 1&\text{otherwise}\end{array}.,"where \(t\) refers to current #step, \(T\) is the total number of fine-tuning steps",r(t)=\left(1-\left(\frac{t-t_{i}}{T-t_{i}-t_{f}}\right)^{3}\right)_{+},"where \(t\) refers to current step, \(T\) is the total number of fine-tuning steps"
2024.acl-short.43,1,"### FastText-based LIDs

In this paper, we explore the use of MaskLID for LIDs based on the FastText Bojanowski et al. (2017) architecture. However, it is also possible to apply MaskLID to other LIDs, as long as they enable to determine how much each feature (e.g., word) contributes to each supported language. FastText is one of the most popular LID architectures due to its open-source nature, high performance, ease of use, and efficiency. FastText classifier is a multinomial logistic classifier that represents the input sentence as a set of feature embeddings, making it easy to assess each feature's contribution to the final prediction.

Given a sentence \(s\), let \(f_{1},f_{2},\ldots,f_{T}\) represent the features extracted from \(s\). Note that these features are linearly ordered, i.e., \(f_{i}\) precedes \(f_{i+1}\) in \(s\). FastText maps these features onto vectors in \(\mathbb{R}^{d}\) via feature embeddings \(\mathbf{x}_{1},\mathbf{x}_{2},\ldots,\mathbf{x}_{T}\). The dimensionality of these embeddings, denoted \(d\), is a hyperparameter. A base LID using FastText architecture computes the posterior probability for a language \(c\in[1:N]\) by applying the softmax function over logits as:",P(c|s)=\frac{\exp(\mathbf{b}_{c}\cdot\frac{1}{T}\sum_{t=1}^{T}\mathbf{x}_{t})}{ \sum_{c^{\prime}=1}^{N}\exp(\mathbf{b}_{c^{\prime}}\cdot\frac{1}{T}\sum_{t=1}^ {T}\mathbf{x}_{t})}.,,P(c|s) = \frac{e^{z_c}}{\sum_{j=1}^{N} e^{z_j}},Where \(z_c\) represents the logit for language \(c\) computed from the feature embeddings.
2024.acl-short.5,2,"\). and \(Y\). are respectively the token probability and logit generated from LMs. \(\mathcal{V}_{\cdot,i}^{\alpha}\) denotes the adaptive plausibility constraint that dynamically restricts the logits from producing the erroneous modes. The adaptive plausibility constraints are calculated as","\mathcal{V}_{\text{ori},i}^{\alpha}=\{w|P_{\mathcal{M}_{e}}(w|x_{<i})> \alpha\max_{w\in\mathcal{V}}P_{\mathcal{M}_{e}}(w|x_{<i})\}, || \mathcal{V}_{\text{imp},i}^{\alpha}=\{w|Y_{\mathcal{M}_{e}}(w| x_{<i})>\log\alpha+\max_{w\in\mathcal{V}}Y_{\mathcal{M}_{e}}(w|x_{<i})\}.",,"\mathcal{V}_{\text{ori},i}^{\alpha} = \{x_{j} \in \mathcal{V} \mid Y_{\mathcal{M}_{e}}(x_{j}|x_{<i}) \geq \alpha\}","The adaptive plausibility constraints for original and improved contrastive decoding, respectively."
2024.acl-short.62,1,"### Token-level reward modeling

In this section, we will first introduce the RL environment and then define our token-level reward modeling scheme.

Language generation can be defined as a Markov Decision Process (MDP) \(\langle\mathcal{S},\mathcal{A},\mathcal{R},\mathcal{P},\gamma\rangle\). \(\mathcal{S}\) refers to the state space and we define the start state \(s_{1}\) as the input prompts \(\{x\}\). An action at t-step \(a_{t}\) is a generated token. The transition function of the environment is denoted as \(\mathcal{P}:\mathcal{S}\times\mathcal{A}\rightarrow\mathcal{S}\), \(s_{t}=\{x,a_{1},...,a_{t-1}\}\). A response \(y\) of length \(T\) is then \(y=\{a_{1},..,a_{T}\}\). In our token-level reward scheme, a reward is assigned to each generated token \(a_{t}\) by \(\mathcal{R}\colon\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}\), where at each time step \(t\) there is a learned reward function \(r_{t}=r_{\phi}(s_{t},a_{t})\). Therefore, for each response, we have a trajectory \(\tau=\{s_{1},a_{1},r_{1},...s_{t},a_{t},r_{t},...s_{T},a_{T},r_{T}\}\).

We define the reward of the whole trajectory as the average of rewards assigned to each token:",R(\tau)=\frac{1}{T}\sum_{t=1}^{T}r_{t}.,,R(\tau) = \frac{1}{T} \sum_{t=1}^{T} r_{t},"The average reward of the trajectory \(\tau\), where \(R(\tau)\) is the total reward, \(T\) is the length of the response, and \(r_{t}\) is the reward assigned to the token at time step \(t\)."
2024.acl-short.71,8,"The three embeddings are then fed into a feed-forward module, which outputs a scalar score:","\text{comet}(\textbf{{hyp}})=\text{score}(\textbf{{hyp}},\ \textbf{{ref}},\ \textbf{{src}}).",,"\text{score}=\text{FFN}(\textbf{{hyp}},\ \textbf{{ref}},\ \textbf{{src}}).","The scalar score output by the feed-forward module based on the embeddings of the hypothesis, reference, and source sequence."
2024.acl-short.72,3,"ng. Following previous work (Overwijk et al., 2022), the DOM nodes can be categorized into six kinds of labels \(y^{k}\), including primary content, heading, title, paragraph, table, and list. Then we calculate the label prediction probability \(P(y^{k}_{i}=1|x_{i})\) of the \(k\)-th category label \(y^{k}_{i}\) of the \(i\)-th node:",P(y^{k}_{i}=1|x_{i})=\text{Sigmoid}(\text{MLP}(\hat{h}_{i})),,"P(y^{k}_{i}=1|x_{i})=\frac{\exp(\hat{h}_{i} \cdot W^{k})}{\sum_{j=1}^{K}\exp(\hat{h}_{i} \cdot W^{j})},",where \(W^{k}\) is the weight vector for the \(k\)-th label and \(K\) is the total number of labels.
ICLR_2024_oral_1,4,"This is in general tight--for example, the maximum likelihood estimator satisfies (1) with equality as \(T\to\infty\)(Van der Vaart, 2000). The Fisher information thus serves as a fundamental lower bound on parameter estimation error, a key motivation for our exploration procedure.

Asid: Targeted Exploration for Test-Time Simulation Construction, Identification, and Policy Optimization

In this section, we present our proposed approach, Asid, a three-stage pipeline illustrated in Figure 2. We describe each component of Asid in the following.

Figure 2: **Overview of Asid: (1) Train an exploration policy \(\pi_{\mathrm{exp}}\) that maximizes the Fisher information, leveraging the vast amount of cheap simulation data. (2) Roll out \(\pi_{\mathrm{exp}}\) in real to collect informative data that can be used to (3) run system identification to identify physics parameters and reconstruct, _e.g._, geometric, collision, and kinematic properties. (4) Train a task-specific policy \(\pi_{\mathrm{task}}\) in the updated simulator and (5) zero-shot transfer \(\pi_{\mathrm{task}}\) to the real world.**

### Exploration via Fisher Information Maximization

As motivated in Section 3, to learn a policy effectively accomplishing our task, it suffices to accurately identify \(\mathbf{\theta}^{\star}\). In the exploration phase, step 1 in our learning protocol, our goal is to then play an exploration policy \(\pi_{\mathrm{exp}}\) which generates a trajectory on the real environment that provides as much information on \(\mathbf{\theta}^{\star}\) as possible. Following Section 3, the Fisher information gives a quantification of the usefulness of the data collected, which motivates our approach.

In our setting, the distribution over trajectories generated during exploration in real, \(\mathbf{\tau}_{\mathrm{real}}\sim p_{\mathbf{\theta}^{\star}}(\cdot\mid\pi_{\mathrm{ exp}})\), depends on the exploration policy, \(\pi_{\mathrm{exp}}\), being played. As the Fisher information depends on the data distribution, it too scales with the choice of exploration policy:","\mathcal{I}(\mathbf{\theta}^{\star},\pi_{\mathrm{exp}}):=\mathbb{E}_{\mathbf{\tau} \sim p_{\mathbf{\theta}^{\star}}(\cdot\mid\pi_{\mathrm{exp}})}[\nabla_{\mathbf{ \theta}}\log p_{\mathbf{\theta}^{\star}}(\mathbf{\tau}\mid\pi_{\mathrm{exp}})\cdot \nabla_{\mathbf{\theta}}\log p_{\mathbf{\theta}^{\star}}(\mathbf{\tau}\mid\pi_{\mathrm{ exp}})^{\top}].",,"\mathcal{I}(\mathbf{\theta}^{\star},\pi_{\mathrm{exp}}):=\mathbb{E}_{\mathbf{\tau}_{\mathrm{real}}\sim p_{\mathbf{\theta}^{\star}}(\cdot\mid\pi_{\mathrm{exp}})} [\nabla_{\mathbf{\theta}}\log p_{\mathbf{\theta}^{\star}}(\mathbf{\tau}_{\mathrm{real}})\cdot\nabla_{\mathbf{\theta}}\log p_{\mathbf{\theta}^{\star}}(\mathbf{\tau}_{\mathrm{real}})^{\top}].",The Fisher information matrix conditioned on the exploration policy \(\pi_{\mathrm{exp}}\) and the true parameter \(\mathbf{\theta}^{\star}\).
ICLR_2024_oral_10,11,"We instantiate this denoising diffusion framework to model the distributions over patchwise rays conditioned on the input images. We do this by simply modifying our ray regression network from Sec. 3.2 to be additionally conditioned on noisy rays (concatenated with patchwise features and pixel coordinates) and a positionally encoded (Vaswani et al., 2017) time embedding \(t\):","\{\hat{\mathcal{R}}\}_{i=1}^{N}=f_{\text{Diffusion}}(\{\mathbf{f}_{i}, \mathbf{u}_{i},\mathbf{r}_{i,t}\}_{i=1}^{N\cdot p^{2}},t), || \mathbf{r}_{i,t}=\sqrt{\bar{\alpha}_{t}}\mathbf{r}_{i}+\sqrt{1-\bar{\alpha}_{t}}\epsilon.","where the noisy rays \(\mathbf{r}_{i,t}\) can be computed as:

\[\mathbf{r}_{i,t}=\sqrt{\bar{\alpha}_{t}}\mathbf{r}_{i}+\sqrt{1-\bar{\alpha}_{t}}\epsilon","\mathbf{r}_{i,t}=\sqrt{\bar{\alpha}_{t}}\mathbf{r}_{i}+\sqrt{1-\bar{\alpha}_{t}}\epsilon","where \(\mathbf{r}_{i,t}\) is the noisy ray, \(\mathbf{r}_{i}\) is the original ray, and \(\epsilon\sim\mathcal{N}(0,\mathbf{I})\) is Gaussian noise."
ICLR_2024_oral_11,1,"### The Proposed Pruning Method

Here, we introduce the proposed CLSP method that aims to eliminate potential false candidate labels in the candidate label set of each training PLL instance. Differently from conventional learning-centric adopt PLL methods that focus on training a desired deep neural network, we pursue a data-centric solution that delves into the PLL data itself. Motivated by the clusterability setting in the conventional supervised learning (Zhu et al., 2022), we further focus on a label distinguishability setting on the PLL dataset, i.e., nearby PLL instances are likely to have the same true label (vs. false candidate label) inside their candidate label sets with a high probability (vs. a limited probability), which could be formally defined below.

**Definition 2** ((\(k,\delta_{k},\rho_{k}\))-label distinguishability).: _A PLL dataset \(\mathcal{D}=\{(\mathbf{x}_{i},Y_{i})\}_{i=1}^{n}\) satisfies (\(k,\delta_{k},\rho_{k}\)) label distinguishability if: \(\forall(\mathbf{x}_{i},Y_{i})\in\mathcal{D}\), the true label \(y_{i}\in Y_{i}\) is inside the candidate label set \(Y^{(j)}_{i}\) of its each \(k\)-NN example \((\mathbf{x}^{(j)}_{i},Y^{(j)}_{i})\), with probability at least \(1-\delta_{k}\), and each false candidate label \(y^{\prime}_{i}\in Y_{i}\backslash\{y_{i}\}\) is inside the candidate label set \(Y^{(j)}_{i}\) of its each \(k\)-NN example \((\mathbf{x}^{(j)}_{i},Y^{(j)}_{i})\) with probability no more than \(\rho_{k}\)._

Definition 2 characterizes the candidate label distribution in the local representation space, which has two-fold implications. First, the true label of instances in the local representation space appears in their candidate label sets with a high probability. Second, each false candidate label of instances in the local representation space appears in their candidate label sets with a limited probability.

Intuitively, the candidate label of an instance that appears most frequently in its \(k\)-NN instances' candidate label sets is more likely to be the true label, and the candidate label that rarely appears in its \(k\)-NN instances' candidate label sets has a high probability of being a false label. Motivated by this intuition, we introduce a per-instance label-wise metric \(O_{ij}\) towards the \(i\)-th instance (\(\mathbf{x}_{i},Y_{i}\)) associated with the \(j\)-th candidate label, to measure the possibility of the \(j\)-th candidate label is _not_ the true label of the instance \(\mathbf{x}_{i}\), which implies that we should prune the \(j\)-th label from the candidate label set if \(O_{ij}\) is large. Concretely, \(O_{ij}\) is calculated by counting the times of its \(j\)-th candidate label not appearing in its \(k\)-NN instances' candidate label set \(Y_{i}^{(k)}\):","O_{ij}=\sum_{v=1}^{k}\mathbb{I}[y_{ij}\neq y_{ij}^{(v)}],\forall j\in Y_{i},",where \(\mathbb{I}[\cdot]\) is the indicator function and \(y_{ij}\) is the \(j\)-th candidate label of \(\mathbf{x}_{i}\),O_{ij} = \frac{1}{k} \sum_{j=1}^{k} \mathbb{I}[y_{ij} \notin Y^{(j)}_{i}]\,where \(\mathbb{I}[\cdot]\) is the indicator function and \(y_{ij}\) is the \(j\)-th candidate label of \(\mathbf{x}_{i}\)
ICLR_2024_oral_11,3,"er. Then, for each instance \((\mathbf{x}_{i},Y_{i})\), we can eliminate a subset of candidate labels that possess a high down-voting value:","\widetilde{Y}_{i}=\mathrm{Top}\text{-}\gamma_{i}\text{-}\mathrm{argmax}_{j \in Y_{i}}(O_{ij}),",where \(\mathrm{Top}\text{-}\gamma_{i}\text{-}\mathrm{argmax}_{i}\) returns a subset of indices (i,"J_{i}=\mathrm{Top}\text{-}\gamma_{i}\text{-}\mathrm{argmax}_{j} O_{ij},\forall j\in Y_{i},","where \(J_{i}\) is the set of indices of the candidate labels to be eliminated for the instance \((\mathbf{x}_{i},Y_{i})\)."
ICLR_2024_oral_14,1,"## 3 Preliminaries

**Probabilistic interpretation of dense correspondence.** Let us denote a pair of images, i.e., source and target, as \(I_{\text{src}}\) and \(I_{\text{tgt}}\) that represent visually or semantically similar images, and feature descriptors extracted from \(I_{\text{src}}\) and \(I_{\text{tgt}}\) as \(D_{\text{src}}\) and \(D_{\text{tgt}}\), respectively. The objective of dense correspondence is to find a correspondence field \(F\) that is defined at each pixel \(i\), which warps \(I_{\text{src}}\) towards \(I_{\text{tgt}}\) such that \(I_{\text{tgt}}(i)\sim I_{\text{src}}(i+F(i))\) or \(D_{\text{tgt}}(i)\sim D_{\text{src}}(i+F(i))\).

This objective can be formulated within probabilistic interpretation (Simoncelli et al., 1991; Sun et al., 2008; Ham et al., 2016; Kim et al., 2017), where we seek to find \(F^{*}\) that maximizes the posterior probability of the correspondence field given a pair of feature descriptors \(D_{\text{src}}\) and \(D_{\text{tgt}}\), i.e., \(p(F|D_{\text{src}},D_{\text{tgt}})\). According to Bayes' theorem (Joyce, 2003), the posterior can be decomposed such that \(p(F|D_{\text{src}},D_{\text{tgt}})\propto p(D_{\text{src}},D_{\text{tgt}}|F )\cdot p(F)\). To find the matching field \(F^{*}\) that maximizes the posterior, we can use the maximum a posteriori (MAP) approach (Greig et al., 1989):","\begin{split} F^{*}&=\operatorname*{argmax}_{F}p(F|D _{\text{src}},D_{\text{tgt}})=\operatorname*{argmax}_{F}p(D_{\text{src}},D_{ \text{tgt}}|F)\cdot p(F)\\ &=\operatorname*{argmax}_{F}\{\underbrace{\log p(D_{\text{src}},D _{\text{tgt}}|F)}_{\text{data term}}+\underbrace{\log p(F)}_{\text{prior term}}\}.\end{split}",,"F^{*} = \arg \max_{F} p(F|D_{\text{src}},D_{\text{tgt}})",Optimal correspondence field that maximizes the posterior probability given the feature descriptors.
ICLR_2024_oral_15,1,"### Generative Gaussian Splatting

Gaussian splatting (Kerbl et al., 2023) represents 3D information with a set of 3D Gaussians. It has been proven effective in reconstruction settings (Kerbl et al., 2023; Laiten et al., 2023) with high inference speed and reconstruction quality under similar modeling time with NeRF. However, its usage in a generative manner has not been explored. We identify that the 3D Gaussians can be efficient for 3D generation tasks too.

Specifically, the location of each Gaussian can be described with a center \(\mathbf{x}\in\mathbb{R}^{3}\), a scaling factor \(\mathbf{s}\in\mathbb{R}^{3}\), and a rotation quaternion \(\mathbf{q}\in\mathbb{R}^{4}\). We also store an opacity value \(\alpha\in\mathbb{R}\) and a color feature \(\mathbf{c}\in\mathbb{R}^{3}\) for volumetric rendering. Spherical harmonics are disabled since we only want to model simple diffuse color. All the above optimizable parameters is presented by \(\Theta\), where \(\Theta_{i}=\{\mathbf{x}_{i},\mathbf{s}_{i},\mathbf{q}_{i},\alpha_{i},\mathbf{ c}_{i}\}\) is the parameter for the \(i\)-th Gaussian. To render a set of 3D Gaussians, we need to project them onto the image plane as 2D Gaussians. Volumetric rendering is then performed for each pixel in front-to-back depth order to evaluate the final color and alpha. In this work, we use the highly optimized renderer implementation from Kerbl et al. (2023) to optimize \(\Theta\).

We initialize the 3D Gaussians with random positions sampled inside a sphere, with unit scaling and no rotation. These 3D Gaussians are periodically densified during optimization. Different from the reconstruction pipeline, we start from fewer Gaussians but density it more frequently to align with the generation progress. We follow the recommended practices from previous works (Poole et al., 2022; Huang et al., 2023; Lin et al., 2023) and use SDS to optimize the 3D Gaussians (Please refer to Section A.1 for more details on SDS loss). At each step, we sample a random camera pose \(p\) orbiting the object center, and render the RGB image \(I_{\text{RGB}}^{p}\) and transparency \(I_{\text{A}}^{p}\) of the current view. Similar to Dreamtime (Huang et al., 2023), we decrease the timestep \(t\) linearly during training, which is used to weight the random noise \(\epsilon\) added to the rendered RGB image. Then, different 2D diffusion priors \(\phi\) can be used to optimize the underlying 3D Gaussians through SDS.

**Image-to-3D.** For the image-to-3D task, an image \(\tilde{I}_{\text{RGB}}^{r}\) and a foreground mask \(\tilde{I}_{\text{A}}^{r}\) are given as input. Zero-1-to-3 XL (Liu et al., 2023; Deitke et al., 2023) is adopted as the 2D diffusion prior. The

Figure 2: **DreamGaussian Framework. 3D Gaussians are used for efficient initialization of geometry and appearance using single-step SDS loss. We then extract a textured mesh and refine the texture image with a multi-step MSE loss.**

SDS loss can be formulated as:","\nabla_{\Theta}\mathcal{L}_{\text{SDS}}=\mathbb{E}_{t,p,\epsilon}[w(t)( \epsilon_{\phi}(I_{\text{RGB}}^{p};t,\tilde{I}_{\text{RGB}}^{r},\Delta p)- \epsilon)\frac{\partial I_{\text{RGB}}^{p}}{\partial\Theta}]","where \(w(t)\) is a weighting function, \(\epsilon_{\phi}(\cdot)\) is the predicted noise by the 2D diffusion prior \(\phi\), and \(\Delta p\) is the relative camera pose change from the reference camera \(r\)","L_{\text{SDS}} = \mathbb{E}_{t, \tilde{I}_{\text{RGB}}^{r}, \tilde{I}_{\text{A}}^{r}} \left[ w(t) \left\| \epsilon_{\phi}(\tilde{I}_{\text{RGB}}^{r}, \tilde{I}_{\text{A}}^{r}) - \epsilon \right\|^{2} \right]","where \(w(t)\) is a weighting function, \(\epsilon_{\phi}(\cdot)\) is the predicted noise by the 2D diffusion prior \(\phi\), and \(\Delta p\) is the relative camera pose change from the reference camera \(r\)."
ICLR_2024_oral_15,2,"\). Additionally, we optimize the reference view image \(I_{\text{RGB}}^{r}\) and transparency \(I_{\text{A}}^{r}\) to align with the input:",\mathcal{L}_{\text{Ref}}=\lambda_{\text{RGB}}||I_{\text{RGB}}^{r}-\tilde{I}_ {\text{RGB}}^{r}||_{2}^{2}+\lambda_{\text{A}}||I_{\text{A}}^{r}-\tilde{I}_{ \text{A}}^{r}||_{2}^{2},where \(\lambda_{\text{RGB}}\) and \(\lambda_{\text{A}}\) are the weights which are linearly increased during training,"\mathcal{L}_{\text{align}} = \lambda_{\text{RGB}} \mathcal{L}_{\text{RGB}}(I_{\text{RGB}}^{r}, I_{\text{RGB}}^{p}) + \lambda_{\text{A}} \mathcal{L}_{\text{A}}(I_{\text{A}}^{r}, I_{\text{A}}^{p})","where \(\mathcal{L}_{\text{RGB}}\) and \(\mathcal{L}_{\text{A}}\) are the loss functions for the RGB image and transparency, respectively."
ICLR_2024_oral_16,5,"When constructing the embedding space, we found that an additional consideration of reconstruction of state \(s\) conditioned on timestep \(t\) improves the quality of feature extraction and constitutes a smoother embedding space. To this end, we develop the deterministic conditional autoencoder (**dCAE**), and the corresponding loss function can be expressed as","\mathcal{L}(\mathbf{\phi},\mathbf{\psi})=(H_{t}-f_{\psi}^{H}(f_{\phi}(s_{t}|t)|t ))^{2}+\lambda_{rcon}||s_{t}-f_{\psi}^{s}(f_{\phi}(s_{t}|t)|t)||_{2}^{2},",where \(f_{\psi}^{H}\) predicts the highest return; \(f_{\psi}^{s}\) reconstructs \(s_{t}\); \(\lambda_{rcon}\) is a scale factor,\mathcal{L}_{dCAE}=(H_{t}-f_{\psi}^{H}(f_{\phi}(s_{t})))^{2}+\lambda_{rcon}(s_{t}-f_{\psi}^{s}(f_{\phi}(s_{t})))^{2}.,where \(f_{\psi}^{H}\) predicts the highest return; \(f_{\psi}^{s}\) reconstructs \(s_{t}\); \(\lambda_{rcon}\) is a scale factor.
ICLR_2024_oral_23,6,"er. Thus, learning the true density model is equivalent to performing optimal denoising at all noise levels. Conversely, a suboptimal denoiser introduces a score approximation error, which in turn can result in an error in the modeled density.

Generally, the optimal denoising function \(f^{\star}\) (as well as the ""true"" distribution, \(p(x)\)) is unknown for photographic images, which makes numerical evaluation of sub-optimality challenging. We can however separate deviations from optimality arising from model bias and model variance. Model variance measures the size of the approximation class, and hence the strength (or restrictiveness) of the inductive biases. It can be evaluated without knowledge of \(f^{\star}\). Here, we define generalization as near-zero model variance (i.e., an absence of overfitting), which is agnostic to model bias. This is the subject of Section 2.2. Model bias measures the distance of the true score to the approximation class, and thus the alignment between the inductive biases and the data distribution. In the context of photographic images, visual quality of generated samples can be a qualitative indicator of the model bias, although high visual quality does not necessarily guarantee low model bias. We evaluate model bias in Section 3.2 by considering synthetic image classes for which \(f^{\star}\) is approximately known.

### Transition from memorization to generalization

DNNs are susceptible to overfitting, because the number of training examples is typically small relative to the model capacity. Since density estimation, in particular, suffers from the curse of dimensionality, overfitting is of more concern in the context of generative models. An overfitted denoiser performs well on training images but fails to generalize to test images, resulting in low-diversity generated images. Consistent with this, several papers have reported that diffusion models can memorize their training data (Sompalli et al., 2023; Carlini et al., 2023; Dar et al., 2023; Zhang et al., 2023). To directly assess this, we compared denoising performance on training and test data for different training set sizes \(N\). We trained denoisers on subsets of the (downsampled) CelebA dataset (Liu et al., 2015) of size \(N=10^{0},10^{1},10^{2},10^{3},10^{4},10^{5}\). We used a UNet architecture (Ronneberger et al., 2015), which is composed of 3 convolutional encoder and decoder blocks with rectifying non-linearities. These denoisers are universal and blind: they operate on all noise levels without having noise level as an input Mohan* et al. (2020). Networks are trained to minimize mean squared error (4). See Appendix A for architecture and training details.

Results are shown in Figure 1. When \(N=1\), the denoiser essentially memorizes the single training image, leading to a high test error. Increasing \(N\) substantially increases the performance on the test set while worsening performance on the training set, as the network transitions from memorization to generalization. At \(N=10^{5}\), empirical test and train error are matched for all noise levels.

To investigate this generalization further, we train denoisers on _non-overlapping_ subsets of CelebA of various size \(N\). We then generate samples using the scores learned by each denoiser, through the reverse diffusion algorithm of Kadkhodaie & Simoncelli (2020)--see Appendix A for details. Figure 2 shows samples generated by these denoisers, initialized from the same noise sample. For small \(N\), the networks memorize their respective training images. However, for large \(N\), the networks converge to the same score function (and thus sample from the same model density), generating nearly identical samples. This surprising behavior provides a much stronger demonstration of convergence than comparison of average train and test performance.

## 3 Inductive biases

The number of samples needed for estimation of an arbitrary probability density grows exponentially with dimensionality (the ""curse of dimensionality""). As a result, estimating high-dimensional distributions is only feasible if one imposes strong constraints or priors over the hypothesis space. In a diffusion model, these arise from the network architecture and the optimization algorithm, and are referred to as the inductive biases of the network (Wilson & Izmailov, 2020; Goyal & Bengio, 2022; Griffiths et al., 2023). In Section 2.2, we demonstrated that DNN denoisers can learn scores (and thus a density) from relatively small training sets. This generalization result, combined with the high quality of sampled images, is evidence that the inductive biases are well-matched to the ""true"" distribution of images, allowing the model to rapidly converge to a good solution through learning. On the contrary, when inductive biases are not aligned with the true distribution, the model will arrive at a poor solution with high model bias.

For diffusion methods, learning the right density model is equivalent to performing optimal denoising at all noise levels (see Section 2.1). The inductive biases on the density model thus arise directly from inductive biases in the denoiser. This connection offers a means of evaluating the accuracy of the learned probability models, which is generally difficult in high-dimensions.

### Denoising as shrinkage in an adaptive basis

The inductive biases of the DNN denoiser can be studied through an eigendecomposition of its Jacobian. We describe the general properties that are expected for an optimal denoiser, and examine several specific cases for which the optimal solution is partially known.

Jacobian eigenvectors as an adaptive basis.To analyze inductive biases, we perform a local analysis of a denoising estimator \(\hat{x}(y)=f(y)\) by looking at its Jacobian \(\nabla f(y)\). For simplicity, we assume that the Jacobian is symmetric and non-negative (we show below that this holds for the optimal denoiser, and it is approximately true of the network Jacobian (Mohan* et al., 2020)). We can then diagonalize it to obtain eigenvalues \((\lambda_{k}(y))_{1\leq k\leq d}\) and eigenvectors \((e_{k}(y))_{1\leq k\leq d}\).

If \(f(y)\) is computed with a DNN denoiser with no additive ""bias"" parameters, its input-output mapping is piecewise linear, as opposed to piecewise affine (Mohan* et al., 2020; Romano et al., 2017). It

Figure 1: Transition from memorization to generalization, for a UNet denoiser trained on face images. Each curve shows the denoising error (output PSNR, ten times log10 ratio of squared dynamic range to MSE) as a function of noise level (input PSNR), for a training set of size \(N\). As \(N\) increases, performance on the training set generally worsens (left), while performance on the test set improves (right). For \(N=1\) and \(N=10\), the train PSNR improves with unit slope, while test PSNR is poor, independent of noise level, a sign of memorization. The increase in test performance on small noise levels at \(N=1000\) is indicative of the transition phase from memorization to generalization. At \(N=10^{5}\), test and train PSNR are essentially identical, and the model is no longer overfitting the training data.

follows that the denoiser mapping can be rewritten in terms of the Jacobian eigendecomposition as","f(y)=\nabla f(y)\,y=\sum_{k}\lambda_{k}(y)\langle y,e_{k}(y)\rangle e_{ k}(y).",,"\hat{x}(y)=f(y)=\sum_{k=1}^{d} e_{k}(y) \cdot \lambda_{k}(y) \cdot \hat{x}_{k}(y),",where \(\hat{x}_{k}(y)\) are the coefficients in the adaptive basis formed by the eigenvectors of the Jacobian.
ICLR_2024_oral_23,10,"A small denoising error thus implies an approximately low-rank Jacobian (with many small eigenvalues) and thus an efficient approximation of \(x\) given \(y\).

In most cases, the optimal adaptive basis \((e_{k}^{\star}(y))_{1\leq k\leq d}\) is not known. Rather than aiming for exact optimality, classical analyses (Donoho, 1995) thus focus on the asymptotic decay of the denoising error as the noise level \(\sigma^{2}\) falls, up to multiplicative constants. This corresponds to finding a basis \((e_{k}(y))_{1\leq k\leq d}\) which captures the asymptotic slope of the PSNR plots in Figure 1 but not necessarily the intercept. This weaker notion of optimality is obtained by showing matching upper and lower-bounds on the asymptotic behavior of the denoising error. To provide intuition, we first consider a fixed orthonormal basis \(e_{k}(y)=e_{k}\), and then consider the more general case of best bases selected from a fixed dictionary.

Denoising in a fixed basis.Consider a denoising algorithm that is restricted to operate in a fixed basis \(e_{k}\) but can adapt its shrinkage factors \(\lambda_{k}(y)\). An unreachable lower-bound on the denoising error--and thus an upper-bound on the PSNR slope--is obtained by evaluating the performance of an ""oracle"" denoiser where the shrinkage factors \(\lambda_{k}\) depend on the unknown clean image \(x\) rather than the noisy observation \(y\)(Mallat, 2008). Appendix D.4 shows that the denoising error of this oracle is","\operatorname*{\mathbb{E}}_{x}\biggl{[}\sum_{k}\Bigl{(}(1-\lambda_{k}(x))^{2} \langle x,e_{k}\rangle^{2}+\lambda_{k}(x)^{2}\sigma^{2}\Bigr{)}\biggr{]},",,"\mathrm{MSE}(f^{\star},\sigma^{2})\geq \mathop{\mathbb{E}}_{y}\left[\sum_{k}\lambda_{k}^{\star}\right].",where \(\lambda_{k}^{\star}\) are the optimal shrinkage factors in the fixed basis
ICLR_2024_oral_24,8,"Henceforth, we refer to the dynamics associated with the Bridge Matching SDE as AGM-SDE, and its corresponding ODE counterpart as AGM-ODE. Meanwhile, the linearity of the system implies the intermediate state \(\mathbf{m}_{t}\) and the close form solution of score term are analytically available. In particular, the mean \(\boldsymbol{\mu}_{t}\) and covariance matrix \(\boldsymbol{\Sigma}_{t}\) of the intermediate marginal \(p_{t}(\mathbf{m}_{t}|\mathbf{x}_{1})=\mathcal{N}(\boldsymbol{\mu}_{t}, \boldsymbol{\Sigma}_{t})\) of such a system can be analytically computed with \(\boldsymbol{\Sigma}_{t}=\begin{bmatrix}\Sigma_{t}^{xx}&\Sigma_{t}^{xx}\\ \Sigma_{t}^{xx}&\Sigma_{t}^{xx}\end{bmatrix}\otimes\boldsymbol{I}_{d}\), and \(\boldsymbol{\mu}_{t}=\begin{bmatrix}\mu_{t}^{x}\\ \mu_{t}^{x}\end{bmatrix}\), provided we have the boundary conditions \(\boldsymbol{\mu}_{0}\) and \(\boldsymbol{\Sigma}_{0}\) in place, as outlined in Sarkka and Solin (2019). Please see Appendix.D.3 for detail. In order to sample from such multi-variant Gaussian, one need to decompose the covariance matrix by Cholesky decomposition, and \(\mathbf{m}_{t}\) is reparametrized as:","\mathbf{m}_{t}=\boldsymbol{\mu}_{t}+\mathbf{L}_{t}\boldsymbol{\epsilon}= \boldsymbol{\mu}_{t}+\begin{bmatrix}L_{t}^{xx}\boldsymbol{\epsilon}_{0}\\ L_{t}^{xv}\boldsymbol{\epsilon}_{0}+L_{t}^{vv}\boldsymbol{\epsilon}_{1}\end{bmatrix},\nabla_{\mathbf{v}}\log p_{t}:=-\ell_{t}\boldsymbol{\epsilon}_{1}","where \(\boldsymbol{\Sigma}_{t}=\mathbf{L}_{t}\mathbf{L}_{t}^{\mathsf{T}}\), \(\epsilon=\begin{bmatrix}\boldsymbol{\epsilon}_{0}\\ \boldsymbol{\epsilon}_{1}\end{bmatrix}\sim\mathcal{N}(\mathbf{0},\mathbf{I}_{2d})\) and \(\ell_{t}=\sqrt{\frac{\Sigma_{t}^{xx}}{\Sigma_{t}^{xx}\Sigma_{t}^{xx}-(\Sigma _{t}^{xx})^{2}}}\)","\mathbf{m}_{t}=\boldsymbol{\mu}_{t}+\mathbf{L}_{t}\epsilon,\quad \boldsymbol{\Sigma}_{t}=\begin{bmatrix}\Sigma_{t}^{xx}&\Sigma_{t}^{xx}\\ \Sigma_{t}^{xx}&\Sigma_{t}^{xx}\end{bmatrix}\otimes\boldsymbol{I}_{d},\quad \epsilon=\begin{bmatrix}\boldsymbol{\epsilon}_{0}\\ \boldsymbol{\epsilon}_{1}\end{bmatrix}\sim\mathcal{N}(\mathbf{0},\mathbf{I}_{2d}),\quad \ell_{t}=\sqrt{\frac{\Sigma_{t}^{xx}}{\Sigma_{t}^{xx}\Sigma_{t}^{xx}-(\Sigma_{t}^{xx})^{2}}}","where \(\boldsymbol{\Sigma}_{t}=\mathbf{L}_{t}\mathbf{L}_{t}^{\mathsf{T}}\), \(\epsilon=\begin{bmatrix}\boldsymbol{\epsilon}_{0}\\ \boldsymbol{\epsilon}_{1}\end{bmatrix}\sim\mathcal{N}(\mathbf{0},\mathbf{I}_{2d})\) and \(\ell_{t}=\sqrt{\frac{\Sigma_{t}^{xx}}{\Sigma_{t}^{xx}\Sigma_{t}^{xx}-(\Sigma_{t}^{xx})^{2}}}\)"
ICLR_2024_oral_24,12,"In Eq.11, \(\Phi(s,t)\) denotes the transition matrix for our system, while \(\mathbf{M}_{i,j}(\tau)\) represents the \(w-\)order multistep coefficient (Hochbruck and Ostermann, 2010). For a comprehensive derivation of these terms, please refer to Appendix.D.9. It is worth noting that the mapping of \(\mathbf{s}_{\theta}\) into both the position and velocity channels significantly emulates the errors introduced by discretization delays.

**Sampling-hop:** In the context of CLD (Dockhorn et al., 2021), their focus is on estimating the score function w.r.t. velocity, which essentially corresponds to estimating scaled \(\mathbf{\epsilon}_{1}\) in our notation. However, relying solely on the aforementioned information is not sufficient for estimating the data point \(\mathbf{x}_{1}\). Additional knowledge regarding \(\mathbf{\epsilon}_{0}\) is also required in order to perform such estimation. In our case, the training objective implicitly includes both \(\mathbf{\epsilon}_{0}\) and \(\mathbf{\epsilon}_{1}\) (see eq.9), hence one can manage to recover \(\mathbf{x}_{1}\) by Proposition.5. Remarkably, our observations have unveiled that when the network is equipped with additional velocity information, it acquires the capability to estimate the target data point during the early stages of the trajectory, as illustrated in fig.2. This estimation can be seamlessly integrated into AGM-SDE and AGM-ODE and we name it sampling-hop. Specifically,

**Proposition 5** (Sampling-Hop).: _Given the state, velocity and trained force term \(\mathbf{F}_{t}^{\theta}\) at time step \(t\) in sampling phase, The estimated data point \(\tilde{\mathbf{x}}_{1}\) can be represented as_","\tilde{\mathbf{x}}_{1}^{SDE}=\frac{(1-t)(\mathbf{F}_{t}^{\theta}+\mathbf{v}_{t })}{g_{t}^{2}P_{11}}+\mathbf{x}_{t},\text{ or }\quad\tilde{\mathbf{x}}_{1}^{ODE}=\frac{\mathbf{F}_{t}^{\theta}+g_{t}^{2}P_{ 11}(\alpha_{t}\mathbf{x}_{t}+\beta_{t}\mathbf{v}_{t})}{4(t-1)^{2}+g_{t}^{2}P_{ 11}(\alpha_{t}\mu_{t}^{x}+\beta_{t}\mu_{t}^{v})}",,"\tilde{\mathbf{x}}_{1}=\mathbf{x}_{t}+(1-t)\mathbf{F}_{t}^{\theta}(\mathbf{m}_{t},t)\end{latex>
<description>where \(\mathbf{F}_{t}^{\theta}\) is the parameterized force term, \(\mathbf{x}_{t}\) is the state at time \(t\), and \(t\) is the time variable.</description>","where \(\mathbf{F}_{t}^{\theta}\) is the parameterized force term, \(\mathbf{x}_{t}\) is the state at time \(t\), and \(t\) is the time variable."
ICLR_2024_oral_26,2,"ix. The edge features form a sparse block matrix where the sparsity pattern depends on the number of nodes per layer: the first diagonal block has size \(d_{0}\), the second \(d_{1}\), and so on. We can verify that putting \(\mathbf{\mathrm{W}}^{(l)}\) as the first off-diagonal blocks means that they have the expected size of \(d_{l}\times d_{l-1}\). The first \(d_{0}\) node features in \(\mathbf{V}\) are set to \(0\) to reflect the fact that there are no biases at the input nodes. We can show that our neural graph has a one-to-one correspondence to the neural network's computation graph. This ensures that distinct MLPs get mapped to distinct neural graphs.

Similar to the example above, many applications of neural networks in parameter space only include the neural network parameters as input. For such applications, an MLP has scalar weights \(\mathbf{\mathrm{W}}^{(l)}_{ij}\in\mathbb{R}\) and biases \(\mathbf{\mathrm{b}}^{(l)}_{i}\in\mathbb{R}\) comprising the elements of \(\mathbf{E},\mathbf{V}\), resulting in one-dimensional features, _i.e._\(d_{\mathbf{V}}=d_{\mathbf{E}}=1\). Depending on the task at hand, however, we have the flexibility to incorporate additional edge and node features. We explore some examples of this in Section 2.4.

### CNNs as graphs

So far, we have only described how to encode basic MLPs as graphs. We now address how to generalize the graph representation to alternative network architectures, namely convolutional networks. To make the exposition of our method clear, we will use the following interpretation of convolutional layers and CNNs. Convolutional layers take as input a multi-channel input image (_e.g._ an RGB image has \(C=3\) channels) or a multi-channel feature map, and process it with a filter bank, _i.e._ a collection of convolutional kernels - often termed filters. Each filter results in a single-channel feature map; applying all filters in the filter bank results in a collection of feature maps, which we concatenate together in a multi-channel feature map. CNNs, in their simplest form, are a stack of convolutional layers mixed with non-linearities in between.

Permutation symmetries in a CNN work similarly to an MLP; permuting the filters in a layer while simultaneously permuting the channels of each filter in the subsequent layer effectively cancels out the permutations, shown visually in Figure 5 in Appendix C.2. Under the aforementioned interpretation, single-channel slices of a multi-channel feature map (or single-channel slices of the input) correspond to nodes in our neural graph. Each node is connected via edges incoming from a particular convolutional kernel from the filter bank. By treating each channel as a node, our CNN neural graph respects the permutation symmetries of the CNN.

We now describe the neural graph representation for each component in a CNN (with more details in Appendix C.2). As a working example, let us consider a CNN with \(L\) convolutional layers. It consists of filters \(\left\{\mathbf{\mathrm{W}}^{(l)}\right\}\) and biases \(\left\{\mathbf{\mathrm{b}}^{(l)}\right\}\) for layers \(l\in\{1,\dots,L\}\), where \(\mathbf{\mathrm{W}}^{(l)}\in\mathbb{R}^{d_{l}\times d_{l-1}\times w_{l}\times h_{l }},\mathbf{\mathrm{b}}^{(l)}\in\mathbb{R}^{d_{l}}\), and \(w_{l},h_{l}\) denote the width and the height of kernels at layer \(l\).

**Convolutional layers.** Convolutional layers are the core operation in a convolutional network. Since channels in a CNN correspond to nodes in the neural graph, we can treat the biases the same way as in an MLP, namely as node features - see Equation (1). The kernels, however, cannot be treated identically due to their spatial dimensions, which do not exist for MLP weights. To resolve this, we represent the kernels by flattening their spatial dimensions to a vector. To ensure spatial self-consistency across kernels of different sizes, we first zero-pad all kernels to a maximum size \(s=(w_{\text{max}},h_{\text{max}})\), and then flatten them. This operation allows for a unified representation across different kernel sizes; we can process all kernels with the same network. The maximum kernel size is chosen as a hyperparameter per experiment; this operation is visualized in Figure 6 in Appendix C.2. After this operation, the kernels can be treated as a multi-dimensional equivalent of linear layer weights. We construct the edge features matrix similarly to Equation (1); the only difference is that this matrix no longer has scalar features. Instead, we have \(\mathbf{E}\in\mathbb{R}^{n\times n\times d_{\mathbf{E}}}\), with \(d_{\mathbf{E}}=w_{\text{max}}\cdot h_{\text{max}}\).

**Flattening layer.** CNNs are often tasked with predicting a single feature vector per image. In such cases, the feature maps have to be converted to a single feature vector. Modern CNN architectures (He et al., 2016) perform adaptive pooling after the last convolutional layer, which pools the whole feature map in a single feature vector, while traditional CNNs (Simonyan and Zisserman, 2015) achieved that by flattening the spatial dimensions of the feature maps. The downside of the latter approach is that CNNs are bound to a specific input resolution and cannot process arbitrary images. Our neural graph is not bound to any spatial resolution, and as such, its construction does not require any modifications to integrate adaptive pooling. While the CNNs in all our experiments employ adaptive pooling, we also propose two mechanisms to address traditional flattening, which we discuss in Appendix C.2.

**Linear layers.** Linear layers are often applied after flattening or adaptive pooling to produce the final feature vector representation for an image. The most straightforward way to treat linear layers in a CNN is in the exact same fashion as in an MLP. The downside of this approach is that linear layers and convolutional layers have separate representations, as their \(d_{\mathbf{E}}\) will typically differ. An alternative is to treat linear layers as \(1\times 1\) convolutions, which allows for a unified representation between linear and convolutional layers. When treated as convolutions, the linear layers are padded to the maximum kernel size and flattened. In our experiments, we explore both options, and choose the most suitable via hyperparameter search.

### Modelling heterogeneous architectures

One of the primary benefits of the neural graph representation is that it becomes straightforward to represent varying network architectures that can all be processed by the same graph neural network. Notably, we do not require any changes to accommodate a varying number of layers, number of dimensions per layer, or even completely different architectures and connectivities between layers. When dealing with a single architecture, we can opt to ignore certain architectural components that are shared across instances, as our method - and related methods - can learn to account for them during training. These include activation functions and residual connections. Thus, we will now describe how we can incorporate varying non-linearities and residual connections in heterogeneous architectures of CNNs or MLPs.

**Non-linearities.** Non-linearities are functions applied elementwise to each neuron, and can thus be encoded as node features. We create embeddings for a list of common activation functions and add them to the node features.

**Residual connections.** Residual connections are an integral component of modern CNN architectures. A residual connection directly connects the input of a layer to its output as \(\mathbf{y}=f(\mathbf{x})+\mathbf{x}\). Incorporating residual connections in our neural graph architecture is straightforward, since we can include edges from each sender node in \(\mathbf{x}\) to the respective receiving node in \(\mathbf{y}\). Since residual connections can be rewritten as \(\mathbf{y}=f(\mathbf{x})+\mathbf{I}\mathbf{x}\), where \(\mathbf{I}\) is the identity matrix, the edge features have a value of 1 for each neuron connected.

Transformers.The feedforward component of transformers (Vaswani et al., 2017) can be treated as an MLP and its neural graph follows a similar schema as previously outlined. We explain the conversion rules for normalization layers in Appendix C.3 and the conversion rules for multi-head self-attention in Appendix C.4. After individually converting the parts, we can compose them into a neural graph for transformers.

### Node and Edge Representation

Our neural graph representation gives us the flexibility to choose what kinds of data serve as node and edge features. Though we mainly focus on weights and biases, there are also other options that we can use, for example we use the gradients in a learning to optimize setting.

**Edge direction.** Our basic encoding only considers the forward pass computations of the neural network, yielding a directed acyclic graph. To facilitate information flow from later layers back to earlier layers we can add reversed edges to the neural graph. Specifically, we include \(\mathbf{E}^{\top}\) as additional edge features. Similarly, we can also include \(\mathbf{E}+\mathbf{E}^{\top}\) as extra features representing undirected features.

**Probe features.** Humans tend to interpret complicated functions by probing the function with a few input samples and inspecting the resulting output. We give the graph neural network a similar ability by adding extra features to every node that correspond to specific inputs. In particular, we learn a set of sample input values that we pass through the input neural network and retain the values for all the intermediate activations and the output. For example, consider the simple input neural network \(f(\mathbf{x})=\mathbf{W}^{(2)}\alpha\big{(}\mathbf{W}^{(1)}\mathbf{x}+\mathbf{b}^{(1) }\big{)}+\mathbf{b}^{(2)}\) for which we acquire an extra node feature:","\mathbf{V}_{\text{probe}}=\Big{(}\mathbf{x},\alpha\Big{(}\mathbf{W}^{(1)}\mathbf{x}+ \mathbf{b}^{(1)}\Big{)},f(\mathbf{x})\Big{)}^{\top},",,\mathbf{V}_{\text{extra}}=\begin{pmatrix}\mathbf{W}^{(1)}\mathbf{x}+\mathbf{b}^{(1)}\\ \mathbf{W}^{(2)}\alpha\big{(}\mathbf{W}^{(1)}\mathbf{x}+\mathbf{b}^{(1)}\big{)}+\mathbf{b}^{(2)}\end{pmatrix},Extra node feature for the input neural network capturing intermediate activations.
ICLR_2024_oral_3,2,"with the objective function \(f:\mathbb{R}^{d}\to\mathbb{R}\) and \(X\) taking values in a finite state space \(\mathcal{N}\) with distribution \(\mathbf{\mu}\triangleq[\mu_{i}]_{i\in\mathcal{N}}\). Leveraging partial gradient information per iteration, these algorithms have been recognized for their scalability and efficiency with large datasets (Bottou et al., 2018; Even, 2023). For any given _noise sequence_\(\{X_{n}\}_{n\geq 0}\subset\mathcal{N}\), and step size sequence \(\{\beta_{n}\}_{n\geq 0}\subset\mathbb{R}_{+}\), most stochastic optimization algorithms can be classified as stochastic approximations (SA) of the form","\mathbf{\theta}_{n+1}=\mathbf{\theta}_{n}+\beta_{n+1}H(\mathbf{\theta}_{n},X_{n+1}),\ \ \ \forall\ n\geq 0,","where, roughly speaking, \(H(\mathbf{\theta},i)\) contains gradient information \(\nabla_{\mathbf{\theta}}F(\theta,i)\), such that \(\mathbf{\theta}^{*}\) solves \(\mathbf{h}(\mathbf{\theta})\triangleq\mathbb{E}_{X\sim\mathbf{\mu}}[H(\mathbf{\theta},X)]= \sum_{i\in\mathcal{N}}\mu_{i}H(\mathbf{\theta},i)=\mathbf{0}\)","\mathbf{h}(\mathbf{\theta})\triangleq\mathbb{E}_{X\sim\mathbf{\mu}}[H(\mathbf{\theta},X)]= \sum_{i\in\mathcal{N}}\mu_{i}H(\mathbf{\theta},i)=\mathbf{0}","where \(H(\mathbf{\theta},i)\) contains gradient information \(\nabla_{\mathbf{\theta}}F(\theta,i)\)."
ICLR_2024_oral_3,7,"featuring distinct asymptotic covariance matrices \(\mathbf{V}_{\boldsymbol{\theta}}^{(1)}(\alpha),\mathbf{V}_{\boldsymbol{ \theta}}^{(2)}(\alpha)\) and \(\mathbf{V}_{\boldsymbol{\theta}}^{(3)}(\alpha)\), respectively. The three matrices coincide when \(\alpha=0\),6. Moreover, the derivation of the CLT for cases (i) and (iii), for which (4) corresponds to _two-timescale_ SA with _controlled_ Markov noise, is the first of its kind and thus a key technical contribution in this paper, as expanded upon in Section 3.

Footnote 6: The \(\alpha=0\) case is equivalent to simply running the base Markov chain, since from (3) we have \(\mathbf{K}[\cdot]=\mathbf{P}\), thus bypassing the SRRWâ€™s effect and rendering all three cases nearly the same.

**3.** For case (i), we show that \(\mathbf{V}_{\boldsymbol{\theta}}^{(1)}(\alpha)\) decreases to zero (in the sense of Loewner ordering introduced in Section 2.1) as \(\alpha\) increases, with rate \(O(1/\alpha^{2})\). This is especially surprising, since the asymptotic performance benefit from using the SRRW kernel with \(\alpha\) in (3), to drive the noise terms \(X_{n}\), is _amplified_ in the context of distributed learning and estimating \(\boldsymbol{\theta}^{*}\); compared to the sampling case, for which the rate is \(O(1/\alpha)\) as mentioned earlier. For case (iii), we show that \(\mathbf{V}_{\boldsymbol{\theta}}^{(3)}(\alpha)\!=\!\mathbf{V}_{\boldsymbol{ \theta}}^{(3)}(0)\) for all \(\alpha\!\geq 0\), implying that using the SRRW in this case provides no asymptotic benefit than the

Figure 1: Visualization of token algorithms using SRRW versus traditional MC in distributed learning. Our CLT analysis, extended from SRRW itself to distributed stochastic approximation, leads to near-zero variance for the SA iteration \(\boldsymbol{\theta}_{n}\). Node numbers on the left denote visit counts.

original base Markov chain, and thus performs worse than case (i). In summary, we deduce that \(\mathbf{V}_{\boldsymbol{\theta}}^{(1)}(\alpha_{2})\!<_{L}\!\mathbf{V}_{\boldsymbol{ \theta}}^{(1)}(\alpha_{1})\!<_{L}\!\mathbf{V}_{\boldsymbol{\theta}}^{(1)}(0)\!= \!\mathbf{V}_{\boldsymbol{\theta}}^{(3)}(0)\!=\!\mathbf{V}_{\boldsymbol{\theta }}^{(3)}(\alpha)\) for all \(\alpha_{2}>\alpha_{1}>0\) and \(\alpha>0\).7

Footnote 7: In particular, this is the reason why we advocate for a more general step size \(\gamma_{n}=(n+1)^{-a}\) in the SRRW iterates with \(a<1\), allowing us to choose \(\beta_{n}=(n+1)^{-b}\) with \(b\in(a,1]\) to satisfy \(\beta_{n}=o(\gamma_{n})\) for case (i).

**4**. We numerically simulate our SA-SRRW algorithm on various real-world datasets, focusing on a binary classification task, to evaluate its performance across all three cases. By carefully choosing the function \(H\) in SA-SRRW, we test the SGD and algorithms driven by SRRW. Our findings consistently highlight the superiority of case (i) over cases (ii) and (iii) for diverse \(\alpha\) values, even in their finite time performance. Notably, our tests validate the variance reduction at a rate of \(O(1/\alpha^{2})\) for case (i), suggesting it as the best algorithmic choice among the three cases.

## 2 Preliminaries and Model Setup

In Section 2.1, we first standardize the notations used throughout the paper, and define key mathematical terms and quantities used in our theoretical analyses. Then, in Section 2.2, we consolidate the model assumptions of our SA-SRRW algorithm (4). We then go on to discuss our assumptions, and provide additional interpretations of our use of generalized step-sizes.

### Basic Notations and Definitions

Vectors are denoted by lower-case bold letters, e.g., \(\mathbf{v}\triangleq[v_{i}]\in\mathbb{R}^{D}\), and matrices by upper-case bold, e.g., \(\mathbf{M}\triangleq[M_{ij}]\in\mathbb{R}^{D\times D}\). \(\mathbf{M}^{-T}\) is the transpose of the matrix inverse \(\mathbf{M}^{-1}\). The diagonal matrix \(\mathbf{D}_{\mathbf{v}}\) is formed by vector \(\mathbf{v}\) with \(v_{i}\) as the \(i\)'th diagonal entry. Let \(\mathbf{1}\) and \(\mathbf{0}\) denote vectors of all ones and zeros, respectively. The identity matrix is represented by \(\mathbf{I}\), with subscripts indicating dimensions as needed. A matrix is _Hurwitz_ if all its eigenvalues possess strictly negative real parts. \(\mathds{1}_{\{\cdot\}}\) denotes an indicator function with condition in parentheses. We use \(\|\cdot\|\) to denote both the Euclidean norm of vectors and the spectral norm of matrices. Two symmetric matrices \(\mathbf{M}_{1},\mathbf{M}_{2}\) follow Loewner ordering \(\mathbf{M}_{1}\!<_{L}\mathbf{M}_{2}\) if \(\mathbf{M}_{2}\!-\!\mathbf{M}_{1}\) is positive semi-definite and \(\mathbf{M}_{1}\neq\mathbf{M}_{2}\). This slightly differs from the conventional definition with \(\leq_{L}\), which allows \(\mathbf{M}_{1}\!=\!\mathbf{M}_{2}\).

Throughout the paper, the matrix \(\mathbf{P}\triangleq[P_{i,j}]_{i,j\in\mathcal{N}}\) and vector \(\boldsymbol{\mu}\triangleq[\mu_{i}]_{i\in\mathcal{N}}\) are used exclusively to denote an \(N\times N\)-dimensional transition kernel of an ergodic Markov chain, and its stationary distribution, respectively. Without loss of generality, we assume \(P_{ij}>0\) if and only if \(a_{ij}>0\). Markov chains satisfying the _detailed balance equation_, where \(\mu_{i}P_{ij}=\mu_{j}P_{ji}\) for all \(i,j\in\mathcal{N}\), are termed _time-reversible_. For such chains, we use \((\lambda_{i},\mathbf{u}_{i})\) (resp. \((\lambda_{i},v_{i})\)) to denote the \(i\)'th left (resp. right) eigenpair where the eigenvalues are ordered: \(-1\!<\!\lambda_{1}\!\leq\!\cdots\!\leq\!\lambda_{N-1}\!<\!\lambda_{N}\!=\!1\), with \(\mathbf{u}_{N}\!=\!\boldsymbol{\mu}\) and \(\mathbf{v}_{N}\!=\!\boldsymbol{1}\) in \(\mathbb{R}^{N}\). We assume eigenvectors to be normalized such that \(\mathbf{u}_{i}^{T}\mathbf{v}_{i}\!=\!1\) for all \(i\), and we have \(\mathbf{u}_{i}\!=\!\mathbf{D}_{\boldsymbol{\mu}}\mathbf{v}_{i}\) and \(\mathbf{u}_{i}^{T}\mathbf{v}_{j}\!=\!0\) for all \(i,j\!\in\!\mathcal{N}\). We direct the reader to Aldous & Fill (2002, Chapter 3.4) for a detailed exposition on spectral properties of time-reversible Markov chains.

### SA-SRRW: Key Assumptions and Discussions

**Assumptions:** All results in our paper are proved under the following assumptions.

* The function \(H:\mathbb{R}^{D}\times\mathcal{N}\to\mathbb{R}^{D}\), is a continuous at every \(\boldsymbol{\theta}\in\mathbb{R}^{D}\), and there exists a positive constant \(L\) such that \(\|H(\boldsymbol{\theta},i)\|\leq L(1+\|\boldsymbol{\theta}\|)\) for every \(\boldsymbol{\theta}\in\mathbb{R}^{D},i\in\mathcal{N}\).
* Step sizes \(\beta_{n}\) and \(\gamma_{n}\) follow \(\beta_{n}\!=\!(n\!+\!1)^{-b}\), and \(\gamma_{n}\!=\!(n\!+\!1)^{-a}\), where \(a,b\in(0.5,1]\).
* Roots of function \(\mathbf{h}(\cdot)\) are disjoint, which comprise the globally attracting set \(\Theta\triangleq\left\{\boldsymbol{\theta}^{*}|\mathbf{h}(\boldsymbol{\theta}^ {*})\!=\!0,\nabla\mathbf{h}(\boldsymbol{\theta}^{*})+\frac{\mathbf{1}_{(n\!=\! 1)}}{2}\mathbf{I}\text{ is Hurwitz}\right\}\neq\emptyset\) of the associated ordinary differential equation (ODE) for iteration (4c), given by \(d\boldsymbol{\theta}(t)/dt\!=\!\mathbf{h}(\boldsymbol{\theta}(t))\).
* For any \((\boldsymbol{\theta}_{0},\mathbf{x}_{0},X_{0})\in\mathbb{R}^{D}\times\text{Int}( \Sigma)\times\mathcal{N}\), the iterate sequence \(\{\boldsymbol{\theta}_{n}\}_{n\geq 0}\) (resp. \(\{\mathbf{x}_{n}\}_{n\geq 0}\)) is \(\mathbb{P}_{\Theta_{0},\mathbf{x}_{0},X_{0}}\)-almost surely contained within a compact subset of \(\mathbb{R}^{D}\) (resp. \(\text{Int}(\Sigma)\)).

**Discussions on Assumptions:** Assumption A1 requires \(H\) to only be locally Lipschitz albeit with linear growth, and is less stringent than the globally Lipschitz assumption prevalent in optimization literature (Li & Wai, 2022; Hendrikx, 2023; Even, 2023).

Assumption A2 is the general umbrella assumption under which cases (i), (ii) and (iii) mentioned in Section 1 are extracted by setting: (i) \(a<b\), (ii) \(a=b\), and (iii) \(a>b\). Cases (i) and (iii) render \(\mathbf{\theta}_{n},\mathbf{x}_{n}\) on different timescales; the polynomial form of \(\beta_{n},\gamma_{n}\) widely assumed in the two-timescale SA literature (Mokkadem & Pelletier, 2006; Zeng et al., 2021; Hong et al., 2023). Case (ii) characterizes the SA-SRRW algorithm (4) as a single-timescale SA with polynomially decreasing step size, and is among the most common assumptions in the SA literature (Borkar, 2022; Fort, 2015; Li et al., 2023). In all three cases, the form of \(\gamma_{n}\) ensures \(\gamma_{n}\leq 1\) such that the SRRW iterates \(\mathbf{x}_{n}\) in (4b) is within \(\text{Int}(\Sigma)\), ensuring that \(\mathbf{K}[\mathbf{x}_{n}]\) is well-defined for all \(n\geq 0\).

In Assumption A3, limiting dynamics of SA iterations \(\{\mathbf{\theta}_{n}\}_{n\geq 0}\) closely follow trajectories \(\{\mathbf{\theta}(t)\}_{t\geq 0}\) of their associated ODE, and assuming the existence of globally stable equilibria is standard (Borkar, 2022; Fort, 2015; Li et al., 2023). In optimization problems, this is equivalent to assuming the existence of at most countably many local minima.

Assumption A4 assumes almost sure boundedness of iterates \(\mathbf{\theta}_{n}\) and \(\mathbf{x}_{n}\), which is a common assumption in SA algorithms (Kushner & Yin, 2003; Chen, 2006; Borkar, 2022; Karmakar & Bhatnagar, 2018; Li et al., 2023) for the stability of the SA iterations by ensuring the well-definiteness of all quantities involved. Stability of the weighted empirical measure \(\mathbf{x}_{n}\) of the SRRW process is practically ensured by studying (4b) with a truncation-based procedure (see Doshi et al., 2023, Remark 4.5 and Appendix E for a comprehensive explanation), while that for \(\mathbf{\theta}_{n}\) is usually ensured either as a by-product of the algorithm design, or via mechanisms such as projections onto a compact subset of \(\mathbb{R}^{D}\), depending on the application context. We now provide additional discussions regarding the step-size assumptions and their implications on the SRRW iteration (4b).

**SRRW with General Step Size:** As shown in Benaim & Cloez (2015, Remark 1.1), albeit for a completely different non-linear Markov kernel driving the algorithm therein, iterates \(\mathbf{x}_{n}\) of (4b) can also be expressed as _weighted_ empirical measures of \(\{X_{n}\}_{n\geq 0}\), in the following form:","\mathbf{x}_{n}=\frac{\sum_{i=1}^{n}\omega_{i}\mathbf{\theta}_{X_{i}}+\omega_{0} \mathbf{x}_{0}}{\sum_{i=0}^{n}\omega_{i}},\ \ \text{where}\ \ \omega_{0}=1,\ \ \text{and}\ \ \omega_{n}=\frac{\gamma_{n}}{\prod_{i=1}^{n}(1-\gamma_{i})},",,"\mathbf{V}_{\boldsymbol{\theta}}^{(k)}(\alpha)\triangleq\mathbb{E}[(\boldsymbol{\theta}_{n}-\boldsymbol{\theta}^{*})(\boldsymbol{\theta}_{n}-\boldsymbol{\theta}^{*})^{T}],\quad k=1,2,3.","where \(\mathbf{V}_{\boldsymbol{\theta}}^{(k)}(\alpha)\) represents the asymptotic covariance matrices for the three cases of convergence in the SA-SRRW algorithm, with \(k\) indicating the respective case."
ICLR_2024_oral_3,8,"for all \(n>0\). For the special case when \(\gamma_{n}=1/(n+1)\) as in Doshi et al. (2023), we have \(\omega_{n}=1\) for all \(n\geq 0\) and \(\mathbf{x}_{n}\) is the typical, unweighted empirical measure. For the additional case considered in our paper, when \(a<1\) for \(\gamma_{n}\) as in assumption A2, we can approximate \(1-\gamma_{n}\approx e^{-\gamma_{n}}\) and \(\omega_{n}\approx n^{-a}e^{n^{(1-a)}/(1-a)}\). This implies that \(\omega_{n}\) will increase at sub-exponential rate, giving more weight to recent visit counts and allowing it to quickly 'forget' the poor initial measure \(\mathbf{x}_{0}\) and shed the correlation with the initial choice of \(X_{0}\). This'speed up' effect by setting \(a<1\) is guaranteed in case (i) irrespective of the choice of \(b\) in Assumption A2, and in Section 3 we show how this can lead to further reduction in covariance of optimization error \(\mathbf{\theta}_{n}=\mathbf{\theta}^{*}\) in the asymptotic regime.

**Additional assumption for case (iii):** Before moving on to Section 3, we take another look at the case when \(\gamma_{n}=o(\beta_{n})\), and replace A3 with the following, stronger assumption only for case (iii).

* For any \(\mathbf{x}\in\text{Int}(\Sigma)\), there exists a function \(\rho:\text{Int}(\Sigma)\operatorname{\rightarrow}\mathbb{R}^{D}\) such that \(\|\rho(\mathbf{x})\|\operatorname{\leq}L_{2}(1+\|\mathbf{x}\|)\) for some \(L_{2}\operatorname{>}0\), \(\mathbb{E}_{i\operatorname{\sim}\mathbf{\pi}[\mathbf{x}]}(H(\rho(\mathbf{x}),i)] \operatorname{=}0\) and \(\mathbb{E}_{i\operatorname{\sim}\mathbf{\pi}[\mathbf{x}]}[\nabla H(\rho(\mathbf{x }),i)]+\frac{1_{\{n=1\}}}{2}\mathbf{I}\) is Hurwitz.

While Assumption A3\({}^{\prime}\) for case (iii) is much stronger than A3, it is not detrimental to the overall results of our paper, since case (i) is of far greater interest as impressed upon in Section 1. This is discussed further in Appendix C.

## 3 Asymptotic Analysis of the SA-SRRW Algorithm

In this section, we provide the main results for the SA-SRRW algorithm (4). We first present the a.s. convergence and the CLT result for SRRW with generalized step size, extending the results in Doshi et al. (2023). Building upon this, we present the a.s. convergence and the CLT result for the SA iterate \(\mathbf{\theta}_{n}\) under different settings of step sizes. We then shift our focus to the analysis of the different asymptotic covariance matrices emerging out of the CLT result, and capture the effect of \(\alpha\) and the step sizes, particularly in cases (i) and (iii), on \(\mathbf{\theta}_{n}-\mathbf{\theta}^{*}\) via performance ordering.

**Almost Sure convergence and CLT:** The following result establishes first and second order convergence of the sequence \(\{\mathbf{x}_{n}\}_{n\geq 0}\), which represents the weighted empirical measures of the SRRW process \(\{X_{n}\}_{n\geq 0}\), based on the update rule in (4b).

**Lemma 3.1**.: _Under Assumptions A1, A2 and A4, for the SRRW iterates (4b), we have_","\mathbf{x}_{n}\xrightarrow[n\to\infty]{a.s.}\boldsymbol{\mu},\quad \text{and}\quad\gamma_{n}^{-1/2}(\mathbf{x}_{n}-\boldsymbol{\mu})\xrightarrow[n \to\infty]{dist.}N(\mathbf{0},\mathbf{V}_{\mathbf{x}}(\alpha)), || \text{where}\quad\mathbf{V}_{\mathbf{x}}(\alpha)=\sum_{i=1}^{N-1 }\frac{1}{2\alpha(1+\lambda_{i})+2-\mathds{1}_{\{a=1\}}}\cdot\frac{1+\lambda_ {i}}{1-\lambda_{i}}\mathbf{u}_{i}\mathbf{u}_{i}^{T}.",,"\mathbf{x}_{n}=\frac{\sum_{i=1}^{n}\omega_{i}\boldsymbol{\theta}_{X_{i}}+\omega_{0} \mathbf{x}_{0}}{\sum_{i=0}^{n}\omega_{i}},\ \ \text{where}\ \ \omega_{0}=1,\ \ \text{and}\ \ \omega_{n}=\frac{\gamma_{n}}{\prod_{i=1}^{n}(1-\gamma_{i})},",for all \(n>0\).
ICLR_2024_oral_31,1,"### Preliminaries

**Static Pruning.** Given a dataset \(\mathcal{D}=\{z_{i}\}|_{i=1}^{|\mathcal{D}|}=\{(x_{i},y_{i})\}|_{i=1}^{|\mathcal{ D}|}\), a score \(\mathcal{H}(z)\) can be defined for each sample. For the pruning process, samples are discarded by a pruning probability \(\mathcal{P}\) defined on top of \(\mathcal{H}\). Static pruning directly discards all samples satisfying a certain condition before training, resulting in \(\mathcal{P}(z;\mathcal{H})\in\{0,1\}\). For examples, Toneva et al. (2018) defines:","\mathcal{P}(z;\mathcal{H})=\mathds{1}(\mathcal{H}(z)<\bar{\mathcal{H}}),",where \(\bar{\mathcal{H}}\) is a threshold and \(\mathds{1}(\cdot)\) is indicator function,\mathcal{P}(z;\mathcal{H}) = \mathds{1}(\mathcal{H}(z) < \bar{\mathcal{H}}),where \(\bar{\mathcal{H}}\) is a threshold and \(\mathds{1}(\cdot)\) is the indicator function
ICLR_2024_oral_39,4,"\).

**Architecture and Training.** We use the video U-Net architecture (Ho et al., 2022) to implement UniSim by employing interleaved temporal and spatial attention and convolution layers in both the downsampling and upsampling passes. For history conditioning, we replicate the conditioning frames at all future frame indices, and concatenate the conditioning frames with the noise sample for each of the future frame to serve as input to the U-Net. UniSim model has 5.6B parameters and requires 512 TPU-v3 and 20 days to train on all data. See more details in Appendix C.

## 3 Simulating Real-World Interactions

We now demonstrate emulating real-world manipulation and navigation environments by simulating both action-rich and long-horizon interactions for both humans and robots.

### Action-Rich, Long-Horizon, and Diverse Interactions

**Action-Rich Simulation.** We first demonstrate action-rich interactions through natural language actions. Figure 3 shows simulation of human manipulation and navigation starting from the same initial observation (left-most column). We can instruct a person in the initial frame to perform various kitchen tasks (top left), press different switches (top right), or navigate scenes (bottom). The model only trained on generic internet data, without action-rich manipulation data such as EPIC-KITCHENS (Damen et al., 2018), fails to simulate action-rich manipulations (Appendix F).

**Long-Horizon Simulation.** Next, we illustrate 8 _sequential_ interactions in Figure 4. We condition the simulation of each interaction on previous observations and new language action as described in Section 2.2. UniSim successfully preserves objects manipulated by previous instructions (e.g., the orange and can are preserved in the drawers in Columns 4, 5, 7, 8 after being put in the drawers). See additional long-horizon interactions in Appendix A.1.

**Diversity and Stochasticity in the Simulator.** UniSim can also support highly diverse and stochastic environment transitions, e.g., diverse objects being revealed after removing the towel on top (Figure 5 left), diverse object colors and locations (cups and pens in Figure 5 right), and real-world variabilities such as change in camera angles. Flexibility in diffusion models promotes simulation of highly stochastic environments that cannot be controlled by actions, so that a policy can learn to only control the controllable part (Yang et al., 2022).

### Ablation and Analysis

**Frame Conditioning Ablations.** We ablate over choices of past frames to condition on using a validation split of the Ego4D dataset (Grauman et al., 2022), which contains egocentric movement requiring proper handling of observation history. We compare UniSim conditioned on different

\begin{table}
\begin{tabular}{l|c|c|c|c} Condition & FID \(\downarrow\) & FVD \(\downarrow\) & IS \(\uparrow\) & CLIP \(\uparrow\) \\ \hline
1 frame & 59.47 & 315.69 & 3.03 & 22.55 \\
4 distant & 34.89 & 237 & 3.43 & 22.62 \\
4 recent & **34.63** & **211.3** & **3.52** & **22.63** \\ \end{tabular}
\end{table}
Table 1: **Ablations of history conditioning using FVD, FID, and Inception score, and CLIP score on Ego4D. Conditioning on multiple frames is better than on a single frame, and recent history has an edge over distant history.**

Figure 4: **Long-horizon simulations.** UniSim sequentially simulates 8 interactions autoregressively. The simulated interactions maintain temporal consistency across long-horizon interactions, correctly preserving objects and locations (can on counter in column 2-7, orange in drawer in column 4-5).

Figure 3: **Action-rich simulations.** UniSim can support manipulation actions such as â€œcut carrotsâ€, â€œwash handsâ€, and â€œpickup bowlâ€ from the same initial frame (top left) and other navigation actions.

numbers of past frames in Table 1. Conditioning on 4 frames is better than conditioning on a single frame, but conditioning on history that is too far in the past (4 frames with exponentially increasing distances) can hurt performance. Increasing the number of conditioning frames beyond 4 did not further improve performance on Ego4D, but it could be helpful for applications that require memory from distant past (e.g., navigation for retrieval).

**Simulating Low-Data Domains.** During joint training of UniSim on diverse data, we found that naively combining datasets of highly varying size can result in low generation quality in low-data domains. While we can increase the weight of these domains in the data mixture during training, we found that attaching a domain identifier such as the name of the dataset to the actions being conditioned on improves generation quality in low-data domains, as shown in Figure 6. While such domain identifier improves in-distribution generation quality, we found domain-specific identifiers to hurt generalization to other domains, and should only be applied with the test domain is in distribution of the training domain.

## 4 Applications of UniSim

We now demonstrate how UniSim can be used to train other types of machine intelligence such as vision-language policies, RL agents, and vision-language models through simulating highly realistic experiences.

### Training Long-Horizon Vision-Language Policies through Hindsight Labeling.

Language models and vision language models (VLM) have recently been used as policies that can operate in image or text based observation and action spaces (Du et al., 2023; Driess et al., 2023; Brohan et al., 2023). One major challenge in learning such agents lies in the need for large amounts of language action labels. The labor intensity in data collection only increases as tasks increase in horizon and complexity. UniSim can generate large amounts of training data for VLM policies through hindsight relabeling.

**Setup and Baseline.** We use data from the Language Table environment (Lynch and Sermanet, 2020) for learning geometric rearrangements of blocks on a table. We train an image-goal conditioned VLM policy to predict language instructions and the motor controls from the start and goal images using the PALM-E architecture (Driess et al., 2023) (See data and model details in Appendix D.1). For the baseline, the goal is set to the last frame of the original short-horizon trajectories. During each evaluation run, we set the long-horizon goal by modifying the location of 3-4 blocks, and measure the blocks' distance to their goal states after executing 5 instructions using the VLM policy. We define the reduction in distance to goal (RDG) metric as","\text{RDG}=\frac{\|s_{0}-s_{\text{goal}}\|_{2}-\|s_{T}-s_{\text{goal}}\|_{2}}{ \|s_{0}-s_{\text{goal}}\|_{2}},","where \(s_{T}\) represents the underlying block locations after executing the policy, \(s_{0}\) and \(s_{\text{goal}}\) represents the initial and goal block locations","RDG = \frac{\|s_{0} - s_{T}\| - \|s_{\text{goal}} - s_{T}\|}{\|s_{0} - s_{\text{goal}}\|},","where \(s_{T}\) represents the underlying block locations after executing the policy, \(s_{0}\) and \(s_{\text{goal}}\) represents the initial and goal block locations."
ICLR_2024_oral_4,3,"Our goal is to train models to sample \(Z\) from this posterior distribution. Intuitively, this allows us to sample likely reasoning chains that lead to the desired outcome \(Y\). Although we take \(Z\) to be a string

\begin{table}
\begin{tabular}{l l l} \hline \hline Object & Meaning & Example 1 (infilling) & Example 2 (subjectivity classification) \\ \hline \(X\) & case / condition / question & _The cat was hungry._ & _A deeply moving storyline._ \\ \(Z\) & mechanism / reasoning chain & _She at a mouse._ & _This revise expresses personal feelings._ \\ \(Y\) & effect / answer & _Now the cat is sleory, not hungry._ & _Answer: Subjective_ \\ \(p(Z\mid X)\) & conditional prior & \(p_{\text{LM}}(Z\mid X)\) \\ \(p(Y\mid X,Z)\) & likelihood of effect given & \(p_{\text{LM}}(Y\mid XZ)\) \\ cause and mechanism & & \(p_{\text{LM}}(ZY\mid X)\) \\ \(p(Z,Y\mid X)\) & conditional joint, reward for \(Z\) & \(p_{\text{LM}}(ZY\mid X)\) \\ \hline \(p(Z\mid X,Y)\) & posterior (**intractable?**) & approximated and amortized by GIFlowNet \(q_{\text{GEN}}(Z\mid X|,Y)\) \\ \(q(Y\mid X)\) & posterior predictive / & approximated as \(\sum_{Z}q_{\text{GEN}}(Z\mid X)p_{\text{LM}}(Y\mid XZ)\), \\ Bayesian model average & sampled as \(Z\to q_{\text{GEN}}(Z\mid X),Y\to p_{\text{LM}}(Y\mid XZ)\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Objects in language posterior inference. Given a pretrained â€˜teacherâ€™ LM \(p_{\text{LM}}\), we train a GIFlowNet \(q_{\text{GFN}}\) to sample the posterior \(p(Z\mid X,Y)\). Amortization and generalization are achieved by making \(X\), and optionally \(Y\), an input to \(q_{\text{GFN}}\).

of tokens, the same formalism and the GFlowNet objectives apply to other structured latent objects, such as trees or sets of natural language statements, as long as one has access to a likelihood model \(p(Y\mid XZ)\). While not investigated in this work, these generalizations could be important for formal reasoning and multi-step chains of inference. See, _e.g._, Yao et al. (2023); Hao et al. (2023); Besta et al. (2024) for approaches to reasoning in language using tree- or list-structured state spaces.

A latent variable model of this form is useful when the marginal distribution \(p_{\text{LM}}(Y\mid X)\) is harder to model than \(p_{\text{LM}}(Z\mid X)\) and \(p_{\text{LM}}(Y\mid XZ)\), _i.e._, a difficult inference is broken down into a chain of easier ones. By training a model to match the Bayesian posterior \(p_{\text{LM}}(Z\mid X,Y)\), we can learn to sample latent reasoning chains that increase the likelihood of producing \(Y\) from \(X\) via the sampled \(Z\).

However, we can also fine-tune the language model \(p_{\text{LM}}(Z\mid XY)\) itself to maximize the likelihood of data pairs \((X,Y)\) under the LVM. While it is generally intractable to directly maximize the data likelihood \(p_{\text{LM}}(X,Y)=\sum_{Z}p_{\text{LM}}(XZY)\) because of the summation over \(Z\), the (variational) expectation-maximization (EM) algorithm (Dempster et al., 1977; Beal, 2003; Koller and Friedman, 2009) can be used for this purpose. In the expectation step (E-step), we draw samples from the posterior over the latent variable \(p_{\text{LM}}(Z\mid X,Y)\), which could come from an amortized sampler of \(Z\). In the maximization step (M-step), we maximize the log-likelihood of the joint probability of the sampled latent variables \(\mathbb{E}_{Z-p_{\text{LM}}(Z\mid X,Y)}\) log \(p_{\text{LM}}(XZY)\) with respect to the parameters of the language model \(p_{\text{LM}}\). This combination of amortized inference (learning to sample the chain of thought) and supervised fine-tuning (optimizing the language model with the'supervision' involving \(Z\) sampled from the amortized posterior) will be illustrated in one of our experiments (SS4.3, Table 3).

### Amortized inference with GFlowNet objectives

For inference in the latent variable model, we leverage the probabilistic framework of generative flow networks (GFlowNets; Bengio et al., 2021, 2023). Using notation for Malkin et al. (2022), we briefly introduce relevant GFlowNet concepts pertaining to autoregressive sequence generation. Here, GFlowNets learn policies to sample sequences \(Z=z_{1}z_{2}\dots z_{n}\top\in\mathcal{Z}\) (where \(\top\) denotes a stop symbol) from a distribution over the space of sequences \(\mathcal{Z}\), given an unnormalized density (reward) \(R:\mathcal{Z}\rightarrow\mathbb{R}_{\geqslant 0}\). The generative process is the same as in autoregressive language models: generation begins with an empty string, and at the \(i\)-th step a token \(z_{i}\) is sampled from a policy \(q_{\text{GN}}(z_{i}\mid z_{1:i-1})\), which is then appended to the sequence. This process continues until a stop symbol \(\top\) is generated.

The marginal likelihood \(q_{\text{FNN}}^{\top}(Z)\) of sampling a terminal state \(Z=z_{1:n}\top\) is given by \(\prod_{i=1}^{n}q_{\text{GN}}(z_{i}\mid z_{1:i-1})q_{\text{GFN}}(\top\mid z)\), where \(z_{1:0}\) is understood to be the empty string. The goal of GFlowNet training is to fit a parametric policy \(q_{\text{GFN}}(\cdot\mid\cdot;\theta)\) such that \(q_{\text{GFN}}^{\top}(Z)\propto R(Z)\), _i.e._, the likelihood of generating a complete sequence is proportional to its reward.

Learning objective.We use a modified version of the subtrajectory balance (SubTB; Madan et al., 2023) objective to account for trajectories being terminable at all states (Deleu et al., 2022). The objective for a sequence \(Z=z_{1:n}\top\) is","\mathcal{L}(Z;\theta)=\sum_{0\leq i<j\leq n}(\log\frac{R(z_{1:i}\top) \prod_{k=i+1}^{j}q_{\text{GFN}}(z_{k}\mid z_{1:k-1})q_{\text{GFN}}(\top\mid z_ {1:j})}{R(z_{1:j}\top)q_{\text{GFN}}(\top\mid z_{1:i})})^{2},",,J(Z)=\mathbb{E}_{Z\sim q_{\text{GFN}}}\left[\sum_{i=1}^{n}R(z_{1:i})\right]-\mathbb{E}_{Z\sim q_{\text{GFN}}}\left[\sum_{i=1}^{n}R(z_{1:i-1})\right].,"where \(J(Z)\) is the learning objective for the GFlowNet, \(R(z_{1:i})\) is the reward for the sequence up to the \(i\)-th token, and \(q_{\text{GFN}}\) is the GFlowNet policy."
ICLR_2024_oral_44,5,"Here, we note that the decomposition \(f(s,z)=\phi(s)^{\top}\psi(z)\) is _universal_; _i.e._, the expressiveness of \(f(s,z)\) is equivalent to that of \(\phi(s)^{\top}\psi(z)\) when \(D\to\infty\). The proof can be found in Appendix C.

Next, we consider a variant of the Wasserstein dependency measure that only depends on the last state: \(I_{\mathcal{W}}(S_{T};Z)\), similarly to VIC (Gregor et al., 2016). This allows us to further decompose the objective with a telescoping sum as follows:","I_{\mathcal{W}}(S_{T};Z)\approx\sup_{\|\phi\|_{L}\leq 1,\| \psi\|_{L}\leq 1}\mathbb{E}_{p(\tau,z)}[\phi(s_{T})^{\top}\psi(z)]-\mathbb{E}_{p( \tau)}[\phi(s_{T})]^{\top}\mathbb{E}_{p(z)}[\psi(z)] || =\sup_{\phi,\psi}\sum_{t=0}^{T-1}(\mathbb{E}_{p(\tau,z)}[( \phi(s_{t+1})-\phi(s_{t}))^{\top}\psi(z)]-\mathbb{E}_{p(\tau)}[\phi(s_{t+1})- \phi(s_{t})]^{\top}\mathbb{E}_{p(z)}[\psi(z)]),",,"I_{\mathcal{W}}(S_{T};Z)\approx\sup_{\|\phi\|_{L}\leq 1,\|\psi\|_{L}\leq 1}\mathbb{E}_{p(s_{T},z)}[\phi(s_{T})^{\top}\psi(z)]-\mathbb{E}_{p(s_{T})}[\phi(s_{T})]^{\top}\mathbb{E}_{p(z)}[\psi(z)].","where \(I_{\mathcal{W}}(S_{T};Z)\) is the Wasserstein dependency measure based on the last state \(S_{T}\), and \(\phi\) and \(\psi\) are functions parameterizing the score function under the given Lipschitz constraints."
ICLR_2024_oral_45,5,(5),"\mathcal{L}^{\text{rep}}(\theta,h_{t},o_{t}) =\beta_{\text{rep}}\max(1,\text{KL}[\quad q_{\theta}(z_{t}\mid o_ {t})\parallel\lessdot\circ g(p(z_{t}\mid h_{t}))\,])",,"\mathcal{L}^{\text{rep}}(\theta,h_{t},o_{t}) = -\beta_{\text{rep}} \mathbb{E}_{z_{t}\sim q_{\theta}(z_{t}\mid o_{t})}[\ln p_{\theta}(z_{t}\mid h_{t})]","where \(\mathcal{L}^{\text{rep}}\) is the representation loss, \(\beta_{\text{rep}}\) is a weighting factor, and \(p_{\theta}(z_{t}\mid h_{t})\) is the predicted distribution of the latent state given the deterministic state."
ICLR_2024_oral_54,9,\). An example for feature distance calculation can be found in Appendix B.2. The MLS for the time \(t\) can be described as:,"\mathcal{S}_{\mathrm{MLS}}(t)=\mathcal{S}_{[n_{t}^{*}]}=\operatorname*{arg\, max}_{\mathcal{S}_{[n]}}(R(\mathcal{S}_{[n]},t))\;\;\text{ where}\;\;n\in[1,N-1].",,"\mathcal{S}[n^{*}]=\arg\max_{n\in[1,N-1]}R(\mathcal{S}_{[n]},t),","where \(\mathcal{S}[n^{*}]\) is the Most Learnable Subset (MLS) selected at time \(t\) based on the highest feature distance reduction rate \(R(\mathcal{S}_{[n]},t)\)."
ICLR_2024_oral_55,7,"In Appendix C, we summarise the supernet training algorithm in pseudocode (Algorithm 1).

### Two-stage search for an optimal path

A supernet \(f_{\theta,\alpha,\phi^{\prime}}\) trained with the method described in Section 2.3 contains \(4^{K}\) models, intertwined via weight sharing. As explained in Section 1, our goal is to search for the best-performing one, but the main challenge is related to the fact that we do not know what data is going to be used for adaptation at test time. One extreme approach would be to search for a single solution during training and simply use it throughout the entire test, regardless of the potential domain shift. Another, would be to defer the search and perform it from scratch each time a new support set is given to us at test time. However, both have their shortcomings. As such, we propose a hybrid, where searching is split into two phases - one during training, and a subsequent one during testing.

Meta-training timeThe search is responsible for pre-selecting a set of \(N\) models from the entire search space. Its main purpose is to mitigate potential overfitting that can happen at test time, when only a small amount of data is available, while providing enough diversity to successfully adjust the architecture to the diverse set of test domains. Formally, we search for a sequence of paths \((p_{1},p_{2},...,p_{N})\) where:","p_{k} =\operatorname*{arg\,max}_{p\in P}\mathbb{E}_{\mathcal{S}, \mathcal{Q}}A(f^{p}_{\theta,\alpha^{\prime},\phi^{\prime\prime}},\mathcal{S}, \mathcal{Q}),\quad\text{s.t.} || \alpha^{*},\phi^{\prime*} =\operatorname*{arg\,min}_{\alpha,\phi^{\prime}}\mathcal{L}(f^{p} _{\theta,\alpha,\phi^{\prime}},\mathcal{S},\mathcal{S})",,"(p_{1},p_{2},...,p_{N}) \in \arg\max_{p \in P} \mathbb{E}_{\mathcal{S},\mathcal{Q}} \mathcal{L}(f^{p}_{\theta,\alpha,\phi^{\prime}}, \mathcal{S},\mathcal{Q}).",where \(N\) denotes the number of selected models
ICLR_2024_oral_6,1,"## 3 Related Work

Representing the problem of computing a Nash equilibrium as an optimization problem is not new. A variety of loss functions and pseudo-distance functions have been proposed. Most of them measure some function of how much each player can exploit the joint strategy by unilaterally deviating:","\epsilon_{k}(\boldsymbol{x})\mathop{=}^{\text{\text{\text{def}}}}u_{k}( \texttt{BR}_{k},x_{-k})-u_{k}(\boldsymbol{x})\text{ where }\texttt{BR}_{k}\in\operatorname*{arg\,max}_{z}u_{k}(z,x_{-k}).",,"f(x) = \sum_{i=1}^{n} \max_{x_i' \in X_i} \left( u_i(x_i', x_{-i}) - u_i(x_i, x_{-i}) \right)","where \( f(x) \) is the loss function, \( n \) is the number of players, \( u_i \) is the utility function for player \( i \), \( x_i \) is the strategy of player \( i \), \( x_{-i} \) represents the strategies of all other players, and \( X_i \) is the strategy space for player \( i \)."
ICLR_2024_oral_63,2,"EBMs are typically trained via contrastive divergence (Hinton, 2002), and new samples are drawn from \(p_{\theta}(x)\) by Markov-Chain Monte Carlo (MCMC). Details of the loss function used in this work are given in Section 3. In Langevin MCMC, samples are initialized from a known data point or random noise and refined with (discretized) Langevin diffusion","x_{k+1}=x_{k}-\delta\,\nabla f_{\theta}(x_{k})+\sqrt{2\delta}\,\varepsilon_{k },\,\varepsilon_{k}\sim\mathcal{N}(0,I_{d}),","where \(\nabla\) denotes the gradient of the energy function with respect to inputs, \(k\) is the sampling step, \(\delta\) is the (discretization) step size, and the noise \(\varepsilon_{k}\) is drawn from the normal distribution at each step",x_{k+1} = x_k - \frac{\delta}{2} \nabla f_{\theta}(x_k) + \sqrt{\delta} \varepsilon_k,"where \(x_k\) is the current sample, \(\nabla\) denotes the gradient of the energy function with respect to inputs, \(k\) is the sampling step, \(\delta\) is the (discretization) step size, and the noise \(\varepsilon_{k}\) is drawn from the normal distribution at each step."
ICLR_2024_oral_63,10,"ns. The critical noise level, \(\sigma_{c}\), is defined as",\sigma_{c}=\max_{ii^{\prime}}\chi_{ii^{\prime}},,"\sigma_{c}=\frac{1}{\sqrt{d}}\end{latex>

<description>The critical noise level, \(\sigma_{c}\), is defined as the threshold for effective sampling in high-dimensional spaces.</description>","The critical noise level, \(\sigma_{c}\), is defined as the threshold for effective sampling in high-dimensional spaces."
ICLR_2024_oral_64,9,"The loss can be understood as first sampling an OOD combination of slots \(\mathbf{z}^{\prime}\) by composing inferred ID slots \(\mathbf{h}_{k}(\mathbf{z}_{\pi(k)})\). The decoder can then render \(\mathbf{z}^{\prime}\) to create an OOD sample \(\hat{\mathbf{f}}(\mathbf{z}^{\prime})\). Re-encoding this sample such that \(\hat{\mathbf{g}}(\hat{\mathbf{f}}(\mathbf{z}^{\prime}))=\mathbf{z}^{\prime}\) then regularizes the encoder to invert the decoder OOD. We discuss how this regularization can be implemented in practice in Sec. 4.

### Putting it All Together

Thm. 1 showed how slot identifiability can be achieved ID on \(\mathcal{Z}^{S}\) if \(\hat{\mathbf{f}}\) satisfies compositionality, and Thm. 2, Def. 6 showed how this identifiability can be generalized to all of \(\mathcal{Z}\) if the decoder is additive and compositional consistency is minimized. Putting these results together, we can now prove conditions for which an autoencoder will generalize compositionally (see Fig. 2 for an overview).

**Theorem 3** (Compositionally generalizing autoencoder).: _Let \(\mathbf{f}:\mathcal{Z}\to\mathcal{X}\) be a compositional and irreducible diffeomorphism. Let \(\mathcal{Z}^{S}\) be a convex, slot-supported subset of \(\mathcal{Z}\). Let \(\big{(}\hat{\mathbf{g}},\hat{\mathbf{f}}\big{)}\) be an autoencoder with additive decoder \(\hat{\mathbf{f}}\) (Def. 5). If \(\hat{\mathbf{f}}\) is compositional on \(\hat{\mathbf{g}}(\mathcal{X}^{S})\) and \(\hat{\mathbf{g}},\hat{\mathbf{f}}\) solve_","\mathcal{L}_{\text{rec}}\big{(}\hat{\mathbf{g}},\hat{\mathbf{f}},\mathcal{X}^{S}\big{)} +\lambda\mathcal{L}_{\text{cons}}\big{(}\hat{\mathbf{g}},\hat{\mathbf{f}},\mathcal{Z }^{\prime}\big{)}=0,\qquad\text{for some }\lambda>0,",,"\mathcal{L}_{\text{rec}}(\mathcal{X}^{S})+\mathcal{L}_{\text{cons}}\big{(}\hat{\mathbf{g}},\hat{\mathbf{f}},\mathcal{Z}^{\prime} \big{)}\to 0,\end{latex> 

<description>The total loss for the autoencoder, combining the reconstruction loss on the training space and the compositional consistency loss on the inferred OOD slots.</description>","The total loss for the autoencoder, combining the reconstruction loss on the training space and the compositional consistency loss on the inferred OOD slots."
ICLR_2024_oral_64,10,"_then the autoencoder \(\big{(}\hat{\mathbf{g}},\hat{\mathbf{f}}\big{)}\) generalizes compositionally w.r.t. \(\mathcal{Z}^{S}\) in the sense of Def. 3._

Moreover, \(\hat{\mathbf{g}}:\mathcal{X}\to\hat{\mathcal{Z}}\) inverts \(\hat{\mathbf{f}}:\mathcal{Z}^{\prime}\to\mathcal{X}\) and also \(\hat{\mathcal{Z}}=\mathcal{Z}^{\prime}=\mathbf{h}_{1}(\mathcal{Z}_{\pi(1)})\times \cdots\times\mathbf{h}_{K}(\mathcal{Z}_{\pi(K)})\).

## 4 Compositional Generalization in Practice

CompositionalityThm. 3 explicitly assumes that the decoder \(\hat{\mathbf{f}}\) satisfies compositionality on \(\hat{\mathbf{g}}(\mathcal{X}^{S})\) but does not give a recipe to enforce this in practice. Brady et al. (2023) proposed a regularizer that enforces compositionality if minimized (see App. B.4), but their objective is computationally infeasible to optimize for larger models, thus limiting its practical use. At the same time, Brady et al. (2023) showed that explicitly optimizing this objective may not always be necessary, as the object-centric models used in their experiments seemed to minimize it implicitly, likely through the inductive biases in these models. We observe a similar phenomenon (see Fig. 4, right) and thus rely on these inductive biases to satisfy compositionality in our experiments in Sec. 6.

AdditivityIt is trivial to implement an additive decoder by parameterizing the slot functions \(\mathbf{\varphi}_{k}\) from Eq. 7 as, e.g., deconvolution neural networks. This resembles the decoders typically used in object-centric learning, with the key difference being the use of _slot-wise masks_\(\mathbf{m}_{k}\). Specifically, existing models commonly use a decoder of the form","\hat{\mathbf{f}}(\mathbf{z})=\sum_{k=1}^{K}\tilde{\mathbf{m}}_{k}\odot\mathbf{x}_{k},\qquad \tilde{\mathbf{m}}_{k}=\sigma(\mathbf{m})_{k},\qquad(\mathbf{m}_{k},\mathbf{x}_{k})=\mathbf{\varphi }_{k}(\mathbf{z}_{k}),",where \(\odot\) is an element-wise multiplication and \(\sigma(\cdot)\) denotes the softmax function,"\mathcal{L}_{\text{rec}}\big{(}\hat{\mathbf{g}},\hat{\mathbf{f}},\mathcal{X}^{S}\big{)} +\lambda\mathcal{L}_{\text{cons}}\big{(}\hat{\mathbf{g}},\hat{\mathbf{f}},\mathcal{Z }^{\prime}\big{)}=0,\qquad\text{for some }\lambda>0.","The combined loss function for the autoencoder, consisting of the reconstruction loss on the training space and the compositional consistency loss, which is minimized to achieve compositional generalization."
ICLR_2024_oral_66,4,"The numerator of Equation 3 introduces a mechanism that selects the pipeline \(x\) that has the largest likelihood to improve the lowest observed validation error at the next unobserved epoch \(\tau(x)\) of pipeline \(x\). The denominator balances out the cost of actually finetuning pipeline \(x\) for \(\Delta t\) epochs. \(\tau(x)\) is defined for pipeline \(x\) as \(\tau(x):=\max\{t^{\prime}|(x,t^{\prime},\cdot,\cdot)\in\mathcal{H}\}+\Delta t\), where \(\Delta t\) denotes the number of epochs to finetune from the last observed epoch in the history. If the pipeline is not in the history, the query epoch is \(\tau(x)=\Delta t\). Simply put, if the validation loss of \(x\) is evaluated after every training epoch/step (\(\Delta t=1\)) and has been evaluated for \(k\) epochs/steps, then \(\tau(x)=k+1\). As a result, we select the configuration with the highest chance of improving the best-measured loss at the next epoch, while trading off the cost of finetuning it. Concretely, the best observed loss is \(\ell_{\tau(x)}^{\min}:=\min\left(\{\ell(x,\tau(x))|(x,\tau(x),\ell\left(x,\tau( x)\right),\cdot)\in\mathcal{H}\}\right)\). If no pipeline has been evaluated until \(\tau(x)\), i.e. \((x,\tau(x),\cdot,\cdot)\notin\mathcal{H}\), then \(\ell_{\tau(x)}^{\min}:=\min\left(\{\ell(x,t)|(x,t,\ell\left(x,t\right),\cdot) \in\mathcal{H},t<\tau(x)\}\right)\).

### Meta-learning the Performance and Cost Estimators

A crucial novelty of our paper is to meta-learn BO surrogates from existing pipeline evaluations on other datasets. Assume we have access to a set of curves for the validation errors \(\ell\) and the runtimes \(c\) of pipelines over a pool of datasets, for a series of \(N\) epochs. We call the collection of such quadruple evaluations a meta-dataset \(\mathcal{H}^{\left(M\right)}:=\bigcup_{x\in\mathcal{X}}\bigcup_{d\in\mathcal{D} }\bigcup_{t\in[1,N]}\left\{\left(x,t,\ell\left(x,t,d\right),c\left(x,t,d,d \right)\right)\right\}\), where we explicitly included the dependency of the performance and cost curves to the dataset. To contextualize the predictions on the characteristics of each dataset, we use descriptive features \(d\in\mathcal{D}\) to represent each dataset (a.k.a. meta-features).

We meta-learn a probabilistic validation error estimator \(\hat{\ell}\left(x,t,d;\theta\right)\), and a point-estimate cost predictor \(\hat{c}\left(x,t,d;\gamma\right)\) from the meta-dataset \(\mathcal{H}^{\left(M\right)}\) by solving the following objective functions:","\theta^{(M)} :=\ \operatorname*{arg\,min}_{\theta}\ \mathbb{E}_{(x,t,\ell(x,t,d),c(x,t,d)) \sim\mathcal{H}^{(M)}}\ \ [-\log p(\ell(x,t,d)\mid x,t,d,\hat{\ell}(x,t,d; \theta))] || \gamma^{(M)} :=\ \operatorname*{arg\,min}_{\gamma}\ \mathbb{E}_{(x,t,\ell(x,t,d),c(x,t,d)) \sim\mathcal{H}^{(M)}}\ \ (c(x,t,d)-\hat{c}(x,t,d;\gamma))^{2}",,"\theta^{*}\ :=\ \operatorname*{arg\,min}_{\theta}\ \mathbb{E}_{(x,t,d,\ell(x,t,d),\cdot)\sim\mathcal{H}^{\left(M\right)}}\ \Big{[}-\log p(\ell(x,t,d)\mid x,t,d,\hat{\ell}(x,t,d;\theta))\Big{]}.",Meta-learned probabilistic validation error estimator for the meta-dataset.
ICLR_2024_oral_74,3,"Therefore, computing Betti numbers for TDA can be viewed as a rank estimation problem (i.e., \(\beta_{k}=\dim\tilde{\mathcal{H}}_{k}-\mathrm{rank}(\Delta_{k})\)). Additional TDA details can be found in Appendix A.2. The problem of normalized Betti number estimation (BNE) is defined as (Gyurik et al., 2020): Given a set of \(n\) points, its corresponding Vietoris-Rips complex \(\Gamma\), an integer \(0\leq k\leq n-1\), and the parameters \((\epsilon,\eta)\in(0,1)\), find the value \(\chi_{k}\in[0,1]\) that satisfies with probability \(1-\eta\) the condition","|\chi_{k}-\frac{\beta_{k}}{|S_{k}|}|\leq\epsilon,","where \(|S_{k}|\) is the the number of \(k\)-simplices \(S_{k}\in\Gamma\) or \(\dim\tilde{\mathcal{H}}_{k}\), the dimension of the Hilbert space spanned by the set of \(k\)-simplices in the complex","\chi_{k}=\frac{\beta_{k}}{|S_{k}|},","where \(|S_{k}|\) is the number of \(k\)-simplices \(S_{k}\in\Gamma\) or \(\dim\tilde{\mathcal{H}}_{k}\), the dimension of the Hilbert space spanned by the set of \(k\)-simplices in the complex."
ICML_2024_oral_1,4,"Based on the causality-aware entropy, then the \(Q\)-value for a fixed policy \(\pi\) could be computed iteratively by applying a modified Bellman operator \(\mathcal{T}_{c}^{\pi}\) with \(\mathcal{H}_{c}(\pi(\cdot|\mathbf{s}))\) term as stated below,","\begin{split}\mathcal{T}_{c}^{\pi}Q(\mathbf{s}_{t},\mathbf{a}_{t })\triangleq& r(\mathbf{s}_{t},\mathbf{a}_{t})+\gamma\mathbb{E}_{ \mathbf{s}_{t+1}\sim P}[\mathbb{E}_{\mathbf{a}_{t}\sim\pi}[Q(\mathbf{s}_{t+1},\mathbf{a}_{t+1})\\ &+\alpha\mathcal{H}_{c}(\pi(\mathbf{a}_{t+1}|\mathbf{s}_{t+1}))]].\end{split}",,"Q^{\pi}(\mathbf{s},\mathbf{a})=\mathcal{T}_{c}^{\pi}Q^{\pi}(\mathbf{s},\mathbf{a})=\mathbb{E}_{\mathbf{s}'\sim\mathcal{P}(\cdot|\mathbf{s},\mathbf{a})}\left[r(\mathbf{s},\mathbf{a})+\gamma Q^{\pi}(\mathbf{s}',\mathbf{a}')+\alpha\mathcal{H}_{c}(\pi(\cdot|\mathbf{s}'))\right]","where \(Q^{\pi}(\mathbf{s},\mathbf{a})\) is the action-value function, \(\mathcal{T}_{c}^{\pi}\) is the modified Bellman operator, \(\mathcal{P}(\cdot|\mathbf{s},\mathbf{a})\) represents the transition dynamics, \(\gamma\) is the discount factor, and \(\alpha\) is a coefficient for the causality-aware entropy term."
ICML_2024_oral_1,5,"For a better understanding of our operator, we conduct a theoretical analysis of its dynamic programming properties in the tabular MDP setting, covering policy evaluation, policy improvement, and policy iteration. All proofs are included in Appendix A.2.

**Proposition 3.5** (Policy evaluation).: _Consider an initial \(Q_{0}:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}\) with \(|\mathcal{A}|<\infty\), and \(Q\)-value iterates by \(Q_{k+1}=\mathcal{T}_{c}^{\pi}Q_{k}\). Then the sequence \(\{Q_{k}\}\) converges to a fixed point \(Q^{\pi}\) as \(k\rightarrow\infty\)._

**Proposition 3.6** (Policy improvement).: _Let \(\pi_{k}\) be the policy at iteration \(k\), and \(\pi_{k+1}\) be the updated policy ( maximize of the \(Q\)-value). Then for all \((s,a)\in\mathcal{S}\times\mathcal{A}\), \(|\mathcal{A}|<\infty\), we have \(Q^{\pi_{k+1}}(\mathbf{s},\mathbf{a})\geq Q^{\pi_{k}}(\mathbf{s},\mathbf{a})\)._

**Proposition 3.7** (Policy iteration).: _Assume \(|\mathcal{A}|<\infty\), by repeating iterations of the policy evaluation and policy improvement, any initial policy converge to the optimal policy \(\pi^{*}\), s.t. \(Q^{\pi^{*}}(\mathbf{s}_{t},\mathbf{a}_{t})\geq Q^{\pi}(\mathbf{s}_{t}, \mathbf{a}_{t}),\forall\pi\in\Pi,\forall(\mathbf{s}_{t},\mathbf{a}_{t})\)._

**Causality-aware off-policy actor-critic (CausalSAC).** Our causality-aware entropy provides a flexible solution that can be seamlessly incorporated into any Max-Entropy RL framework. For example, as a plug-and-play component, an algorithm instantiation CausalSAC can be implemented within SAC Haarnoja et al. (2018) by integrating our \(\mathcal{H}_{c}\) into the policy optimization objective, \(J(\pi)=\sum_{t=0}^{\infty}\mathbb{E}_{(\mathbf{s}_{t},\mathbf{a}_{t})\sim p( \pi)}\left[\gamma^{t}(r(\mathbf{s}_{t},\mathbf{a}_{t})+\alpha\mathcal{H}_{c}( \pi(\cdot|\mathbf{s}_{t}))\right]\).

### Gradient-dormancy-guided Reset

Guided by causality-aware entropy, the agent efficiently explores and masters primitive behaviors for different learning stages. However, causality-aware exploration introduces the risk of getting stuck in local optima and overfitting to specific primitive behaviors. To address this challenge, we analyze the gradient dormancy phenomenon during RL training and introduce a soft reset mechanism. This mechanism, guided by gradient dormancy, regularly perturbs the agents' neural networks to maintain network expressivity, thereby improving the agent's performance.

The dormancy phenomenon of neural networks in RL, signifying a loss of expressive capacity, has been previously discussed in existing works Sokar et al. (2023); Xu et al. (2023). However, the dormant phenomenon defined in these works is not evident in state-based RL, and it cannot effectively enhance our algorithm in exploration scheduling. We are the first to investigate dormancy from the perspective of gradients. Here, we introduce definitions for gradient -dormant neurons and the gradient dormancy degree of a neural network.

**Definition 3.8** (Gradient-dormant Neurons).: For a fully connected layer in a neural network, where \(N^{l}\) represents the number of neurons in layer \(l\), the L2 norm of gradients of the weights for neuron \(i\) is denoted as \(n_{i}^{l}\). Neuron \(i\) is classified as a gradient-dormant neuron if it satisfies","\frac{n_{i}^{l}(x)}{\frac{1}{N^{l}}\sum_{k\in l}n_{k}^{l}}\leq\tau,",where \(\tau\) is a constant serving as a threshold to determine the gradient dormancy of neurons in each layer,n_{i}^{l} < \tau,"where \(n_{i}^{l}\) is the L2 norm of gradients of the weights for neuron \(i\) in layer \(l\), and \(\tau\) is a constant serving as a threshold to determine the gradient dormancy of neurons in each layer."
ICML_2024_oral_110,10,"_Here \(C_{1}\) is a constant depending on \(d\), \(p\), \(\Omega\), and the lower and upper bound on the density of \(\mathbb{P}\)._

The proof of this result, including the extension to the undercomplete case can be found in Appendix E. Let us continue our discussion on the meaning of approximate identifiability in the context of this theorem. First, we note that if \(\Theta_{p}(f,\Omega)\) is not small we obtain no useful statement, except that our transformed data is some function of the original data. This is not surprising: We cannot hope to recover any mixing function because the problem of learning \(f\) from \(f_{*}\mathbb{P}\) is not identifiable (even when assuming \(\mathbb{P}\) known). Moreover, our statement only applies to one specific unmixing \(g\) which is again unavoidable for the same reason. What we show is that if \(f\) and \(g\) are both close to being locally isometric, then the concatenation \(g^{-1}f\) will be close to a linear (even orthogonal) map. Note that \(g\) does not appear on the right-hand side of (9) because we choose \(g\) to be the maximally isometric representation of our observations, but we know that this representation is more isometric than any alternative representation, in particular more isometric than \(f\). This allows us to bound the non-linearity \(h\) in terms of \(f\) only. While this result does not have the simplicity of a standard identifiability result it provides a more general viewpoint. Indeed, if \(\Theta_{p}(f,\Omega)=0\), i.e., when \(f\) is a local isometry we have \(h=0\) and we recover \(f\) up to a linear transformation which is the standard identifiability result for local isometries (see Theorem 2 of Horan et al. (2021)). Our result extends this gracefully to functions that are approximate local isometries in the sense that \(\Theta_{p}(f,\Omega)\) is small. Let us add some remarks about this result.

_Remark 4.2_.:
* The optimization problem for \(g\) in (8) is non-convex, and difficult to optimize in practice.
* For \(D=d\) the introduction of \(g\) is not necessary, instead we can directly apply Theorem E.1 to \(f\) and just work with the original data \(X\) and directly apply Theorem E.1 to \(X=f(S)\).
* The assumptions that \(\Omega\) is connected and that the density of \(\mathbb{P}\) is lower bounded are necessary. In particular the result does not apply to distributions \(\mathbb{P}\) with disconnected support.
* There are alternative assumptions that allow us to remove (or bound) the second term in (2), e.g., assuming \(Df^{\top}Df>c_{1}>0\), i.e., the smallest singular value of \(Df\) is bounded below is sufficient (see Lemma C.2).

## 5 Perturbed linear ICA

In this section, we consider the problem of independent component analysis where the mixing is a slight perturbation of a linear function. This is a problem of general interest beyond the main setting considered in this paper because in typical real-world applications the mixing will only be approximately linear, so understanding the effect of the non-linear part is important. Concretely, we assume that data is generated by a perturbed linear model",x=f(s)=As+\eta h(s),where \(h:\mathbb{R}^{d}\to\mathbb{R}^{d}\) is a non-linear function and \(\eta\in\mathbb{R}\) is a small constant,X = A S + h(S) + \eta,"where \(A\) is a linear transformation, \(h:\mathbb{R}^{d}\to\mathbb{R}^{d}\) is a non-linear function, and \(\eta\in\mathbb{R}\) is a small constant."
ICML_2024_oral_110,13,"thus satisfy \(\bar{w}_{i}^{\top}\Sigma_{X}^{-\frac{1}{2}}X=S_{i}\) and \(|\bar{w}_{i}|=1\). We also consider the matrix \(\bar{W}=A^{-1}(AA^{\top})^{\frac{1}{2}}\) which has rows \(\bar{w}_{i}=\bar{W}^{\top}e_{i}\) and satisfies \(\bar{W}\Sigma_{X}^{-\frac{1}{2}}X=S\) for \(\eta=0\).

The main goal of this section is to show that this general picture remains approximately true in the perturbed setting, i.e., under minor regularity assumptions on \(G\) there is a matrix \(W\) close to \(\bar{W}\) such that its rows are local extrema of \(H\). Let us collect the necessary assumptions for our results.

**Assumption 5.2**.: The function \(G\) is even, three times differentiable, and there are constants \(C_{g}\) and \(d_{g}\) such that for \(k\leq 3\)","|G^{(k)}(x)|\leq C_{g}(1+|x|)^{\max(d_{g}-k,0)}",where \(G^{(k)}\) denotes the \(k\)-th derivative of \(G\),H(w)=\mathbb{E}G(w^{\top}\Sigma_{X}^{-\frac{1}{2}}X),where \(\Sigma_{X}\) denotes the covariance matrix of \(X\) so that \(\Sigma_{X}^{-\frac{1}{2}}X\) is whitened and \(G\) is the so-called contrast function.
ICML_2024_oral_111,8,"ro. When the overlap ratio \(s_{1}^{k}\) exceeds a predefined threshold \(inter_{1}\), the \(k\)-th segment \(\widetilde{M}_{i,:,k}^{u}\) generated by SAM is selected and subsequently merged to replace the pseudo-labels. This method is referred to as Composite Parts Integration for Under-segmentation (CPI-U). Conversely, over-segmentation introduces erroneous regions into the segmentation. To mitigate this, we seek to leverage SAM's segmentation to filter out the extraneous noise. The selection is based on the overlap ratio with the candidate mask, computed as:","s_{2}^{k}=\frac{\sum_{j=1}^{H\times W}(\hat{M}_{i,j}^{u}\cap\widetilde{M} _{i,j,k}^{u})}{\sum_{j=1}^{H\times W}(\widetilde{M}_{i,j,k}^{u} )}.",,"s_{2}^{k}=\frac{\sum_{j=1}^{H\times W}(\hat{M}_{i,j}^{u}\cap\widetilde{M}_{i,j,k}^{u})}{\sum_{j=1}^{H\times W}(\widetilde{M}_{i,j,k}^{u})+\epsilon},",where \(\epsilon\) is the smoothing factor to prevent a denominator of zero
ICML_2024_oral_113,4,". Note that when \(i=j\), then \(\pi_{j}=\pi_{i,old}\) and this reduces to the on-policy update as expected. This is then scaled and combined with the on-policy term (eq. 2)",L(\pi_{i})=L_{on}(\pi_{i})+\lambda\cdot L_{off}(\pi_{i};\mathcal{X}),,\begin{split} L(\pi_{i};\mathcal{X}) &= L_{off}(\pi_{i};\mathcal{X}) + \alpha L_{on}(\pi_{i}) \end{split},"where \(L(\pi_{i};\mathcal{X})\) is the combined loss for policy \(\pi_{i}\), \(L_{off}(\pi_{i};\mathcal{X})\) is the off-policy loss, \(L_{on}(\pi_{i})\) is the on-policy loss, and \(\alpha\) is a scaling factor for the on-policy term."
ICML_2024_oral_117,2,"Input Attention Head.The purpose of this block is to retrieve relevant information from both the previous policies and the output attention head. It provides the necessary information for the decision-making process of the internal policy (the next block) by attending to the important features from past policies and the tentative vector \(\mathbf{v}\) from the output attention head. Similarly to the previous block, it employs an attention head conditioned on \(\mathbf{h}_{\mathbf{s}}\), but unlike the preceding block, the attention head returns a linear combination over learnable transformations of its inputs. Specifically, the query vector is computed as \(\mathbf{q}=\mathbf{h}_{\mathbf{s}}W_{\text{in}}^{Q}\), where \(W_{\text{in}}^{Q}\in\mathbb{R}^{d_{\text{max}}\times d_{\text{model}}}\). Following Figure 2, the keys are computed as \((P+E_{\text{in}})W_{\text{in}}^{K}\), where \(P\) is the row-wise concatenation of the output of the previous block (\(\mathbf{v}\)) and \(\Phi^{k;\mathbf{s}}\), while \(E_{\text{in}}\) is a positional encoding matrix of the same size as \(P\) and \(W_{\text{in}}^{K}\in\mathbb{R}^{|\mathcal{A}|\times d_{\text{model}}}\). In turn, the values matrix is obtained as the linear transformation \(V=PW_{\text{in}}^{V}\), where \(W_{\text{in}}^{V}\in\mathbb{R}^{|\mathcal{A}|\times d_{\text{model}}}\). Once \(\mathbf{q}\), \(K\), and \(V\) have been computed, the output of this block is the dot-product attention of these three elements, see Equation (1). Note that the learnable parameters of this block are \(W_{\text{in}}^{Q}\), \(W_{\text{in}}^{K}\), and \(W_{\text{in}}^{V}\).

Internal Policy.This block is used to adjust, overwrite, or retain the tentative vector \(\mathbf{v}\) from the output attention head, considering the contextual information provided by the input attention head and the representation of the current state. It is comprised of a feed-forward multi-layerperceptron network which takes the result of the previous block and \(\mathbf{h_{s}}\) as input, generating a real-valued vector of size \(|\mathcal{A}|\). Notably, this vector is not the direct output of the self-composing policy module; instead, it is added to the tentative vector \(\mathbf{v}\) to form the final output of the module. Finally, depending on the nature of the task at hand, this addition might require normalization, as the output of the module usually represents a categorical distribution over \(\mathcal{A}\) or continuous actions within some bounds.

In summary, the output attention head proposes an output for the module based on the current state and the information of the previous policies. Subsequently, the input attention head retrieves relevant information from both the previous policies and the output attention head. Then, the internal policy utilizes this information along with the current state representation to adjust, overwrite, or retain the tentative output from the output attention head. Note that the output attention head proposes outputs based on preceding policies and the current state representation, while the input attention head retrieves relevant information from previous modules for guiding the decision-making process of the internal policy.

In Section 3, we categorized three scenarios concerning the current task and previously learned policy modules that motivated the design of CompoNet. The subsequent lines review these scenarios within the described architecture:

1. If a previous policy solves the current task, the output attention head assigns high attention to it, and the internal policy may output a vector of zeros to retain this result, akin to residual connections in deep learning.
2. If a function over the previous policies and the current state can solve the task at hand, then the three blocks of the module can be used to learn such a function.
3. When previous policies offer no relevant information for the task at hand, the internal policy independently learns a policy from scratch based on current state information, superseding the result of the output attention head in the last addition step.

### Computational Cost

As mentioned in Section 2, computational cost stands as the primary drawback of growing NN architectures. Consequently, memory and inference costs are focal points in this study. First, the self-composing policy module is designed to mitigate the memory complexity of the model. As a result, CompoNet grows linearly in the number of parameters with respect to the number of tasks while being able to encompass the information of all previously learned modules (see Appendix B for further details). The rightmost plot in Figure 3 contrasts the memory costs of CompoNet with progressive NNs (ProgressiveNet). The latter method, introduced by Rusu et al. (2016) is one of the best-known growing NNs, and shares multiple similarities with CompoNet. Regarding the computational cost of inference, while the theoretical cost of CompoNet is quadratic with respect to the number of tasks (elaborated in Appendix C.1), the results presented in Figure 3 indicate that the empirical computational cost of CompoNet does not exhibit quadratic growth up to the 300 tasks tested, effectively scaling to very long task sequences in practice.

## 5 Experiments

In this section, we validate the presented architecture across sequences of tasks from multiple environments and domains. The central hypothesis of these experiments is that CompoNet should be able to benefit from forward knowledge transfer to solve the task at hand in the scenarios presented in Section 4.

### Evaluation Metrics

We start by describing the CRL-relevant metrics commonly used in the literature (Wolczyk et al., 2021, 2022). Consider \(p_{i}(t)\in[0,1]\) to be the success rate4 in task \(i\) at time \(t\), indicating whether the task is solved5, \(p_{i}(t)=1\), or not,

Figure 2: Diagram of the self-composing policy module. _Vstack_ and _Hstack_ successively represent row-wise and column-wise concatenation operations, while the normalization operation has been delimited with a dashed line to denote that it is optional and dependent on the nature of the action space. Finally, note that the only blocks with learnable parameters are the feed-forward block and the linear transformations.

\(p_{i}(t)=0\). Note that the metric is task-specific and defined by the problem itself. Moreover, the interaction of an agent with each task is limited to \(\Delta\) timesteps, being the total number of timesteps \(T=N\cdot\Delta\), where \(N\) is the number of tasks. Continuing the standard practice in CRL, we consider an agent trained from scratch in each task as the baseline for the following metrics (Diaz-Rodriguez et al., 2018; Wolczyk et al., 2021, 2022).

Average Performance.The average performance at timestep \(t\) is computed as \(\text{P}(t)=\frac{1}{N}\sum_{i=1}^{N}p_{i}(t)\). In the next sections, we report the final performance value \(\text{P}(T)\) as it is a commonly used metric in the CL literature (Wolczyk et al., 2021).

Forward Transfer.The forward transfer is defined as the normalized area between the training curve of the method and the training curve of the baseline. Considering \(p_{i}^{b}(t)\in[0,1]\) to be the performance of the baseline, the forward transfer \(\text{FTr}_{i}\) on task \(i\) is,","\text{FTr}_{i}=\frac{\text{AUC}_{i}-\text{AUC}_{i}^{b}}{1-\text{AUC}_{i}^{ b}},\quad\text{AUC}_{i}=\frac{1}{\Delta}\int_{(i-1)\cdot\Delta}^{i\cdot \Delta}p_{i}(t)\mathrm{d}t, || \text{AUC}_{i}^{b}=\frac{1}{\Delta}\int_{0}^{\Delta}p_{i}^{b}(t)\mathrm{d}t",,FTr_{i} = \frac{\int_{0}^{T} p_{i}(t) dt - \int_{0}^{T} p_{i}^{b}(t) dt}{\int_{0}^{T} p_{i}^{b}(t) dt},"Forward transfer metric for task \(i\), comparing the performance of the proposed method with the baseline."
ICML_2024_oral_122,8,"In the subsequent sections, we explore two strategies for obtaining the target probabilities \(p_{i}(S,A;\tilde{\theta})\).

### Categorical Distributions from Scalars

The first set of methods we outline will project the scalar target \((\widehat{\mathcal{T}}Q)(S,A;\tilde{\theta})\) onto the categorical distribution supported on \(\{z_{i}\}_{i=1}^{m}\). A prevalent but naive approach for the projection step involves discretizing the scalar into one of \(m\) bins where \(z_{i}\) represents the center of the bin. The resulting one-hot distribution is ""lossy"" and induces errors in the \(Q\)-function. These errors would compound as more Bellman backups are performed, resulting in more biased estimates, and likely worse performance. To combat this, we first consider the ""two-hot"" approach (Schrittwieser et al., 2020) that represents a scalar target _exactly_ via a unique categorical distribution that puts non-zero densities on two locations that the target lies between (see Figure 1; Left).

**A Two-Hot Categorical Distribution.** Let \(z_{i}\) and \(z_{i+1}\) be the locations which lower and upper-bound the TD target \(y=(\widehat{\mathcal{T}}Q)(S,A;\tilde{\theta})\), i.e., \(z_{i}\leq y\leq z_{i+1}\). Then, the probability, \(p_{i}\) and \(p_{i+1}\), put on these locations is:","p_{i}(S,A;\tilde{\theta})=\frac{y-z_{i}}{z_{i+1}-z_{i}},\ p_{i+1}(S,A;\tilde{ \theta})=\frac{z_{i+1}-y}{z_{i+1}-z_{i}}\,.",,"p_{i}=\frac{z_{i+1}-y}{z_{i+1}-z_{i}},\ p_{i+1}=\frac{y-z_{i}}{z_{i+1}-z_{i}}.","where \(p_{i}\) and \(p_{i+1}\) are the probabilities assigned to the locations \(z_{i}\) and \(z_{i+1}\) respectively, based on the scalar target \(y\)."
ICML_2024_oral_13,3,"This yields an estimated probability measure \(\mathbb{P}_{n}^{\leftarrow}\) on \(\mathcal{V}^{n}\).

In this paper, we will speak of _forward/backward (FW/BW) model_ to refer to the same (architectural) model trained with the same hyperparameters (learning rate, batch size, training time,...) but fed with (batches of) \((x_{1},\ldots,x_{n})\) and \((x_{n},\ldots,x_{1})\) from the same dataset respectively. In other words, both models are the same, except that the FW model is trained to predict the _next_ token, while the BW one is trained to predict the _previous_ token.

**Problem 1**.: For a measure \(\mathbb{P}\) and a given model, how do the _forward and backward measures_\(\mathbb{P}_{n}^{\rightarrow}\) and \(\mathbb{P}_{n}^{\leftarrow}\) differ from one another?

For certain \(\mathbb{P}\)s, we will see universal asymmetries: for any given architecture and hyperparameters, a substantial difference between the way \(\mathbb{P}_{n}^{\rightarrow}\) and \(\mathbb{P}_{n}^{\leftarrow}\) approximate \(\mathbb{P}\) arises.

### Cross-Entropy Loss and Perplexity

LLMs are trained as follows: sample sequences of \(n\) consecutive tokens \((x_{1},\ldots,x_{n})\) from the dataset; then, for \(i=1,\ldots,n\), get a prediction \(p_{i}:\mathcal{V}\rightarrow[0,1]\) for \(X_{i}\) (given previous tokens for the FW model/next tokens for the BW one), compute the loss \(\sum_{i=1}^{n}\ell\left(p_{i},x_{i}\right)\) on the observed tokens \(x_{i}\) for a loss function \(\ell\), perform a gradient step to optimize \(\ell\), and start again.

In the training of most LLMs, the prime choice for \(\ell\) is the _cross-entropy loss_, defined by \(\ell_{\mathcal{C}}\left(\mathbf{p}_{k},x_{k}\right)=-\ln\mathbf{p}_{k}\left( x_{k}\right)\): the negative log of the _predicted probability of the observed token_. It is a _proper scoring rule_(Savage, 1971; Gneiting and Raftery, 2007), uniquely identified by certain modularity properties (Hanson, 2012); in expectation, it gives the number of nats (\(\ln 2\) times the number of bits) needed to compress \((x_{1},\ldots,x_{n})\) when using a coding scheme based on the model's estimated probabilities. Finally, and crucially for us, we have the following:

_Remark 2_.: For \(i=1,\ldots,n\), let \(\left(\mathbf{p}_{i}^{\rightarrow}\right)_{i}\) and \(\left(\mathbf{p}_{i}^{\leftarrow}\right)_{i}\) denote the predictions of the FW and BW models respectively. Setting \(\ell_{i}^{\leftarrow}:=\ell_{\mathcal{C}}\left(\mathbf{p}_{i}^{\leftarrow},x_ {i}\right)\), we have","\sum_{i=1}^{n}\ell_{i}^{arrow}=-\ln\mathbb{P}_{n}^{arrow}\{X_ {1}=x_{1},\cdots,X_{n}=x_{n}\}.",,\ell_{i}^{\leftarrow}=-\ln\mathbf{p}_{i}^{\leftarrow}\left( x_{i}\right),"Cross-entropy loss for the backward model, where \(\ell_{i}^{\leftarrow}\) is the loss for the \(i\)-th token prediction from the backward model."
ICML_2024_oral_13,4,"In particular, if the FW and BW measures coincide, the cross-entropy losses are identical.

_Remark 3_.: If \((x_{1},\ldots,x_{n})\) is sampled from \(\mathbb{P}_{n}\), denoting by \(\mathcal{L}_{n}^{\leftarrow}\) the expectations of \(\sum_{i=1}^{n}\ell_{i}^{\leftarrow}\) (estimated by the test loss of the models during training), we have","\mathcal{L}_{n}^{arrow}=\mathrm{D}_{\mathrm{KL}}(\mathbb{P}_{n} \big{|}\big{|}\mathbb{P}_{n}^{arrow})+H(\mathbb{P}_{n}),",where \(H\) denotes the entropy and \(\mathrm{D}_{\mathrm{KL}}\) the Kullback-Leibler divergence,\mathcal{L}_{n}^{\leftarrow} = H(X) + \mathrm{D}_{\mathrm{KL}}\left(\mathbb{P}_{n}^{\leftarrow} \| \mathbb{P}_{n}\right),where \(H\) denotes the entropy and \(\mathrm{D}_{\mathrm{KL}}\) the Kullback-Leibler divergence
ICML_2024_oral_15,2,"\). This formalization allows for a reconsideration of the optimal \(T\) given \(X\) for previous works, as will be analyzed in Section 4.

Information retrieval involves another term, \(Q\), and the optimization can be extended as follows:",\min_{p(T|X)} I(X;T),,"\min_{p(T|X,Q)}\quad I(X;T|Q),",where \(I(X;T|Q)\) denotes the conditional mutual information of \(X\) and \(T\) given \(Q\).
ICML_2024_oral_15,9,(8),"=\mathbb{E}_{X,T,Q}\log\frac{p(X|Q)}{p(T|Q)}\frac{p(T)}{p(X)}.",,"I(X;T) =\mathbb{E}_{X,T}\log\frac{p(X|T)}{p(X)},",where \(I(X;T)\) denotes the mutual information between \(X\) and \(T\).
ICML_2024_oral_24,1,"### Measuring Debater and Judge Performance

We are interested in oversight protocols with no access to labels, but existing work uses metrics that assume access to the ground truth. In this section, we introduce metrics to measure debater capabilities without this assumption. Given a dataset \(\mathcal{D}\) consisting of \(N\) questions (\(q\)) and candidate answers (\(a_{1},a_{2}\)), \(\mathcal{D}=\{q_{i},a_{i1},a_{i2}\}_{i=1}^{N}\), a debate _match_ is defined over a dataset by \((D_{1},D_{2},J)\), comprising two models \(D_{1}\) and \(D_{2}\), and a Judge \(J\). The first argument refers to which model is assigned \(a_{1}\) to support (in this case \(D_{1}\)), and the second argument refers to which model is assigned \(a_{2}\). A consultancy match with a consultant \(D\) is expressed as \((D,-,J)\) or \((-,D,J)\).

**Win rate** -- We define the win rate as the frequency with which a judge picks a specific debater's answer. For the match \((D_{1},D_{2},J)\), the win rate, \(\omega_{1}\), for Debater \(D_{1}\) is:","\omega_{1}(D_{1},D_{2},J)=\frac{1}{N}\sum_{i=1}^{N}\mathds{1}\{J(q_{i},a_{i1}, a_{i2})=a_{i1}\}",,\omega_{1} = \frac{N_{1}}{N},"Where \(\omega_{1}\) is the win rate for Debater \(D_{1}\), \(N_{1}\) is the number of times Judge \(J\) selects \(D_{1}\)'s answer, and \(N\) is the total number of matches."
ICML_2024_oral_24,2,"In practice, this win rate depends on the assignment of the answer choice, which gives an unfair advantage, as some assignments may be easier to argue for than others (e.g. some answers are easier to defend than others). To mitigate this, we flip assignments such that \(D_{1}\) and \(D_{2}\) argue for the opposite assignments and then take the average, \(\bar{\omega}_{1}\). Assuming debaters perform equally well across all questions, we say \(D_{1}\) is _more persuasive_ than \(D_{2}\) if:","\bar{\omega}_{1}(D_{1},D_{2},J)>\frac{1}{2}",,"\bar{\omega}_{1}(D_{1},D_{2},J)=\frac{1}{2}\left(\omega_{1}(D_{1},D_{2},J) + \omega_{1}(D_{2},D_{1},J)\right)",Average win rate for Debater \(D_{1}\) considering flipped assignments.
ICML_2024_oral_24,3,"**Elo rating** -- To capture a general notion of persuasiveness, we evaluate aggregate win rates for each debater across a population of co-debaters. We parameterise win rates by a latent skill, using the Elo ranking metric (Elo, 1978). We calculate ratings by minimising predicted win rate error (see Appendix D.5). _Aggregate rating_ is defined as the latent skill for a debater independent of assignment. Given two debaters, \(D_{1}\) and \(D_{2}\), and a judge \(J\), we define each player's aggregate rating, \(E_{1}\) and \(E_{2}\), as the solution to the equality:","\bar{\omega}_{1}(D_{1},D_{2},J)=\frac{1}{1+10^{(E_{2}-E_{1})/400}}",,"E_{1} = E_{2} + \log\left(\frac{1 - \bar{\omega}_{1}(D_{1},D_{2},J)}{\bar{\omega}_{1}(D_{1},D_{2},J)}\right)",Aggregate rating for debater \(D_{1}\) in relation to debater \(D_{2}\) and judge \(J\).
ICML_2024_oral_26,3,"It can be observed that the computational and memory complexity for calculating each output in a Transformer model is \(\mathcal{O}(nd)\), where \(n\) is the sequence length and \(d\) is the dimensionality of the representation. Consequently, the time and memory complexity for processing a sentence of length \(n\) scales quadratically, becoming \(\mathcal{O}(n^{2}d)\). This quadratic scaling poses a significant computational burden, particularly for longer sequences where \(n\) is large, making processing resource-intensive and challenging.

To mitigate this complexity, the concept of a kernel mechanism has been introduced as a means to reduce the computational demands of attention mechanisms, which has been introduced in Tsai et al. (2019); Katharopoulos et al. (2020); Choromanski et al. (2020). Specifically, this involves the introduction of a kernel function \(K(\cdot,\cdot)\), which acts as a positive-definite kernel capable of measuring similarity. By utilizing this kernel, the attention mechanism can be reformulated as:","o_{i}=\sum_{j=1}^{n}\frac{K(q_{i},k_{j})}{\sum_{j^{\prime}=1}^{n}\mathcal{K}( q_{i},k_{j^{\prime}})}v_{j},",,"\begin{split}&\text{Attention}(Q,K,V) = \sum_{i=1}^{n} \sum_{j=1}^{n} K(q_{i}, k_{j}) v_{j},\\ &\text{where } K(\cdot,\cdot) \text{ is a positive-definite kernel.}\end{split}","where \(K(q_{i}, k_{j})\) denotes the kernel function measuring similarity between query \(q_{i}\) and key \(k_{j}\)."
ICML_2024_oral_44,3,"A sampling method that realizes this distribution is first to initialize a graph \(G^{\prime}\leftarrow(N,E)\). Then, iteratively sample whether to include edge \(e_{i}\) in \(G^{\prime}\) for all \(i\)'s. If including \(e_{i}\) causes a cycle in current \(G^{\prime}\), then the edge would not be included. Otherwise, add the edge to \(G^{\prime}\) with probability \(\theta_{i}\).

#### 2.3.3 Optimization Algorithm

To optimize the objective function (Equation (1)), we apply the REINFORCE algorithm (Williams, 1992) by applying a gradient ascent variant (e.g., Adam (Kingma and Ba, 2014)) with an unbiased gradient estimation:","\nabla_{\theta}\operatorname{\mathbb{E}}_{G_{\mathcal{E}}\sim D_{\theta}} [u_{\tau}(G_{\mathcal{E}})]\approx\frac{1}{M}\sum_{i=1}^{M}\hat{u}_ {\tau}(G_{i})\nabla_{\theta}\log(p_{\theta}(G_{i})),","where \(G_{1},G_{2},\ldots,G_{N}\sim D_{\theta}\) are mutually independent and \(\hat{u}_{\tau}(G_{i})\) is an independent unbiased estimate of \(u_{\tau}(G_{i})\) for all \(i\) and some \(M\in\mathbb{N}\)","\nabla_{\theta}\mathbb{E}_{G\sim D_{\theta}}[\hat{u}_{\tau}(G)]\approx\frac{1}{M}\sum_{i=1}^{M}\nabla_{\theta}\log P(G_{i})\hat{u}_{\tau}(G_{i}),","where \(G_{1},G_{2},\ldots,G_{N}\sim D_{\theta}\) are mutually independent and \(\hat{u}_{\tau}(G_{i})\) is an independent unbiased estimate of \(u_{\tau}(G_{i})\) for all \(i\) and some \(M\in\mathbb{N}\)"
ICML_2024_oral_5,8,"rs.

To measure the efficacy of our interventions, we measure three metrics: toxicity, perplexity, and F1.

**Toxicity.** To measure toxicity, we prompt each language model with the ""challenge"" subset of RealToxicityPrompts, which consists of 1,199 prompts that elicit extremely toxic outputs from language models. We follow prior work (Geva et al., 2022) and use Perspective API,3 an automated tool for toxicity detection, to assign toxicity scores to each generation.

Footnote 3: [https://github.com/conversational/perspectiveapi](https://github.com/conversational/perspectiveapi)

Perplexity.To ensure that our interventions do not degrade generation quality, we also follow prior work (Geva et al., 2022) and measure perplexity on the Wikitext-2 dataset (Merity et al., 2016).

F1.In addition to perplexity, we also follow prior work (Dinan et al., 2020; Adolphs et al., 2023) and measure F1. Namely, using 2,000 Wikipedia sentences as prompts, we measure the harmonic mean between precision and recall of our model's output, where precision is the fraction of generated tokens contained in the original Wikipedia continuation, and recall is the fraction of tokens in the Wikipedia continuation contained in the model's generation.

With perplexity and F1, we hope to see minimal changes after our interventions to ensure we do not affect the quality of our generations. Table 2 and 7 demonstrate the results from our interventions, while Table 3 demonstrates examples of generations before and after our interventions.

Note that our interventions depend on how much we scale each vector (\(\alpha\)). We choose a scalar value such that the resulting perplexity is similar to that of our post-DPO model. For details regarding our post-DPO model see Section 4.

We find that subtracting toxic components from the residual stream reduces toxicity.

## 4 Toxicity Alignment Using DPO

We next describe our alignment procedure using DPO.

### Background: DPO

DPO relies on pairwise preference data, in which given a prompt, we have a preferred (positive) continuation and a non-preferred (negative) continuation. Given each preference pair, the algorithm promotes the likelihood of the positive sample, while suppressing the likelihood of the negative sample, using the following loss term:","\mathcal{L}_{\text{DPO}}=-\mathbb{E}[\log\sigma(\beta \log P-\beta\log N)], || P=\frac{\pi_{\theta}(y_{+}\mid\mathbf{w})}{\pi_{ref}(y_{+}\mid \mathbf{w})},N=\frac{\pi_{\theta}(y_{-}\mid\mathbf{w})}{\pi_{ref}(y_{-}\mid \mathbf{w})},",,\mathcal{L}_{DPO} = -\log\left(\frac{p_{+}}{p_{+} + p_{-}}\right),"where \(p_{+}\) and \(p_{-}\) are the probabilities of the positive and negative samples, respectively"
ICML_2024_oral_58,1,"### World Model Learning

The world model learns representations of all sensory modalities that the agent receives and then predicts the sequence of these latent representations given actions. Predicting future representations not only provides a rich learning signal to ground language in visual experience but also allows planning and policy optimization from imagined sequences. The world model is shown in Figure 2(a). At each time step, it receives an image \(x_{t}\), a language token \(l_{t}\), and an action \(a_{t}\). The image and language observations are compressed into a discrete representation \(z_{t}\) and fed together with the action into the sequence model to predict the next representation \(\hat{z}_{t+1}\). The multimodal world model consists of the following components, where \(h_{t}\) is a recurrent state:","\text{Sequence model:} \hat{z}_{t},\,h_{t}=\text{seq}(z_{t-1},\,h_{t-1},\,a_{t-1}) || \text{Multimodal encoder:} z_{t}\sim\text{enc}(x_{t},\,l_{t},\,h_{t}) || \text{Multimodal decoder:} \hat{x}_{t},\,\hat{l}_{t},\,\hat{r}_{t},\,\hat{c}_{t}=\text{ dec}(z_{t},\,h_{t})",,"\hat{z}_{t+1} = f(z_{t}, a_{t}, h_{t})","Predicted next representation based on current representation, action, and recurrent state."
ICML_2024_oral_93,2,"rm.

### Heatmap Generation

In the context of large-scale TSP, recent state-of-the-art approaches blend ML and OR, where ML models do not predict a solution (i.e., a permutation \(\mathbf{\pi}=(\pi_{1},\pi_{2},...,\pi_{n})\) of all the vertices) outright but alter the solution space distribution. Specifically, trained models predict an \(n\times n\) heatmap \(\Phi\), where \(\Phi_{i,j}\) indicates the suitability of including edge \((i,j)\) in the solution. The optimization problem's objective is defined as:","\mathcal{L}(\theta)=\mathbb{E}_{\mathbf{\pi}\sim\mathcal{S}}[\mathbb{E}_{\Phi \sim f_{\theta}(s)}[\mathbb{E}_{\mathbf{\pi}\sim g(s,\Phi)}[c(\bm {\pi})]]],","where \(s\) represents an instance from distribution \(\mathcal{S}\), \(\theta\) is the trainable parameters of model \(f\), \(\mathbf{\pi}\) is the solution outputed by post-hoc search algorithm \(g\) given \(\Phi\), and \(c(\mathbf{\pi})\) is calculated based on Equation 1","\min_{\mathbf{\pi}} \mathbb{E}_{s \sim \mathcal{S}} \left[ c(\mathbf{\pi}) + \lambda \sum_{i=1}^{n} \sum_{j=1}^{n} \Phi_{i,j} \cdot \mathbb{I}(\mathbf{\pi}) \right],","where \(s\) represents an instance from distribution \(\mathcal{S}\), \(\theta\) is the trainable parameters of model \(f\), \(\mathbf{\pi}\) is the solution outputted by post-hoc search algorithm \(g\) given \(\Phi\), and \(c(\mathbf{\pi})\) is calculated based on Equation 1."
ICML_2024_oral_93,3,"1.

Given the non-differentiable and computationally intensive nature of \(\mathbb{E}_{\mathbf{\pi}\sim g(s,\Phi)}\left[c\left(\mathbf{\pi}\right)\right]\), a surrogate loss \(\ell\left(s,\Phi\right)\), which is both differentiable and easy to compute, is often em ployed, leading to a surrogate objective:","\mathcal{L}_{\textit{surrogate}}(\theta)=\mathbb{E}_{s\sim\mathcal{S}}[ \mathbb{E}_{\Phi\sim f_{\theta}(s)}[\ell(s,\Phi)]].",,"\mathcal{L}_{\text{surrogate}}(\theta) = \mathbb{E}_{s \sim \mathcal{S}}[\ell(s,\Phi)],","where \(\ell(s,\Phi)\) is the surrogate loss function based on the heatmap \(\Phi\) and the TSP instance \(s\)."
ICML_2024_oral_93,4,"This surrogate loss, designed heuristically, can take forms of supervised (Fu et al., 2021; Sun and Yang, 2023), unsupervised (Min et al., 2023), or reinforcement learning (Qiu et al., 2022), where the optimized \(\theta^{*}\) from minimizing \(\mathcal{L}_{\textit{surrogate}}(\theta)\) is aimed to approximate the optimal \(\theta\) obtained from the original loss, i.e., \(\theta^{*}\approx\text{argmin}_{\theta}\mathbb{E}_{s\sim\mathcal{S}}\left[ \mathbb{E}_{\Phi\sim f_{\theta}(s)}\left[\mathbb{E}_{\mathbf{\pi}\sim g(s,\Phi)} \left[c\left(\mathbf{\pi}\right)\right]\right]\right]\). However, this approximation often lacks a rigorous theoretical foundation, making it uncertain whether minimizing the surrogate loss genuinely aligns with optimizing the original TSP objective. Consequently, despite optimizing \(\theta^{*}\) for the surrogate loss, its efficacy in guiding MCTS to find optimal solutions during testing remains questionable. During inference, the output heatmap \(\Phi^{*}\) from \(f_{\theta^{*}}(s)\) is fed into the search algorithm \(g\), yielding the solution \(\mathbf{\pi}^{*}\sim g(s,\Phi^{*})\). This disconnect between training and test phases--where training focuses on heatmap generation without involving MCTS, while testing relies on MCTS guided by these heatmaps--highlights a potential misalignment in the approach, as depicted in Figure 2.

### Monte Carlo Tree Search

MCTS is utilized as a guided \(k\)-opt process, which iteratively refines a complete TSP solution \(\mathbf{\pi}\) by alternating edge deletions and additions. The selection of edges during \(k\)-opt is influenced by a weight matrix \(W\) and an access matrix \(Q\), both of which are dynamically updated based on \(k\)-opt outcomes. Here, \(W_{i,j}\) scores the suitability of edge \((i,j)\) in the solution, while \(Q_{i,j}\) records the number of times edge \((i,j)\) is selected. Note that this section covers only the key aspects of MCTS. For a detailed understanding, please refer to Fu et al. (2019, 2021); Min et al. (2023).

Initialization.The heatmap \(H\) initializes \(W\) (\(W_{i,j}=100\times H_{i,j}\)). The access matrix \(Q\) starts with all elements set to zero. Edge potential matrix \(Z\) guides the \(k\)-opt process, balancing exploitation and exploration. The edge potential \(Z_{i,j}\) is formulated as \(Z_{i,j}=\frac{W_{i,j}}{\Omega_{i}}+\alpha\sqrt{\frac{\ln(M+1)}{Q_{i,j}+1}}\), where \(\Omega_{i}\), the average weight of edges connected to vertex \(i\), is \(\Omega_{i}=\frac{\sum_{j\neq i}W_{i,j}}{\sum_{j\neq i}1}\), \(\alpha\) balances exploitation and exploration, and \(M\) is the total number of actions sampled so far.

A random initial tour \(\mathbf{\pi}\) is constructed and optimized using 2-opt. The initial tour construction probability is formulated as \(p(\mathbf{\pi})=p(\pi_{1})\prod_{i=2}^{n}p(\pi_{i}|\pi_{i-1})\), where \(p(\pi_{i}|\pi_{i-1})\) is the conditional probability of choosing the next vertex, calculated by the edge potential:","p(\pi_{i}|\pi_{i-1})=\frac{Z_{\pi_{i-1},\pi_{i}}}{\sum_{l\in\mathbb{X}_{\pi_{ i-1}}}Z_{\pi_{i-1},l}},",,"p(\pi_{i}|\pi_{i-1})=\frac{Z_{i,\pi_{i}}}{\sum_{j\neq \pi_{i-1}}Z_{i,j}},","where \(Z_{i,j}\) is the edge potential for edge \((i,j)\), \(\Omega_{i}\) is the average weight of edges connected to vertex \(i\), \(\alpha\) is a balancing parameter, and \(M\) is the total number of actions sampled."
naacl_2024_short_23,2,"\). The AttributePredictor (see section 3.1) is a novel formulation of geographical text classification, the CandidateGenerator (see section 3.2) is the best ranking system from prior work, and the Constrainter (see section 3.3) is a novel deterministic constraint-based algorithm.

### Attribute Predictor

This function predicts the country, state, and feature class of \(m\). It is formulated as a text classification model, based on a novel input prompt coupled with a masked language modeling objective. The prediction targets are defined as:

**Feature Class** is one of the nine types defined by GeoNames: \(A\), Administrative boundaries (e.g., countries, states, provinces); \(P\), Populated places (e.g., cities, towns, villages); \(U\), Undersea features (e.g., oceanic ridges, trenches), etc.
**State** is the canonical name of one of the 3871 first-order administrative divisions in GeoNames, such as states, provinces, or regions.
**Country** is the canonical name of one of the 252 countries in GeoNames.

Figure 1: The architecture of our model: GEOgraphical normalization by Predicting Attributes to Constrain Ontology Entries (GeoPLACE). The figure shows how GeoPLACE normalizes a mention of _Paris_.

We implement prediction of these targets as:","Z =\textsc{transformer}(\textsc{Toinput}(m,M)) || \hat{C}_{m} =\mathrm{softmax}(Z_{c}W_{c}) || \hat{S}_{m} =\mathrm{softmax}(Z_{s}W_{s}) || \hat{F}_{m} =\mathrm{softmax}(Z_{f}W_{f})",,\hat{C}_{m} = \textsc{predictCountry}(m),"Predictions for the country, state, and feature class of the mention \(m\) using respective prediction functions."
naacl_2024_short_23,3,"),..., \(m_{|M|}\) in which \(m\) is [MASK] located in [MASK] of [MASK] [SEP]1; \(f\), \(s\), \(c\), are the indexes of the three [MASK] tokens; \(W_{c},W_{s},W_{f}\in\mathbb{R}^{N\times H}\) are the learnable parameters of the three classification heads; \(N\) is the size of the transformer tokenizer's vocabulary; and \(H\) is the size of the transformer's contextualized representations. We add new tokens to the transformer's tokenizer to ensure that every country, state, and feature class is a single token in the classifier output, e.g., making _United States_ a single token. This single-token prediction approach compares favorably to a multi-token sequence-to-sequence prediction approach, as shown in section 4.

Footnote 1: This prompt dramatically reduces the size of the input while still providing most of the critical document-level information for disambiguating toponyms

The model is trained on the labeled data in the toponym datasets with cross-entropy loss:",L=C_{m}log(\hat{C}_{m})+S_{m}log(\hat{S}_{m})+F_{m}log(\hat{F}_{m}),"where \(C_{m}\), \(S_{m}\), and \(F_{m}\) are one-hot vectors of size \(N\) representing the true country, state, and feature class for mention \(m\)",\mathcal{L} = -\frac{1}{|M|}\sum_{m\in M}\left(C_{m}\log(\hat{C}_{m}) + S_{m}\log(\hat{S}_{m}) + F_{m}\log(\hat{F}_{m})\right),"where \(C_{m}\), \(S_{m}\), and \(F_{m}\) are one-hot vectors of size \(N\) representing the true country, state, and feature class for mention \(m\)."
naacl_2024_short_26,3,"\).

### Prototypical Contrastive Learning

One may notice that \(\mathcal{L}_{gen}\) maximizes the likelihood over the entire sentence. In fact, more optimizations may be beneficial especially when decoding the stance label. To this end, we decouple the stance embedding \(\boldsymbol{z}\in\mathbb{R}^{embed\_size}\) (the embedding used to generate the stance token) from BART decoder outputs. We then project it into a low-dimensional vector \(\boldsymbol{\hat{z}}\in\mathbb{R}^{low\_embed\_size}\) to prevent \(\boldsymbol{z}\) losing much semantics and being over-corrected. To regularize \(\boldsymbol{\hat{z}}\) to be more discriminative in the latent space, inspired by Li et al. (2021), we introduce the concept of prototypes, they are widely adopted in data-efficient learning Li et al. (2024) and can be viewed as the representatives of class-wise embeddings. However, different from Li et al. (2021), we view prototypes as the representatives of class-wise projected embeddings instead of class-agnostic ones. By interacting with prototypes, a contrastive loss is employed to increase the intra-class similarity but decrease the inter-class similarity of projected stance embeddings. In the following, we will detail how to estimate class-wise prototypes and formulate the contrastive loss.

Online Prototype UpdateFor each stance class \(c\), we randomly initialize a vector \(\boldsymbol{v}_{c}\) as its prototype before normalizing it into a unit one at the beginning. Along the training progress, we update \(\boldsymbol{v}_{c}\) at each step in a moving average manner by,",\boldsymbol{v}_{c}arrow\text{Normalize}(\beta\boldsymbol{v}_{c}+(1- \beta)\boldsymbol{v}_{c}^{\prime}),"where \(\beta\) is a momentum coefficient, \(\text{Normalize}(\cdot)\) is the normalization function, and \(\boldsymbol{v}_{c}^{\prime}\) is the centroid of embeddings belonging to class \(c\) in the batch",\boldsymbol{v}_{c} \leftarrow (1 - \beta) \boldsymbol{v}_{c} + \beta \cdot \text{Normalize}(\boldsymbol{v}_{c}^{\prime}),"where \(\beta\) is a momentum coefficient, \(\text{Normalize}(\cdot)\) is the normalization function, and \(\boldsymbol{v}_{c}^{\prime}\) is the centroid of embeddings belonging to class \(c\) in the batch"
naacl_2024_short_34,3,"Eq. (2) encapsulates the recurrence relation of S4 (Gu et al., 2022; Gupta et al., 2022), S5 (Smith et al., 2023), and Linear Recurrent Unit (Orvieto et al., 2023). For example, \(A\) represents the HiPPO matrix family (Gu et al., 2023) of S4 or a complex diagonal matrix of Linear Recurrent Unit. We show in Proposition 1 that such an input-independent matrix \(A\) cannot represent subtraction.

**Proposition 1**.: _An input-independent LRNN is inconsistent in representing subtraction._

Proof.: Denote \(u_{0}\), \(u_{-}\), and \(u_{1}\) as the input vector w.r.t. input characters 0, -, and 1. Denote \(z\) as the initial state vector. The sequences ""0-1"" and ""1-0"" are represented as","\begin{split} x_{0-1}&=A^{3}z+A^{2}u_{0}+Au_{-}+u_{ 1},\ \ \text{for ""0-1""}\\ x_{1-0}&=A^{3}z+A^{2}u_{1}+Au_{-}+u_{0},\ \ \text{for ""1-0""} \end{split}",,"z_{0} = A z + B u_{0}, \quad z_{1} = A z_{0} + B u_{-}, \quad z_{2} = A z_{1} + B u_{1}","Denote \(z_{0}\), \(z_{1}\), and \(z_{2}\) as the state vectors representing the sequences ""0-1"" and ""1-0"" in the LRNN framework."
naacl_2024_short_39,2,"The prompt tuning method proposed in Lester et al. (2021) is represented by Equation 1. The parameter of a pre-trained language model \(\theta\) is fixed, and only the prompt parameter \(\theta_{\mathbf{P}}\) of the soft prompt \(\mathbf{P}=[p_{1},p_{2},\dots,p_{l}]\in\mathbb{R}^{l\times d}\) is learnable. We use the prompt length \(l=100\), and \(d\) is the input dimension of the model.

### Datasets

Following the two classification systems from Rogers et al. (2023), we show 16 QA datasets2 used in our analysis in Table 1. Detailed descriptions of each dataset are provided in Appendix A.

Footnote 2: In cases where only one of valid or test datasets was available such as Rajpurkar et al. (2016), we used it in the testing process. Additionally, we split the train datasets into a 9:1 ratio, and used it in the train and valid process, respectively. The number of datasets we used is shown in Table 1.

First, the amount of evidence is how much evidence is provided to answer the question. _Single Source_ indicates that the information required to answer the question is explicitly contained within a context. Partial Source means that although some evidence is available, it needs to be integrated with external knowledge to answer the question. **No Source** needs to find answers solely from implicit knowledge. The more evidence available to answer a question, the more explicit knowledge exists; conversely, the less evidence, the more implicit knowledge exists.

Second, the answer format is divided into four types. Extractive format refers to when the answer span can be found within the provided context. Categorical format denotes that the correct answer is in a pre-defined option, exclusively employing yes or no formats in our dataset. Multi-choice format indicates that answer options are given, and the answer is to be chosen from among them. Lastly, Freeform format refers to cases where the model generates answers without following a specific format.

## 3 Results and Analysis

To study the transferability of soft prompts, we used 16 QA datasets as the source and target tasks. The main terms referred to in this section are as follows: (1) vanilla prompt tuning (Vanilla PT), the result of training the prompt in Equation 1 after random initializing; (2) zero-shot performance, the result of solving the target task using the source prompt without additional training; and (3) prompt transfer (PoT), the result of initializing the target prompt with the selected source prompt and training it as shown in Equation 1. For our experiments, we used the T5BASE3 as our base LM. Further experimental details are in Appendix B.

Footnote 3: [https://huggingface.co/t5-base](https://huggingface.co/t5-base)

### Transferability with Initialization

Can transferability be interpreted as cosine similarity?As shown in Figure 1, we investigated the prompt transferability with cosine-similarity. We can observe that prompt embeddings with the

\begin{table}
\begin{tabular}{l|l l r r r} \hline
**Dataset** & **Answer format** & **Amount of evidence** & **Train** & **Valid** & **Test** \\ \hline \hline DuoRC (Saha et al., 2018) & Freeform & Partial & 60,094 & 12,845 & 12,415 \\ NQ-Open (Lee et al., 2019) & Freeform & **No** & 79,132 & 8,793 & 3,610 \\ WQ (Berant et al., 2013) & Freeform & **No** & 3,400 & 378 & 2,032 \\ MRQA-NewSQA (Trischler et al., 2017) & Extractive & _Single_ & 66,744 & 7,416 & 4,212 \\ SQuAD (Rajpurkar et al., 2016) & Extractive & _Single_ & 78,839 & 8,760 & 10,570 \\ BoolQ (Clark et al., 2019) & Categorical & _Single_ & 8,484 & 943 & 3,270 \\ MultiRC (Khashabi et al., 2018) & Categorical & _Single_ & 24,518 & 2,725 & 4,848 \\ TQA (Joshi et al., 2017) & Freeform & Partial & 78,859 & 8,763 & 11,313 \\ CosmosQA (Huang et al., 2019) & Multi-choice & Partial & 22,735 & 2,527 & 2,985 \\ SIQA (Sap et al., 2019) & Multi-choice & Partial & 30,069 & 3,341 & 1,954 \\ \hline SQuAD w/o ctx & Freeform & **No** & 78,839 & 8,760 & 10,570 \\ BoolQ w/o ctx & Categorical & **No** & 8,484 & 943 & 3,270 \\ MultiRC w/o ctx & Categorical & **No** & 24,518 & 2,725 & 4,848 \\ TQA w/o ctx & Freeform & **No** & 78,859 & 8,763 & 11,313 \\ CosmosQA w/o ctx & Multi-choice & **No** & 22,735 & 2,527 & 2,985 \\ SIQA w/o ctx & Multi-choice & **No** & 30,069 & 3,341 & 1,954 \\ \hline \end{tabular}
\end{table}
Table 1: The details of QA datasets. â€w/o ctxâ€ refers to the removal of context from the original dataset to evaluate the influence of the amount of evidence.

same answer formats are clustered together in Figure 1(b). However, Figure 1(a) demonstrates that the high similarity score between the source and target task does not necessarily result in positive transferability. For example, even though the transfer BoolQ (Clark et al., 2019) \(\rightarrow\) MultiRC (Khashabi et al., 2018) has the highest similarity score of \(0.9\), it yields a negative transferability of \(-2.8\%\). We note that the PoT performance varies significantly depending on the target task. Therefore, prompt initialization with high cosine-similarity does not guarantee performance improvement. As a result, we find that it is not suitable to interpret transferability through cosine-similarity in the QA task.

Can transferability be interpreted as zero-shot performance?To verify the effectiveness of selecting the best zero-shot prompt when used for initialization, we compare PoT performance between the best and worst zero-shot prompts in Table 2. When initialized with the best zero-shot prompt, it only outperforms the worst one in 7 out of 16

\begin{table}
\begin{tabular}{l||c||l c c|l c c||c} \hline \hline
**Target Task** & **Random** & **Best Source Task** & **Zero-shot** & **PoT** & **Worst Source Task** & **Zero-shot** & **PoT** & \(\Delta\) \\ \hline \hline
**DuoRC** & 2.14 & SQuAD & 32.86 & 35.56 & BoolQ & 0.77 & 36.79 & -1.23 \\
**NQ-Open** & 0.00 & SQuAD w/o ctx & 1.66 & 2.30 & MultiRC w/o ctx & 0.00 & 1.99 & +0.31 \\
**WQ** & 0.00 & NQ-Open & 3.69 & 3.99 & MultiRC & 0.00 & 2.51 & +1.48 \\
**MRQA-NewsQA** & 4.80 & SQuAD & 38.39 & 41.90 & MultiRC & 1.16 & 38.49 & +3.41 \\
**SQuAD** & 13.96 & DuoRC & 78.90 & 81.57 & CosmosQA & 1.07 & 81.28 & +0.29 \\
**BoolQ** & 0.00 & MultiRC & 67.37 & 76.70 & SIQA w/o ctx & 0.00 & 78.38 & -1.68 \\
**MultiRC** & 0.06 & BoolQ & 69.68 & 74.05 & TBA & 0.00 & 78.57 & -4.52 \\
**TQA** & 13.21 & DuoRC & 39.51 & 43.58 & MultiRC & 1.87 & 44.06 & -0.48 \\
**CosmosQA** & 2.91 & SIQA & 78.22 & 82.81 & MultiRC & 0.00 & 82.81 & 0.00 \\
**SIQA** & 0.61 & CosmosQA & 99.28 & 99.59 & BoolQ & 0.00 & 99.64 & -0.05 \\
**SQuAD w/o ctx** & 0.00 & NQ-Open & 0.96 & 1.74 & BoolQ & 0.00 & 1.65 & +0.09 \\
**BoolQ w/o ctx** & 19.27 & BoolQ & 47.83 & 51.13 & SIQA w/o ctx & 0.00 & 62.17 & -11.04 \\
**MultiRC w/o ctx** & 43.05 & MultiRC & 57.86 & 58.15 & SQuAD w/o ctx & 0.00 & 58.54 & -0.39 \\
**TQA w/o ctx** & 0.15 & SQuAD w/o ctx & 5.09 & 4.06 & BoolQ w/o ctx & 0.02 & 4.15 & -0.09 \\
**CosmosQA w/o ctx** & 0.20 & SIQA w/o ctx & 74.64 & 82.65 & MultiRC & 0.00 & 82.45 & +0.20 \\
**SIQA w/o ctx** & 0.46 & SQuAD & 26.46 & 99.39 & BoolQ & 0.00 & 99.33 & +0.06 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Relativeness of zero-shot and PoT performance. **Random** indicates the performance after random initialization. **Best Source Task** represents the best performance task in a zero-shot setting. **Worst Source Task** represents the worst performance task in a zero-shot setting. Each score is EM. The difference in the PoT scores between **Best Source Task** and **Worst Source Task** is denoted by \(\Delta\). When the **Zero-shot** scores are equal, we chose the source task with the higher **PoT** score.

Figure 1: (a) Heatmap of our task transferability results. (b) Heatmap of the cosine similarities between the source prompt embeddings. The colors of the task names indicate the answer format type: Blue, Extractive; Green, Categorical; Brown, Freeform; Yellow, Multi-choice.

cases. The mean absolute error was \(1.58\), indicating that the performance difference is approximate. Additionally, Figure 5 and Figure 6 indicate that most cases converge to similar values as the epoch progresses, regardless of which source prompt is selected. It can therefore be seen that the method proposed in Su et al. (2022) cannot assure better or comparable transfer performance in the QA task.

Effect of Dataset SizeIn Table 4, the PoT performance varies considerably depending on the target task. Therefore, we applied min-max normalization4 to each target task to compare the correlation between the source and target tasks. We classified the QA datasets based on the number of train datasets into small, medium, and large (see Appendix D). Subsequently, we divided into three groups 5 founded on the difference in size between the source task and target task as follows: _Target > Source_, _Same_ and _Target < Source_.

Footnote 4: See the formula in Appendix C.

Footnote 5: For example, _Target > Source_, indicating the train dataset group of the target task is larger than the source task (_e.g._, target task: large, source task: small).

As shown in Figure 2, the normalized task transferability results are based on the difference in the dataset group size between the source task and the target task. Regarding the _Target < Source_ group, most cases show positive transferability. The median (Q2) of each box plot indicates a tendency to drop in the sequence of _Target < Source_, _Same_, and _Target > Source_. We demonstrate that the dataset size of the source and target tasks in the QA task is a key factor in transferability.

### Investigating Catastrophic Forgetting

Catastrophic forgetting (Kirkpatrick et al., 2017) is the tendency for previously learned task knowledge to be abruptly lost as information relevant to the current task is incorporated. However, there is still no clear method for measuring this phenomenon.

Therefore, we propose a novel metric for evaluating catastrophic forgetting:",\frac{(Zero\text{-}shot\ correct)\cap(PoT\ incorrect)}{Zero\text{-}shot\ correct},"where _Zero-shot correct_ is the case of correct responses in a zero-shot setting, and _PoT incorrect_ is the case of incorrect answers after prompt transfer in the target task",\text{Catastrophic Forgetting} = \frac{\text{Zero-shot correct} - \text{PoT incorrect}}{\text{Zero-shot correct}},"where _Zero-shot correct_ is the case of correct responses in a zero-shot setting, and _PoT incorrect_ is the case of incorrect answers after prompt transfer in the target task."
naacl_2024_short_43,1,"## 2 Motivation & Approach

The **CoCo-CroLa** benchmark (CCCL) evaluates a T2I model's ability to generate images of an inventory of tangible concepts when prompted in different languages (Saxon and Wang, 2023). Given a tangible concept \(c\), written in language \(\ell\) as phrase \(c_{\ell}\), the \(i\)-th image produced by a multilingual T2I model \(f\) on the concept \(c_{\ell}\) can be expressed as:","I_{c,i}\sim f(c_{\ell})",,I_i = f(c_{\ell}),\(I_i\): the \(i\)-th image produced by the T2I model; \(f\): the T2I model function; \(c_{\ell}\): the tangible concept written in language \(\ell\).
naacl_2024_short_43,3,"\). In practice, the default source language \(\ell_{s}\) is English and \(F\) is the CLIP visual feature extractor (Radford et al., 2021).

### Translation Errors in CoCo-CroLa

CCCL requires correct translations of each concept \(c\) from the source language \(\ell_{s}\) into a set of semantically-equivalent translations in each test language \(\ell\). Saxon and Wang (2023) built CCCL v1's concept translation list using an automated approach so as to allow new languages to be easily added without experts in each new language.

They used an ensemble of commercial machine translation systems to generate candidate translations and the BabelNet knowledge graph (Navigli and Ponzetto, 2010) to enforce word sense agreement. Unfortunately, this approach introduces translation errors (Table 1).

We check the Spanish, Chinese, and Japanese translations using a group of proficient speakers, following a protocol described in Appendix A.1.1, who identify a set of _translation error candidates_ that may not sufficiently capture a concept's intended semantics in English, for various reasons.

Some of the candidate errors, such as the error for _rock_ in JA (Table 1), represent severe failures to translate a concept into its common, tangible sense--it is incoherent to test a model's ability to generate pictures of rocks by prompting it with ""rock music."" However, other candidate errors, such as _father_ in ZH are still potentially acceptable translations, but deviate from the annotators' preferred level of formality or specificity.

To decide which corrections ought to be integrated in future T2I multilinguality benchmarks, quantifying both the significance of each translation correction is and its impact on the CCCL score for its concept is desirable.

### Quantifying Error Correction & Impact

Characterizing the _impact_ of a translation correction on model behavior is simple; we check \(\Delta X_{c}\), the change in the CCCL score going from the original concept translation \(c_{\ell}\) to the corrected \(c^{\prime}_{\ell}\),","\Delta X_{c}(c,\ell)=X_{c}(f,c^{\prime}_{\ell},c_{\ell_{s}})-X_{c}(f,c_{\ell}, c_{\ell_{s}})",,"\Delta X_{c} = X_{c}(f,c^{\prime}_{\ell},c_{\ell_{s}}) - X_{c}(f,c_{\ell},c_{\ell_{s}})",where \(c^{\prime}_{\ell}\) is the corrected translation of the concept \(c\).
naacl_2024_short_45,2,"Intrinsic ApproachFor our intrinsic approach using SelfExplain (Rajagopal et al., 2021), we augment the dialect classifier with a Local Interpretability Layer (LIL) during training. This layer quantifies the relevance of each feature \(x_{i}\) in input \(\mathbf{X}\) to the final label distribution \(\boldsymbol{\ell}\) via activation difference (Shrikumar et al., 2017), and is trained jointly with the final classifier layer. Taking \(\boldsymbol{\ell}\) in Equation 1, the loss is the negative log probability, summed over all training instances:",L_{\mathit{dialect-classifier}}=-\sum_{i}\log\boldsymbol{\ell}[y_{i}^{*}],where \(y_{i}^{*}\) is the correct label for instance \(i\),\mathcal{L} = -\sum_{i} y_{i}^{*} \log(\boldsymbol{\ell}),where \(y_{i}^{*}\) is the correct label for instance \(i\)
naacl_2024_short_46,3,"ly. Following Liu et al. (2022), to entangle span representations with soft prompts, the probability distribution over all prompts is calculated as \(Z_{q}=\text{FFN}(\mathbf{Q}^{k})\cdot\overline{\mathbf{x}}^{i}_{t}\), where \(\cdot\) is the inner product. \(Z_{q}\) is then combined with \(Z^{i}_{t}\) to optimize the cross entropy loss:","\mathcal{L}_{\text{new}}=-\sum_{(\overline{x}^{i}_{t},y^{i}_{t})\in\mathcal{ D}^{k}_{\text{train}}}\text{CE}(Z^{i}_{t}+Z_{q},y^{i}_{t})",,"L = -\sum_{j} Z^{i}_{t,j} \log(Z_{q,j})","where \(Z_{q}\) is the probability distribution over all prompts, and \(Z^{i}_{t}\) is the logits for the predicted event type"
naacl_2024_short_5,1,"#### 3.2.1 Unsupervised SKICSE

Given an unlabeled input sentence \(x\), SKICSE creates a positive example \(x^{\text{ski}}\) for \(x\) by obtaining its SKI. We can constitute a triplet of sentences \((x,x^{\prime},x^{\text{ski}})\) as shown in Figure 2(a). Here, \(x\) and \(x^{\prime}\) have the same text, but different hidden dropout masks. By using the BERTbase encoder \(f\), we can get a triplet of sentence embeddings \((f(x),f(x^{\prime}),f(x^{\text{ski}}))=(\mathbf{h},\mathbf{h}^{\prime}, \mathbf{h}^{\text{ski}})\), and objective functions can be formulated as:","\mathcal{L}^{\text{unsup}}_{\text{simce}}=-\log\frac{e^{\text{sim}( \mathbf{h}_{i},\mathbf{h}_{i}^{\prime})/\tau}}{\sum_{j=1}^{N}e^{\text{ sim}(\mathbf{h}_{i},\mathbf{h}_{j}^{\prime})/\tau}}, || \mathcal{L}^{\text{unsup}}_{\text{skice}}=-\log\frac{e^{\text{sim}( \mathbf{h}_{i},\mathbf{h}_{i}^{\text{ski}})/\tau}}{\sum_{j=1}^{N}e^{ \text{sim}(\mathbf{h}_{i},\mathbf{h}_{j}^{\text{ski}})/\tau}},",,\mathcal{L}_{\text{SKICSE}} = -\log \frac{\exp(\mathbf{h} \cdot \mathbf{h}^{\text{ski}} / \tau)}{\sum_{j=1}^{N} \exp(\mathbf{h} \cdot \mathbf{h}^{\prime}_{j} / \tau) + \exp(\mathbf{h} \cdot \mathbf{h}^{\text{ski}} / \tau)},"Loss function for the SKICSE objective, where \(\tau\) is the temperature parameter, \(\mathbf{h}\) is the embedding of the original sentence, and \(\mathbf{h}^{\text{ski}}\) is the embedding of the SKI sentence."
naacl_2024_short_52,4,"The same argument works if the normal distribution has spherical covariance \(\sigma\mathbf{I}_{d}\) for any \(\sigma\), and thus, since the cosine loss is norm-invariant, uniform initialization is exactly equivalent to the standard initialization of transformer embeddings.

Hypercube.The corners of the hypercube \(\{-1,1\}^{d}\) all have norm \(\sqrt{d}\) and thus form a discrete subset of a hypersphere. This motivates us to consider drawing embeddings from a scaled Rademacher distribution:",\mathbf{E}(y_{i})=\mathbf{r}_{i}/\sqrt{d};\quad\mathbf{r}_{i}\sim\text{Rademacher}(d).,,"\mathbf{E}(y_{i})=\sigma \cdot \mathbf{r}_{i};\quad \mathbf{r}_{i}\sim\text{Rademacher}(\{-1,1\}^{d}).",The scaled Rademacher distribution for generating embeddings.
naacl_2024_short_67,5,"h). \(W\) is a linear layer of \((L-1)\times 1\), \(b\in\mathbb{R}^{L-1}\) is the bias, and \(A\in\mathbb{R}^{L-1}\) denotes the final weights. The final representation \(d^{*}\in\mathbb{R}^{d}\) is obtained by weighting \(H\) using \(A\).

#### 2.2.3 Contrastive Learning

When \(N\) input instances \(\{x_{1},...,x_{N}\}\) and their corresponding relation descriptions \(\{d_{1},...,d_{N}\}\) are input into the encoder within a mini-batch, we obtain the representations of instance \(x_{i}^{vec}\) and description \(d_{i}^{vec}\), \(i\in[1,N]\). To effectively learn the matching relationship between \(x_{i}^{vec}\) and \(d_{i}^{vec}\), we utilize a contrastive learning method, where \(d_{i}^{vec}\) serves as a positive sample and other \(N-1\) samples within the mini-batch \(d_{j}^{vec}(j\neq i)\) serve as negative samples. The goal of contrastive learning is to minimize the distance between \(x_{i}^{vec}\) and \(d_{i}^{vec}\) while maximizing the distance from \(d_{j}^{vec}\).

We utilize cosine similarity as the measurement and employ the infoNCE(van den Oord et al., 2018) as the contrastive loss function:","\mathcal{L}_{i}=-\log\frac{e^{\text{sim}(x_{i}^{vec},d_{i}^{vec})/\tau}}{\sum_ {j=1}^{N}e^{\text{sim}(x_{i}^{vec},d_{j}^{vec})/\tau}}",where \(\tau\) is a temperature hyperparameter and \(\text{sim}(\cdot)\) is the cosine similarity,"\mathcal{L}_{contrastive}=-\frac{1}{N}\sum_{i=1}^{N}\log\frac{\exp(\text{sim}(x_{i}^{vec},d_{i}^{vec})/\tau)}{\sum_{j=1}^{N}\mathbb{I}_{[j\neq i]}\exp(\text{sim}(x_{i}^{vec},d_{j}^{vec})/\tau)}",where \(\tau\) is a temperature hyperparameter and \(\text{sim}(\cdot)\) is the cosine similarity
naacl_2024_short_8,2,"on.

Some of the previous works term the models as ""mono[BACKBONE]"" when initialized from non-BERT pretrained Transformers, such as mono-ELECTRA (Pradeep et al., 2022). However, since the underlying model structure remains the same, we refer to them all as monoBERT while specifying the backbones where the models are initialized.

**Mean-Pooling**. To study whether the improvement of CELI is attributed to the interaction between the query and the documents tokens, or simply the additional token information, we add the Mean-Pooling method as a baseline. Instead of computing the similarity score based solely on the [CLS] representation as in Eq. (1), it uses the mean representation of all the tokens:","s_{m}(q,d)=\frac{1}{n}\sum_{i}^{n}(T_{tok_{i}}W+b),","where \(T_{tok_{i}}\) is the final-layer representation of the \(i\)-th token, and \(n\) is the total number of tokens in the input sequence","s_{mp}(q,d)=\frac{1}{n}\sum_{i=1}^{n}T_{tok_{i}}","where \(T_{tok_{i}}\) is the final-layer representation of the \(i\)-th token, and \(n\) is the total number of tokens in the input sequence"
neurips_2024_oral_10,7,We can obtain this by eliminating \(Y_{t-1}\) from Eq.(8) and Eq.(9) and replacing \(Y_{t}\) with \(WX_{t}+b\):,\begin{split} Y_{t-2}=W^{\prime\prime}X_{t}+C^{\prime\prime}\\ W^{\prime\prime}=\frac{1}{\sqrt{a_{t}-1}}\{\frac{W}{\sqrt{a_{t} }}-[C_{1}(t)+C_{1}(t-1)]WW_{D}+\sqrt{a_{t}}C_{1}(t-1)C_{1}(t)WW_{D}W_{D}\}\\ C^{\prime\prime}=\frac{1}{\sqrt{a_{t}-1}}[WC_{2}(t)+\sqrt{a_{t}}WC_{ 2}(t-1)-\sqrt{a_{t}}C_{1}(t-1)C_{2}(t)WW_{D}]Z+b\end{split},,\frac{1}{\sqrt{a_{t-1}}}(WX_{t}+b)-Y_{t-2}=C_{1}(t-1)D_{\theta}(WX_{t}+b)-\sigma_{t-1}z,"where \(W\) denotes the parameters of the linear layer, \(b\) denotes the bias term, and \(Y_{t-2}\) denotes the result after denoising in two steps of \(Y_{t}\)"
neurips_2024_oral_13,1,"### Preliminaries

As the common modules within each transformer block of LLMs, both Multi-head Self-Attention (MSA) and Feed-Forward Network (FFN) fundamentally consist of basic linear layers, which can be represented as, \(\mathbf{Y}=\mathbf{X}\cdot\mathbf{W}\in\mathbb{R}^{T\times C_{out}}\). Here, \(\mathbf{X}\in\mathbb{R}^{T\times C_{in}}\) is the activation input and \(\mathbf{W}\in\mathbb{R}^{C_{in}\times C_{out}}\) denotes the weight matrix. In this paper, we focus on integer uniform quantization [25] of both activation and weight, aiming to achieve better hardware support. Specifically, the \(b\)-bit quantization process maps the FP16 tensor \(\mathbf{X}\) to low-bit integer \(\mathbf{X}_{q}\):","\mathbf{X}_{q}=\text{clamp}(\,\lfloor\frac{\mathbf{X}}{\Delta} \rceil\!+\!z,0,2^{b}-1),\text{where }\Delta=\frac{\max(\mathbf{X})-\min(\mathbf{X})}{2^{b}-1},z=-\lfloor \frac{\min(\mathbf{X})}{\Delta}\rfloor.",,\mathbf{X}_{q} = \text{round}(\mathbf{X} \cdot 2^{b-1}),"Here, \(\mathbf{X}_{q}\) is the quantized activation, and \(\text{round}(\cdot)\) denotes the rounding function applied to the scaled input tensor."
neurips_2024_oral_15,6,"_Here, the asymptotic variance depends on a problem-specific constant, \(\zeta^{2}\), with an upper bounded:_",\zeta^{2}\leq\|y\|_{(\sum_{x\in\mathcal{X}_{\text{sample}}} [\min_{x^{\prime}\in\mathcal{X}_{\text{sample}}}\mathbb{E}[t_{x^{ \prime}}]]\cdot xx^{\top})^{-1}}^{-1}.,,\zeta^{2}\coloneqq\frac{1}{\sum_{x\in\mathcal{X}_{\text{sample}}}\frac{|\mathbb{E}[c_{x}]|^{2}}{\mathbb{E}[t_{x}]^{2}}}.,asymptotic variance constant
neurips_2024_oral_15,10,"eq. (4)).

Notably, the choice-only estimator in eq. (6) aligns with the EZ-diffusion model's drift estimator (67, eq. (5)). Moreover, the estimators in Xiang Chiong et al. (73, eq. (6)) and Berlinghieri et al. (8, eq. (7)) combine elements of both estimators from eqs. (5) and (6). In section 5.2, we demonstrate that both estimators from Wagenmakers et al. (67, eq. (5)) and Xiang Chiong et al. (73, eq. (6)) are outperformed by our proposed estimator in eq. (3) for the full bandit problem.

Assuming the utility difference \(u_{x}\neq 0\), the choice-decision-time estimator in eq. (5) satisfies the following non-asymptotic concentration bound, proven in appendix C.3.1:

**Theorem 3.3** (Non-asymptotic concentration of \(\widehat{u}_{x,\text{CH,DT}}\)).: _For each query \(x\in\mathcal{X}\) with \(u_{x}\neq 0\), given a fixed i.i.d. dataset \(\left\{\left(c_{x,s_{x},i},t_{x,s_{x},i}\right)\right\}_{i\in[n_{x}]}\), for any \(\epsilon>0\) satisfying \(\epsilon\leq\min\left\{|u_{x}|/(\sqrt{2}a),\left(1+\sqrt{2}\right)a|u_{x}|/ \mathbb{E}\left[t_{x}\right]\right\}\), the following holds:_","\mathbb{P}(|\widehat{u}_{x,\text{CH,DT}}-\frac{u_{x}}{a}|> \epsilon)\leq 4\exp(-[m_{\text{CH,DT}}^{\text{non-axym}}(x^{ \top}\theta^{*})]^{2}\,n_{x}\,[\epsilon\cdot a]^{2} ),",,"\left|\widehat{u}_{x,\text{CH,DT}}-u_{x}/a\right|\leq\epsilon\text{ with probability at least }1-\delta,\text{ for } \delta\in(0,1).\",Non-asymptotic concentration bound for the choice-decision-time estimator.
neurips_2024_oral_16,2,"lf. In Eq. 1, \(\forall j\in\mathcal{N}_{i},\mathbf{x}_{j}\sim p_{i}(\mu_{i},\sigma_{i}^{2})\). For node \(v_{i}\), subsequent to GNN message passing, we compute the mean and variance of the aggregated representation \(\mathbf{h}_{i}\) as follows:","\begin{split}\mathbb{E}[\mathbf{h}_{i}]&=\mathbb{E} [\sum_{j\in\mathcal{N}(i)}\alpha_{ij}\mathbf{x}_{j}\cdot\mathbf{W}^{ \prime}]=\sum_{j\in\mathcal{N}(i)}\alpha_{ij}\mathbb{E}[\mathbf{x}_{j}] \cdot\mathbf{W}^{\prime}=\sum_{j\in\mathcal{N}(i)}\alpha_{ij}\mu_{i}\cdot \mathbf{W}^{\prime},\\ \text{Var}(\mathbf{h}_{i})&=\text{Var}(\sum_{ j\in\mathcal{N}(i)}\alpha_{ij}\mathbf{x}_{j}\cdot\mathbf{W}^{\prime})= \sum_{j\in\mathcal{N}(i)}\alpha_{ij}\text{Var}(\mathbf{x}_{j})\cdot\mathbf{W} ^{\prime 2}=\sigma_{i}^{2}\sum_{j\in\mathcal{N}(i)}\alpha_{ij}^{2}\cdot \mathbf{W}^{\prime 2}.\end{split}",,"\mu_{i}=\frac{1}{|\mathcal{N}_{i}|}\sum_{j\in\mathcal{N}_{i}}\mathbf{x}_{j}, \quad \sigma_{i}^{2}=\frac{1}{|\mathcal{N}_{i}|}\sum_{j\in\mathcal{N}_{i}}(\mathbf{x}_{j}-\mu_{i})^{2},",where \(\mu_{i}\) is the mean of the aggregated representation for node \(v_{i}\) and \(\sigma_{i}^{2}\) is the variance of the aggregated representation for node \(v_{i}\).
neurips_2024_oral_18,4,"In this work, we regard all the Gaussians as isotropic kernels, which has been demonstrated as an effective way to simplify the model and better reconstruct the scene [6; 53]. We should note that

Figure 1: Overview. (a) **Continuum Generation:** Given a series of multi-view images capturing a moving object, the motion-factorized dynamic 3D Gaussian network is trained to reconstruct the dynamic object as 3D Gaussian point sets across different time states. From the reconstructed results, we employ a coarse-to-fine strategy to generate density fields to recover the continuums and extract object surfaces. The continuum is endowed with Gaussian attributes to allow mask rendering. (b) **Identification:** The MPM simulates the trajectory with the initial continuum \(\mathbb{P}(0)\) and the physical parameters \(\Theta\). The simulated object surfaces and the rendered masks are then compared against the previously extracted surfaces (colored in blue) and the corresponding masks from the dataset. The differences are quantified to guide the parameter estimation process. (c) **Simulation:** Digital twin demonstrations are displayed. Simulated objects (colored by stress increasing from blue to red), characterized by the properties estimated from observation, exhibit behavior consistent with real-world objects.

although previous works [29; 54] also perform motion decomposition modeling, our pipeline shows two major differences: 1) instead of modeling each basis with an independent neural network, our module shares a common backbone. Our key observation is that for reconstructing a dynamic object, all points on the object should follow a similar moving tendency, and the final heads of the neural network are sufficient to model the details of different parts of the object; 2) to increase the ability to fit high rank of the dynamic scene [16], we model the motion coefficients as time-variant variables rather than constant Gaussian attributes [29].

**Optimization**. We employ the same setting in [16] to train our pipeline. Concretely, the canonical 3D Gaussians are initialized with points randomly sampled from the given bounding box of the scene. We start training the deformation network after 3,000 iterations of warm-up for the 3D Gaussians. Similar to previous works [16; 29], we optimize the pipeline by computing the L1 norm and Structural Similarity Index Measure (SSIM) between the rendered image \(I\) and the ground truth image \(\tilde{I}\). Moreover, since large scales may lead to inaccurate reconstructed shapes [55], we thus perform L1 norm on the scale attributes of all the points to recover more fine-grand shapes of the object. Therefore, the overall loss function is defined as:","\mathcal{L}_{gs}=\mathcal{L}_{1}(I,\tilde{I})+\lambda_{1}\mathcal{L}_{ssim}(I,\tilde{I})+\lambda_{2}\mathcal{L}_{1}(s(t)),",where \(\lambda_{1}\) and \(\lambda_{2}\) are balancing hyperparameters,"L = \lambda_{1} \|I - \tilde{I}\|_{1} + \lambda_{2} \text{SSIM}(I, \tilde{I}) + \|s - s_{0}\|_{1},","where \(L\) is the overall loss function, \(I\) is the rendered image, \(\tilde{I}\) is the ground truth image, \(\lambda_{1}\) and \(\lambda_{2}\) are balancing hyperparameters, and \(s\) and \(s_{0}\) are the current and initial scale attributes, respectively."
neurips_2024_oral_2,1,"## 2 _Aligner_

Preliminary: Supervised Fine-Tuning (SFT)SFT aims to finetune the pretrained LLM to generate target answers using supervised learning -- specifically, maximum likelihood estimation -- on a curated high-quality dataset \(\mathcal{D}_{\text{SFT}}=\{\mathbf{x}^{(i)},\mathbf{y}^{(i)}\}_{i=1}^{N}\). The goal is to obtain a model \(\pi_{\mathbf{\theta}}^{\text{SFT}}\) with the following training objective:","\operatorname*{minimize}_{\mathbf{\theta}}\mathcal{L}(\mathbf{\theta};\mathcal{D}_{ \text{SFT}})=-\mathbb{E}_{(\mathbf{x},\mathbf{y})\sim\mathcal{D}_{\text{SFT}}}[\log \pi_{\mathbf{\theta}}(\mathbf{y}|\mathbf{x})].",,\mathcal{L}_{\text{SFT}} = -\frac{1}{N} \sum_{i=1}^{N} \log \pi_{\mathbf{\theta}}^{\text{SFT}}(\mathbf{y}^{(i)} | \mathbf{x}^{(i)}),"Loss function for Supervised Fine-Tuning (SFT), where \(\mathcal{L}_{\text{SFT}}\) is the average negative log-likelihood of the target answers given the inputs."
neurips_2024_oral_24,3,"The matrices \(B_{i}\in\mathbb{R}^{d\times r}\) and shared \(A\in\mathbb{R}^{r\times k}\). The hyper-parameter \(N\) denotes the number of \(B\) matrices. The term \(\omega_{i}\) modulates these contribution weights for head \(B_{i}\).

### Workflow of _HydraLoRA_

Figure 4 illustrates the workflow of _HydraLoRA_. Initially, _HydraLoRA_ delves into the adaptive identification and initialization of LoRA modules within a heterogeneous corpus, aligning them with

Figure 3: Breakdown analysis of LoRA modules. Compare fine-tuned LoRA modules of Dolly-15K [8] with three subtasks of Dolly-15K including â€œ_summarization (Sum)_â€, â€œ_closed QA (QA)_â€ and â€œ_information extraction (IE)_â€ using t-SNE. Consider LLaMA2-7B (random seed=42), which contains 32 decoder layers, corresponding to 32 adaptive modules. Each module consists of (**0**: q_proj of A, I: q_proj of B, **2**: v_proj of A, **3**: v_proj of B) submodules. This makes a total of \(32\times 4\) submodules. Left displays all submodules. Center shows all even submodules, i.e. the A matrix. Right represents all odd submodules, i.e. the B matrix. It can be seen that the differences in the fine-tuned LoRA modules for different tasks arise mainly from the B matrix.

task relevance through the application of \(k\)-means or developer-specified size. Subsequently, we propose a Mixture-of-Experts (MoE) framework that handles \(B\) matrices as expert adapters to ensure computational efficiency throughout the fine-tuning (Section 3.2.1) and inference (Section 3.2.2) stages by freezing the rest of the LLM parameters. During inference, it flexibly and dynamically merges multiple \(B\) matrices through the MoE router.

#### 3.2.1 Fine-tuning

Motivated by Mixture-of-Experts (MoEs; [20; 40]), where experts are selectively activated by a gating mechanism (Router) in response to different inputs. In _HydraLoRA_, we substitute each expert with a lightweight LoRA adapter. During fine-tuning, while weights of LLMs remain frozen, the experts and router layers are trained from scratch. In order to achieve a unified approach to the distinct forward processes of multiple \(B\) matrices, we define a set of experts, denoted as \((E_{1},\ldots,E_{N})\), to learn the updated matrix \(\Delta W\). As _HydraLoRA_ fine-tunes the experts using the heterogeneous corpus, the shared matrix \(A\) inherently captures collaborative knowledge to augment intra-gains, and different matrices \(B\) foster knowledge modularity to mitigate fine-tuning inter-offsets. Based on this structure, the forward process of _HydraLoRA_ is expressed as:",y=\mathit{W}_{0}x+\sum_{i=1}^{N}\omega_{i}E_{i}Ax\quad(MoE),"where \(N\) denotes the number of experts, i",\Delta W = \sum_{i=1}^{N} E_{i} \cdot B_{i} A,"where \(N\) denotes the number of experts, \(E_{i}\) represents the expert weights, \(B_{i}\) are the distinct matrices, and \(A\) is the shared matrix."
neurips_2024_oral_25,8,"_where \(\omega_{\mathbb{G}}=\mathrm{TSLS}(X,Y,\mathbb{G})\)._

This ""pseudo-residual"" is applied to measure the uncorrelated relationship with some genetic variants. Note that concepts to ""pseudo-residual"" have been developed to address different tasks (Dtron and Richardson, 2004; Chen et al., 2017; Cai et al., 2019; Xie et al., 2020), but our formalization is different from theirs in terms of the parameter \(\omega_{\mathbb{G}}\). To the best of our knowledge, it has not been realized that the uncorrelated property involving such pseudo-residuals reflects the validity of the IVs in the bi-directional MR causal model (see Propositions 2 \(\sim\) 3).

As analyzed by Chu et al. (2001), within the framework of linear models, a variable being a valid IV imposes no constraints on the joint marginal distribution of the observed variables. In other words, there is no available test to determine whether a variable is a valid IV without making further assumptions. Therefore, we introduce the following assumption.

**Assumption 1** (Valid IV Set).: _For a given causal relationship (if the relationship exists), there exists a valid IV set that consists of at least two valid IVs. For example, for the causal relationship \(X\to Y\), \(|\mathbf{G}_{\mathcal{V}}^{X\to Y}|\geq 2\)._

Figure 2: An illustrative example where valid and invalid IV sets induce distinct constraints, where \(\mathbf{G}_{\mathcal{V}}^{X\to Y}=(G_{1},G_{3})^{\intercal}\) is a valid IV set, while \(\mathbf{G}_{\mathcal{I}}^{X\to Y}=(G_{2},G_{4},G_{5})^{\intercal}\) is invalid due to pathways \(G_{2}\to Y\), \(G_{4}\to Y\) and \(G_{5}\to Y\).

Assumption 1 is the same as in (Silva and Shimizu, 2017; Cheng et al., 2023) if the model is a one-directional MR model. Notice that this assumption is much milder than the _Majority rule_ assumption (Kang et al., 2016; Bowden et al., 2016; Windmeijer et al., 2019; Hartford et al., 2021) and _Plurality rule_ assumption (Guo et al., 2018; Windmeijer et al., 2021): Assumption 1 does not rely on the total number of genetic variants, while _Majority rule_ and _Plurality rule_ assumptions do.

We now show that under Assumption 1, one can identify the class of invalid IV sets.

**Proposition 2** (Identifying Invalid IV Sets).: _Let \(\mathbb{G}=\{G_{1},\ldots,G_{p}\},p\geq 2\) be a subset of candidate genetic variants \(\mathbf{G}\). Suppose that Assumption 1 and the size of participants \(n\rightarrow\infty\) hold. If there exists a \(G_{j}\in\mathbb{G}\) such that,_","\mathrm{corr}(\mathcal{PR}_{\langle X,Y\,|\,\{G_{j}\}\rangle},G_{j })\neq 0,",,"G_{j}\text{ is a valid IV, then } \mathrm{corr}(Y-X\omega_{\mathbb{G}\setminus\{G_{j}\}},G_{j})=0.\end{equation}","where \(\omega_{\mathbb{G}\setminus\{G_{j}\}}=\mathrm{TSLS}(X,Y,\mathbb{G}\setminus\{G_{j}\})\)."
neurips_2024_oral_26,1,"## 3 Background: Diffusion and Distribution Matching Distillation

This section gives a brief overview of diffusion models and distribution matching distillation (DMD).

**Diffusion Models** generate images through iterative denoising. In the forward diffusion process, noise is progressively added to corrupt a sample \(x\sim p_{\text{real}}\) from the data distribution into pure Gaussian noise over a predetermined number of steps \(T\), so that, at each timestep \(t\), the diffused samples follow the distribution \(p_{\text{real},t}(x_{t})=\int p_{\text{real}}(x)q(x_{t}|x)dx\), with \(q_{t}(x_{t}|x)\sim\mathcal{N}(\alpha_{t}x,\sigma_{t}^{2}\mathbf{I})\), where \(\alpha_{t},\sigma_{t}>0\) are scalars determined by the noise schedule [47, 48]. The diffusion model learns to iteratively reverse the corruption process by predicting a denoised estimate \(\mu(x_{t},t)\), conditioned on the current noisy sample \(x_{t}\) and the timestep \(t\), ultimately leading to an image from the data distribution \(p_{\text{real}}\). After training, the denoised estimate relates to the gradient of the data likelihood function, or score function [48] of the diffused distribution:","s_{\text{real}}(x_{t},t)=\nabla_{x_{t}}\log p_{\text{real},t}(x_{t})=-\frac{x_ {t}-\alpha_{t}\mu_{\text{real}}(x_{t},t)}{\sigma_{t}^{2}}.",,"p_{\text{real},t}(x_{t})=\int p_{\text{real}}(x)q(x_{t}|x)dx",Distribution of the diffused samples at timestep \(t\).
neurips_2024_oral_26,4,"on. While gathering this data incurs negligible cost for small datasets like CIFAR-10, it becomes a significant bottleneck with large-scale text-to-image synthesis tasks, or models with complex conditioning [55, 56, 57]. For instance, generating one noise-image pair for SDXL [58] takes around 5 seconds, amounting to about 700 A100 days to cover the 12 million prompts in the LAION 6.0 dataset [59], as utilized by Yin et al. [22]. This dataset construction cost alone is already more than \(4\times\) our total training compute (asdetailed in Appendix J). This regularization objective is also at odds with DMD's goal of matching the student and teacher in _distribution_, since it encourages adherence to the teacher's sampling paths.

## 4 Improved Distribution Matching Distillation

We revisit multiple design choices in the DMD algorithm [22] and identify significant improvements.

### Removing the regression loss: true distribution matching and easier large-scale training

The regression loss [16] used in DMD [22] ensures mode coverage and training stability, but as we discussed in Section 3, it makes large-scale distillation cumbersome, and is at odds with the distribution matching idea, thus inherently limiting the performance of the distilled generator to that of the teacher model. Our first improvement is to remove this loss.

### Stabilizing pure distribution matching with a Two Time-scale Update Rule

Naively omitting the regression objective, shown in Eq. (3), from DMD leads to training instabilities and significantly degrades quality (Tab. 3). For example, we observed that the average brightness, along with other statistics, of generated samples fluctuates significantly, without converging to a stable point (See Appendix G). We attribute this instability to approximation errors in the fake diffusion model \(\mu_{\text{fake}}\), which does not track the fake score accurately, since it is dynamically optimized on the non-stationary output distribution of the generator. This causes approximation errors and biased generator gradients (as also discussed in [30]).

We address this using the two time-scale update rule inspired by Heusel et al. [60]. Specifically, we train \(\mu_{\text{fake}}\) and the generator \(G\) at different frequencies to ensure that \(\mu_{\text{fake}}\) accurately tracks the generator's output distribution. We find that using 5 fake score updates per generator update, without the regression loss, provides good stability and matches the quality of the original DMD on ImageNet (Tab. 3) while achieving much faster convergence. Further analysis are included in Appendix G.

### Surpassing the teacher model using a GAN loss and real data

Our model so far achieves comparable training stability and performance to DMD [22] without the need for costly dataset construction (Tab. 3). However, a performance gap remains between the distilled generator and the teacher diffusion model. We hypothesize this gap could be attributed to approximation errors in the real score function \(\mu_{\text{real}}\) used in DMD, which would propagate to the generator and lead to suboptimal results. Since DMD's distilled model is never trained with real data, it cannot recover from these errors.

We address this issue by incorporating an additional GAN objective into our pipeline, where the discriminator is trained to distinguish between _real_ images and images produced by our generator.

Figure 3: Our method distills a costly diffusion model (gray, right) into a one- or multi-step generator (red, left). Our training alternates between 2 steps: 1. optimizing the generator using the gradient of an implicit distribution matching objective (red arrow) and a GAN loss (green), and 2. training a score function (blue) to model the distribution of â€œfakeâ€ samples produced by the generator, as well as a GAN discriminator (green) to discriminate between fake samples and real images. The student generator can be a one-step or a multi-step model, as shown here, with an intermediate step input.

Trained using real data, the GAN classifier does not suffer from the teacher's limitation, potentially allowing our student generator to surpass it in sample quality. Our integration of a GAN classifier into DMD follows a minimalist design: we add a classification branch on top of the bottleneck of the fake diffusion denoiser (see Fig. 3). The classification branch and upstream encoder features in the UNet are trained by maximizing the standard non-saturing GAN objective:","\mathcal{L}_{\text{GAN}}=\mathbb{E}_{x\sim p_{\text{noise}},t\sim[0,T]}[\log D (F(x,t))]+\mathbb{E}_{z\sim p_{\text{noise}},t\sim[0,T]}[-\log(D(F(G_{\theta}(z ),t)))],","where \(D\) is the discriminator, and \(F\) is the forward diffusion process (i","\mathcal{L}_{\text{GAN}}=\mathbb{E}_{x\sim p_{\text{real}}}[\log D(x)] + \mathbb{E}_{z\sim \mathcal{N}(0,\mathbf{I})}[\log(1 - D(G_{\theta}(z)))],","where \(D\) is the discriminator, and \(F\) is the forward diffusion process."
neurips_2024_oral_34,11,"rm.

Policy Improvement. The estimated value is used to improve the diffusion model. For each \(\mathbf{x}_{t}\) in a trajectory \(\mathbf{x}_{0:T}\) sampled from \(\pi_{\phi}(\mathbf{x}_{0:T})\), the diffusion model is optimized to minimize the next-state value and the running costs.",\min_{\phi}\mathbb{E}_{\pi_{\phi}(\mathbf{x}_{t+1}|\mathbf{x}_{t})}[V_{ \psi}^{t+1}(\mathbf{x}_{t+1})+\tau\log\pi_{\phi}(\mathbf{x}_{t+1}|\mathbf{x} _{t})+\frac{\tau}{2s_{t}^{2}}||\mathbf{x}_{t}-\mathbf{x}_{t+1}||^{2}\bigg{|} \mathbf{x}_{t}].,,"\min_{\phi}\mathbb{E}_{\pi_{\phi}(\mathbf{x}_{0:T})}[V_{\psi}^{t}(\mathbf{x}_{t})+\tau\log\pi_{\phi}(\mathbf{x}_{t+1}|\mathbf{x}_{t})+\frac{\tau}{2s_{t}^{2}}||\mathbf{x}_{t+1}-\mathbf{x}_{t}||^{2}],",where \(V_{\psi}^{t}(\mathbf{x}_{t})\) is the value function and \(\tau\) is the temperature parameter.
neurips_2024_oral_34,12,"In practice, each iteration of policy evaluation and improvement involves a single gradient step.

Adaptive Velocity Regularization (AVR). We additionally propose a method for systematically determining the hyperparameter \(s_{t}\)'s of the auxiliary distribution \(\tilde{q}(\mathbf{x}_{0:T-1}|\mathbf{x}_{T})\). We can optimize \(s_{t}\) such that the inequality Eq. (6) is as tight as possible by solving \(\min_{s_{0},\dots,s_{T-1}}KL(\pi_{\phi}(\mathbf{x}_{0:T})||q_{\theta}(\mathbf{ x}_{T})\tilde{q}(\mathbf{x}_{0:T-1}|\mathbf{x}_{T}))\). After calculation (details in Appendix A), the optimal \(s_{t}^{*}\) can be obtained analytically: \((s_{t}^{*})^{2}=\mathbb{E}_{\mathbf{x}_{t},\mathbf{x}_{t+1}\sim\pi}[||\mathbf{ x}_{t}-\mathbf{x}_{t+1}||^{2}]/D\). In practice, we can use exponential moving average to compute the expectation \(\mathbb{E}_{\mathbf{x}_{t},\mathbf{x}_{t+1}\sim\pi}[||\mathbf{x}_{t}-\mathbf{ x}_{t+1}||^{2}]\) during training: \(s_{t}^{2}\leftarrow\alpha s_{t}^{2}+(1-\alpha)||\mathbf{x}_{t}-\mathbf{x}_{t+1} ||^{2}/D\) where we set \(\alpha=0.99\) for all experiment.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline Method & T & Pretrain & \(\tau\) & \(SW\) (\(\downarrow\)) & AUC (\(\uparrow\)) \\ \hline DDPM & 5 & - & - & 0.967\(\pm\)0.005 & - \\ DDPM & 10 & - & - & 0.824\(\pm\)0.002 & - \\ DDPM & 100 & - & - & 0.241\(\pm\)0.003 & - \\ DDPM & 1000 & - & - & 0.123\(\pm\)0.014 & - \\ \hline DxMI & 5 & \(\bigcirc\) & 0 & 0.074\(\pm\)0.018 & 0.707 \\ DxMI & 5 & \(\bigcirc\) & 0.01 & 0.074\(\pm\)0.017 & 0.751 \\ DxMI & 5 & \(\bigcirc\) & 0.1 & **0.068\(\pm\)**0.004** & **0.898** \\ DxMI & 5 & \(\bigcirc\) & 1 & 1.030\(\pm\)0.004 & 0.842 \\ DxMI & 5 & \(\times\) & 0.1 & 0.076\(\pm\)0.011 & 0.883 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Quantitative results for 8 Gaussians experiment. \(SW\) denotes the sliced Wasserstein distance between samples and data. AUC is computed for classification between data and uniform noise using the energy. The standard deviation is computed from 5 independent samplings. The ideal maximum value of AUC is about 0.906.

Figure 2: 2D density estimation on 8 Gaussians. Red shades indicate the energy (white is low), and the dots are generated samples.

### Techniques for Image Generation Experiments

When using DxDP for image generation, one of the most common applications of diffusion models, we introduce several design choices to DxDP to enhance performance and training stability. The resulting algorithm is summarized in Algorithm 2.

Time-Independent Value Function. In image generation experiments (Section 5.2), we let the value function be independent of time, i.e., \(V_{\psi}^{t}(\mathbf{x}_{t})=V_{\psi}(\mathbf{x}_{t})\). Removing the time dependence reduces the number of parameters to be trained. More importantly, a time-independent value function can learn better representation because the value function is exposed to diverse inputs, including both noisy and clean images. On the contrary, a time-dependent value function \(V_{\psi}^{t}(\mathbf{x}_{t})\) never observes samples having different noise levels than the noise level of \(\mathbf{x}_{t}\).

Time Cost. Also, in the value update (Eq. (10)) step of image generation experiments, we introduce _time cost_ function \(R(t)>0\), which replaces the running cost terms \(\tau\log\pi_{\phi}(\mathbf{x}_{t+1}|\mathbf{x}_{t})+\tau||\mathbf{x}_{t}- \mathbf{x}_{t+1}||^{2}/(2s_{t}^{2})\). The time cost \(R(t)\) only depends on time \(t\). The modified value update equation is given as follows:","\min_{\psi}\mathbb{E}_{\mathbf{x}_{t},\mathbf{x}_{t+1}\sim\pi}[(\text{s} \text{g}[V_{\psi}(\mathbf{x}_{t+1})]+R(t)-V_{\psi}(\mathbf{x}_{t}))^{2}].",,\min_{\phi}\mathbb{E}_{\pi_{\phi}(\mathbf{x}_{t+1}|\mathbf{x}_{t})}[V_{\psi}(\mathbf{x}_{t+1})+R(t)],where \(R(t)>0\) is a time cost function that replaces the running cost terms in the value update step for image generation experiments.
neurips_2024_oral_40,11,"Comparing this expression to Eq.2 reveals that the max-divergence is indeed a worst-case estimation of the KL divergence.

The KL divergence --also known as _relative entropy_-- between two distributions \(P\) and \(Q\) is always non-negative and equal to zero if and only if \(P=Q\). Moreover, in our setting, it is always finite due to the following remark.3

Footnote 3: We only need \(P\) to be absolutely continuous with respect to \(Q\); i.e., that for any event \(A\), we have \(P(A)=0\) whenever \(Q(A)=0\). We express our results in terms of identical supports for the sake of simplicity as they can be readily generalized to only require absolute continuity.

**Remark 1**.: In the execution Algorithm1, every distribution \(D_{\ell}\), for \(\ell\in[pR]\), has the same support. This must be the case since Line20 always preserves the support of \(D_{1}\).

On the other hand, the KL divergence is not a proper metric as it is not symmetric and it does not satisfy the triangle inequality, unlike the max-divergence. This introduces a number of difficulties in bounding the divergence between \(\mathcal{D}_{0}\) and \(\mathcal{D}_{r}\). Overcoming these challenges requires a deeper and highly novel analysis. Our results reveal that the KL divergence captures particularly well the behavior of our Boosting algorithm. We remark that we are not the first to relate KL divergence and Boosting, see e.g. Schapire and Freund (2012, Chapter 8 and the references therein), yet we make several new contributions to this connection.

To study the probability of obtaining a \(\gamma/2\)-approximation for \(D_{r}\) when sampling from \(D_{0}\), rather than using advanced composition, we employ the _duality formula for variational inference_(Donsker and Varadhan, 1975) --also known as _Gibbs variational principle_, or _Donsker-Varadhan formula_--to estimate such a probability in terms of \(\mathrm{KL}(D_{r}\parallel D_{0})\).

**Lemma 2.4** (Duality formula4).: _Given finite probability spaces \((\Omega,\mathcal{F},P)\) and \((\Omega,\mathcal{F},Q)\), if \(P\) and \(Q\) have the same support, then for any real-valued random variable \(\mathbf{X}\) on \((\Omega,\mathcal{F},P)\) we have that_

Footnote 4: Corollary of, e.g., Dembo and Zeitouni (1998, Lemma 6.2.13) or Lee (2022, Theorem 2.1). Presented here in a weaker form for the sake of simplicity.",\ln\mathbb{E}_{P}\big{[}e^{\mathbf{X}}\big{]}\geq\mathbb{E}_{Q}[\mathbf{X}]- \mathrm{KL}(Q\parallel P).,,\mathrm{KL}(D_{r}\parallel D_{0})\coloneqq\sum_{x\in\mathcal{X}}D_{r}(x)\ln \frac{D_{r}(x)}{D_{0}(x)}.,The Kullback-Leibler divergence between two distributions \(D_{r}\) and \(D_{0}\).
neurips_2024_oral_41,4,"Notice that, necessarily, \(M_{P}(x)\in\mathbb{R}^{d}_{\leq c}\) and \(\phi(M_{P}(x))\leq\phi(M_{P}(x))\leq\phi(x).\) Evaluating \(M_{P}\) amounts to solving an unconstrained optimization problem in a \(c\)-dimensional space.

\(\diamond\)**The V step (fixing \(V\)).** Similarly, given \(y\in\mathbb{R}^{d}_{c}\), we define the mapping","M_{V}(y)=M_{V,\phi}(y):=\arg\min_{x\in\mathbb{R}^{d}}\{\phi(x)\,:\,V(x)\subseteq V (y)\}.",,"V_{P}(y)=V_{P,\phi}(y):=\arg\min_{x\in\mathbb{R}^{d}}\{\phi(x)\,:\,V(x)=V(y)\}.",where \(V_{P}(y)\) is the mapping that minimizes the objective function \(\phi\) while keeping the vector \(V\) fixed.
neurips_2024_oral_48,3,"es. Observe that they are different for each layer, with the input and output neurons held fixed.

**Graph Metanetworks (GMNs).** A recently introduced weight space architecture [33, 44], takes advantage of the permutation symmetries and treats FFNNs (among others, e.g. CNNs) as graphs, processing them with conventional GNNs. In particular, let \(G=(\mathcal{V},\mathcal{E})\) be the computational graph, \(i\in\mathcal{V}\) an arbitrary vertex in the graph (neuron) and \((i,j)\in\mathcal{E}\) an arbitrary edge from vertex \(j\) to vertex \(i\).4 Additionally, let \(\mathbf{x}_{V}\in\mathbb{R}^{|\mathcal{V}|\times d_{\ell}}\) be the vertex features and \(\mathbf{x}_{E}\in\mathbb{R}^{|\mathcal{E}|\times d_{\ell}}\) the edge features (i.e. biases and weights resp. in a FFNN). The general form of a \(T\) iteration (layer) GMN reads:

Footnote 4: We use this convention to align with the indexing of the weights \(\mathbf{W}(i,j)\).","\mathbf{h}_{V}^{0}(i)=\mathrm{INIT}_{V}(\mathbf{x}_{V}(i )),\quad\mathbf{h}_{E}^{0}(i,j)=\mathrm{INIT}_{E}(\mathbf{x}_{ E}(i,j))",,"\mathbf{x}_{V}^{(t)}=\sigma\left(\mathbf{W}^{(t)}\mathbf{x}_{V}^{(t-1)}+\mathbf{b}^{(t)}\right),\quad \mathbf{x}_{E}^{(t)}=\mathbf{W}_{E}^{(t)}\mathbf{x}_{E}^{(t-1)}+\mathbf{b}_{E}^{(t)}","where \(t\): the iteration (layer) index, \(\mathbf{W}^{(t)}\): the weight matrix for vertex features at iteration \(t\), \(\mathbf{b}^{(t)}\): the bias vector for vertex features at iteration \(t\), \(\mathbf{W}_{E}^{(t)}\): the weight matrix for edge features at iteration \(t\), \(\mathbf{x}_{V}^{(t)}\): the vertex features at iteration \(t\), \(\mathbf{x}_{E}^{(t)}\): the edge features at iteration \(t\), \(\sigma\): activation function applied element-wise."
neurips_2024_oral_48,7,"4.1. Common examples are those discussed in Section 4, e.g. \(D_{i}=\{1,-1\}\)**or**\(D_{i}=\mathbb{R}^{+}\). The first case, i.e. _sign symmetries_, has been discussed in recent work [43, 40]. Here we generalise their architecture into arbitrary scaling groups. In specific, _Scale Equivariant_ networks follow the methodology of [40], i.e. they are compositions of multiple linear transformations multiplied elementwise with the output of _Scale Invariant_ functions:","\mathsf{ScaleInv}^{k}(\mathbf{X})=\rho^{k}(\tilde{\mathbf{x} }_{1},\ldots,\tilde{\mathbf{x}}_{n}),",,"g_{i}\big{(}q_{1}\mathbf{x}_{1},\ldots,q_{n}\mathbf{x}_{n}\big{)}=q_{i}g_{i} \mathbf{(x}_{1},\ldots,\mathbf{x}_{n}\big{)},\forall q_{i}\in D_{i},i\in\{1, \ldots,n\}",where \(D_{i}\) a 1-dimensional scaling group as defined in Proposition 4
neurips_2024_oral_48,10,"We call these functions _rescale equivariant_. Note, that this is an unusual symmetry in equivariant NN design. Our approach is based on the observation that _any n-other monomial containing variables from all vectors \(\mathbf{x}_{i}\) is rescale-equivariant_. Collecting all these monomials into a single representation is precisely the _outer product_\(\textbf{X}_{n}=\mathbf{x}_{1}\otimes\cdots\otimes\mathbf{x}_{n}\), where \(\textbf{X}_{n}(j_{1},\ldots,j_{n})=\prod_{i=1}^{n}\mathbf{x}_{i}(j_{i})\). Therefore, the general form of our proposed Rescale Equivariant Network is as follows:","\mathsf{ReScaleEq}(\mathbf{x}_{1},\ldots\mathbf{x}_{n})=\mathsf{ScaleEq} \big{(}\mathrm{vec}(\textbf{X}_{n})\big{)}.",,"\mathbf{h}^{\prime}_{V}(i) = g\big{(}q_{\ell}(i)\mathbf{h}_{V}(\pi_{\ell}(i)),\ldots,q_{\ell}(n)\mathbf{h}_{V}(\pi_{\ell}(n))\big{)},\quad\ell=\text{layer}(i) \in\{1,\ldots,L-1\}",where \(g\) is a rescale equivariant function that combines the vertex representations scaled by their respective multipliers \(q_{\ell}(i)\).
neurips_2024_oral_48,13,"Finally, to summarise our graph into a single scalar/vector we require a scale and permutation-invariant readout. The former is once more achieved using canonicalised/symmetrised versions of the vertex representations of hidden neurons, while the latter using a DeepSets architecture as usual:","\mathrm{READ}_{V}(\mathbf{X}):=\mathsf{DeepSets}(\tilde{ \mathbf{x}}_{1},\ldots,\tilde{\mathbf{x}}_{n}),\quad\tilde{\mathbf{x}}_{ i}=\mathsf{canon}_{i}(\mathbf{x}_{i})\text{ or }\tilde{\mathbf{x}}_{i}=\mathsf{symm}_{i}(\mathbf{x}_{i})",,\mathrm{READ}(\mathbf{h}_{V})=\mathsf{ScaleInv}(\mathsf{symm}(\mathbf{h}_{V})),"where \(\mathbf{h}_{V}\) are the vertex representations of hidden neurons, \(\mathsf{ScaleInv}\) is a scale invariant function, and \(\mathsf{symm}\) is a symmetrisation function."
neurips_2024_oral_58,2,"Training an autoregressive model \(p_{\theta}\) involves optimizing \(p_{\theta}(x_{t}\mid x_{1},x_{2},\dots,x_{t-1})\) over a dataset. This is known as the ""next-token prediction"", and the trained \(p_{\theta}\) can generate new sequences.

**Tokenization.** Images are inherently 2D continuous signals. To apply autoregressive modeling to images via next-token prediction, we must: 1) tokenize an image into several _discrete_ tokens, and 2) define a 1D _order_ of tokens for unidirectional modeling. For 1), a quantized autoencoder such as [30] is often used to convert the image feature map \(f\in\mathbb{R}^{h\times w\times C}\) to discrete tokens \(q\in[V]^{h\times w}\):","f=\mathcal{E}(im),\ \ \ \ \ q=\mathcal{Q}(f),","where \(im\) denotes the raw image, \(\mathcal{E}(\cdot)\) a encoder, and \(\mathcal{Q}(\cdot)\) a quantizer",q=\mathcal{Q}(\mathcal{E}(im)),"where \(im\) denotes the raw image, \(\mathcal{E}(\cdot)\) an encoder, and \(\mathcal{Q}(\cdot)\) a quantizer"
neurips_2024_oral_58,4,"\). To train the quantized autoencoder, \(Z\) is looked up by every \(q^{(i,j)}\) to get \(\hat{f}\), the approximation of original \(f\). Then a new image \(\hat{im}\) is reconstructed using the decoder \(\mathcal{D}(\cdot)\) given \(\hat{f}\), and a compound loss \(\mathcal{L}\) is minimized:","\hat{f} =\text{lookup}(Z,q),\ \ \ \ \ \ \ \ \hat{im}=\mathcal{D}(\hat{f}), || \mathcal{L} =\|im-\hat{im}\|_{2}+\|f-\hat{f}\|_{2}+\lambda_{\text{P}} \mathcal{L}_{\text{P}}(\hat{im})+\lambda_{\text{G}}\mathcal{L}_{\text{G}}( \hat{im}),",,"\hat{f}=\text{lookup}(Z,q),\ \ \ \ \ \hat{im}=\mathcal{D}(\hat{f}),\ \ \ \ \ \mathcal{L}=\|\hat{f}-f\|_{2}^{2}+\lambda\|\hat{im}-im\|_{2}^{2}.","where \(\hat{f}\) is the reconstructed feature map, and \(\mathcal{D}(\cdot)\) is the decoder function."
neurips_2024_oral_58,7,"Consequently, the main parameter count \(N\) of a VAR transformer with depth \(d\) is given by3:

Footnote 3: Due to resource limitation, we use a single shared adaptive layernorm (AdaLN) across all attention blocks in 512\(\times\)512 synthesis. In this case, the parameter count would be reduced to around \(12dw^{2}+6w^{2}\approx 49152\,d^{3}\).","N(d)=\underbrace{d\cdot 4w^{2}}_{\text{self-attention}}+\underbrace{d\cdot 8w^{2}} _{\text{feed-forward}}+\underbrace{d\cdot 6w^{2}}_{\text{adaptive layernorm}}=18\,dw^{2}=73728 \,d^{3}.",,"N \approx 12dw^{2}+6w^{2} \approx 49152\,d^{3}.",The main parameter count \(N\) of a VAR transformer with depth \(d\) is given by
neurips_2024_oral_59,3,"Here, \(g^{(t)},h\) are functions on the domain of multisets and \(f^{(t)}\) is a function on the domain of tuples. For each \(t\), the colorings \(c^{(t)}\) are graph invariants. When the subsets of nodes with the same colors cannot be further split into different color groups, the algorithm terminates; the stable coloring after convergence is denoted by \(c(G)\).

Choosing injective functions for all \(f^{(t)}\) and setting \(g^{(t)}\) and \(h\) as the identity function results in 1-WL (Weisfeiler et al., 1968). If \(f^{(t)},g^{(t)},h\) are chosen as suitable neural networks, one obtains a Message Passing Neural Network (MPNN). Xu et al. (2019) proved that MPNNs are as powerful as \(1\)-WL if the functions \(f^{(t)},g^{(t)}\), and \(h\) are injective on their respective domains. The \(k\)-WL algorithms uplift the expressive power of \(1\)-WL by considering interactions between \(k\)-tuples of nodes. This results in a hierarchy of strictly more powerful graph invariants (see Appendix B.1 for a formal definition).

### Homomorphism and Subgraph Counting Expressivity

A more nuanced graph invariant can be built by considering the occurrences of a motif \(F\).

**Definition 4**.: _Let \(F\in\mathcal{G}\). A graph invariant \(\zeta\) can homomorphism-count \(F\) if for all pairs \(G,H\in\mathcal{G}\)\(\zeta(G)=\zeta(H)\) implies \(\hom(F,G)=\hom(F,H)\). By analogy, \(\zeta\) can subgraph-count \(F\) if for all pairs \(G,H\in\mathcal{G}\), \(\zeta(G)=\zeta(H)\) implies \(\sub(F,G)=\sub(F,H)\)._

If \(\mathcal{F}\) is a family of graphs, we say that \(\zeta\) can homomorphism-count \(\mathcal{F}\) if \(\zeta\) can homomorphism-count every \(F\in\mathcal{F}\); we denote the vector of homomorphism-count by \(\hom(\mathcal{F},G)\coloneqq(\hom(F,G))_{F\in\mathcal{F}}\). Interpreting \(\hom(\mathcal{F},\cdot)\) as a graph invariant, given by \(G\mapsto\hom(\mathcal{F},G)\), another graph invariant \(\zeta\) can homomorphism-count \(\mathcal{F}\) if and only if \(\zeta\sqsubseteq\hom(\mathcal{F},\cdot)\).

The ability of a graph invariant to count homomorphisms is highly relevant because \(\hom(\mathcal{G},\cdot)\) is a complete graph invariant. Conversely, if \(\zeta\) is a complete graph invariant, then \(\zeta\) can homomorphism-count all graphs (Lovasz, 1967). Additionally, homomorphism-counting serves as a quantitative expressivity measure to compare different WL variants and GNNs, such as \(k\)-WL, Subgraph GNNs, and other methods (Lanzinger et al., 2024; B. Zhang et al., 2024), and allows for relating them to our proposed \(r\)-\(\ell\)WL variant, as detailed in Corollary 2.

## 4 Loopy Weisfeiler-Leman Algorithm

In this section, we introduce a new graph invariant by enhancing the direct neighborhood of nodes with _simple paths_ between neighbors.

**Definition 5**.: _Let \(G\in\mathcal{G}\). A simple path of length \(r\) is a collection \(\mathbf{p}=\{p_{i}\}_{i=1}^{r+1}\) of \(r+1\) nodes such that \(\{p_{i},p_{i+1}\}\in E(G)\) and \(i\neq j\implies p_{i}\neq p_{j}\) for every \(i,j\in\{1,\ldots,r\}\),._

Simple paths are the building blocks of \(r\)-neighborhoods, which in turn are the backbone of our \(r\)-\(\ell\)WL algorithm. The following definition is inspired by (Cantwell et al., 2019; Kirkley et al., 2021).

**Definition 6**.: _Let \(G\in\mathcal{G}\) and \(r\in\mathbb{N}\setminus\{0\}\), we define the \(r\)-neighborhood \(\mathcal{N}_{r}(v)\) of \(v\in V(G)\) as_","\mathcal{N}_{r}(v)\coloneqq\{\mathbf{p}\mid\mathbf{p}\text{ simple path of length }r,\,p_{1},p_{r+1}\in\mathcal{N}(v),v\notin\mathbf{p}\}\,.",,\mathcal{N}_{r}(v) \coloneqq \{u \in V(G) \mid \text{there exists a simple path of length } r \text{ from } v \text{ to } u\}.,The \(r\)-neighborhood of a node \(v\) in graph \(G\) is defined as the set of nodes \(u\) that can be reached from \(v\) via a simple path of length \(r\).
neurips_2024_oral_60,3,"le. Then, cross-decoder layers are stacked after self-decoder to obtain the final output \(X^{L}\). The KV caches \(\hat{K},\hat{V}\) are reused by all the \(\frac{L}{2}\) cross-decoder modules:","Q^{l} =\mathrm{LN}(X^{l})W_{Q}^{l} || Y^{l} =\mathrm{Attention}(Q^{l},\hat{K},\hat{V})+X^{l} || X^{l+1} =\mathrm{SwiGLU}(\mathrm{LN}(Y^{l}))+Y^{l}",,"\hat{K}_{i}=\hat{K},\quad\hat{V}_{i}=\hat{V}, \quad \text{for } i=1,\ldots,\frac{L}{2}",where \(\hat{K}_{i}\) and \(\hat{V}_{i}\) are the KV caches reused by each of the \(\frac{L}{2}\) cross-decoder modules.
neurips_2024_oral_60,6,"6]. The data-controlled decay is head-wise [19] rather than element-wise so that the computation can fully utilize NVIDIA tensor cores. Refer to [35] for more details about the other designs.

The Recurrent RepresentationBeing equivalent to Equation (5), the output of gated retention can be computed recurrently. For the \(n\)-th timestep, the output is obtained via:","\begin{split} S_{n}=\gamma_{n}S_{n-1}+K_{n}^{\intercal}V_{n}\\ g\mathrm{Ret}(X_{n})=Q_{n}S_{n},\quad n=1,\cdots,|x|\end{split}","where \(Q,K,V,\gamma\) are the same as in Equation (5)",X^{n} = \mathrm{gRet}(X^{n-1}) + X^{n-1},"where \(X^{n}\) is the output at the \(n\)-th timestep, and \(X^{n-1}\) is the input from the previous timestep."
neurips_2024_oral_8,3,"Subsequently, the MFP module calculates a binary mask matrix \(\mathbf{M}_{i}^{t}\in\{0,1\}^{|\mathbf{w}_{i}^{t}|}\) for pruning the fused local model \(\mathbf{w}_{i}^{t}\) (in line 3). The matrix \(\mathbf{M}_{i}^{t}\) is derived through the channel-wise \(\ell_{1}\) norm, which has proven to be effective and efficient in assessing the importance of parameters [47; 48]. The elements in \(\mathbf{M}_{i}^{t}\) with a value of ""0"" indicate the corresponding parameters that need to be pruned, while those with a value of ""1"" indicate the parameters that will be retained. The pruning ratio \(\rho_{i}\) determines the proportion of ""0"" in \(\mathbf{M}_{i}^{t}\).3 Finally, the MFP module prunes the local model \(\mathbf{w}_{i}^{t}\) with the binary mask matrix \(\mathbf{M}_{i}^{t}\) (in line 4). The pruned model can be represented as \(\mathbf{w}_{i}^{t}\odot\mathbf{M}_{i}^{t}\). It is noteworthy that, the pruning strategy used in our DapperFL is structural pruning strategy (channel pruning), which is a hardware-friendly approach that can be easily implemented with popular machine learning libraries such as PyTorch, making it particularly suitable for deployment on edge devices.

Footnote 3: Following the conventional setting in heterogeneous FL [21; 11; 5; 12], we make the fundamental assumption that the system capabilities of the devices are available to the server and the pruning ratios for all devices are appropriately determined according to the system information.

### Domain Adaptive Regularization

In FL, each client \(i\in\mathcal{C}\) possess private local data \(\mathcal{D}_{i}=\{x_{i},y_{i}\}^{N_{i}}\), where \(x\in\mathcal{X}\) denotes the input, \(y\in\mathcal{Y}\) denotes the corresponding label, and \(N_{i}\) represents the local data sample size. The data distribution \(p_{i}(x,y)\) of client \(i\) typically varies from that of other clients, i.e., \(p_{i}(x)\neq p_{j}(x),p_{i}(x|y)\neq p_{j}(x|y)\), leading to the domain shifts problem. Due to the existence of domain shifts, representation \(z_{i}\) generated by the local encoder varies among different clients, resulting in degraded prediction results of the local predictor. To address the domain shifts problem, we design a DAR module to enhance the performance of DapperFL across multiple domains while maintaining compatibility with the MFP module.

Specifically, the DAR module introduces a regularization term to the local objective to alleviate the bias of representations \(z_{i}\) on different clients adaptively. To achieve this goal, we first segment each pruned local model \(\mathbf{w}\odot\mathbf{M}\) into two parts, i.e., an encoder \(\mathbf{w}_{e}\odot\mathbf{M}_{e}\) and a predictor \(\mathbf{w}_{p}\odot\mathbf{M}_{p}\), where \(\mathbf{w}=\{\mathbf{w}_{e},\mathbf{w}_{p}\}\) and \(\mathbf{M}=\{\mathbf{M}_{e},\mathbf{M}_{p}\}\).4 The encoder is responsible for learning a representation \(z_{i}\) given an input \(x_{i}\), denoted as \(z_{i}=g_{e}(\mathbf{w}_{e}\odot\mathbf{M}_{e};x_{i})\). While the predictor is responsible for predicting label \(\hat{y}_{i}\) given the representation \(z_{i}\), denoted as \(\hat{y}_{i}=g_{p}(\mathbf{w}_{p}\odot\mathbf{M}_{p};z_{i})\). Subsequently, we construct a regularization term \(\mathcal{L}^{DAR}\) on the local objective as follows:

Footnote 4: We omit the client index \(i\) and the communication round index \(t\) for notation simplicity. In this work, all layers except the final linear layer act as the encoder, while the last linear layer of the model acts as the predictor.",\mathcal{L}^{DAR}_{i}=||g_{e}(\mathbf{w}_{e}\odot\mathbf{M}_{e};x_{i})||_{2}^{2}.,,"\mathcal{L}^{DAR}=\frac{1}{N_{i}}\sum_{j=1}^{N_{i}}\mathcal{L}(y_{i}^{(j)},\hat{y}_{i}^{(j)})+\lambda\|\mathbf{w}_{e}\odot\mathbf{M}_{e}\|_{2}^{2},","where \(\mathcal{L}(y_{i}^{(j)},\hat{y}_{i}^{(j)})\) is the loss function for the prediction, \(\lambda\) is the regularization coefficient, and \(\|\mathbf{w}_{e}\odot\mathbf{M}_{e}\|_{2}^{2}\) represents the \(\ell_{2}\) norm of the pruned encoder weights."
neurips_2024_oral_9,2,"These examples are regarded as ""logic heuristics"" that inspire the model to decompose questions in a manner closely aligned with human reasoning.

After obtaining \(lh_{Q}\), we utilize them to decompose the sub-question \(q_{t}\) at level \(t\) into multiple sub-questions at level \(t+1\). Specifically, given question \(q_{t}\), if its coherence score \(s_{t}\) (Eq. (5)) is higher than a threshold \(\epsilon_{1}\), We ask the LLM whether it needs to be further decomposed. If \(q_{t}\) requires decomposition, we then prompt the LLM to autonomously break it down into several sub-questions \(\{q_{t+1}^{j},j=1,...,J\}\). It is worth noting that in our decomposition approach, we do not pre-specify the number \(J\) of sub-questions; instead, we allow LLMs to adapt tively determine it based on the logic of each question. However, the number of sub-questions is capped at a predefined maximum branch limit to ensure computational efficiency and manageability 5.1.2. This enhances adaptability and more closely aligns with human logical characteristics when compared to existing methods like ToT [49] and GoT [3], etc. To facilitate this process, we design a heuristic-enhanced prompt that consists of a prompt head \(h_{1}\) and ""logic heuristics"" \(lh_{Q}\). The prompt head describes the question decomposition task in natural language. This process is formulated in Eq. (2). Additionally, we validate the effectiveness of using logic heuristics, and provide detailed explanations and templates in _Appendix_A.1.","\{q_{t+1}^{j},j=1,...,J\}\gets Decompose(p_{\theta},\;h_{1},\;lh_{Q},\;q_ {t}).",,"q_{t+1}^{j} = \text{LLM}(q_{t}, s_{t}, \epsilon_{1}, lh_{Q}) \quad (j=1,...,J).",Sub-questions generated by the LLM based on the coherence score and logic heuristics.
neurips_2024_oral_9,7,"(4.3) and _Decompose_ (4.1).

### _Rethink_ Stage

According to self-reflection theories [11; 13; 6] in cognitive science, humans constantly update and reflect on their previous reasoning results based on the current information. This allows us to correct past mistakes and ultimately achieve a consistent and stable answer. For example in Figure 2, a person might initially answer question \(Q\) (""Janet's ducks... How much... market?"") with the rationale \(r_{0}\) ""She makes \(9\times 3=\$27\) per day"". However,after considering responses to sub-questions \(q_{1}^{1}\) (""Whatis the selling price of one egg?"") and \(q_{1}^{2}\) (""How many eggs does Janet have per day?""), he/she realizes an error in \(r_{0}\). The correct calculation, using the values ""\(2\)"" for the price per egg and ""\(9\)"" for the daily number of eggs, should be ""\(2\times 9=\$18\)"".

Nevertheless, existing methods like ToT [48] search reasoning paths based solely on preceding steps, lacking the ability to retrospectively update earlier content based on the influence of later steps. To address this, we introduce a _Rethink_ stage that mirrors the human reflective process.

Specifically, during the rethinking process, humans first identify which existing reasoning steps may require revision. We aim to automate this by using LLMs to detect logical connections between ancestral and newly generated nodes, updating ancestral nodes based on insights from the rationales of new nodes. In our proposed ""Reasoning Tree"", we essentially use information from lower-level nodes to ""rethink"" higher-level nodes, closely mirroring the human cognitive simplification process in problem-solving [33].

To achieve this, after obtaining node \(n_{t+1}^{j}\) in _Analyze_ Stage, we first check its coherence score \(s_{t+1}^{j}\) (Eq. (5)). If \(s_{t+1}^{j}\) exceeds the threshold \(\epsilon_{2}\), we then examine the correlation between \(q_{t+1}^{j}\) and all sub-questions above level \(t\), specifically, \(\{q_{l},\ l\leq t\}\). Next, we extract a subset of \(k\) most related nodes \(L_{k}\) from \(L\triangleq\{n_{l},\ l\leq t\}\) (the specific nodes to be extracted are determined by the LLM):","L_{k}\gets Extract(p_{\theta},\ h_{5},\ L,q_{t+1}^{j}),\ L_{k}\subseteq L.",where \(h_{5}\) is a prompt head (_Appendix_ A,"L_{k}=\{n_{l},\;l\leq t\;|\;Correlate(q_{t+1}^{j},\;q_{l})\;>\;0\}.",where \(L_{k}\) is the subset of the most related nodes extracted based on their correlation with the sub-question \(q_{t+1}^{j}\).
