paper_id,equation_id,ground_truth_eq,ground_truth_description,generated_equation,generated_description
2024.acl-short.12,1,ECE=\sum_{i=1}^{B}\frac{n_{b_{i}}}{N}|acc(b_{i})-conf(b_{i})|,"where \(i\) denotes \(i\)-th bin, \(N\) is the total instance count of the dataset, \(acc(b_{i})\) and \(conf(b_{i})\) represent the accuracy and confidence of the bin \(b_{i}\), and \(n_{b_{i}}\) is the instance number of the bin \(b_{i}\)",ECE=\sum_{i=1}^{N}\frac{n_{b_{i}}}{N}\left| acc(b_{i})-conf(b_{i})\right|,"where \(i\) denotes \(i\)-th bin, \(N\) is the total instance count of the dataset, \(acc(b_{i})\) and \(conf(b_{i})\) represent the accuracy and confidence of the bin \(b_{i}\), and \(n_{b_{i}}\) is the instance number of the bin \(b_{i}\)"
2024.acl-short.12,2,"GECE=\frac{|M(pred,ref)-\frac{1}{n}\sum_{i=1}^{n}p(t_{i})|}{\alpha\cdot[E(\bigtriangledown_{ins})\cdot\bigtriangledown_{ins}]}","where \(pred\) and \(ref\) represent the generated text and the referenced ground truth, respectively","GECE=\frac{1}{B}\sum_{i=1}^{B}\left(METEOR(pred_i, ref_i)+avg\_prob(pred_i)+avg\_freq(pred_i)+dot\_prod(\nabla D,\nabla pred_i)\right)","where \(pred\) and \(ref\) represent the generated text and the referenced ground truth, respectively"
2024.acl-short.14,1,"P(cot,T|KG) || =P((s_{1},t_{1}),\cdots,(s_{n},t_{n}),T|KG) || =\prod_{i=1}^{n}P((s_{i},t_{i})|(s_{1},t_{1}),\cdots,(s_{i-1},t_ {i-1}),KG)\cdot || P(T|(s_{1},t_{1}),\cdots,(s_{n},t_{n}),KG) || =\prod_{i=1}^{n}P(t_{i}|(s_{1},t_{1}),\cdots,(s_{i-1},t_{i-1}),KG)\cdot || \prod_{i=1}^{n}P(s_{i})|t_{i},(s_{1},t_{1}),\cdots,(s_{i-1},t_{i-1}),KG)\cdot || P(T|(s_{1},t_{1}),\cdots,(s_{n},t_{n}),KG)",,"y_t=f(x_t, h_{t-1})","y_t: the generated word at time t, x_t: the input at time t, h_{t-1}: the hidden state from the previous time step"
2024.acl-short.14,2,"L_{seq}=-logP((s_{1},t_{1}),\cdots,(s_{n},t_{n}),T|KG) || =-\sum_{i=1}^{n}logP(t_{i}|(s_{1},t_{1}),\cdots,(s_{i-1},t_{i-1}) || KG)-\sum_{i=1}^{n}logP(s_{i})|t_{i},(s_{1},t_{1 }),\cdots,(s_{i-1} || t_{i-1}),KG)-\sum_{i=1}^{n}logP(T|(s_{1},t_{1 }),\cdots,(s_{n} || t_{n}),KG)",,"L=-\frac{1}{N}\sum_{i=1}^{N}\log P(s_{i}|t_{i},KG)","where \(L\) is the cross-entropy loss, \(N\) is the number of sentences, \(s_{i}\) is the \(i\)-th generated sentence, \(t_{i}\) is the corresponding triplet subset, and \(\mathbf{KG}\) is the knowledge graph."
2024.acl-short.14,3,"L_{re}=-\textbf{log}P(r|h,t,T) || =-\textbf{log}\prod_{i=1}^{m}P(r_{i}|r_{<i},h,t,T)",,"L_{rel}=-\sum_{j=1}^{m}logP(r_{j}|h_{j},t_{j},T)\","The backward relation extraction loss function, where \(L_{rel}\) is the loss, \(m\) is the number of sampled triples, \(r_{j}\) is the relation for the \(j\)-th triple, \(h_{j}\) is the head entity, \(t_{j}\) is the tail entity, and \(T\) is the generated text."
2024.acl-short.14,4,L=\alpha_{1}L_{seq}+\alpha_{2}L_{re},where \(\alpha_{1}\) and \(\alpha_{2}\) are parameters to tune,L=\alpha_{1} L_{seq}+\alpha_{2} L_{re},"where \(L\) is the joint loss, \(L_{seq}\) is the sequential sentence-level generation loss, \(L_{re}\) is the backward relation extraction loss, and \(\alpha_{1}\) and \(\alpha_{2}\) are parameters to tune."
2024.acl-short.15,1,"(o^{I},o^{S})=f(x)",where \(f(\cdot)\) is the joint model; \(\mathbf{o}^{I}\) and \(\mathbf{o}^{S}\) denotes an intent label and a slot sequence,"f(x)=(o^{I},o^{S})",where \(f(\cdot)\) is the joint model; \(\mathbf{o}^{I}\) and \(\mathbf{o}^{S}\) denotes an intent label and a slot sequence
2024.acl-short.15,2,"\hat{Q}_{[i,j]}=norm(Q_{[i,j]})","where \(\mathrm{norm}(\cdot)\) denotes row normalization, which constrains the values to lie between 0 and 1",\hat{Q}=norm(Q),"where \(\mathrm{norm}(\cdot)\) denotes row normalization, which constrains the values to lie between 0 and 1"
2024.acl-short.15,3,"L_{RLA}=-\sum_{[i,j]}\hat{Q}_{[i,j]}\log(\sigma(1-C_{[i,j]}))","where \(\sigma\) denotes the sigmoid function, and \(1-\mathbf{C}_{[i,j]}\) denotes the cosine similarity between \(\mathbf{h}_{i}\) and \(\mathbf{h}^{\prime}_{j}\)","L_{align}=\sum_{[i,j]}\sigma(\hat{Q}_{[i,j]})(1-C_{[i,j]})","where \(\sigma\) denotes the sigmoid function, and \(1-\mathbf{C}_{[i,j]}\) denotes the cosine similarity between \(\mathbf{h}_{i}\) and \(\mathbf{h}^{\prime}_{j}\)"
2024.acl-short.15,4,P^{I}_{x}=softmax(W^{I}h_{CLS}+b^{I}) || P^{I}_{x^{\prime}}=softmax(W^{I}h^{\prime}_{CLS}+b^{I}),,"o^{I}=decoder^{I}(h_{CLS},h^{\prime}_{CLS})","where \(\mathbf{o}^{I}\) is the predicted intent label, and \(\text{decoder}^{I}\) is the classification layer for intent detection."
2024.acl-short.15,5,P^{S}_{x}=softmax(W^{S}h_{[1:-1]}+b^{ S}) || P^{S}_{x^{\prime}}=softmax(W^{S}h^{\prime}_{[1:-1]}+b^{S}),,P^{S}_{x}=softmax(W^{S}h_{[1:-1]}+b^{S}),"where \(\mathbf{P}^{S}_{\mathbf{x}}\) is the predicted slot sequence for the input sentence \(\mathbf{x}\), \(\mathbf{W}^{S}\) is the weight matrix for the slot filling task, and \(\mathbf{b}^{S}\) is the bias term."
2024.acl-short.15,6,"L_{I}=CE(P^{I}_{x},P^{I}) || L_{S}=\frac{1}{L}\sum_{i=1}^{L}CE(P^{S}_{[x,i]},P^{S}_{i})",,"L_{I}=-\sum_{k}y_{k}\log(P^{I}_{x}[k]),\quadL_{S}=-\sum_{l}y_{l}\log(P^{S}_{x}[l])","where \(y_{k}\) and \(y_{l}\) are the ground truth labels for intent and slot filling tasks, respectively."
2024.acl-short.15,7,L_{PLA}=\underbrace{KL(P^{I}_{x^{\prime}} || P^{I}_{x})}_{Intent PLA}+\underbrace{KL(P^{S}_{x^{\prime}} || P^{S}_{x}) }_{Slot PLA},"where \(\mathrm{KL}(\cdot)\) denotes Kullback-Leibler divergence (Kullback and Leibler, 1951) to measure the difference between two distributions",L_{PLA}=KL(P^{I}_{x}\|P^{I}_{x^{\prime}})+KL(P^{S}_{x}\|P^{S}_{x^{\prime}}),"where \(\mathrm{KL}(\cdot)\) denotes Kullback-Leibler divergence (Kullback and Leibler, 1951) to measure the difference between two distributions"
2024.acl-short.15,8,"L_{SD}=\underbrace{KL(P^{[I,t-1]}_{ kT} || P^{[I,t]}_{latent SD})}_{Intent SD}+\underbrace{\frac{1}{L}\sum_{i=1}^{L}KL(P^{[S,t-1]}_{[x,i]} || P^{[S,t]}_{[x,i]}) }_{Slot SD}","where \(\mathbf{P}^{[I,t]}_{\mathbf{x}}\) denotes the probability distribution of intent, \(\mathbf{P}^{[S,t]}_{[\mathbf{x},i]}\) of slot at \(i\)-th token","L_{SD}=KL(P^{[I,t]}_{x} || P^{[I,t-1]}_{x})+\frac{1}{L}\sum_{i=1}^{L}KL(P^{[S,t]}_{[x,i]} || P^{[S,t-1]}_{[x,i]})","where \(\mathbf{P}^{[I,t]}_{\mathbf{x}}\) denotes the probability distribution of intent, \(\mathbf{P}^{[S,t]}_{[\mathbf{x},i]}\) of slot at \(i\)-th token"
2024.acl-short.15,9,L=L_{I}+L_{S}+L_{RLA}+L _{PLA}+L_{SD},,L_{total}=L_{RLA}+L_{PLA}+L_{SD}+L_{I}+L_{S},"where \(\mathcal{L}_{\text{total}}\) represents the overall loss function combining representation-level alignment, prediction-level alignment, self-distillation, intent loss, and slot filling loss."
2024.acl-short.16,1,Y=W_{0}^{l}X+\Lambda_{b}^{l}B^{l}\Lambda_{d}^{l}A^{l}X,,Y=lora_{B}\cdot (s_{b}\odot (lora_{A}\cdot (s_{d}\odot X))),"The output \(Y\) is computed by applying the up-projection layer \(lora_{B}\) to the result of the down-projection layer \(lora_{A}\), which processes the input \(X\) after being transformed by the feature vectors \(s_{d}\) and \(s_{b}\)."
2024.acl-short.16,2,"I_{A^{l}}=|\nablaL(\theta)|,\overline{T}^{(t)}_{A^{l }}=\beta_{1}\overline{T}^{(t-1)}_{A^{l}}+(1-\beta_{1})I^{(t)}_{A^{l}} || U^{(t)}_{A^{l}}=|I^{(t)}_{A^{l}}-\overline{I}^{(t)}_{A^{l}}|,\overline{U}^{(t)}_{A^{l}}=\beta_{2}\overline{U}^{(t-1)}_{A^{l}}+(1-\beta_{2} )U^{(t)}_{A^{l}} || s^{(t)}_{A^{l}}=mean(\overline{T}^{(t)}_{A^{l}}\circ\overline{U}^{(t)}_{A^{l}})",,FS(A^{l})=\frac{1}{n}\sum_{i=1}^{n} |\nabla A_{i}^{l}|,FS(A^{l})
2024.acl-short.16,3,r(t)=\{{ll}0&0\leq t<t_{i}\\1-(1-\frac{t-t_{i}}{T-t_{i}-t_{f}})^{3}&t_{i}\leq t<T-t_{f}\\1&otherwise,"where \(t\) refers to current #step, \(T\) is the total number of fine-tuning steps",r(t)=\left(1-\left(\frac{t-t_{i}}{T-t_{i}-t_{f}}\right)^{3}\right)_{+},"where \(t\) refers to current step, \(T\) is the total number of fine-tuning steps"
2024.acl-short.20,1,"E(h,r,t)=\sum_{i=1}^{k}w_{i}(q)M_{i}(h,r,t)","where \(\mathtt{E}(\mathtt{h},\mathbf{r},\mathbf{t})\) is the ensemble score for \(\mathbf{t}\) given query \(\mathtt{q}=(\mathbf{h},\mathbf{r},?)\)","E(h,r,t)=\sum_{i=1}^{k}\alpha_i\cdotM_{i}(h,r,t)",where \(\alpha_i\) represents the weight assigned to the \(i\)-th model \(\mathtt{M}_{i}\).
2024.acl-short.20,2,"M_{i}(h,r,t)\getsM_ {i}(h,r,t)-\min_{t^{\prime}\inE}M_{i}(h,r,t^{\prime}) || M_{i}(h,r,t)arrow\frac{M_{i}(h,r,t)}{\max_{t^{\prime}\inE}M_{i}(h,r,t^{\prime})}",,"M_{i}^{norm}(h,r,t)=\frac{M_{i}(h,r,t)-\min_{t'\inE}M_{i}(h,r,t')}{\max_{t'\inE}M_{i}(h,r,t')-\min_{t'\inE}M_{i}(h,r,t')}","where \(\mathtt{M}_{\mathtt{i}}^{\text{norm}}(\mathbf{h},\mathbf{r},\mathbf{t})\) is the normalized score for model \(\mathtt{M}_{\mathtt{i}}\)"
2024.acl-short.20,3,"f(M_{i},q)=\underset{t^{\prime}\inE}{mean}(M_{i}(h,r,t^{\prime})) || \underset{t^{\prime}\inE}{ var}(M_{i}(h,r,t^{\prime}))",,"f_{i}(h,r)=\left[\min_{t^{\prime}\inE}M_{i}(h,r,t^{\prime}),\max_{t^{\prime}\inE}M_{i}(h,r,t^{\prime}),mean_{t^{\prime}\inE}M_{i}(h,r,t^{\prime})\right]","where \(\mathbf{f}_{\mathtt{i}}(\mathbf{h},\mathbf{r})\) represents the feature vector extracted from the score distribution of model \(\mathtt{M}_{\mathtt{i}}\) for the query \(\mathbf{q}=(\mathbf{h},\mathbf{r},?)\)"
2024.acl-short.20,4,"w_{1}(q)=MLP_{1}(f(M_{1},q) || f(M_{2},q) ||  || f(M_ {k},q))",,"f=f(M_{1},q)\\f(M_{2},q)\\\vdots\\f(M_{k},q)",where \(\mathbf{f}\) is the concatenated feature vector for all models \(\mathtt{M}_{\mathtt{i}}\) used to learn the query-dependent weights \(\mathtt{w}_{\mathtt{i}}\).
2024.acl-short.20,5,"L=\sum_{t\inN}\max(E(h,r,t)-E(h,r,t^{*})+m,0)",where \(\mathtt{m}\) is the margin hyperparameter,"L(q)=\max(0,M_{1}(h,r,t^{*})-\max_{t\inN}M_{1}(h,r,t)+m)",where \(\mathtt{m}\) is the margin hyperparameter
2024.acl-short.25,1,"s_{i}=\sigma(\frac{1}{\beta_{i}}(log\frac{\mu_{i}}{1-\mu_{i}}+\textit{log}\alpha_{i})) || m_{i}=min(1,max(0,s_{i}(\zeta-\gamma)+\gamma))",,"m_{i}\simHardConcrete(\log\alpha_{i},\beta_{i})",Mask \(m_{i}\) corresponding to each neuron as an independent random variable following a hard concrete distribution.
2024.acl-short.25,2,"min(1,max(0,\sigma(log\alpha_{i})(\zeta-\gamma)+\gamma))",,"m_{i}=min(1,max(0,s_{i}(\zeta-\gamma)+\gamma))","Here, \(m_{i}\) is the approximately discrete activation value of the mask score \(s_{i}\), where \(\gamma\) and \(\zeta\) are constants."
2024.acl-short.25,3,"L_{m}(f(m\odot\theta),x)=\sum_{i=1}^{I}log(P(x_{p+i}|x_{<p+i}))",,"L_{PII}=-\sum_{t=1}^{T}\log P(x_{t} | x_{1},\ldots, x_{t-1}, m)","where \(\mathcal{L}_{PII}\) is the training objective for PII localization, \(P(x_{t} | x_{1}, \ldots, x_{t-1}, m)\) is the probability of token \(x_{t}\) given the previous tokens and the mask \(m\)."
2024.acl-short.25,4,"L_{adv}(f(m\odot\theta),x)=-\sum_{t=1}^{T}log(P(x_{t}|x_{<t}))",,"L_{adv}(f(m\odot\theta),x)=-\sum_{t=1}^{T}log(P(x_{t}|x_{<t}))","Adversarial loss for the language model, where \(P(x_{t}|x_{<t})\) is the probability of token \(x_{t}\) given the preceding tokens."
2024.acl-short.25,5,R(m)=-\frac{1}{|m|}\sum_{i=1}^{|m|}\sigma(log\alpha_{i}-\beta_{i}log \frac{-\gamma}{\zeta}),,L_{0}(m)=\sum_{i=1}^{N}I(m_{i}=0),"\(\mathcal{L}_{0}(m)\) is the \(L_{0}\) complexity of the mask scores, where \(\mathbb{I}\) is the indicator function that counts the number of neurons with zero mask scores."
2024.acl-short.3,1,"\Delta(G_{1},G_{2})=\min_{ops}\sum_{i=1}^{n}w(op_{i})",,"\Delta(G_{1}, G_{2})=APTED(G_{1}, G_{2})",Tree edit distance between the predicted code's AST \(G_{1}\) and the ground-truth AST \(G_{2}\).
2024.acl-short.3,2,"TSED=\max\{1-\frac{\delta}{MaxNodes(G_{1},G_{2})},0\}",,"N=\max(|G_{1}|, |G_{2}|)+\alpha\cdot\Delta(G_{1}, G_{2})","Where \(N\) is the normalized tree edit distance, \(|G_{1}|\) and \(|G_{2}|\) are the number of nodes in the respective ASTs, and \(\alpha\) is a scaling factor."
2024.acl-short.39,1,"Bio(m,co(m))=f_{gen}(m,co(m))",where biography is generated by the model \(f_{gen}\) given the personal attribute of interest (\(m\)) and the co-occurring attributes (\(co(m)\)),"b=f_{gen}(m, co(m))","where \(b\) represents the generated biography, \(f_{gen}\) is the generation model, \(m\) is the personal attribute of interest, and \(co(m)\) denotes the co-occurring attributes."
2024.acl-short.39,2,"Bio(\phi,co(m))=f_{gen}(\phi,co(m))",,"S_{true}(m)=\frac{1}{N}\sum_{i=1}^{N} S(Bio(m, co(m))_i)","where \(S_{true}(m)\) is the semantic score for biographies generated with the true attribute \(m\), \(N\) is the number of generated biographies, and \(S(Bio(m, co(m))_i)\) is the semantic score of the \(i\)-th generated biography."
2024.acl-short.39,3,"Bio(f,co(m))=f_{gen}(f,co(m)),do(m\to f)","where \(do(m\to f)\) denotes the do operator (Pearl, 2009), e","Bio(do(m\to f), co(m))=f_{gen}(do(m\to f), co(m))","where \(do(m \to f)\) denotes the do operator, indicating the manipulation of the personal attribute \(m\) to a different attribute \(f\) while keeping the co-occurring attributes \(co(m)\) unchanged."
2024.acl-short.40,1,"L_{e}=\frac{1}{N}\sum_{i=1}^{N}(\cos(S_{1,i},S_{2,i})-\cos(E_{1,i},E_{2,i}))^{2}","where \(N\) is the batch size, and \(S\) and \(E\) contain the target text SEM vectors and the predicted output SEM vectors respectively",L=\frac{1}{N}\sum_{i=1}^{N} (S_i-E_i)^2,"where \(N\) is the batch size, and \(S\) and \(E\) contain the target text SEM vectors and the predicted output SEM vectors respectively."
2024.acl-short.40,2,"L_{o}=CE(T,O)=-\frac{1}{N}\sum_{i=1}^{N}\sum_{j=1}^{M} (T_{ij}\cdot\log(O_{ij}))","where \(N\) is the batch size, \(M\) the vocabulary size, \(T\) is the target text and \(O\) is the output text","L_{t}=-\frac{1}{N}\sum_{i=1}^{N}\sum_{j=1}^{M}T_{i,j}\log(O_{i,j})","where \(N\) is the batch size, \(M\) the vocabulary size, \(T\) is the target text and \(O\) is the output text"
2024.acl-short.41,1,E(y)=h(\beta+\sum_{j=1}^{J}f_{j}(x_{j})),"where \(h(\cdot)\) is the activation function used in the output layer, e",y=h\left(\sum_{i=1}^{n} w_i x_i+b\right),"where \(y\) is the output of the model, \(w_i\) are the weights, \(x_i\) are the input features, and \(b\) is the bias term."
2024.acl-short.41,2,h(E[y])=\beta+\sum_{j=1}^{J}f_{j}(x_{j(tab)})+\sum_{k=1}^{ K}f_{k}(x_{k(top)}),,E(y)=h\left(\beta+\sum_{j=1}^{J}f_{j}(x_{j(tab)})+\sum_{k=1}^{K}f_{k}(x_{k(top)})\right),"where \(f_{k}:\mathbb{R}\rightarrow\mathbb{R}\) represents the shape-functions for the \(k\)-th topic, and \(K\) is the number of topics."
2024.acl-short.43,1,P(c|s)=\frac{\exp(b_{c}\cdot\frac{1}{T}\sum_{t=1}^{T}x_{t})}{\sum_{c^{\prime}=1}^{N}\exp(b_{c^{\prime}}\cdot\frac{1}{T}\sum_{t=1}^ {T}x_{t})},,P(c|s)=\frac{e^{z_c}}{\sum_{j=1}^{N} e^{z_j}},Where \(z_c\) represents the logit for language \(c\) computed from the feature embeddings.
2024.acl-short.43,2,"V_{c,t}(s)=b_{c}\cdotx_{t}",,"V(s)=V_{1,1}(s) &V_{1,2}(s) &\cdots &V_{1,W}(s)\\V_{2,1}(s) &V_{2,2}(s) &\cdots &V_{2,W}(s)\\\vdots &\vdots &\ddots &\vdots\\V_{N,1}(s) &V_{N,2}(s) &\cdots &V_{N,W}(s)",\mathbf{V}(s) is the \(N \times W\) matrix representing the logits for each language \(c\) and word-level feature \(\mathbf{x}_{t}\) in the sentence \(s\).
2024.acl-short.49,1,CEF=\frac{\sum_{i=0}^{n}(I_{i}-\overline{I} )(M_{i}-\overline{M})}{\sqrt{\sum_{i=1}^ {n}(I_{i}-\overline{I})^{2}}\sqrt{\sum_{i=1}^ {n}(M_{i}-\overline{M})^{2}}},where \(\overline{x}=\frac{1}{n}\sum_{i=1}^{n}x_{i}\) (the sample mean),r=\frac{\sum_{i=1}^{n}(I_i-\overline{I})(M_i-\overline{M})}{\sqrt{\sum_{i=1}^{n}(I_i-\overline{I})^2}\sqrt{\sum_{i=1}^{n}(M_i-\overline{M})^2}},"where \(r\) is the Pearson correlation coefficient, \(I_i\) is the prediction impact for intervention \(i\), \(M_i\) is the mention importance for intervention \(i\), and \(\overline{I}\) and \(\overline{M}\) are the sample means of prediction impacts and mention importances, respectively."
2024.acl-short.49,2,"TVD(P,Q)=\frac{1}{2}\sum_{x}|P(x)-Q(x)|",where P and Q are probability distributions over discrete classes,"TVD(P, Q)=\frac{1}{2}\sum_{i} |P(i)-Q(i)|",where \(P\) and \(Q\) are probability distributions over discrete classes
2024.acl-short.49,3,CCT=\frac{E_{M}(TVD)-E_{-M}(TVD)}{STD(TVD)}\sqrt{\frac{|M || \neg M|}{|M\cup\neg M|^{2}}},"where \(M\) indicates that the explanation mentions the IA, and \(|M|\) indicates the number of examples with explanation mentions",CCT=\frac{\sum_{i=1}^{n}(I_{i}-\overline{I})M_{i}}{\sqrt{\sum_{i=1}^{n}(I_{i}-\overline{I})^{2}}|M|},"where \(M\) indicates that the explanation mentions the IA, and \(|M|\) indicates the number of examples with explanation mentions"
2024.acl-short.5,1,"s_{ori}(x_{i}|x_{<i})= || \{{ll}\log P_{M_{e}}(x_{i}|x_{<i})-\log P_{M_{a}}(x_{i}|x_{<i}),&x_{i}\inV_{ori,i}^{\alpha}\\-\infty,&x_{i}\notinV_{ori,i}^{\alpha}\\s_{imp}(x_{i}|x_{<i})=\\\{{ll}(1+\beta)Y_{M_{a}}(x_{i}|x_{<i})-\beta Y_{M_{a}}(x_{i}|x_{<i}),&x_{i}\inV_{imp,i}^{\alpha}\\-\infty,&x_{i}\notinV_{imp,i} || s_{int}(x_{i}|x_{<i})= || \{{ll}(1+\beta)Y_{M_{e}}(x_{i}|x_{< i})-\beta Y_{M_{a}}(x_{i}|x_{<i}),&x_{i}\inV_{imp,i}^{\alpha}\\-\infty,&x_{i}\notinV_{imp,i}",,s_{ori}(x_{i}|x_{<i})=\log\frac{p_{amateur}(x_{i}|x_{<i})}{p_{expert}(x_{i}|x_{<i})},"The contrastive logit score for the original contrastive decoding, where \(p_{\text{amateur}}(x_{i}|x_{<i})\) is the probability of the token \(x_{i}\) given the preceding tokens \(x_{<i}\) from the amateur language model, and \(p_{\text{expert}}(x_{i}|x_{<i})\) is the corresponding probability from the expert language model."
2024.acl-short.5,2,"V_{ori,i}^{\alpha}=\{w|P_{M_{e}}(w|x_{<i})>\alpha\max_{w\inV}P_{M_{e}}(w|x_{<i})\} || V_{imp,i}^{\alpha}=\{w|Y_{M_{e}}(w| x_{<i})>\log\alpha+\max_{w\inV}Y_{M_{e}}(w|x_{<i})\}",,"V_{ori,i}^{\alpha}=\{x_{j}\inV\mid Y_{M_{e}}(x_{j}|x_{<i})\geq\alpha\}","The adaptive plausibility constraints for original and improved contrastive decoding, respectively."
2024.acl-short.53,1,"L_{FT}=\frac{1}{|D_{M}|}\sum_{d\inD_{M}}L(d;\Phi _{0},\Delta\Phi)","where \(\mathcal{D}_{M}\) is the KE dataset and \(d\) is a fact edit example, \(L\) is the cross entropy loss function applied to autoregressive models, \(\Phi_{0}\) denotes the set of original weights of the language model that are kept frozen, and \(\Delta\Phi\) denotes the additional parameters used by the PEFT adapters","L=\sum_{d\inD_{M}} L(\Phi_{0}, d)+\lambda\|\Delta\Phi\|^2","where \(L\) is the total loss function, \(\mathcal{D}_{M}\) is the KE dataset, \(d\) is a fact edit example, \(\Phi_{0}\) denotes the set of original weights of the language model that are kept frozen, \(\Delta\Phi\) denotes the additional parameters used by the PEFT adapters, and \(\lambda\) is the regularization parameter."
2024.acl-short.53,2,h=W_{0}x+BAx=(W_{0}+BA)x,,"L_{LoRA}=\frac{1}{|D_{M}|}\sum_{d\inD_{M}}L(d;\Phi_{0}+BA,\Delta\Phi)","where \(B\) and \(A\) are the low-rank matrices used in the LoRA approach, and \(r\) is the rank of the decomposition."
2024.acl-short.56,1,h^{\prime}=SoftMax(h\cdoth^{\prime T})\cdoth^{\prime},,"H=(z^{\prime}_{s}+M[1],\cdots,z^{\prime}_{s}+M[k])",Hybrid latent variables \(H\) are constructed by adding the sentence-level continuous latent variable \(z^{\prime}_{s}\) to the discrete latent variables \(M\).
2024.acl-short.56,2,"\mu^{\prime}_{1},...,\mu^{\prime}_{n}\\\log(\sigma^{\prime 2}_{1}),...,\log(\sigma^{\prime 2}_{n})=[h_{1};h^{\prime}_{1}]\\\cdots\\[h_{n};h^{\prime}_{n}]W^{\prime}_{u}\","where \(W^{\prime}_{u}\) is trainable parameters of \(q_{\phi}(z|r,c)\)","q_{\phi}(z|c,r)\simN(\mu^{\prime},\sigma^{\prime 2}I)","where \(W^{\prime}_{u}\) are trainable parameters of \(q_{\phi}(z|r,c)\)"
2024.acl-short.57,1,"a_{k}^{pred}=\operatorname*{arg\,max}_{a_{k}}\!P(a_{k}|q_{k},H_{k},D)",,"H_{k}=\{(q_{0},a_{0}^{pred}),\cdots,(q_{k-1},a_{k-1}^{pred})\}",History of questions and predictions up to turn \(k\).
2024.acl-short.57,2,"P(a_{k}|q_{k},H_{k},D)=P(a_{k}|q_{k},H_{k}^{\star},D)",,"P(a_{k}|q_{k},H_{k}^{\star},D)=P(a_{k}|q_{k},H_{k},D)","The probability of the predicted answer \(a_{k}\) given the question \(q_{k}\), the augmented history \(H_{k}^{\star}\), and the document \(D\) is equal to the probability of the predicted answer given the original history \(H_{k}\)."
2024.acl-short.57,3,"L_{CE}=CE(QA_{\theta^{\prime}}(q_{k},H_{k},D),a_{k}^{gold}) || L_{Cons}=D_{KL}(QA_{\theta^{\prime}}(q_{k},H_{k},D) || QA_{\theta^{\prime}}(q_{k},H_{k}^{\star},D)) || L_{T}=L_{CE}+\lambda L_{Cons}",,L_{T}=L_{CE}+\lambda L_{Cons},"Total loss for training the QA network, combining cross-entropy loss and consistency loss."
2024.acl-short.62,1,R(\tau)=\frac{1}{T}\sum_{t=1}^{T}r_{t},,R(\tau)=\frac{1}{T}\sum_{t=1}^{T} r_{t},"The average reward of the trajectory \(\tau\), where \(R(\tau)\) is the total reward, \(T\) is the length of the response, and \(r_{t}\) is the reward assigned to the token at time step \(t\)."
2024.acl-short.62,2,p(\tau^{i}\succ\tau^{j})&=\frac{\exp(R(\tau^{i}))}{\exp(R(\tau^{i}))+\exp(R(\tau^{j}))}\\&=\sigma(R(\tau^{i})-R(\tau^{j})),where \(\tau^{i}\) and \(\tau^{j}\) represent two different responses generated from the same prompt,P(\tau^{i}\succ\tau^{j})=\frac{e^{R(\tau^{i})}}{e^{R(\tau^{i})}+e^{R(\tau^{j})}},where \(\tau^{i}\) and \(\tau^{j}\) represent two different responses generated from the same prompt.
2024.acl-short.62,3,"&L=-E_{(\tau^{i},\tau^{j})\simD}[\log\sigma(R(\tau^{i})-R(\tau^{j}))]\\&=-E_{(\tau^{i},\tau^{j})\simD}[\log\sigma((\frac{1}{T^{i}}-\frac{1}{T^{j}})\sum_{t\in U_{0}}r_{t}\\&+\frac{1}{T^{i}}\sum_{t\in U_{1}}r_{t}^{i}-\frac{1}{T^{j}}\sum_ {t\in U_{1}}r_{t}^{j})]",,L=-\sum_{t\in U_{1}}\log(p(\tau^{i}\succ\tau^{j}))-\sum_{t\in U_{0}}\log(1-p(\tau^{i}\succ\tau^{j})),"where \(U_{0}\) and \(U_{1}\) represent the unchanged and changed parts of the responses, respectively."
2024.acl-short.62,4,"L\approx-E_{(\tau^{i},\tau^{j} )\simD}[\log\sigma(\frac{1}{T^{i}}\sum_{t\in U_{1}}r_{t}^{i}-\frac{ 1}{T^{j}}\sum_{t\in U_{1}}r_{t}^{j})]",,"L=-E_{(\tau^{i},\tau^{j})\simD}[\log\sigma((\frac{1}{T^{i}}\sum_{t\in U_{1}}r_{t}^{i}-\frac{1}{T^{j}}\sum_ {t\in U_{1}}r_{t}^{j})]","where \(T^{i}\) and \(T^{j}\) are the lengths of responses \(\tau^{i}\) and \(\tau^{j}\), respectively."
2024.acl-short.66,1,"H(t,a)=\mathds{1}[\{(i,t)\in a\}=\varnothing]",,\hat{y}_{t} is a hallucination\iff a(\hat{y}_{t})=\emptyset,A target word \(\hat{y}_{t}\) is a hallucination if there is no alignment to any source word \(x_{j}\).
2024.acl-short.66,2,"HR(x,\hat{y},a)=\frac{1}{|\hat{y}|}\sum_{t=1}^{|\hat{y}|}H(t,a)",,HR=\frac{H}{N},"HR: Hallucination Rate, H: Number of hallucinated words, N: Total number of target words"
2024.acl-short.66,3,"H_{wait-k}(t,a)=\mathds{1}[\{(s,t)\in a\mid s\geq t+k\}=\varnothing]",,"H_{G}(t,a)=\mathds{1}[\{(i,t)\in a\}=\varnothing\land t\leq k]",A target word \(\hat{y}_{t}\) is considered a hallucination in GHall if it does not align with the current source and its position \(t\) is less than or equal to \(k\).
2024.acl-short.66,4,"R(y_{i},x_{j})=P(y_{i}\midy_{<i},x_{\leq i+k-1}) || \quad-P(y_{i}\midy_{<i},x_{\leq i+k-1, (j,\textbf{0})}) || R(y_{i},y_{j})=P(y_{i}\midy_{<i},x_{\leq i+k-1}) || \quad-P(y_{i}\midy_{<i,(j,\textbf{0} )},x_{\leq i+k-1})",,"R(y_{i},x_{j})=\mathds{1}[dropout(x_{j})]\cdot f(y_{i},x_{j})",Relevance of target word \(\text{y}_{i}\) to source word \(\text{x}_{j}\) based on the dropout effect.
2024.acl-short.66,5,"R(y_{i})_{source-side}=\max\{|R(y_{i},x_ {j})|\} || R(y_{i})_{target-side}=\max\{|R(y_{i},y _{j})|\}",,"R_{max}=\max\left( |R(y_{i},x_{j})|, |R(y_{i},y_{j})|\right)",The maximum relevance \(R_{max}\) is determined by the maximum absolute value of the relevance of the target-side and source-side words to the next word to be generated.
2024.acl-short.66,6,TSSR(y_{i})=\frac{R(y_{i})_{target-side}}{R(y_{i} )_{source-side}},,TSSR=\frac{R(y_{i})_{target-side}}{R(y_{i})_{source-side}},"The ratio of target-side relevance to source-side relevance (TSSR), indicating the usage of target-side context in generating the next word \(y_{i}\)."
2024.acl-short.68,1,"T_{i}=\operatorname*{Top\_}{d\inM}k\f(s_{i},d)",,"T_{i}=Top-K(R_{Q}(s_{i}),M)",Top-k passages retrieved for medical code \(c_{i}\) based on its surface name \(s_{i}\).
2024.acl-short.68,2,"e_{i}=LLM([Prompt,t_{i,1},\cdots,t_{i,k}])",where \(t_{i}\in\mathcal{T}_{i}\) stands for the retrieved passages in Eq,"e_{i}=LLM(t_{1}, t_{2},\ldots, t_{k})",where \(t_{i}\in\mathcal{T}_{i}\) stands for the retrieved passages in Eq
2024.acl-short.68,3,"h_{i}^{k}=PLM(X_{i}^{k}),\;\;\widehat{y}_{i,1}=MLP ( || _{k\inS}h_{i}^{k})",,"X_{i}^{d}=\{[CLS],D_{t},D_{t-1},\ldots,D_{1}\},\quad D_{i}= || _{c\in D_{i}}(c,e)",where \(D_{i}\) is the concatenation of disease code \(c\) and its summarized knowledge \(e\) for the \(i\)-th visit.
2024.acl-short.68,4,"e_{i}=HyGT(G,V_{i}),\widehat{y}_{i,2}=MLP(e_{i})",where \(\mathbf{e}_{i}\) is the representation of patient \(i\) after hypergraph transformer,"\widehat{y}_{i,2}=f_{\theta}(e_{i})",where \(\mathbf{e}_{i}\) is the representation of patient \(i\) after hypergraph transformer.
2024.acl-short.68,5,"L_{aug}=E_{(V_{i},y_{i})\simP}\;\ell(\widehat{y}_{i,1},y_{i})+\lambdaD_{KL}(\widehat{y}_{i,1},\widetilde{y}) || L_{loc}=E_{(V_{i},y_{i})\simP}\;\ell(\widehat{y}_{i,2},y_{i})+\lambdaD_{KL}(\widehat{y}_{i,2},\widetilde{y})",,L=L_{1}+L_{2},"where \(\mathcal{L}_{1}\) and \(\mathcal{L}_{2}\) are the loss functions for the models \(g_{\phi}\) and \(f_{\theta}\), respectively."
2024.acl-short.71,1,"hyp^{*}=\operatorname*{arg\,max}_{hyp\inhyps}utility(hyp)",,"hyp^{*}=\arg\max_{hyp\in\{hyp_{1},\ldots,hyp_{n}\}} U(hyp)",\mathit{hyp}^{*}: the selected translation hypothesis; \mathit{hyp}: the set of translation hypotheses; U(\mathit{hyp}): the utility function for a given hypothesis.
2024.acl-short.71,2,"utility(hyp)\approx\frac{1}{m}\sum_{\textit{ref}\in\textit{refs}} metric(hyp,\textit{ref})",,"refs=\{ref_{1},\ldots,ref_{m}\}\sim p(refs|src)",The set of pseudo-references sampled from the model distribution given the source input.
2024.acl-short.71,3,\overline{\textit{ref}}=\frac{1}{m}\sum_{\textit{ref}\in\textit{refs}}\textit{ ref},,\overline{\textit{ref}}=\frac{1}{m}\sum_{\textit{ref}\in\textit{refs}}\textit{ref},"Aggregate representation of references, calculated as the average of all reference representations."
2024.acl-short.71,4,"utility(hyp)\approxmetric(hyp,\overline{\textit{ref}})",,"utility(hyp)\approxmetric(hyp,\overline{\textit{ref}})","utility(hyp): approximated utility of the hypothesis; metric(hyp, \overline{ref}): metric score between the hypothesis and the aggregate reference representation."
2024.acl-short.71,5,ChrF_{\beta}=\frac{(1+\beta^{2})\cdotChrP\cdotChrR}{\beta^{2}\cdotChrP+ChrR} || ChrP=\frac{|hyp\cap\textit{ref}|}{|hyp|} and ChrR=\frac{|hyp\cap\textit{ref}|}{|\textit{ref}|},"where

\[\text{ChrP}=\frac{|hyp\cap\textit{ref}|}{|hyp|}\text{ and }\text{ChrR}=\frac{|hyp \cap\textit{ref}|}{|\textit{ref}|},\]

and the parameter \(\beta\) controls the relative importance of precision and recall",ChrF=(1+\beta^2)\cdot\frac{ChrP\cdotChrR}{\beta^2\cdotChrP+ChrR},"ChrF is defined as an F-score over character n-grams, where \(\text{ChrP}\) is the character precision, \(\text{ChrR}\) is the character recall, and \(\beta\) controls the relative importance of precision and recall."
2024.acl-short.71,6,\overline{\textit{ref}}=\frac{1}{m}\underbrace{\biguplus}_{\textit{ref}\in\textit{refs}}\textit{ref},where \(\biguplus\) is an operation that sums up the counts of each n-gram,\overline{ChrP}=\frac{1}{m}\sum_{\textit{ref}\in\textit{refs}}\frac{|hyp\cap\textit{ref}|}{|hyp|} and \overline{ChrR}=\frac{1}{m}\sum_{\textit{ref}\in\textit{refs}}\frac{|hyp\cap\textit{ref}|}{|\textit{ref}|},"where \(\overline{\text{ChrP}}\) and \(\overline{\text{ChrR}}\) are the averaged precision and recall across all references, respectively."
2024.acl-short.71,7,"\textbf{{hyp}},\\textbf{{ref}},\\textbf{{src}}=emb(\textbf{{hyp}}),\emb(\textbf{{ref}}),\emb(\textbf{{src}})",,"utility(hyp)\approxChrF_{\beta}(hyp,\overline{\textit{ref}})",Expected utility of a hypothesis approximated by calculating ChrF with respect to the aggregate reference representation.
2024.acl-short.71,8,"comet(\textbf{{hyp}})=score(\textbf{{hyp}},\\textbf{{ref}},\\textbf{{src}})",,"score=FFN(\textbf{{hyp}},\\textbf{{ref}},\\textbf{{src}})","The scalar score output by the feed-forward module based on the embeddings of the hypothesis, reference, and source sequence."
2024.acl-short.71,9,\overline{\textbf{{ref}}}=\frac{1}{m}\sum_{\textbf{{ref}}\in\textbf{{ref}}}emb(\textbf{{ref}}),,\overline{\textbf{ref}}=\frac{1}{m}\sum_{\textbf{ref}\in\textit{refs}}emb(\textbf{ref}),where \(\overline{\textbf{ref}}\) is the aggregated embedding of the references.
2024.acl-short.71,10,"comet(\textbf{{hyp}})\approxscore(\textbf{{hyp}},\\overline{\textbf{{ref}}},\\textbf{{src}})",,"comet(hyp)=score(\textbf{{hyp}},\\overline{\textbf{{ref}}},\\textbf{{src}})",where \(\text{comet}(hyp)\) is the predicted quality score for the hypothesis \(\textbf{hyp}\) based on the aggregated reference embeddings \(\overline{\textbf{ref}}\) and the source sequence \(\textbf{src}\).
2024.acl-short.72,1,h_{i}=XLMRoberta-Layer^{1}(x_{i}),"where \(h_{i}\) is the representation of the ""[CLS]"" token",h_{i}=XLM-Roberta(x_{i})\inR^{768},"where \(h_{i}\) is the representation of the ""[CLS]"" token"
2024.acl-short.72,2,\hat{h}_{i}=Transformer(Linear(h_{i})),where the linear layer projects \(h_{i}\) to 256-dimensional embeddings for efficient modeling,"H=Transformer(h_{1}, h_{2},\ldots, h_{n})",where the linear layer projects \(h_{i}\) to 256-dimensional embeddings for efficient modeling
2024.acl-short.72,3,P(y^{k}_{i}=1|x_{i})=Sigmoid(MLP(\hat{h}_{i})),,P(y^{k}_{i}=1|x_{i})=\frac{\exp(\hat{h}_{i}\cdot W^{k})}{\sum_{j=1}^{K}\exp(\hat{h}_{i}\cdot W^{j})},where \(W^{k}\) is the weight vector for the \(k\)-th label and \(K\) is the total number of labels.
2024.acl-short.72,4,"L=\sum_{k=1}^{6}\sum_{i=1}^{n}CrossEntropy(P(y^{k}_{i}|x_{i}), Y^{k}_{i})",where \(\mathcal{Y}^{k}_{i}\) is the ground truth label,L=-\sum_{i=1}^{n}\sum_{k=1}^{K}Y^{k}_{i}\log(P(y^{k}_{i}=1|x_{i}))+(1-Y^{k}_{i})\log(1-P(y^{k}_{i}=1|x_{i})),where \(\mathcal{Y}^{k}_{i}\) is the ground truth label
ICLR_2024_oral_1,1,I(\theta):=E_{\Upsilon\sim p_{\theta}} [\nabla_{\theta}\log p_{\theta}(\Upsilon)\cdot\nabla_{\bm {\theta}}\log p_{\theta}(\Upsilon)^{\top}],,"I_{\theta}=E_{\theta}\left[\left(\nabla_{\theta}\log p_{\theta}(s,a)\right)\left(\nabla_{\theta}\log p_{\theta}(s,a)\right)^{\top}\right]","The Fisher information matrix \(I_{\mathbf{\theta}}\) is defined in terms of the expected value of the outer product of the gradient of the log-likelihood of the distribution \(p_{\mathbf{\theta}}(s,a)\) with respect to the parameter \(\mathbf{\theta}\)."
ICLR_2024_oral_1,2,E_{D\sim p_{\theta^{\star}}}([\widehat{\theta} (D)-\theta^{\star})(\widehat{\theta}(D)-\theta^{\star})^{\top}]\succeq T^{-1}\cdotI(\theta^{\star})^ {-1},,Cov(\widehat{\theta}(D))\succeqI(\theta^{\star})^{-1},The covariance of the unbiased estimator \(\widehat{\mathbf{\theta}}(\mathfrak{D})\) is bounded below by the inverse of the Fisher information matrix evaluated at the true parameter \(\mathbf{\theta}^{\star}\).
ICLR_2024_oral_1,3,E_{D\sim p_{\theta^{\star}}}[\|\widehat{\theta} (D)-\theta^{\star}\|_{2}^{2}]=tr(E_{ D\sim p_{\theta^{\star}}}[(\widehat{\theta}(D)-\theta^{\star})(\widehat{\theta}(D)-\theta^{\star})^{\top}])\geq T^{-1}\cdottr(I(\theta^{\star})^{-1}),,E_{D\sim p_{\theta^{\star}}}[\|\widehat{\theta}(D)-\theta^{\star}\|^{2}]\succeq\frac{1}{T}\cdotI(\theta^{\star})^{-1},The mean-squared error of the unbiased estimator \(\widehat{\mathbf{\theta}}(\mathfrak{D})\) is bounded below by the inverse of the Fisher information matrix scaled by the number of samples \(T\).
ICLR_2024_oral_1,4,"I(\theta^{\star},\pi_{exp}):=E_{\tau\sim p_{\theta^{\star}}(\cdot\mid\pi_{exp})}[\nabla_{\theta}\log p_{\theta^{\star}}(\tau\mid\pi_{exp})\cdot\nabla_{\theta}\log p_{\theta^{\star}}(\tau\mid\pi_{ exp})^{\top}]",,"I(\theta^{\star},\pi_{exp}):=E_{\tau_{real}\sim p_{\theta^{\star}}(\cdot\mid\pi_{exp})} [\nabla_{\theta}\log p_{\theta^{\star}}(\tau_{real})\cdot\nabla_{\theta}\log p_{\theta^{\star}}(\tau_{real})^{\top}]",The Fisher information matrix conditioned on the exploration policy \(\pi_{\mathrm{exp}}\) and the true parameter \(\mathbf{\theta}^{\star}\).
ICLR_2024_oral_1,5,"\arg\min_{\pi}tr(I(\theta^{\star},\pi)^{-1})",,"I(\theta^{\star},\pi_{exp})^{-1}=argmin_{\pi}tr(I(\theta^{\star},\pi))",The Fisher information matrix under the exploration policy \(\pi_{\mathrm{exp}}\) is minimized to achieve the smallest estimation error for \(\mathbf{\widehat{\theta}}\).
ICLR_2024_oral_1,6,"s_{h+1}=f_{\theta}(s_{h},a_{h})+w_{h}","where \(s_{h}\) and \(a_{h}\) are the current state and action, \(w_{h}\sim\mathcal{N}(0,\sigma_{w}^{2}\cdot I)\) is Gaussian process noise, and \(f_{\mathbf{\theta}}\) are the nominal dynamics","s_{h+1}=f_{\theta}(s_{h}, a_{h})+w_{h}","where \(s_{h}\) and \(a_{h}\) are the current state and action, \(w_{h}\sim\mathcal{N}(0,\sigma_{w}^{2}\cdot I)\) is Gaussian process noise, and \(f_{\mathbf{\theta}}\) are the nominal dynamics."
ICLR_2024_oral_1,7,"I(\theta,\pi)=\sigma_{w}^{-2}\cdotE_{p_{\theta}(\cdot\mid\pi)}[\sum_{h=1}^{H}\nabla_{\theta}f_{\theta}(s_{h},a_ {h})\cdot\nabla_{\theta}f_{\theta}(s_{h},a_{h})^{\top}]",,"I(\theta^{\star},\pi_{exp})=E_{\tau\sim p_{\theta^{\star}}(\cdot\mid\pi_{exp})}\left[\nabla_{\theta} f_{\theta}(s_h, a_h)\cdot\nabla_{\theta} f_{\theta}(s_h, a_h)^{\top}\right]","Fisher information matrix under the specified dynamics, where \(f_{\mathbf{\theta}}(s_h, a_h)\) represents the nominal dynamics function."
ICLR_2024_oral_1,8,"\pi_{exp}=\arg\min_{\pi}E_{\theta\sim q_{0}}[tr (I(\theta,\pi)^{-1})]",,"\arg\min_{\pi}E_{\theta\simP}[tr(I(\theta,\pi)^{-1})]",where \mathcal{P} is the known parametric family of dynamics.
ICLR_2024_oral_1,9,E_{\theta\sim q_{\theta}}[E_{\tau_{ sim}\sim p_{\theta}(\cdot\midA(\uptau_{real}) )}[\|\uptau_{real}-\uptau_{sim}\|_{2}^{2}]],"where \(p_{\mathbf{\theta}}(\cdot\mid\mathcal{A}(\mathbf{\uptau}_{\mathrm{real}}))\) denotes the distribution over trajectories generated by the simulator with parameter \(\mathbf{\theta}\), and playing the same sequence of actions as were played in \(\mathbf{\uptau}_{\mathrm{real}}\)",q_{\phi}^{\star}=\arg\min_{q_{\phi}}E_{\uptau_{real}}\left[D_{KL}\left(p_{\theta}(\cdot\midA(\uptau_{real}))\|p_{\theta^{\star}}(\cdot\midA(\uptau_{real}))\right)\right],where \(D_{\mathrm{KL} }\) denotes the Kullback-Leibler divergence between the distributions over trajectories generated by the simulator and the real environment.
ICLR_2024_oral_10,1,"R=\{r_{1},\dots,r_{m}\}",where each ray \(\mathbf{r}_{i}\in\mathbb{R}^{6}\) is associated with a known pixel coordinate \(\mathbf{u}_{i}\),r_{i}=u_{i}\\d_{i}\inR^{6},"where \(\mathbf{r}_{i}\) is the ray representation, \(\mathbf{u}_{i}\) is the pixel coordinate, and \(\mathbf{d}_{i}\) is the direction vector of the ray."
ICLR_2024_oral_10,2,"r=\langled,m\rangle\inR^{6}","where \(\mathbf{m}=\mathbf{p}\times\mathbf{d}\in\mathbb{R}^{3}\) is the moment vector, and importantly, is agnostic to the specific point on the ray used to compute it",r=d\\m=d\\p\timesd,"where \(\mathbf{d}\in\mathbb{R}^{3}\) is the direction vector, \(\mathbf{p}\in\mathbb{R}^{3}\) is a point on the ray, and \(\mathbf{m}=\mathbf{p}\times\mathbf{d}\in\mathbb{R}^{3}\) is the moment vector."
ICLR_2024_oral_10,3,"d=R^{\top}K^{-1}u,\qquadm=(-R^{\top}t)\timesd",,"d_{i}=K^{-1}(u_{i},1),\quadm_{i}=p\timesd_{i},\quad\forall i\in\{1,\dots,m\}","where \(\mathbf{d}_{i}\) is the direction of the ray corresponding to pixel coordinate \(\mathbf{u}_{i}\), \(\mathbf{m}_{i}\) is the moment vector for the ray, and \(\mathbf{p}\) is the camera center."
ICLR_2024_oral_10,4,"c=\operatorname*{arg\,min}_{p\inR^{3}}\\sum_{\langled,m\rangle\inR}\lVertp\timesd-m\rVert^{2}",,c=\arg\min_{x}\sum_{i=1}^{m}\left\|r_{i}(x)\right\|^2,where \(\mathbf{c}\) is the camera center in 3D space.
ICLR_2024_oral_10,5,"P=\operatorname*{arg\,min}_{\|H\|=1}\sum_{i=1}^{m}\|Hd_{i}\timesu_{i}\|",,P=\sum_{i=1}^{m}u_{i}d_{i}^{\top},where \(\mathbf{d}_{i}\) is the predicted ray direction corresponding to pixel coordinate \(\mathbf{u}_{i}\).
ICLR_2024_oral_10,6,f_{feat}(I)=f\inR^{p\times p\times d},,t=-Rc,"where \(\mathbf{t}\) is the translation vector, \(\mathbf{R}\) is the rotation matrix, and \(\mathbf{c}\) is the camera center."
ICLR_2024_oral_10,7,"\{\hat{R}\}_{i=1}^{N}=f_{Regress}(\{f_{i},u_{i}\}_{i=1}^{N\cdot p^{2}})",,"f_{input}=concat(f_{feat}(I),u)\inR^{p^{2}\times (d+2)}","where \(\mathbf{f}_{\text{input}}\) is the concatenated feature vector for each patch, and \(\mathbf{u}\) is the pixel coordinate in normalized device coordinates."
ICLR_2024_oral_10,8,L_{recon}=\sum_{i=1}^{N}\|\hat{R}_{i}- R_{i}\|_{2}^{2},,L_{recon}=\sum_{i=1}^{N}\sum_{j=1}^{m}\|\hat{r}_{ij}-r_{ij}\|^{2},"where \(\hat{\mathbf{r}}_{ij}\) is the predicted ray for the \(j\)-th pixel of the \(i\)-th image, and \(\mathbf{r}_{ij}\) is the ground truth ray."
ICLR_2024_oral_10,9,x_{t}=\sqrt{\alpha_{t}}x_{0}+\sqrt{1-\bar{\alpha}_{t}}\epsilon,"where \(\epsilon\sim\mathcal{N}(0,\mathbf{I})\) and \(\alpha_{t}\) is a hyper-parameter schedule of noise weights such that \(x_{T}\) can be approximated as a standard Gaussian distribution",x_{t}=\sqrt{\alpha_{t}}x_{0}+\sqrt{1-\alpha_{t}}\epsilon,"where \(\epsilon\sim\mathcal{N}(0,\mathbf{I})\) and \(\alpha_{t}\) is a hyper-parameter schedule of noise weights such that \(x_{T}\) can be approximated as a standard Gaussian distribution."
ICLR_2024_oral_10,10,"L(\theta)=E_{t,x_{0},\epsilon}\|x_{0}-f_{\theta }(x_{t},t)\|^{2}",,x_{0}=f_{\theta}(x_{t}),where \(f_{\theta}\) is the denoising network parameterized by \(\theta\).
ICLR_2024_oral_10,11,"\{\hat{R}\}_{i=1}^{N}=f_{Diffusion}(\{f_{i},u_{i},r_{i,t}\}_{i=1}^{N\cdot p^{2}},t) || r_{i,t}=\sqrt{\bar{\alpha}_{t}}r_{i}+\sqrt{1-\bar{\alpha}_{t}}\epsilon","where the noisy rays \(\mathbf{r}_{i,t}\) can be computed as:

\[\mathbf{r}_{i,t}=\sqrt{\bar{\alpha}_{t}}\mathbf{r}_{i}+\sqrt{1-\bar{\alpha}_{t}}\epsilon","r_{i,t}=\sqrt{\bar{\alpha}_{t}}r_{i}+\sqrt{1-\bar{\alpha}_{t}}\epsilon","where \(\mathbf{r}_{i,t}\) is the noisy ray, \(\mathbf{r}_{i}\) is the original ray, and \(\epsilon\sim\mathcal{N}(0,\mathbf{I})\) is Gaussian noise."
ICLR_2024_oral_11,1,"O_{ij}=\sum_{v=1}^{k}I[y_{ij}\neq y_{ij}^{(v)}],\forall j\in Y_{i}",where \(\mathbb{I}[\cdot]\) is the indicator function and \(y_{ij}\) is the \(j\)-th candidate label of \(\mathbf{x}_{i}\),O_{ij}=\frac{1}{k}\sum_{j=1}^{k}I[y_{ij}\notin Y^{(j)}_{i}]\,where \(\mathbb{I}[\cdot]\) is the indicator function and \(y_{ij}\) is the \(j\)-th candidate label of \(\mathbf{x}_{i}\)
ICLR_2024_oral_11,2,\gamma_{i}=\lceil\tau(|Y_{i}|-1)\rceil,where \(\lceil\cdot\rceil\) is the ceiling function that returns the least integer greater than or equal to the given number,"N_{i}=\lceil O_{ij}\cdot\tau\rceil,\forall j\in Y_{i}",where \(N_{i}\) is the number of eliminated candidate labels for the instance \(\mathbf{x}_{i}\) and \(\tau\) is the specified parameter controlling the pruning extent.
ICLR_2024_oral_11,3,\widetilde{Y}_{i}=Top-\gamma_{i}-argmax_{j\in Y_{i}}(O_{ij}),where \(\mathrm{Top}\text{-}\gamma_{i}\text{-}\mathrm{argmax}_{i}\) returns a subset of indices (i,"J_{i}=Top-\gamma_{i}-argmax_{j} O_{ij},\forall j\in Y_{i}","where \(J_{i}\) is the set of indices of the candidate labels to be eliminated for the instance \((\mathbf{x}_{i},Y_{i})\)."
ICLR_2024_oral_11,4,"P(O_{iy^{\prime}}^{(\xi_{i})}<O_{iy})\leq\sum_{j=1}^{k}\sum_{m=\xi_{i}} ^{|Y_{i}^{\prime}|}\binom{|Y_{i}^{\prime}|}{m}\eta^{m}(1-\eta)^{(|Y_{i}^{\prime }|-m)}b_{\delta_{k}}(k,j)",,P(incorrect pruning)\leq\sum_{j=1}^{\gamma_{i}} P\left(O_{iy^{\prime}}^{(j)}\geq O_{iy}\right),where \(O_{iy^{\prime}}^{(j)}\) is the \(j\)-th order statistic of the down-voting statistics of the false candidate labels.
ICLR_2024_oral_11,5,"P(O_{iy}^{(\xi_{i}^{2})}<O_{iy})-P(O_{iy}^{(\xi_{i}^{1})}<O _{iy})\leqslant\sum_{j=1}^{k}\sum_{m=\xi_{i}^{2}}^{\xi_{i}^{1}-1}\binom{|Y^{\prime}_{i}|}{m}\eta^{m}(1-\eta)^{|Y^{\prime}_{i}|-m}b_{\delta_{k}}(k,j)",,"P(O_{iy^{\prime}}^{(\xi_{i}^{2})}<O_{iy})-P(O_{iy^{\prime}}^{(\xi_{i}^{1})}<O_{iy})\leq\sum_{j=1}^{k}\sum_{m=\xi_{i}^{2}}^{|Y_{i}^{\prime}|}\binom{|Y_{i}^{\prime}|}{m}\eta^{m}(1-\eta)^{(|Y_{i}^{\prime}|-m)}b_{\delta_{k}}(k,j)-\sum_{j=1}^{k}\sum_{m=\xi_{i}^{1}}^{|Y_{i}^{\prime}|}\binom{|Y_{i}^{\prime}|}{m}\eta^{m}(1-\eta)^{(|Y_{i}^{\prime}|-m)}b_{\delta_{k}}(k,j)",where \(\xi_{i}^{1}=(|Y^{\prime}_{i}|-\gamma_{i}^{1}+1)\) and \(\xi_{i}^{2}=(|Y^{\prime}_{i}|-\gamma_{i}^{2}+1)\).
ICLR_2024_oral_13,1,"q(x_{t}|x_{t-1})=N(x_{t};\sqrt{1-\beta_{t}}x_{t},\beta_{t}I ),\quadfor t\in\{1,...,T\}","where \(\beta_{t}\in(0,1)\) is the scheduled variance at step \(t\)","\beta_{t}\in (0, 1)",where \(\beta_{t}\) is the scheduled variance at step \(t\)
ICLR_2024_oral_13,2,x_{t}=\sqrt{\bar{\alpha}_{t}}x_{0}+\sqrt{1-\bar{\alpha}_{t}}\epsilon,"where, \(\bar{\alpha}_{t}=\prod_{i=1}^{t}(1-\beta_{t})\)","\bar{q}(x_{t}|x_{0})=N(x_{t};\sqrt{\bar{\alpha}_{t}}x_{0},(1-\bar{\alpha}_{t})I),\quadfor t\in\{1,...,T\}","where, \(\bar{\alpha}_{t}=\prod_{i=1}^{t}(1-\beta_{i})\)"
ICLR_2024_oral_13,3,\hat{x}_{0}^{t}=\frac{x_{t}-\sqrt{1-\bar{\alpha}_{t}}\epsilon_{\theta}(x_{t}) }{\sqrt{\bar{\alpha}_{t}}},,x_{0}=\frac{x_{t}-\sqrt{1-\bar{\alpha}_{t}}\epsilon_{\theta}(x_{t})}{\sqrt{\bar{\alpha}_{t}}},where \(\epsilon_{\theta}(x_{t})\) is the predicted noise at step \(t\).
ICLR_2024_oral_13,4,"x_{t-1}=\sqrt{\bar{\alpha}_{t-1}}\hat{x}_{0}^{t}+\sqrt{1-\Text-conditional diffusion models, such as Stable Diffusion (Rombach et al., 2022), employ classifier-free diffusion guidance (Rombach et al., 2022) to steer the sampling process. Given a text prompt\(p\), its embedding\(e_{p}=f(p)\) is computed using a pre-trained CLIP text encoder\(f(\cdot)\)(Radford et al., 2021; Cherti et al., 2023). In the reverse process, the conditional sampling adheres to Eq. (3) and Eq. (4), but the predicted noise\(\epsilon_{\theta}(x_{t})\) is changed to:\[\epsilon_{\theta}(x_{t},e_{\theta})+s(\underbrace{\epsilon_{\theta}(x_{t},e_ {p})-\epsilon_{\theta}(x_{t},e_{\theta})}_{text-conditional noise prediction})","where, \(e_{\theta}\) represents the prompt embedding of an empty string, and \(s\) determines the guidance strength, controlling the alignment of the generation to the prompt",x_{t-1}=\frac{1}{\sqrt{1-\beta_{t}}}\left(x_{t}+\sqrt{\beta_{t}}e_{\theta}+\sqrt{s}\epsilon_{\theta}(x_{t})\right),"where, \(e_{\theta}\) represents the prompt embedding of an empty string, and \(s\) determines the guidance strength, controlling the alignment of the generation to the prompt."
ICLR_2024_oral_13,5,"d=\frac{1}{T}\sum_{t=1}^{T}\|\epsilon_{\theta}(x_{t},e_{p})-\epsilon_{\theta} (x_{t},e_{\theta})\|_{2}",,"M=\left\|\epsilon_{\theta}(x_{T}, e_{p})-\epsilon_{\theta}(x_{T}, e_{\theta})\right\|",where \(M\) denotes the magnitude of the text-conditional noise prediction.
ICLR_2024_oral_13,6,"L(x_{t},e)=\|\epsilon_{\theta}(x_{t},e)-\epsilon_{\theta}(x_{t},e_{\emptyset})\|_{2}",,"\min_{e}\|\epsilon_{\theta}(x_{t},e)-\epsilon_{\theta}(x_{t},e_{\theta})\|_{2}",where \(e\) is the prompt embedding of the prompt \(p\) with \(N\) tokens.
ICLR_2024_oral_13,7,"SS_{e^{i}}=\frac{1}{T}\sum_{t=1}^{T}\|\nabla_{e^{i}}L(x_{t}, e)\|_{2}",,"s_{i}=\|\epsilon_{\theta}(x_{t},e_{i})-\epsilon_{\theta}(x_{t},e_{\emptyset})\|_{2}",where \(s_{i}\) represents the significance score for the token at position \(i\) in the prompt embedding \(e\).
ICLR_2024_oral_14,1,"F^{*}&=\operatorname*{argmax}_{F}p(F|D _{src},D_{tgt})=\operatorname*{argmax}_{F}p(D_{src},D_{tgt}|F)\cdot p(F)\\&=\operatorname*{argmax}_{F}\{\underbrace{\log p(D_{src},D _{tgt}|F)}_{data term}+\underbrace{\log p(F)}_{prior term}\}",,"F^{*}=\arg\max_{F} p(F|D_{src},D_{tgt})",Optimal correspondence field that maximizes the posterior probability given the feature descriptors.
ICLR_2024_oral_14,2,"X_{t}=\sqrt{\alpha_{t}}X_{0}+\sqrt{1-\alpha_{t}}Z,\quad Z\simN(0,I)",where \(\alpha_{t}=\prod_{i=1}^{t}(1-\beta_{i})\),\alpha_{t}=\prod_{i=1}^{t}(1-\beta_{i}),where \(\alpha_{t}\) is the cumulative product of the variance schedule up to time step \(t\).
ICLR_2024_oral_14,3,"X_{t-1}=\sqrt{\alpha_{t-1}}F_{\theta}(X_{t},t;K)+\frac{\sqrt{1-\alpha_{t-1}-\sigma_{t}^{2}}}{\sqrt{1-\alpha_{t}}}\Big{(}X_{t}-\sqrt{\alpha_{ t}}F_{\theta}(X_{t},t;K)\Big{)}+\sigma_{t}Z",where \(\sigma_{t}\) is the covariance value of Gaussian distribution at time step \(t\),"X_{t-1}=\mu_{\theta}(X_{t},t)+\sigma_{\theta}(X_{t},t)Z',\quad Z'\simN(0,I)",where \(\sigma_{t}\) is the covariance value of Gaussian distribution at time step \(t\)
ICLR_2024_oral_14,4,"F^{*}=F_{\theta}(D_{src},D_{tgt})\approx\underset{F }{argmax}\\underbrace{\log p(D_{src},D_{tgt}|F)}_{data term}","where \(\mathcal{F}_{\theta}(\cdot)\) and \(\theta\) represent a feed-forward network and its parameters, respectively","F^{*}=\operatorname*{argmax}_{F}\log p(D_{src},D_{tgt}|F)+F_{\theta}(F;\theta)","where \(\mathcal{F}_{\theta}(\cdot)\) and \(\theta\) represent a feed-forward network and its parameters, respectively"
ICLR_2024_oral_14,5,"F^{*}=F_{\theta}(D_{src},D_{tgt})\approx\underset{F}{argmax}\p(F|D_{src},D_{tgt}) || =\underset{F}{argmax}\{\underbrace{\log p(D_{ src},D_{tgt}|F)}_{data term}+\underbrace{\log p(F)}_{prior term}\}",,"F^{*}=\underset{F}{argmax}_{F}p(F|D_{src},D_{tgt})=\underset{F}{argmax}_{F}\left\{\underbrace{p(D_{src},D_{tgt}|F)}_{data term}\cdot\underbrace{p(F)}_{prior term}\right\}",Objective function for learning both the data and prior terms in the conditional generative model for dense correspondence.
ICLR_2024_oral_14,6,"F_{t}=\sqrt{\alpha_{t}}F_{0}+\sqrt{1-\alpha_{t}}Z,\quad Z\simN(0,I)",where \(F_{0}\) is the ground-truth correspondence,"F_{t}=\sqrt{\alpha_{t}}F_{0}+\sqrt{1-\alpha_{t}}Z,\quad Z\simN(0,I)",where \(F_{0}\) is the ground-truth correspondence
ICLR_2024_oral_14,7,"F_{t-1}=\sqrt{\alpha_{t-1}}F_{\theta}(X_{t},t;D_{src},D_{tgt})+\frac{\sqrt{1-\alpha_{t-1}-\sigma_{t}^{2}}}{\sqrt{1-\alpha_{t }}}\Big{(}X_{t}-\sqrt{\alpha_{t}}F_{\theta}(F_{t},t;D_{src },D_{tgt})\Big{)}+\sigma_{t}Z","where \(\mathcal{F}_{\theta}(F_{t},t;D_{\mathrm{src}},D_{\mathrm{tgt}})\) directly predicts the denoised correspondence \(\hat{F}_{0,t}\) with source and target features, \(D_{\mathrm{src}}\) and \(D_{\mathrm{tgt}}\), as conditions","F_{t-1}=\sqrt{\alpha_{t-1}}F_{\theta}(F_{t},t;D_{src},D_{tgt})+\frac{\sqrt{1-\alpha_{t-1}-\sigma_{t}^{2}}}{\sqrt{1-\alpha_{t}}}\Big{(}F_{t}-\sqrt{\alpha_{t}}F_{\theta}(F_{t},t;D_{src},D_{tgt})\Big{)}+\sigma_{t}Z",where \(\sigma_{t}\) is the covariance value of Gaussian distribution at time step \(t\)
ICLR_2024_oral_14,8,"C(i,j)=\frac{D_{src}(i)\cdot D_{tgt}(j)}{\|D_{src}(i)\|\|D_{tgt}(j)\|}","where \(i\in[0,h_{\text{src}})\times[0,w_{\text{src}})\), \(j\in[0,h_{\text{tgt}})\times[0,w_{\text{tgt}})\), and \(\|\cdot\|\) denotes \(l\)-2 normalization","C_{i,j}=\frac{D_{src}(i)\cdot D_{tgt}(j)}{\|D_{src}(i)\|\|D_{tgt}(j)\|},\quad i\in[0,h_{src})\times[0,w_{src}),\j\in[0,h_{tgt})\times[0,w_{tgt})","where \(C_{i,j}\) is the matching cost, \(D_{\text{src}}(i)\) and \(D_{\text{tgt}}(j)\) are the feature descriptors at locations \(i\) and \(j\), respectively, and \(\|\cdot\|\) denotes \(l\)-2 normalization."
ICLR_2024_oral_14,9,"L=E_{F_{0},t,Z\simN(0,I),D_{src},D_{tgt}}[\|F_{0}-F_{\theta}(F_{t},t;F_{init },C^{l})\|^{2}]",,"L=E_{t}\left[\|\hat{F}_{0,t}-F_{0}\|^{2}\right]+\lambdaE_{t}\left[\|F_{t}-F_{0}\|^{2}\right]","where \(\mathcal{L}\) denotes the loss function, \(\lambda\) is a weighting factor, and \(\hat{F}_{0,t}\) is the denoised matching field at time step \(t\)"
ICLR_2024_oral_15,1,"\nabla_{\Theta}L_{SDS}=E_{t,p,\epsilon}[w(t)(\epsilon_{\phi}(I_{RGB}^{p};t,\tilde{I}_{RGB}^{r},\Delta p)-\epsilon)\frac{\partial I_{RGB}^{p}}{\partial\Theta}]","where \(w(t)\) is a weighting function, \(\epsilon_{\phi}(\cdot)\) is the predicted noise by the 2D diffusion prior \(\phi\), and \(\Delta p\) is the relative camera pose change from the reference camera \(r\)","L_{SDS}=E_{t,\tilde{I}_{RGB}^{r},\tilde{I}_{A}^{r}}\left[ w(t)\left\|\epsilon_{\phi}(\tilde{I}_{RGB}^{r},\tilde{I}_{A}^{r})-\epsilon\right\|^{2}\right]","where \(w(t)\) is a weighting function, \(\epsilon_{\phi}(\cdot)\) is the predicted noise by the 2D diffusion prior \(\phi\), and \(\Delta p\) is the relative camera pose change from the reference camera \(r\)."
ICLR_2024_oral_15,2,L_{Ref}=\lambda_{RGB} || I_{RGB}^{r}-\tilde{I}_ {RGB}^{r} || _{2}^{2}+\lambda_{A} || I_{A}^{r}-\tilde{I}_{A}^{r} || _{2}^{2},where \(\lambda_{\text{RGB}}\) and \(\lambda_{\text{A}}\) are the weights which are linearly increased during training,"L_{align}=\lambda_{RGB}L_{RGB}(I_{RGB}^{r}, I_{RGB}^{p})+\lambda_{A}L_{A}(I_{A}^{r}, I_{A}^{p})","where \(\mathcal{L}_{\text{RGB}}\) and \(\mathcal{L}_{\text{A}}\) are the loss functions for the RGB image and transparency, respectively."
ICLR_2024_oral_15,3,"\nabla_{\Theta}L_{SDS}=E_{t,p,\epsilon}[w(t)(\epsilon_{\phi}(I_{RGB}^{p};t,e)-\epsilon)\frac{\partial I_{RGB }^{p}}{\partial\Theta}]",where \(e\) is the CLIP embeddings of the input text description,"\nabla_{\Theta}L_{SDS}^{text}=E_{t,p,\epsilon}[w(t)(\epsilon_{\phi}(I_{RGB}^{p};t,e)-\epsilon)\frac{\partial I_{RGB}^{p}}{\partial\Theta}]",where \(e\) is the CLIP embeddings of the input text description
ICLR_2024_oral_15,4,d(x)=\sum_{i}\alpha_{i}\exp(-\frac{1}{2}(x-x_{i})^ {T}\Sigma_{i}^{-1}(x-x_{i})),where \(\Sigma_{i}\) is the covariance matrix built from scaling \(\mathbf{s}_{i}\) and rotation \(\mathbf{q}_{i}\),"\alpha(x)=\sum_{i}\alpha_{i}\cdotN(x;x_{i},\Sigma_{i})",where \(\Sigma_{i}\) is the covariance matrix built from scaling \(\mathbf{s}_{i}\) and rotation \(\mathbf{q}_{i}\)
ICLR_2024_oral_15,5,"I^{p}_{fine}=f_{\phi}(I^{p}_{coarse}+\epsilon(t_{start}) ;t_{start},c)","where \(\epsilon(t_{\text{start}})\) is a random noise at timestep \(t_{\text{start}}\), \(c\) is \(\Delta p\) for image-to-3D and \(e\) for text-to-3D respectively","I^{p}_{refined}=f_{\phi}(I^{p}_{coarse}+\epsilon(t_{start}),c,e)","where \(\epsilon(t_{\text{start}})\) is a random noise at timestep \(t_{\text{start}}\), \(c\) is \(\Delta p\) for image-to-3D and \(e\) for text-to-3D respectively"
ICLR_2024_oral_15,6,L_{MSE}= || I^{p}_{fine}-I^{p}_{coarse} || ^{2}_{2},,L_{Texture}= || I^{p}_{fine}-I^{p}_{coarse} || _{2}^{2},where \(I^{p}_{\text{fine}}\) is the refined image and \(I^{p}_{\text{coarse}}\) is the initial coarse texture image.
ICLR_2024_oral_16,1,"H(x_{t})=\{{cc}\max\{H(\hat{x}_{t}),R_{t}(s_{t},a_{t })\},&if\ || \hat{x}_{t}-x_{t} || _{2}<\delta\\R_{t}(s_{t},a_{t}),&otherwise\","where \(R_{t}(s_{t},\mathbf{a_{t}})\) is the return of a given \((s_{t},\mathbf{a_{t}})\); \(\delta\) is a threshold value of state-embedding difference; and \(\hat{x}_{t}=f_{\phi}(\hat{s}_{t})\) is \(x_{t}=f_{\phi}(s_{t})\)'s nearest neighbor in \(\mathcal{D}_{E}\)","H(x_{t})\leftarrow R_{t}(s_{t},a_{t}) &if  H(x_{t}) < R_{t}(s_{t},a_{t})\\H(x_{t}) &if \|x_{t}-\hat{x}_{t}\|\leq\delta","where \(R_{t}(s_{t},\mathbf{a_{t}})\) is the return of a given \((s_{t},\mathbf{a_{t}})\); \(\delta\) is a threshold value of state-embedding difference; and \(\hat{x}_{t}=f_{\phi}(\hat{s}_{t})\) is \(x_{t}=f_{\phi}(s_{t})\)'s nearest neighbor in \(\mathcal{D}_{E}\)"
ICLR_2024_oral_16,2,"Q_{EC}(f_{\phi}(s_{t}),a_{t})=r_{t}(s_{t},a_{t})+\gamma H(f_{\phi}(s_ {t+1}))",,"Q_{EC}(f_{\phi}(s_{t}),a_{t})=R_{t}(s_{t},a_{t})+\gamma H(x_{t+1})","where \(Q_{EC}(f_{\phi}(s_{t}),\mathbf{a_{t}})\) is the one-step TD memory target for the action \(\mathbf{a_{t}}\) given the embedded state \(f_{\phi}(s_{t})\)."
ICLR_2024_oral_16,3,"L_{\theta}^{EC}=(y(s,a)-Q_{tot}(s,a;\theta))^{2}+\lambda(Q_{EC}(f_{\phi}(s),a)-Q_{tot}(s,a;\theta))^{2}","where \(y(s,\mathbf{a})\) is one-step TD target; \(Q_{tot}\) is the joint Q-value function parameterized by \(\theta\); and \(\lambda\) is a scale factor","L_{\theta}^{EC}=E_{s_{t},a_{t}}\left[\left(y(s_{t},a_{t})-Q_{EC}(f_{\phi}(s_{t}),a_{t})\right)^{2}\right]+\lambdaE_{s_{t},a_{t}}\left[\left(Q_{tot}(s_{t},a_{t})-H(f_{\phi}(s_{t}))\right)^{2}\right]","where \(y(s,\mathbf{a})\) is one-step TD target; \(Q_{tot}\) is the joint Q-value function parameterized by \(\theta\); and \(\lambda\) is a scale factor."
ICLR_2024_oral_16,4,"L(\phi,\psi)=(H_{t}-f_{\psi}(f_{\phi}(s_{t})))^ {2}",,"L_{\phi,\psi}=E_{s_{t}}[(H_{t}-\bar{H}_{t})^{2}]","where \(\mathcal{L}_{\phi,\psi}\) is the loss function for training the embedding functions \(f_{\phi}\) and \(f_{\psi}\); \(H_{t}\) is the highest return of the global state \(s_{t}\); and \(\bar{H}_{t}\) is the predicted highest return from the decoder structure."
ICLR_2024_oral_16,5,"L(\phi,\psi)=(H_{t}-f_{\psi}^{H}(f_{\phi}(s_{t}|t)|t ))^{2}+\lambda_{rcon} || s_{t}-f_{\psi}^{s}(f_{\phi}(s_{t}|t)|t) || _{2}^{2}",where \(f_{\psi}^{H}\) predicts the highest return; \(f_{\psi}^{s}\) reconstructs \(s_{t}\); \(\lambda_{rcon}\) is a scale factor,L_{dCAE}=(H_{t}-f_{\psi}^{H}(f_{\phi}(s_{t})))^{2}+\lambda_{rcon}(s_{t}-f_{\psi}^{s}(f_{\phi}(s_{t})))^{2},where \(f_{\psi}^{H}\) predicts the highest return; \(f_{\psi}^{s}\) reconstructs \(s_{t}\); \(\lambda_{rcon}\) is a scale factor.
ICLR_2024_oral_16,6,"\eta^{*}(s^{\prime}):=V^{*}(s^{\prime})-\max_{a^{\prime}}\!Q_{\theta^{-}} (s^{\prime},a^{\prime})",,"\hat{\eta}(s^{\prime})=V^{*}(s^{\prime})-\max_{a^{\prime}}Q_{\theta^{-}}(s^{\prime},a^{\prime})","where \(V^{*}(s^{\prime})\) is the true value of state \(s^{\prime}\); \(Q_{\theta^{-}}(s^{\prime},\mathbf{a}^{\prime})\) is the predicted Q-value for action \(\mathbf{a}^{\prime}\) at state \(s^{\prime}\) using the target network."
ICLR_2024_oral_16,7,"r^{p}=\gamma\hat{\eta}(s^{\prime})=\gammaE_{\pi_{\theta}}[\eta(s^{\prime})]\simeq\gamma\frac{N_{\xi}(s^{\prime})}{N_{call}(s^{\prime})}\eta_{\max}(s^{\prime})=\gamma\frac{N_{\xi}(s^{\prime})}{N_{call}(s^{\prime})}(H(f_ {\phi}(s^{\prime}))-\max_{a^{\prime}}\!Q_{\theta^{-}}(s^{\prime},a^{\prime}))",where \(N_{call}(s^{\prime})\) is the number of visits on \(\hat{x}^{\prime}=\mathrm{NN}(f_{\phi}(s^{\prime}))\in\mathcal{D}_{E}\); and \(N_{\xi}\) is the number of desirable transition from \(\hat{x}^{\prime}\),r^{p}=\gamma\hat{\eta}(s^{\prime})=\gamma\frac{N_{call}(s^{\prime})}{N_{\xi}}H(f_{\phi}(s^{\prime})),where \(N_{call}(s^{\prime})\) is the number of visits on \(\hat{x}^{\prime}=\mathrm{NN}(f_{\phi}(s^{\prime}))\in\mathcal{D}_{E}\); and \(N_{\xi}\) is the number of desirable transitions from \(\hat{x}^{\prime}\).
ICLR_2024_oral_16,8,"L_{\theta}^{p}=(r(s,a)+r^{p}+\gamma\!\max_{a^{\prime}}\!Q_{\theta^{-} }(s^{\prime},a^{\prime})-Q_{\theta}(s,a))^{2}",,"L_{\theta}=(y(s,a)+r^{p}-Q_{tot}(s,a;\theta))^{2}","where \(y(s,\mathbf{a})\) is the one-step TD target; \(r^{p}\) is the episodic incentive; and \(Q_{tot}\) is the joint Q-value function parameterized by \(\theta\)."
ICLR_2024_oral_16,9,"\nabla_{\theta}L_{\theta}^{p}=-2\nabla_{\theta}Q_{\theta}(s,a)(\Delta\varepsilon _{TD}+r^{p})=-2\nabla_{\theta}Q_{\theta}(s,a)(\Delta\varepsilon_{TD}+\gamma\frac{N_{\xi}(s^{\prime})}{N_{call}(s^{\prime})}\eta_{\max}(s^{\prime}))",,"\nabla_{\theta}L_{\theta}^{p}=\nabla_{\theta}(r(s,a)+\gamma\hat{\eta}(s^{\prime})+\gamma\max_{a^{\prime}}Q_{\theta^{-}}(s^{\prime},a^{\prime})-Q_{\theta}(s,a))","where \(r(s,\mathbf{a})\) is the immediate reward; \(\hat{\eta}(s^{\prime})\) is the estimated desirability of the next state; and \(Q_{\theta^{-}}(s^{\prime},\mathbf{a}^{\prime})\) is the target Q-value for the next state-action pair."
ICLR_2024_oral_16,10,"L_{\theta}^{p}=(r(s,a)+r^{p}+\beta_{c}r^{c}+\gamma max_{a^{\prime}}Q_{tot}(s^{\prime},a^{\prime};\theta^{-})-Q_{tot}(s,a;\theta))^{2}",where \(\beta_{c}\) is a scale factor,"L_{\theta}^{p}=(r(s,a)+r^{p}+\gamma\!\max_{a^{\prime}}\!Q_{\theta^{-} }(s^{\prime},a^{\prime})-Q_{\theta}(s,a))^{2}+\beta_{c}L_{intrinsic}",where \(\beta_{c}\) is a scale factor.
ICLR_2024_oral_21,1,"g(G)=\operatorname*{arg\,max}_{c\in\{1,2,\cdots,C\}}N_{c}",where a label with a smaller index is taken by our ensemble classifier when there are ties,"g=\arg\min_{c}\{N_{c}\mid c\in\{1,2,\cdots,C\}\}\",where a label with a smaller index is taken by our ensemble classifier when there are ties
ICLR_2024_oral_21,2,"N_{c}-M\leq N_{c}^{p}\leq N_{c}+M,\,\forall c\in\{1,2,\cdots,C\}",,"N_{c}^{p}=\sum_{t=1}^{N}I(f(G_{t}^{p})=c),\forall c\in\{1,2,\cdots,C\}",where \(N_{c}^{p}\) measures the number of sub-graphs created from \(G^{p}\) that are predicted as the class \(c\) by the base graph classifier \(f\).
ICLR_2024_oral_21,3,"M\leq M^{p}=\lfloor\frac{N_{l}-\max_{c\in\{1,2,\cdots,C\}\setminus\{l\}}(N_{c }-I(l<c))}{2}\rfloor",,"N_{l}^{p}>\max_{c\in\{1,2,\cdots,C\}\setminus\{l\}}(N_{c}^{p}-I(l<c))",where \(N_{l}^{p}\) is the number of sub-graphs predicted as label \(l\) from the perturbed graph \(G^{p}\).
ICLR_2024_oral_23,1,"p_{\sigma}(y)=\int p(y|x)\,p(x)\,dx=\int g_{\sigma}(y-x)\,p(x)\,dx",where \(g_{\sigma}(z)\) is the density of \(z\),"p_{\sigma}(y)=\int p(x) g_{\sigma}(y-x)\, dx",where \(g_{\sigma}(z)\) is the density of \(z\)
ICLR_2024_oral_23,2,"D_{KL}(p(x)\,\|\,p_{\theta}(x))\leq\int_{0}^{\infty}E_{y}\big{[}\|\nabla\log p_{\sigma}(y)-s_{\theta}(y)\|^{2}\big{]}\,\sigma\, d\sigma",,"D_{KL}(p(x)\| p_{\theta}(x))=\int\left(\nabla\log p_{\sigma}(y)-s_{\theta}(y)\right)^{2} p_{\sigma}(y)\,dy",where \(D_{\text{KL}}\) denotes the Kullback-Leibler divergence.
ICLR_2024_oral_23,3,"\nabla\log p_{\sigma}(y)=(\mathop{E}_{x}[x\,|\,y]-y)/\sigma^{2}",,\nabla\log p_{\sigma}(y)=E_{x|y}\left[\nabla\log p(x|y)\right],The gradient of the log density \(p_{\sigma}(y)\) is equal to the expected value of the gradient of the log posterior \(p(x|y)\) given \(y\).
ICLR_2024_oral_23,4,"MSE(f_{\theta},\sigma^{2})=\mathop{E}_{x,y}\Bigl{[}\|x-f_{\theta}(y)\|^{2}\Bigr{]}",,"L(\theta)=E_{y,x}\left[\|f_{\theta}(y)-x\|^{2}\right]",The loss function \(\mathcal{L}(\theta)\) represents the mean squared error between the denoised output \(f_{\theta}(y)\) and the clean image \(x\).
ICLR_2024_oral_23,5,"D_{KL}(p(x)\ || \,p_{\theta}(x))\leq\int_{0}^{\infty}\Bigl{(} MSE(f_{\theta},\sigma^{2})-MSE(f^{\star},\sigma^{2})\Bigr{)}\,\sigma^{-3}\,d\sigma","where \(f^{\star}(y)=\mathop{\mathbb{E}}_{x}[x\,|\,y]\) is the optimal denoiser","D_{opt}(f_{\theta},\sigma^{2})\leq\int_{0}^{\infty}E_{y}\Bigl{[}\|f_{\theta}(y)-f^{\star}(y)\|^{2}\Bigr{]}\,\sigma\,d\sigma","where \(D_{\mathrm{opt}}(f_{\theta},\sigma^{2})\) is the optimality gap of the denoiser across noise levels."
ICLR_2024_oral_23,6,"f(y)=\nabla f(y)\,y=\sum_{k}\lambda_{k}(y)\langle y,e_{k}(y)\rangle e_{ k}(y)",,\hat{x}(y)=f(y)=\sum_{k=1}^{d} e_{k}(y)\cdot\lambda_{k}(y)\cdot\hat{x}_{k}(y),where \(\hat{x}_{k}(y)\) are the coefficients in the adaptive basis formed by the eigenvectors of the Jacobian.
ICLR_2024_oral_23,7,"MSE(f,\sigma^{2})=\mathop{E}_{y}\![2\sigma^{2}\, tr\,\nabla f(y)+\|y-f(y)\|^{2}-\sigma^{2}d]",,"f(y)=\nabla f(y)\,y=\sum_{k}\lambda_{k}(y)\langle y,e_{k}(y)\rangle e_{ k}(y)","The denoiser mapping expressed in terms of the Jacobian eigendecomposition, where \(\lambda_{k}(y)\) are the eigenvalues and \(e_{k}(y)\) are the eigenvectors of the Jacobian."
ICLR_2024_oral_23,8,"f^{\star}(y)=y+\sigma^{2}\nabla\log p_{\sigma}(y)=\operatorname*{E}_{ x}[x|y] || \nabla f^{\star}(y)=Id+\sigma^{2}\nabla^{2}\log p_{\sigma}(y)=\sigma^ {-2}Cov[x\,|\,y]",,"f^{\star}(y)=\nabla f^{\star}(y)\,y=\sum_{k}\lambda_{k}^{\star}(y)\langle y,e_{k}^{\star}(y)\rangle e_{k}^{\star}(y)","where \(f^{\star}(y)\) is the optimal denoiser, \(\lambda_{k}^{\star}(y)\) are the optimal eigenvalues, and \(e_{k}^{\star}(y)\) are the optimal eigenvectors."
ICLR_2024_oral_23,9,"MSE(f^{\star},\sigma^{2})=\operatorname*{E}_{y}[tr \,Cov[x\,|\,y]]=\sigma^{2}\operatorname*{E}_{y}\bigl{[}tr\,\nabla f^{\star}(y)\bigr{]}=\sigma^{2}\operatorname*{E} _{y}\biggl{[}\sum_{k}\lambda_{k}^{\star}(y)\biggr{]}",,D_{opt}(y)=\mathop{E}_{x}[\|x-f^{\star}(y)\|^{2} | y]=\sigma^{2}tr\nabla f^{\star}(y)-\sigma^{2}d,"where \(D_{\mathrm{opt}}(y)\) is the optimal denoising error, and \(\mathrm{tr}\) denotes the trace of the Jacobian."
ICLR_2024_oral_23,10,"\operatorname*{E}_{x}\biggl{[}\sum_{k}\Bigl{(}(1-\lambda_{k}(x))^{2}\langle x,e_{k}\rangle^{2}+\lambda_{k}(x)^{2}\sigma^{2}\Bigr{)}\biggr{]}",,"MSE(f^{\star},\sigma^{2})\geq\mathop{E}_{y}\left[\sum_{k}\lambda_{k}^{\star}\right]",where \(\lambda_{k}^{\star}\) are the optimal shrinkage factors in the fixed basis
ICLR_2024_oral_23,11,"\sigma^{2}{\sum_{k}\lambda_{k}(x)}=\sum_{k}\frac{\sigma^{2}\langle x,e_{k}\rangle^{2}}{\langle x,e_{k}\rangle^{2}+\sigma^{2}}\sim\sum_{k}\min(\langle x,e_{k}\rangle^{2},\sigma^{2})=M\sigma^{2}+\|x-x_{M}\|^{2}","where \(x_{M}=\sum_{\langle x,e_{k}\rangle^{2}>\sigma^{2}}\langle x,e_{k}\rangle\,e_{k}\) is the \(M\)-term approximation of \(x\) with the \(M\) basis coefficients \(\langle x,e_{k}\rangle\) above the noise level, and \(\sim\) means that the two terms are of the same order up to multiplicative constants (here smaller than 2)","\operatorname*{E}_{x}\biggl{[}\sum_{k}\Bigl{(}(1-\lambda_{k}(x))^{2}\langle x,e_{k}\rangle^{2}+\lambda_{k}(x)^{2}\sigma^{2}\Bigr{)}\biggr{]}","where \(x_{M}=\sum_{\langle x,e_{k}\rangle^{2}>\sigma^{2}}\langle x,e_{k}\rangle\,e_{k}\) is the \(M\)-term approximation of \(x\) with the \(M\) basis coefficients \(\langle x,e_{k}\rangle\) above the noise level, and \(\sim\) means that the two terms are of the same order up to multiplicative constants (here smaller than 2)."
ICLR_2024_oral_23,12,M\sigma^{2}+\|x-x_{M}\|^{2}\sim\sigma^{2\alpha/(\alpha+1)},,"MSE(f^{\star},\sigma^{2})\sim\sigma^{2}\sum_{k}\min\left(\langle x,e_{k}\rangle^{2},\sigma^{2}\right).\",the denoising error behaves asymptotically as a function of the noise level
ICLR_2024_oral_24,1,dx_{t}=f_{t}(x_{t})dt+g(t)d w_{t}\quadx_{0}\sim p_{data}(x),,"\frac{dx(t)}{dt}=f(x(t), t)+g(t)z(t)","where \(\mathbf{x}(t)\) is the state of the system at time \(t\), \(f\) is the drift function, \(g\) is the diffusion coefficient, and \(\mathbf{z}(t)\) is a standard Wiener process."
ICLR_2024_oral_24,2,"dx_{t}=[f_{t}(x_{t})-g_{t}^{2}\nabla_{ x}\log p(x_{t},t)]dt+g(t)dw_{t},\quadx_{1}\simN(0,I_{d})","where \(p(\cdot,t)\) is the marginal density of (1) at time \(t\) and \(\nabla_{\mathbf{x}}\log p_{t}\) is known as the score function","dx_{t}=-f_{t}(x_{t})dt+\sigma(t)dw_{t}\quadx_{1}\simN(0,I_{d})","where \(p(\cdot,t)\) is the marginal density of (1) at time \(t\) and \(\nabla_{\mathbf{x}}\log p_{t}\) is known as the score function"
ICLR_2024_oral_24,3,"dx_{t}=[f_{t}(x_{t})-\frac{1}{2}g_{t}^{2}\nabla_{x}\log p(x_{t},t)]dt,\quad x_{1}\simN(0,I_{d})",,"dx_{t}=[f_{t}(x_{t})-g_{t}^{2}\nabla_{x}\log p(x_{t},t)]dt+g(t)dw_{t},\quadx_{1}\simN(0,I_{d})","where \(\mathbb{E}_{\mathbf{x}_{t},t}\|\mathbf{s}_{t}^{\theta}(\mathbf{x}_{t},t;\theta )-\nabla_{\mathbf{x}}\log p(\mathbf{x}_{t},t|\mathbf{x}_{0})\|_{2}^{2}\) represents the scaled Stein Score used for learning the score function."
ICLR_2024_oral_24,4,"dx_{t}=v_{t}(x,t)dt+g_{t}dw_{t}\quad s.t.\quad(x_{0},x_{1})\sim\Pi_{0,1}(x_ {0},x_{1}):=p_{0}\times p_{1}",,"dx_{t}=[f_{t}(x_{t})-g_{t}^{2}\nabla_{x}\log p_{0}(x_{t})]dt+g(t)dw_{t},\quadx_{0}\sim p_{0}(x)",where \(p_{0}(\cdot)\) is the initial distribution and \(\nabla_{\mathbf{x}}\log p_{0}(\mathbf{x}_{t})\) is the score function associated with the initial distribution.
ICLR_2024_oral_24,5,"\min_{a_{t}}\int_{\tau}^{1}\lVerta_{t}\rVert_{2}^{2}dt+(m_{1}-m_{1})^{T} R(m_{1}-m_{1})& s.t\underbrace{dx_{t}\\dv_{t}}_{dm_{t}}&=v_{t}\\a_{t}(x_{t},v_{t},t)dt+\underbrace{0&0\\0&g_{t}}_{g_{t}}dw_{t},\\m_{\tau}:=x_{\tau}\\v_{\tau}&=x_{\tau}\\v_{\tau},&R=r&0\\0&r\otimesI_{d},&x_{1}\sim p_{data}",,"dx_{t}=v_{t}(x,t)dt+g_{t}dw_{t},\quadv_{t}(x,t)=\frac{x_{1}-x_{t}}{1-t}","where \(\mathbf{v}_{t}(\mathbf{x},t)\) is the drift term representing the velocity of the trajectory, and \(g_{t}\) is the diffusion coefficient."
ICLR_2024_oral_24,6,"a^{*}(m_{t},t)=g_{t}^{2}P_{11}(\frac{x_{1}-x_{t}}{1-t}-v_{t})\quadwhere:\quad P_{11}=\frac{-4}{g_{t}^{2}(t-1)}",,"m_{t}=x_{t}+v_{t}(t-\tau),\quadv_{t}=\frac{x_{1}-x_{t}}{1-t}","The solution to the optimization problem 5, representing the trajectory in the phase space as a function of time, where \(\mathbf{m}_{t}\) is the state at time \(t\) and \(\mathbf{v}_{t}\) is the velocity at time \(t\)."
ICLR_2024_oral_24,7,"dx_{t}\\dv_{t}=v_{t}\\F_{t}dt+0&0\\0&h_{t}dw_{t}\quads.t\quadm_{0}:=x_{0}\\v_{0}\simN(\mu_{0},\Sigma_{0}) || Bridge Matching SDE:F_{t}:=F_{t}^{b}(m_{t},t)\equiva_{t}^{*}(m_{t},t), h(t):=g(t) || Probabilistic ODE:F_{t}:=F_{t}^{p}(m_{t},t)\equiva_{t}^{*}(m_{t},t)-\frac{1}{2}g_{t}^{2 }\nabla_{v}\log p(m,t), h(t):=0",,"dm_{t}=v_{t}\\a^{*}(m_{t},t)dt+0\\g_{t}dw_{t},\quadm_{1}\sim p_{data}","where \(\mathbf{m}_{t} = \begin{bmatrix} \mathbf{x}_{t} \\ \mathbf{v}_{t} \end{bmatrix}\) represents the state vector comprising position and velocity, and \(p_{\text{data}}\) is the target data distribution."
ICLR_2024_oral_24,8,"m_{t}=\mu_{t}+L_{t}\epsilon=\mu_{t}+L_{t}^{xx}\epsilon_{0}\\L_{t}^{xv}\epsilon_{0}+L_{t}^{vv}\epsilon_{1},\nabla_{v}\log p_{t}:=-\ell_{t}\epsilon_{1}","where \(\boldsymbol{\Sigma}_{t}=\mathbf{L}_{t}\mathbf{L}_{t}^{\mathsf{T}}\), \(\epsilon=\begin{bmatrix}\boldsymbol{\epsilon}_{0}\\ \boldsymbol{\epsilon}_{1}\end{bmatrix}\sim\mathcal{N}(\mathbf{0},\mathbf{I}_{2d})\) and \(\ell_{t}=\sqrt{\frac{\Sigma_{t}^{xx}}{\Sigma_{t}^{xx}\Sigma_{t}^{xx}-(\Sigma _{t}^{xx})^{2}}}\)","m_{t}=\mu_{t}+L_{t}\epsilon,\quad\Sigma_{t}=\Sigma_{t}^{xx}&\Sigma_{t}^{xx}\\\Sigma_{t}^{xx}&\Sigma_{t}^{xx}\otimesI_{d},\quad\epsilon=\epsilon_{0}\\\epsilon_{1}\simN(0,I_{2d}),\quad\ell_{t}=\sqrt{\frac{\Sigma_{t}^{xx}}{\Sigma_{t}^{xx}\Sigma_{t}^{xx}-(\Sigma_{t}^{xx})^{2}}}","where \(\boldsymbol{\Sigma}_{t}=\mathbf{L}_{t}\mathbf{L}_{t}^{\mathsf{T}}\), \(\epsilon=\begin{bmatrix}\boldsymbol{\epsilon}_{0}\\ \boldsymbol{\epsilon}_{1}\end{bmatrix}\sim\mathcal{N}(\mathbf{0},\mathbf{I}_{2d})\) and \(\ell_{t}=\sqrt{\frac{\Sigma_{t}^{xx}}{\Sigma_{t}^{xx}\Sigma_{t}^{xx}-(\Sigma_{t}^{xx})^{2}}}\)"
ICLR_2024_oral_24,9,"a^{*}(m_{t},t)=4x_{1}(1-t)^{2}-g_{t}^{2}P_{11}[ (\frac{L_{t}^{xx}}{1-t}+L_{t}^{xv})\epsilon_{0}+L_{t}^{vv}\epsilon_{1}]",,"F_{t}=F_{t}^{d}(m_{t},t)+F_{t}^{g}(m_{t},t),\quadF_{t}^{d}(m_{t},t)=a_{t}^{*}(m_{t},t),\quadF_{t}^{g}(m_{t},t)=-\frac{1}{2}g_{t}^{2}\nabla_{v}\log p(m_{t},t)","where \(\mathbf{F}_{t}\) is the force term, \(\mathbf{F}_{t}^{d}\) represents the deterministic component, and \(\mathbf{F}_{t}^{g}\) denotes the stochastic component influenced by the score function."
ICLR_2024_oral_24,10,"\min_{\theta}E_{t\in[0,1]}E_{x_{1}\sim p_{ data}}E_{m_{t}\sim p_{t}(m_{t}|x_{1})}\lambda(t)[\|F_{t}^{\theta}(m_{t},t;\theta)-F _{t}(m_{t},t)\|_{2}^{2}]",Where \(\lambda(t)\) is known as the reweight of the objective function across the time horizon,"L(s_{t}^{\theta},z_{t})=E_{m_{t},t}\left[\lambda(t)\|F_{t}^{\theta}-F_{t}\|_{2}^{2}\right]",where \(\lambda(t)\) is known as the reweight of the objective function across the time horizon.
ICLR_2024_oral_24,11,"x_{t_{i+1}}\\v_{t_{i+1}}=\Phi(t_{i+1},t_{i})x _{t}\\v_{t}+\sum_{j=0}^{w}\int_{t_{i}}^{t_{i+1} }(t_{i+1}-\tau)z_{\tau}\cdotM_{i,j}(\tau) d\tau\s_{t}^{\theta}(m_{t_{i-j}},t_{i-j}))\\\int_{t_{i}}^{t_{i+1}}z_{\tau}\cdotM_{i,j}(\tau) d\tau\cdots_{t}^{\theta}(m_{t_{i-j}},t_{i-j})",,"m_{t}=\mu_{t}+L_{t}\epsilon=\mu_{t}+L_{t}^{xx}\epsilon_{0}\\L_{t}^{xv}\epsilon_{0}+L_{t}^{vv}\epsilon_{1},\nabla_{v}\log p_{t}:=-\ell_{t}\epsilon_{1}","where \(\boldsymbol{\Sigma}_{t}=\mathbf{L}_{t}\mathbf{L}_{t}^{\mathsf{T}}\), \(\epsilon=\begin{bmatrix}\boldsymbol{\epsilon}_{0}\\ \boldsymbol{\epsilon}_{1}\end{bmatrix}\sim\mathcal{N}(\mathbf{0},\mathbf{I}_{2d})\) and \(\ell_{t}=\sqrt{\frac{\Sigma_{t}^{xx}}{\Sigma_{t}^{xx}\Sigma_{t}^{xx}-(\Sigma _{t}^{xx})^{2}}}\)"
ICLR_2024_oral_24,12,"\tilde{x}_{1}^{SDE}=\frac{(1-t)(F_{t}^{\theta}+v_{t })}{g_{t}^{2}P_{11}}+x_{t}, or \quad\tilde{x}_{1}^{ODE}=\frac{F_{t}^{\theta}+g_{t}^{2}P_{ 11}(\alpha_{t}x_{t}+\beta_{t}v_{t})}{4(t-1)^{2}+g_{t}^{2}P_{ 11}(\alpha_{t}\mu_{t}^{x}+\beta_{t}\mu_{t}^{v})}",,"\tilde{x}_{1}=x_{t}+(1-t)F_{t}^{\theta}(m_{t},t)_{t}^{\theta}\) is the parameterized force term,\(x_{t}\) is the state at time\(t\), and\(t\) is the time variable.</description>","where \(\mathbf{F}_{t}^{\theta}\) is the parameterized force term, \(\mathbf{x}_{t}\) is the state at time \(t\), and \(t\) is the time variable."
ICLR_2024_oral_25,1,"\underbrace{\nu(x)>0,\\\forall x\inInterior(M_{o})}_{Case 1: inside the open surface},\quad\underbrace{\nu(x)=0,\\\forall x\in\partialM_{o}}_{Case 2: on the surface boundary},\quad\underbrace{\nu(x)<0,\\Otherwise}_{ Case 3: outside the open surface}",where \(\nu\) can be instantiated as the signed geodesic distance to the open surface boundary living on the watertight template,\nu:M\rightarrowR,where \(\nu\) can be instantiated as the signed geodesic distance to the open surface boundary living on the watertight template
ICLR_2024_oral_25,2,L_{mSDF-reg}(\theta_{mSDF})=\underbrace{\sum_{u:v_{\theta_{ ndSF}}(u)\geq 0}L_{nuber}(v_{\theta_{noster}}(u))}_{ Encourage hole opening}+\underbrace{\tau\cdot\sum_{{c}u^{\prime}:v_{\theta_{ ndSF}}(u^{\prime})=0\\u^{\prime} visible from some q\in Q}L_{nuber}(v_{\theta_{n ober}}(u^{\prime})-\epsilon)}_{Regularize holes from being too large},,"L_{hole-opening}=\sum_{x\inM_{o}}\max(0,-\nu(x))","where \(\mathcal{L}_{\text{hole-opening}}\) is the loss function for regularizing the mSDF values, and \(x\) represents points on the open surface \(\mathcal{M}_{o}\)."
ICLR_2024_oral_25,3,"u^{\prime}=\frac{|\nu_{1}|}{|\nu_{1}-\nu_{2}|}\cdot u_{2}-\frac{|\nu_{2}|}{|\nu_{1}-\nu_{2}|}\cdot u_{1},\quad\nu_{1}<0<\nu_{2}",,\nu^{\prime}=\frac{\nu_{i}+\nu_{j}}{2},"where \(\nu^{\prime}\) is the interpolated mSDF value at the extracted mesh vertex, and \(\nu_{i}\) and \(\nu_{j}\) are the mSDF values at the grid cell vertices."
ICLR_2024_oral_26,1,E=({cccc}W^{(l)\top}&W^{(2)\top}&W^{(l)\top}&W^{(l)\top}\\\hline\ddots&W^{(2)\top}&W^{(2)\top}&W ^{(l)\top}\\),where \(\mathbf{\mathrm{W}}^{(l)\top}\) denotes the transposed weight matrix,"V=b^{(1)}\\b^{(2)}\\\vdots\\b^{(L)} ,\quadE=W^{(1)}\\W^{(2)}\\\vdots\\W^{(L)}",Node feature matrix \(\mathbf{V}\) containing biases and edge feature matrix \(\mathbf{E}\) containing weights for each layer of the MLP.
ICLR_2024_oral_26,2,"V_{probe}=\Big{(}x,\alpha\Big{(}W^{(1)}x+b^{(1)}\Big{)},f(x)\Big{)}^{\top}",,V_{extra}=W^{(1)}x+b^{(1)}\\W^{(2)}\alpha\big{(}W^{(1)}x+b^{(1)}\big{)}+b^{(2)},Extra node feature for the input neural network capturing intermediate activations.
ICLR_2024_oral_26,3,"e_{ij}^{(k+1)}=\phi_{e}^{(k+1)}\Big{(}\Big{[}v_{i}^{(k)},e_{ij}^{(k)},v_{j}^{(k)}\Big{]}\Big{)}",where \(k\) is the layer index in our network,E^{(k)}=\phi_{e}\left(E^{(k-1)}\right),where \(k\) is the layer index in our network
ICLR_2024_oral_26,4,"m_{ij}=\phi_{scale}(e_{ij})\odot\phi_{m}([v_ {i},v_{j}])+\phi_{shift}(e_{ij})",,m_{ij}^{(k)}=e_{ij}^{(k)}\cdotv_{i}^{(k)},"where \(\mathbf{v}_{i}^{(k)}\) and \(\mathbf{e}_{ij}^{(k)}\) are the node and edge features at layer \(k\), respectively."
ICLR_2024_oral_26,5,v_{ij}=(W_{scale}^{value}e_{ij} )\odot(W_{n}^{value}v_{j})+ W_{shift}^{value}e_{ij},,v_{j}^{(k+1)}=\phi_{v}^{(k+1)}\Big{(}v_{j}^{(k)}+\sum_{i}m_{ij}\cdote_{ij}^{(k)}\Big{)},where \(\mathbf{v}_{j}\) is the value matrix and \(\mathbf{e}_{ij}\) is the edge feature connecting nodes \(i\) and \(j\)
ICLR_2024_oral_27,1,\sum_{i=1}^{N_{k}}W(A_{i}^{k})=W(\bigcup_{i=1}^{N_{k}}A_{i}^{k})=W(A^{0}),,"\int_{A_{i}^{k}} W(A)\, d\nu\simN\left(0,\nu(A_{i}^{k})\right)",Integral of the white Gaussian noise over the subset \(A_{i}^{k}\) with variance \(\nu(A_{i}^{k})\).
ICLR_2024_oral_27,2,"(W(A^{k})|W(A^{0})=x)\simN(\bar{\mu},\bar{\Sigma}),\quadwith \bar{\mu}=\frac{x}{N_{k}}u,\bar{\Sigma}=\frac{1}{N_{k}} (I_{N_{k}}-\frac{1}{N_{k}}uu^{\top})","where \(\mathbf{u}=(1,\ldots,1)^{\top}\)","\left(W(A^{k})|W(A^{0})=x\right)\simN\left(\frac{x}{N_{k}}u,\frac{1}{N_{k}}I\right)","where \(\mathbf{u}=(1,\ldots,1)^{\top}\)"
ICLR_2024_oral_27,3,"(W(A^{k})|W(A^{0})=x)=\bar{\mu}+UZ=\frac{x}{N_{k}} u+\frac{1}{\sqrt{N_{k}}}(Z-\langle Z\rangleu),\quadwith Z\sim(0,I)",where \(\langle Z\rangle\) is the mean of \(Z\),"W(A^{k})=\bar{\mu}+UZ,\quad Z\simN(0,I_{N_{k}})",where \(\langle Z\rangle\) is the mean of \(Z\)
ICLR_2024_oral_27,4,T(W)(A)=\int_{x\in A}\frac{1}{|\nablaT(T^{-1}(x))|^{\frac{1}{2}}}W(T^{-1}(x))\dx,where \(\left|\nabla\mathcal{T}\right|\) is the determinant of the Jacobian of \(\mathcal{T}\),T(W(A))=\int_{A}W(x)\left|\nablaT(x)\right|dx,where \(\left|\nabla\mathcal{T}\right|\) is the determinant of the Jacobian of \(\mathcal{T}\)
ICLR_2024_oral_27,5,G(p)=\frac{1}{\sqrt{|\Omega_{p}|}}\sum_{A_{s}^{k}\in\Omega_ {p}}W_{k}(A_{s}^{k})\,,W(A^{k})=\frac{1}{|A|}\sum_{i=1}^{N_{k}}W(A_{i}^{k})I_{inside}(A_{i}^{k}),where \(|A|\) is the area of the original pixel and \(\mathbb{I}_{\text{inside}}(A_{i}^{k})\) is an indicator function that checks if the sub-pixel \(A_{i}^{k}\) is inside the warped polygon.
ICLR_2024_oral_27,6,"z_{i}=\alpha x_{i-1}+(1-\alpha)x_{i},\quad z_{i}\simN(0,\,\sigma_{z }^{2}),\quadwith \sigma_{z}^{2}=\alpha^{2}+(1-\alpha)^{2}",,G(p)=\frac{1}{\sqrt{|\Omega_{p}|}}\sum_{A_{s}^{k}\in\Omega_ {p}}W_{k}(A_{s}^{k})\,the discrete noise transport for the noise pixel at position \(\mathbf{p}\)
ICLR_2024_oral_27,7,"z_{i}|x_{i},x_{i-1}\simN(\mu_{\infty},\sigma_{\infty}^{2}),\quadwith \{{l}\mu_{\infty}=\alpha x_{i-1}+(1-\alpha)x_{i}\\\sigma_{\infty}^{2}=1-(\alpha^{2}+(1-\alpha)^{2})=1-\sigma_{z}^{2}",,"z|x_{i-1},x_{i}\simN\left(\frac{x_{i-1}+x_{i}}{2},\frac{1}{2}\right)",the conditional distribution of \(z\) given \(x_{i-1}\) and \(x_{i}\)
ICLR_2024_oral_28,1,\lVertA\hat{x}^{\star}-b\rVert_{2}^{2}\leq(1+\epsilon)\lVertAx^{\star}-b\rVert_{2}^{2},,"\hat{x}^{\star}=\operatorname*{arg\,min}_{x}\lVertAx-\hat{b}\rVert_{2}^{2}",Estimated optimal model parameters using queried target values.
ICLR_2024_oral_28,2,\tau_{i}=\|u_{i}\|_{2}^{2}=a_{i}^{T}(A^{T} A)^{-1}a_{i}=\max_{x\inR^{d}}(a_{i}^{T}x)^{2}/\|Ax\|_{2}^{2},,\tau_{i}=\lVertu_{i}\rVert_{2}^{2}=a_{i}A^{+}a_{i}^{\top},Leverage score of the \(i\)-th row in matrix \(\mathbf{A}\).
ICLR_2024_oral_28,3,\|A\tilde{x}^{*}-b\|_{2}^{2}\leq(1+\epsilon)\|Ax^{*}-b\|_{2}^{2},,k\geq c\cdot\left(d\log d\cdot D_{inf}^{2}+\frac{d}{\epsilon}\cdot D_{inf}\right),"where \(k\) is the number of sampled rows, \(c\) is a fixed positive constant, \(d\) is the number of features, \(\epsilon\) is the error parameter, and \(D_{\text{inf}}\) is the parameter related to one-sided \(\ell_{\infty}\) independence."
ICLR_2024_oral_28,4,\|\tilde{p}-b\|_{2}^{2}\leq(1+\epsilon)\underset{degree d polynomial p}{\min}\|p-b\|_{2}^{2},,\|\tilde{p}-b\|_{2}^{2}\leq(1+\epsilon)\|p^{\star}-b\|_{2}^{2},where \(p^{\star}\) is the optimal degree \(d\) polynomial approximation to \(b\).
ICLR_2024_oral_28,5,"I_{\mu}^{S}(i,j)=\Pr_{\mu}[\xi_{j}=1|\xi_{i}=1\wedge\xi_{\ell}=1\forall\ell\inS]-\Pr_{\mu}[\xi_{j}=1|\xi_{\ell}=1\forall\ell\inS]",,\|\tilde{p}_{i}\|_{2}^{2}\leq(1+\epsilon)\|p\|_{2}^{2},where \(\|\tilde{p}_{i}\|_{2}^{2}\) denotes the average squared magnitude of the sampled probabilities and \(\|\mathbf{p}\|_{2}^{2}\) denotes the average squared magnitude of the original probabilities.
ICLR_2024_oral_29,1,"\frac{dx}{d\sigma}=-\sigma\nabla_{x}\log p _{\sigma}(x)\quad\sigma\in[\sigma_{min},\sigma_{max}]",where the term \(\nabla_{\mathbf{x}}\log p_{\sigma}(\mathbf{x})\) is known as the _score function_ of \(p_{\sigma}(\mathbf{x})\)(Song et al,"\frac{dx}{dt}=f(x, t)=\nabla_{x}\log p_{\sigma}(x)","where \(\mathbf{f}(\mathbf{x}, t)\) represents the flow of the data distribution over time."
ICLR_2024_oral_29,2,"f_{\theta}(x,\sigma)=c_{skip}(\sigma)x+c_{out}(\sigma)F_{\theta}(x,\sigma)","where \(\mathbf{F}_{\mathbf{\theta}}(\mathbf{x},\sigma)\) is a free-form neural network, while \(c_{\text{skip}}(\sigma)\) and \(c_{\text{out}}(\sigma)\) are differentiable functions such that \(c_{\text{skip}}(\sigma_{\text{min}})=1\) and \(c_{\text{out}}(\sigma_{\text{min}})=0\)","f_{\theta}(x,\sigma)=F_{\theta}(x,\sigma)+c_{skip}(\sigma)(x-F_{\theta}(x,\sigma))\quadwith\quad c_{skip}(\sigma_{min})=1,\; c_{out}(\sigma_{min})=0","where \(\mathbf{F}_{\mathbf{\theta}}(\mathbf{x},\sigma)\) is a free-form neural network, while \(c_{\text{skip}}(\sigma)\) and \(c_{\text{out}}(\sigma)\) are differentiable functions."
ICLR_2024_oral_29,3,"L^{N}(\theta,\theta^{-})=E[\lambda(\sigma_{i} )d(f_{\theta}(x_{\sigma_{i+1}},\sigma_{i+1}),f_{\theta}_{-}(\tilde{x}_{\sigma_{i}},\sigma_{i}))]",where \(\tilde{\mathbf{x}}_{\sigma_{i}}=\mathbf{x}_{\sigma_{i+1}}-(\sigma_{i}-\sigma_{ i+1})\sigma_{i+1}\nabla_{\mathbf{x}}\log p_{\sigma_{i+1}}(\mathbf{x})|_{\mathbf{x}= \mathbf{x}_{\sigma_{i+1}}}\),"L_{CM}(\theta)=\frac{1}{N-1}\sum_{i=1}^{N-1}\left\|\tilde{x}_{\sigma_{i}}-f_{\theta}(x_{\sigma_{i}},\sigma_{i})\right\|^{2}",where \(\tilde{\mathbf{x}}_{\sigma_{i}}=\mathbf{x}_{\sigma_{i+1}}-(\sigma_{i}-\sigma_{i+1})\sigma_{i+1}\nabla_{\mathbf{x}}\log p_{\sigma_{i+1}}(\mathbf{x})|_{\mathbf{x}=\mathbf{x}_{\sigma_{i+1}}}\)
ICLR_2024_oral_29,4,\theta^{-}arrowstopgrad(\mu\theta^{-}+(1-\mu)\theta),,"\theta^{-}=\alpha\theta^{-}+(1-\alpha)\theta,\quad\alpha\in(0,1)",where \(\alpha\) is the decay rate for the Exponential Moving Average (EMA).
ICLR_2024_oral_29,5,"L_{CT}^{N}(\theta,\theta^{-})=E[\lambda(\sigma_{i})d(f_{\theta}(x+\sigma_{i+1}z,\sigma_ {i+1}),f_{\theta^{-}}(x+\sigma_{i}z,\sigma_{i}))]",,"L^{CT}(\theta)=E[\lambda(\sigma_{i})d(f_{\theta}(\tilde{x}_{\sigma_{i}},\sigma_{i}),f_{\theta}_{-}(\tilde{x}_{\sigma_{i}},\sigma_{i}))]",where \(\tilde{\mathbf{x}}_{\sigma_{i}}=\mathbf{x}+\sigma_{i}\mathbf{z}\) is the approximation used in consistency training.
ICLR_2024_oral_29,6,"\lim_{N\to\infty}L^{N}(\theta,\theta^{-})=\lim_{N\to\infty}L^{N}_{CT}(\theta,\theta^{-})=E\Big{[}\big{(} 1-\frac{\sigma_{min}}{\sigma_{i}}\big{)}^{2}(\theta-\theta^{-})^{2}\Big{]}\quadif\theta^{-}\neq\theta || \lim_{N\to\infty}\frac{1}{\Delta\sigma}\frac{d L^{N}(\theta,\theta^{-})}{d\theta}=\{{ll}\frac{dL}{d\theta}E\Big{[}\frac{\sigma_ {min}}{\sigma_{i}^{2}}\Big{(}1-\frac{\sigma_{min}}{\sigma_{i}}\Big{)}(\theta-\xi)^{2}\Big{]},&\theta^{-}=\theta\\+\infty,&\theta^{-}\prec\theta\\-\infty,&\theta^{-}>\theta",,"\lim_{N\to\infty}L^{N}(\theta,\theta^{-})\neqL_{CT}^{N}(\theta,\theta^{-})","where \(\mathcal{L}^{N}(\mathbf{\theta},\mathbf{\theta}^{-})\) is the consistency matching loss and \(\mathcal{L}_{\mathrm{CT}}^{N}(\mathbf{\theta},\mathbf{\theta}^{-})\) is the consistency training loss."
ICLR_2024_oral_29,7,"d(x,y)=\sqrt{\lVertx-y\rVert_{2}^{2}+c^{2}}-c",,"d(x,y)=\sqrt{\|x-y\|_{2}^{2}+c^{2}}-c,\end{latex> <description>where\(c\) is a small constant that controls the transition between the squared loss and absolute loss.</description>",where \(c\) is a small constant that controls the transition between the squared loss and absolute loss.
ICLR_2024_oral_29,8,"N(k)=\min(s_{0}2^{\lfloor\frac{1}{N}\rfloor},s_{1})+1,\quad K^{\prime}=\Big{|}\frac{K}{\log_{2}[s_{1}/s_{0}]+1}\Big{|}",,"N(k)=\min(s_{0}2^{\frac{k}{K}},s_{1})+1,\quad k\in[0,K]",where \(s_{0}\) and \(s_{1}\) are parameters controlling the growth of \(N\) and \(K\) is the total training iterations.
ICLR_2024_oral_3,1,"\theta^{*}\in\operatorname*{arg\,min}_{\theta\inR^{d}}f(\theta),\\\where f(\theta)\triangleqE_{X\sim\mu}[F(\theta,X) ]=\sum_{i\inN}\mu_{i}F(\theta,i)",,"f(x)=E_{\xi\simD}[F(x,\xi)]","where \( f(\mathbf{x}) \) is the expected value of the loss function \( F(\mathbf{x}, \xi) \) over the random variable \( \xi \) drawn from the distribution \( \mathcal{D} \)."
ICLR_2024_oral_3,2,"\theta_{n+1}=\theta_{n}+\beta_{n+1}H(\theta_{n},X_{n+1}),\\\\forall\n\geq 0","where, roughly speaking, \(H(\mathbf{\theta},i)\) contains gradient information \(\nabla_{\mathbf{\theta}}F(\theta,i)\), such that \(\mathbf{\theta}^{*}\) solves \(\mathbf{h}(\mathbf{\theta})\triangleq\mathbb{E}_{X\sim\mathbf{\mu}}[H(\mathbf{\theta},X)]= \sum_{i\in\mathcal{N}}\mu_{i}H(\mathbf{\theta},i)=\mathbf{0}\)","h(\theta)\triangleqE_{X\sim\mu}[H(\theta,X)]=\sum_{i\inN}\mu_{i}H(\theta,i)=0","where \(H(\mathbf{\theta},i)\) contains gradient information \(\nabla_{\mathbf{\theta}}F(\theta,i)\)."
ICLR_2024_oral_3,3,"K_{ij}[x]\triangleq\frac{P_{ij}(x_{j}/\mu_{j})^{-\alpha}}{\sum_{k\inN}P_{ik}(x_{k}/\mu_{k})^{-\alpha}},\quad\quad\forall\,i,j\in N",,"K[x]=P+\alpha\left(I-P\right)\operatorname*{diag}(x)1^{\top},\\\where \alpha>0,\\\I is the identity matrix, and 1 is a vector of ones.","where \(\mathbf{K}[\mathbf{x}]\) is the non-linear transition kernel, \(\mathbf{P}\) is the original transition kernel, \(\alpha\) is a positive scalar controlling the strength of the self-repellence behavior, \(\mathbf{I}\) is the identity matrix, and \(\mathbf{1}\) is a vector of ones."
ICLR_2024_oral_3,4,"Draw:\qquad X_{n+1}\simK_{X_{n},\cdot}[x_{n}]",,"\theta_{n+1}=\theta_{n}+\beta_{n+1}H(\theta_{n},X_{n+1})+z_{n+1},\\\\forall\n\geq 0",where \(\mathbf{z}_{n+1}\) represents the noise introduced by the Self Repellent Random Walk (SRRW) process.
ICLR_2024_oral_3,5,"\quadx_{n+1}=x_{n}+\gamma_{n+1}(\delta_{X_{n+1}}-x_{n}) || \quad\theta_{n+1}=\theta_{n}+\beta_{n+1}H(\theta_{n},X_{ n+1})",,"\theta_{n+1}=\theta_{n}+\beta_{n+1}H(\theta_{n},X_{n+1}),\\\\forall\n\geq 0","where \(H(\mathbf{\theta},X)\) is the gradient information at the new sample point \(X_{n+1}\) under the SRRW framework."
ICLR_2024_oral_3,6,"\theta_{n}\xrightarrow[n\to\infty]{a.s.}\theta^{*}\quadand\quad(\theta_{n}-\theta^{*})/\sqrt{\beta_{n}}\xrightarrow[n\to\infty]{dist.}N(0,V_{\theta}^{(k)}(\alpha))",,"\lim_{n\to\infty}\frac{\|\theta_{n}-\theta^{*}\|}{\|x_{n}-\mu\|}=O\left(\frac{1}{\alpha}\right),\quadfor  k=1,2,3","where \(\boldsymbol{\theta}_{n}\) is the iterate of the optimization algorithm, \(\boldsymbol{\theta}^{*}\) is the optimal parameter, \(\mathbf{x}_{n}\) is the empirical distribution, and \(\boldsymbol{\mu}\) is the target distribution."
ICLR_2024_oral_3,7,"x_{n}=\frac{\sum_{i=1}^{n}\omega_{i}\theta_{X_{i}}+\omega_{0}x_{0}}{\sum_{i=0}^{n}\omega_{i}},\\where\\\omega_{0}=1,\\and\\\omega_{n}=\frac{\gamma_{n}}{\prod_{i=1}^{n}(1-\gamma_{i})}",,"V_{\theta}^{(k)}(\alpha)\triangleqE[(\theta_{n}-\theta^{*})(\theta_{n}-\theta^{*})^{T}],\quad k=1,2,3","where \(\mathbf{V}_{\boldsymbol{\theta}}^{(k)}(\alpha)\) represents the asymptotic covariance matrices for the three cases of convergence in the SA-SRRW algorithm, with \(k\) indicating the respective case."
ICLR_2024_oral_3,8,"x_{n}\xrightarrow[n\to\infty]{a.s.}\mu,\quadand\quad\gamma_{n}^{-1/2}(x_{n}-\mu)\xrightarrow[n\to\infty]{dist.}N(0,V_{x}(\alpha)) || where\quadV_{x}(\alpha)=\sum_{i=1}^{N-1 }\frac{1}{2\alpha(1+\lambda_{i})+2-\mathds{1}_{\{a=1\}}}\cdot\frac{1+\lambda_ {i}}{1-\lambda_{i}}u_{i}u_{i}^{T}",,"x_{n}=\frac{\sum_{i=1}^{n}\omega_{i}\theta_{X_{i}}+\omega_{0}x_{0}}{\sum_{i=0}^{n}\omega_{i}},\\where\\\omega_{0}=1,\\and\\\omega_{n}=\frac{\gamma_{n}}{\prod_{i=1}^{n}(1-\gamma_{i})}",for all \(n>0\).
ICLR_2024_oral_3,9,\tfrac{d}{dt}z(t)=g(z(t))\triangleqH(\theta(t))^{T}\pi[x(t)]\\\pi[x(t)]-x(t)\inR^{D+N},"where matrix \(\mathbf{H}(\boldsymbol{\theta})\triangleq[H(\boldsymbol{\theta},1),\cdot,H( \boldsymbol{\theta},N)]^{T}\in\mathbb{R}^{N\times D}\) for any \(\boldsymbol{\theta}\in\mathbb{R}^{D}\)","H(\theta)\triangleq[H(\theta,1),\cdots,H(\theta,N)]^{T}",where matrix \(\mathbf{H}(\boldsymbol{\theta})\) is constructed from the function \(H\) evaluated at each node \(i\) in the set \(\mathcal{N}\).
ICLR_2024_oral_3,10,J(\alpha)\!\triangleq\!\nabla g(z^{*})\!=\!\nablah(\theta^{*})&-\alphaH(\theta^{*})^{T}(P^{T}\!+\mathds{I})\\0_{N\!\times\!D}&2\alpha\mu\mathds{1}^{T}\!-\!\alpha\mathds{P}^{T}\!\!-\!(\alpha\!+\!1)\mathds{I}\!\triangleq\!J_{11}&J_{12}(\alpha)\\J_{21}&J_{22}(\alpha),,J(\alpha)\triangleq\nabla_{\theta}H(\theta^{*}) &0\\0 &-I .}H(\theta^{*})\) is the gradient of\(H\) evaluated at the equilibrium\(\theta^{*}\) and\(I\) is the identity matrix.</description>,where \(\nabla_{\boldsymbol{\theta}}\mathbf{H}(\boldsymbol{\theta}^{*})\) is the gradient of \(\mathbf{H}\) evaluated at the equilibrium \(\boldsymbol{\theta}^{*}\) and \(\mathbf{I}\) is the identity matrix.
ICLR_2024_oral_3,11,"\beta_{n}^{-1/2}(\theta_{n}-\theta^{*} )\\\gamma_{n}^{-1/2}(x_{n}-\mu)\xrightarrow[n\to\infty]{ait.}N(0,V^{(k)}(\alpha))",,J(\alpha)\triangleq\nabla g(z^{*})=\nablah(\theta^{*})&-\alphaH(\theta^{*})^{T}(P^{T}+\mathds{I})\\0_{N\times D}&2\alpha\mu\mathds{1}^{T}-\alpha\mathds{P}^{T}-(\alpha+1)\mathds{I},"The Jacobian matrix \(\mathbf{J}(\alpha)\) evaluated at equilibria \(\mathbf{z}^{*}=(\boldsymbol{\theta}^{*},\boldsymbol{\mu})\) captures the behavior of solutions of the mean-field ODE."
ICLR_2024_oral_3,12,U\triangleq\sum_{i=1}^{N-1}\frac{1+\lambda_{i}}{1-\lambda_{i}}\cdotH^{T}u_{i}u_{i}^{T}H&H^{T}u_{i}u_{i}^{T}\\u_{i}u_{i}^{T}H&u_{i}u_{i}^{T}\triangleqU_{11}&U_{12}\\U_{21}&U_{22},,"V^{(1)}(\alpha) and V^{(3)}(\alpha) can be derived similarly, capturing the effects of the different step sizes and the parameter \alpha","where \(\mathbf{V}^{(1)}(\alpha)\) and \(\mathbf{V}^{(3)}(\alpha)\) represent the asymptotic covariance matrices for cases (i) and (iii), respectively, and their derivations follow analogous principles as that of \(\mathbf{V}^{(2)}(\alpha)\)."
ICLR_2024_oral_3,13,V^{(k)}(\alpha)=V^{(k)}_{\theta }(\alpha)&0_{D\times\!N}\\0_{N\!\times\!D}&V_{x}(\alpha),,"V^{(1)}(\alpha)=V_{\theta}^{(1)}(\alpha)&0\\0&V_{x}(\alpha),\quadV^{(3)}(\alpha)=V_{\theta}^{(3)}(\alpha)&0\\0&V_{x}(0)","where \(\mathbf{V}_{\boldsymbol{\theta}}^{(1)}(\alpha)\) and \(\mathbf{V}_{\boldsymbol{\theta}}^{(3)}(\alpha)\) are distinct asymptotic covariance matrices corresponding to cases (i) and (iii), respectively."
ICLR_2024_oral_31,1,P(z;H)=\mathds{1}(H(z)<\bar{H}),where \(\bar{\mathcal{H}}\) is a threshold and \(\mathds{1}(\cdot)\) is indicator function,P(z;H)=\mathds{1}(H(z) <\bar{H}),where \(\bar{\mathcal{H}}\) is a threshold and \(\mathds{1}(\cdot)\) is the indicator function
ICLR_2024_oral_31,2,P_{t}(z)=P(z;H_{t}),,P(z;H_{t})=\mathds{1}(H_{t}(z) <\bar{H}),where \(\mathcal{H}_{t}(z)\) is the score at time step \(t\) and \(\bar{\mathcal{H}}\) is a threshold.
ICLR_2024_oral_31,3,"P_{t}(z)=r,&H_{t}(z)<\bar{H}_{t}\\0,&H_{t}(z)\geq\bar{H}_{t}","where \(\bar{\mathcal{H}}_{t}\) is the mean value of all the scores \(\mathcal{H}_{t}\) and \(r\in(0,1)\) is a predefined hyper-parameter as the pruning probability",P_{t}(z)=\mathds{1}(H_{t}(z) < r\cdot\bar{H}_{t}),"where \(\bar{\mathcal{H}}_{t}\) is the mean value of all the scores \(\mathcal{H}_{t}\) and \(r\in(0,1)\) is a predefined hyper-parameter as the pruning probability."
ICLR_2024_oral_31,4,"H_{t+1}(z)=H_{t}(z),&z\inD\backslashS_{t}\\L(z),&z\inS_{t}",,H_{t+1}(z)=L(z),where \(\mathcal{L}(z)\) is the loss value of sample \(z\) at epoch \(t\).
ICLR_2024_oral_31,5,"\operatorname*{arg\,min}_{\theta\in\Theta}\mathop{E}_{z\inD }[L(z,\theta)]=\int_{z}L(z,\theta)\rho(z)dz",,L=E_{z\sim\rho(z)}[L(z)],where \(\mathcal{L}(z)\) is the loss function for each sample \(z\) and \(\rho(z)\) is the underlying distribution of the dataset.
ICLR_2024_oral_31,6,"\operatorname*{arg\,min}_{\theta\in\Theta}\mathop{E}_{z\inS _{t}}[\gamma_{t}(z)L(z,\theta)]=\operatorname*{arg\,min}_{\theta\in\Theta}\frac{1}{c_{t}}\int_{z}L(z,\theta)\rho(z)dz","where \(c_{t}=\mathbb{E}_{z\sim\rho}[1-\mathcal{P}_{t}(z)]=\int_{z}\rho(z)(1-\mathcal{ P}_{t}(z))dz\), \(c_{t}\in(0,1)\) is a constant for temporal status \(t\)","\operatorname*{arg\,min}_{\theta\in\Theta}\mathop{E}_{z\inS_{t}}[\gamma_{t}(z)L(z,\theta)]=c_{t}\int_{z}L(z,\theta)\rho(z)dz","where \(c_{t}=\mathbb{E}_{z\sim\rho}[1-\mathcal{P}_{t}(z)]=\int_{z}\rho(z)(1-\mathcal{P}_{t}(z))dz\), \(c_{t}\in(0,1)\) is a constant for temporal status \(t\)"
ICLR_2024_oral_31,7,\frac{1}{c_{t}}=\frac{|D|}{|\sum_{z\inD}(1-P_{t}( z))|}\simeq\frac{|D|}{|S_{t}|}\RightarrowE[\nabla_{\theta}L(S_{t})]\simeq\frac{|D|}{|S_{t}|}\mathop{E}[\nabla_{\theta}L(D)],,"N=|D|,\).</description>",where \(N\) is the total number of samples in the dataset \(\mathcal{D}\).
ICLR_2024_oral_31,8,"P_{t}(z)=r,&H_{t}(z)<\tilde{H}_{t}\wedge t<\delta\cdot C\\0,&H_{t}(z)\geq\tilde{H}_{t}\lor t\geq\delta\cdot C",,"P_{t}(z)=1,& t\geq\delta\cdot C\\P_{t}(z),& t<\delta\cdot C",where \(C\) is the total number of training epochs and \(\delta\) is a hyper-parameter controlling the pruning duration.
ICLR_2024_oral_32,1,"sim(I,t)=\langle M_{image}(I),M_{text}(t)\rangle/( || M_{image}(I) || _{2} || M_{text}(t) || _{2})",,"sim(I, t)=\frac{M_{image}(I)\cdot M_{text}(t)}{\|M_{image}(I)\|\|M_{text}(t)\|}",Similarity between the image \(I\) and text \(t\) computed using cosine similarity.
ICLR_2024_oral_32,2,M_{image}(I)=PViT(I),,M_{image}(I)=P\cdotViT(I),CLIP image representation as a linear projection of the ViT output to a joint vision-and-language space.
ICLR_2024_oral_32,3,"\hat{Z}^{l}=MSA^{l}(Z^{l-1})+Z^{l-1},\quad Z^{l}=MLP^{l}(\hat{ Z}^{l})+\hat{Z}^{l}",,Z^{l}=Z^{l-1}+MSA(Z^{l-1}),"Where \(Z^{l}\) is the updated state after the \(l\)-th layer, and \(\mathrm{MSA}(Z^{l-1})\) represents the multi-head self-attention operation applied to the previous state."
ICLR_2024_oral_32,4,M_{image}(I)=PViT(I)=P[Z^{0}]_{cls}+\underbrace{\sum_{ l=1}^{L}P[MSA^{l}(Z^{l-1})]_{cls}}_{MSA\terms}+\underbrace{\sum_{l=1}^{L}P[MLP^{l}(\hat{Z}^{l}) ]_{cls}}_{MLP\perms},,M_{image}(I)=P\left(\sum_{l=0}^{L} [Z^{l}]_{cls}\right),The image representation \(M_{\text{image}}(I)\) as a linear projection of the sum of the contributions from each layer's class token in the ViT output.
ICLR_2024_oral_32,5,"[MSA^{l}(Z^{l-1})]_{cls}=\sum_{h=1}^{H}\sum_{i=0}^{N}x_{i} ^{l,h},\\\x_{i}^{l,h}=\alpha_{i}^{l,h}W_{VO}^{l,h}z_{i}^{l-1}","where \(W_{VO}^{l,h}\in\mathbb{R}^{d\times d}\) are transition matrices and \(\alpha_{i}^{l,h}\in\mathbb{R}\) are the attention weights from the class token to the \(i\)-th token (\(\sum_{i=0}^{N}\alpha_{i}^{l,h}=1\))","MSA^{l}(Z^{l-1})=\sum_{h=1}^{H}\sum_{i=0}^{N}\alpha_{i}^{l,h}W_{VO}^{l,h}Z^{l-1}_{i}","where \(W_{VO}^{l,h}\in\mathbb{R}^{d\times d}\) are transition matrices and \(\alpha_{i}^{l,h}\in\mathbb{R}\) are the attention weights from the class token to the \(i\)-th token (\(\sum_{i=0}^{N}\alpha_{i}^{l,h}=1\))."
ICLR_2024_oral_32,6,"\sum_{l=1}^{L}P[MSA^{l}(Z^{l-1})]_{cls}=\sum_{l=1}^{L}\sum_{h=1 }^{H}\sum_{i=0}^{N}c_{i,l,h},\\c_{i,l,h}=Px_{i}^{l,h}",,"M_{image}(I)=P[Z^{0}]_{cls}+\underbrace{\sum_{l=1}^{L}P\left(\sum_{h=1}^{H}\sum_{i=0}^{N}x_{i}^{l,h}\right) }_{MSA\terms}+\underbrace{\sum_{l=1}^{L}P[MLP^{l}(\hat{Z}^{l}) ]_{cls}}_{MLP\perms}","where \(x_{i}^{l,h}=\alpha_{i}^{l,h}W_{VO}^{l,h}z_{i}^{l-1}\) are the contributions from the attention heads and tokens."
ICLR_2024_oral_32,7,"V_{explained}(T)=\frac{1}{K}\sum_{k=1}^{K}\| Proj_{T}(c_{k}-c_{avg})\|_{2}^{2}, where c_{avg}=\frac{1}{K}\sum_{k=1}^{K}c_{k}",,Var_{T}(c)=\left\| c-Proj_{T}(c)\right\|^{2},where \(c\) is the head output vector and \(\mathcal{T}\) is the set of text descriptions used for projection.
ICLR_2024_oral_35,1,P_{d}\approx 12lh^{2}+Vh\hskip 28.452756pt(1)\hskip 36.135ptP_{e}\approx(1-\rho)P_{d}+\rho(4h^{2}+8h^{2}N_{e})l,,P_{d}=2h^{2}l+hV,"The parameter count for a dense LLM, where \(h\) is the hidden size, \(l\) is the number of layers, and \(V\) is the vocabulary size."
ICLR_2024_oral_35,2,"L(P,D)=\frac{A}{P^{\alpha}}+\frac{B}{D^{\beta}}+E\hskip 14.226378pt(3)\hskip 28.452756ptTC\approx 6PD\hskip 14.226378pt(4)\hskip 28.452756ptIC\approx 2PD\hskip 14.226378pt(5)",,L=E+A\left(\frac{P}{D}\right)^{\alpha}\hskip 28.452756pt(3),"L: test loss, E: irreducible term, A: fitting constant, P: parameter count, D: training dataset size, α: fitting constant"
ICLR_2024_oral_35,3,eff_{re}=\gamma_{0}\cdot\frac{re}{n}\cdoteff_{n }&re<n\\\gamma_{1}\cdot\frac{n}{re}\cdoteff_{n}+\gamma_{2}&re>n || eff_{re}=\gamma_{0}\cdot\frac{re}{n}\cdoteff_{n }&re<n\\\gamma_{1}\cdot\frac{n}{re}\cdoteff_{n}+\gamma_{2}&re>n,,n=t\cdot p\cdot d\hskip 28.452756pt(6),"The number of devices required to achieve optimal hardware efficiency for dense LLM processing, where \(t\) is tensor parallelism, \(p\) is pipeline parallelism, and \(d\) is the degree of data parallelism."
ICLR_2024_oral_35,4,energy_{hard}=\sum_{i\in hardware\_set}(P_{i}\cdoteff_{i}\cdot n_{i}\cdot t_{i}) || energy_{oper}=energy_{hard}\cdotPUE,,eff_{re}=\gamma_{0}\cdot\frac{re}{n}\cdoteff_{n }&re<n\\\gamma_{1}\cdot\frac{n}{re}\cdoteff_{n}+\gamma_{2}&re>n\hskip 28.452756pt(6),Efficiency with \(re\) devices based on fitting constants and optimal efficiency.
ICLR_2024_oral_35,5,CO2eq_{oper}=energy_{oper}\cdot carb\_int,,time_{dev}=\frac{TFLOP}{FLOP_{peak}\cdoteff}\hskip 14.226378pt(6)\hskip 28.452756ptenergy_{hard}=\sum_{i\in hardware\_set}(P_{i}\cdoteff_{i}\cdot n_{i}\cdot t_{i})\hskip 14.226378pt(7),"Execution time of a device based on FLOP count, hardware efficiency, and peak throughput."
ICLR_2024_oral_35,6,CO2eq_{emb}=\sum_{i\in hardware\_set}\frac{t_{i}\cdotCO2eq_{chip_{i}}}{lifetime_{i}},,energy_{oper}=energy_{hard}\cdotPUE\hskip 14.226378pt(9),Operational energy associated with LLM processing.
ICLR_2024_oral_36,1,"\gamma,\beta=MLP^{mod}(\tilde{c}) || ModLN_{c}(f_{j})=LN(f_{j})\cdot(1+\gamma)+\beta",,ModLN_{c}(f_{j})=f_{j}+c\odotLN(f_{j}),Modulation function that combines the feature vector \(\mathbf{f}_{j}\) with the camera feature \(\mathbf{c}\) using layer normalization.
ICLR_2024_oral_36,2,f_{j}^{cross}=CrossAttn(ModLN_{c}(f_{j}^{in});\{h_{i}\}_{i=1}^{n})+f_{j}^{in} || f_{j}^{self}=SelfAttn(ModLN_{c}(f_{j}^{cross} );\{ModLN_{c}(f_{j}^{cross})\}_{j})+f_{j}^{cross},,"f^{out}_{j}=MLP^{tfm}(SelfAttn(CrossAttn(f^{in}_{j},\{h_{i}\}_{i=1}^{n})))","Output triplane features for the \(j\)-th entry after passing through cross-attention and self-attention layers, followed by a multi-layer perceptron."
ICLR_2024_oral_36,3,f_{j}^{out}=MLP^{tfm}(ModLN_{c}(f_{j}^{self}))+f_{j}^{self},,f_{j}^{out}=MLP^{tfm}(f_{j}^{self})+f_{j}^{self},Output of the multi-layer perceptron sub-layer for the \(j\)-th triplane entry.
ICLR_2024_oral_36,4,"L_{recon}(x)=\frac{1}{V}\sum_{v=1}^{V}( L_{MSE}(\hat{x}_{v},x_{v}^{GT})+\lambdaL_{LPIPS}(\hat{x}_{v},x_{v}^{GT}))","where \(\mathcal{L}_{\mathrm{MSE}}\) is the normalized pixel-wise L2 loss, \(\mathcal{L}_{\mathrm{LPIPS}}\) is the perceptual image patch similarity (Zhang et al","L=L_{MSE}(\hat{x},x^{GT})+\lambdaL_{LPIPS}(\hat{x},x^{GT})","where \(\mathcal{L}_{\mathrm{MSE}}\) is the normalized pixel-wise L2 loss, \(\mathcal{L}_{\mathrm{LPIPS}}\) is the perceptual image patch similarity."
ICLR_2024_oral_37,1,"z_{t}=z_{0}+\int_{0}^{t}f(z_{s},s)ds","where the hidden state \(z_{t}\in\mathbb{R}^{d}\) evolves with certain dynamics characterized by a neural network \(f:\mathbb{R}^{d}\rightarrow\mathbb{R}^{d}\), \(z_{0}\) is the initial state, and \(s\) represents time in integrals","\frac{dz_{t}}{dt}=f(z_{t}, t),\quad z(0)=z_{0}","where \(z_{t}\in\mathbb{R}^{d}\) is the hidden state, \(f:\mathbb{R}^{d}\rightarrow\mathbb{R}^{d}\) is the neural network defining the dynamics, \(z_{0}\) is the initial state, and \(t\) represents time."
ICLR_2024_oral_37,2,"z_{t}=z_{0}+\int_{0}^{t}f(z_{s},s)ds+\int_{0}^{t}g(z_{s},s)dB_{s}","where \(z_{t}\) is a latent state that evolves over time, \(f:\mathbb{R}^{d}\times\mathbb{R}\rightarrow\mathbb{R}^{d}\) is the drift function to capture the evolving dynamics, \(g:\mathbb{R}^{d}\times\mathbb{R}\rightarrow\mathbb{R}^{d\times\omega}\) is the diffusion function to reflect the uncertainties, and \(B_{s}\) is an \(d\)-dimensional Brownian motion (Wiener Process)","dz_{t}=f(z_{t},t)dt+g(z_{t},t)dB_{t}","where \(z_{t}\) is a latent state that evolves over time, \(f:\mathbb{R}^{d}\times\mathbb{R}\rightarrow\mathbb{R}^{d}\) is the drift function to capture the evolving dynamics, \(g:\mathbb{R}^{d}\times\mathbb{R}\rightarrow\mathbb{R}^{d\times\omega}\) is the diffusion function to reflect the uncertainties, and \(B_{s}\) is an \(d\)-dimensional Brownian motion (Wiener Process)."
ICLR_2024_oral_37,3,"\min_{\theta}R_{\nu}(h_{\theta})=\min_{\theta}E_{(z,y)\simD (z_{M+1:M+L},y_{M+1:M+L})}[h_{\theta}(z)\neq y]","where \(\nu\) is the distribution of the stochastic path (Boue & Dupuis, 1998) of \(\mathcal{D}\) along timestamps \(T\) to \(T+T^{*}\), \(z_{M+1:M+L}\) and \(y_{M+1:M+L}\) are short for \(z\) and \(y\) at the timestamps \(\{t_{M+1},\dots,t_{M+L}\}\), \(R_{\nu}\) is the risk of a learning model \(h_{\theta}\) parameterized by parameters \(\theta\)","R_{\nu}=E_{\nu}\left[L(h_{\theta}(z_{M+1:M+L}),y_{M+1:M+L})\right]",where \(\mathcal{L}\) is the loss function measuring the discrepancy between the predicted labels \(h_{\theta}(z_{M+1:M+L})\) and the true labels \(y_{M+1:M+L}\).
ICLR_2024_oral_37,4,"\hat{z}_{i|m+1}^{k}=\underset{z_{i|m+1}^{k}=\underset{z_{i|m+1}^{k}\in S_{m+1}^{k}}{argmin}}Dist(z_{i|m}^{k},z_{j|m+1}^{k})","where \(\text{Dist}:\mathcal{Z}\times\mathcal{Z}\rightarrow[0,+\infty)\) is a distance metric defined over the embedding space, \(\mathbb{S}_{m+1}^{k}\) be the set of \(N_{B}\) data points sampled from \(\mathcal{D}_{m+1}\) (short for \(\mathcal{D}(z,y|t_{m+1})\)) with class \(y=k\in\{1,","z_{m+1}^{k}=\arg\min_{z_{j}\inS_{m+1}^{k}}Dist(z_{m}^{k},z_{j})","where \(\text{Dist}:\mathcal{Z}\times\mathcal{Z}\rightarrow[0,+\infty)\) is a distance metric defined over the embedding space, \(\mathbb{S}_{m+1}^{k}\) be the set of \(N_{B}\) data points sampled from \(\mathcal{D}_{m+1}\) (short for \(\mathcal{D}(z,y|t_{m+1})\)) with class \(y=k\in\{1,\ldots,C\}\)."
ICLR_2024_oral_37,5,"\hat{z}_{i|m+\lambda}=Interp(z_{i|m}^{k},\hat{z}_{i|m+1}^{k},\lambda)=(1-\lambda)z_{i|m}^{k}+\lambda\hat{z}_{i|m+1}^{k},\forall z_{i|m}^{k}\inS_{m}^{k}","where the interpolation rate \(\lambda\in(0,1)\) is sampled from a Beta distribution \(\mathcal{B}(\beta_{1},\beta_{2})\), \(\beta_{1}\) and \(\beta_{2}\) are the parameters of the Beta distribution, and \(\mathbb{S}_{m}^{k}\) consists of instances sampled from \(k\)-th class of \(m\)-th domain","\hat{z}_{i|m+1}^{k}=\lambda z_{i|m}^{k}+(1-\lambda) z_{j|m+1}^{k},\quad\lambda\simB(\beta_{1},\beta_{2})","where the interpolation rate \(\lambda\in(0,1)\) is sampled from a Beta distribution \(\mathcal{B}(\beta_{1},\beta_{2})\), \(\beta_{1}\) and \(\beta_{2}\) are the parameters of the Beta distribution, and \(\mathbb{S}_{m}^{k}\) consists of instances sampled from \(k\)-th class of \(m\)-th domain."
ICLR_2024_oral_37,6,"\hat{z}_{m^{\prime}}^{k}=z_{m}^{k}+\int_{t_{m}}^{t_{m^{\prime}}}f_{k}(\hat{z}_ {s}^{k},s)ds+\int_{t_{m}}^{t_{m^{\prime}}}g_{k}(\hat{z}_{s}^{k},s)dB_{s}","where the latent variable \(\hat{z}_{m^{\prime}}^{k}\) is transformed from \(m\)-th domains latent variable \(z_{m}^{k}\), and \(f_{k}\) is the drift function of the \(k\)-th class to capture the evolving patterns, and \(g_{k}\) is the diffusion function of the \(k\)-th class to characterize the stochastics of the latent representations","\hat{z}_{m^{\prime}}^{k}=z_{m}^{k}+\int_{t_{m}}^{t_{m^{\prime}}}f_{k}(\hat{z}_{s}^{k},s)ds+\int_{t_{m}}^{t_{m^{\prime}}}g_{k}(\hat{z}_{s}^{k},s)dB_{s}","where the latent variable \(\hat{z}_{m^{\prime}}^{k}\) is transformed from \(m\)-th domains latent variable \(z_{m}^{k}\), and \(f_{k}\) is the drift function of the \(k\)-th class to capture the evolving patterns, and \(g_{k}\) is the diffusion function of the \(k\)-th class to characterize the stochastics of the latent representations."
ICLR_2024_oral_37,7,J_{mle}=\sum_{m=1}^{M}\sum_{k=1}^{K}\sum_{i=1}^{N_{B}}-\frac{1}{MKN_ {B}}\Big{(}\logD(z=\hat{z}_{i|m+1}^{k}|z=z_{i|m}^{k})+\logD \big{(}z=\hat{z}_{i|m+\lambda}^{k}|z=z_{i|m}^{k}\big{)}\Big{)},,"L_{Path Alignment}=-E_{(z,\hat{z})\simD(z_{i|m}^{k},\hat{z}_{i|m+\lambda}^{k},\hat{z}_{i|m+1}^{k})}[\log p(\hat{z}|z)","where \(\mathcal{L}_{\text{Path Alignment}}\) is the path alignment loss, \(p(\hat{z}|z)\) is the likelihood of the synthetic feature given the latent representation, and the expectation is taken over the distribution of the aligned samples."
ICLR_2024_oral_37,8,"D(y=k|z,t=t_{m})=\frac{D(z|y=k,t=t_{m})\timesD(y=k|t=t_{m})}{\sum_{k^{\prime}=1}^{K}D(z|y=k^{\prime},t=t_{m})\timesD(y=k^{\prime}|t=t_{m})}","where we model \(\mathcal{D}(z|y=k,t=t_{m})\) with non-parametric model, and \(\mathcal{D}(y|t=t_{m})\) as a neural net with input as timestamp \(t\), function denoted as \(r(t)\)","D(y|t=t_{m})=\frac{D(z|y=k,t=t_{m})D(y=k)}{D(z|t=t_{m})}","where \(\mathcal{D}(z|y=k,t=t_{m})\) is the conditional distribution of the latent variable given the class \(k\) at timestamp \(t_{m}\), \(\mathcal{D}(y=k)\) is the prior distribution of the class \(k\), and \(\mathcal{D}(z|t=t_{m})\) is the marginal distribution of the latent variable at timestamp \(t_{m}\)."
ICLR_2024_oral_37,9,"D(z|y=k,t=t_{m})=\frac{\sum_{\hat{z}_{i}\in\hat{S}_{m}^{k}}- exp(-Dist(z,\hat{z}_{i}))}{|\hat{S}_{m}^{k}|}",where \(\hat{S}_{m}^{k}\) includes instances sample from learned SDE-EDG belong to \(k\)-th class of \(m\)-th domain,"D(z|y=k,t=t_{m})\approx\frac{1}{|\hat{S}_{m}^{k}|}\sum_{z_{i}\in\hat{S}_{m}^{k}}K_h(z-z_{i})",where \(K_h\) is the kernel function with bandwidth \(h\) used for density estimation.
ICLR_2024_oral_37,10,"J_{cls}=\sum_{m=1}^{M}\sum_{k=1}^{K}\sum_{i=1}^{N_{B}}-\frac{1}{MKN _{B}}\logD(y=k|z=z_{i},t=t_{m})",,"L_{pred}=-\sum_{m=1}^{M}\sum_{k=1}^{K}\sum_{i=1}^{N_{B}}\logD(y=k|z=\hat{z}_{i|m}^{k},t=t_{m})","where \(\mathcal{L}_{pred}\) is the prediction loss, and the summation is over all domains \(m\), classes \(k\), and instances \(i\) in the batch."
ICLR_2024_oral_38,1,P_{F}^{\top}(x)\propto\exp(-E(x)),where \(P_{F}^{\top}(x)\) is the distribution of sampling an object \(x\) induced from marginalizing over the trajectories conditioned on \(x=s_{T}\),P_{F}^{\top}(x)=\frac{1}{Z} e^{-E(x)},where \(Z\) is the normalization constant ensuring that the distribution sums to one over all possible objects \(x\).
ICLR_2024_oral_38,2,"L_{DB}(s,s^{\prime})=(\log F(s)+\log P_{F}(s^{\prime}|s)-\log F(s^{\prime})-\log P_{B}(s|s^{\prime}))^{2}","where the flow \(F(s)\) for the terminal state \(s_{T}=x\) is defined to be identical to the exponent of the negative energy \(\exp\left(-\mathcal{E}(x)\right)\), i",L=E_{\tau\sim P_{F}}\left[\sum_{t=0}^{T-1}\log P_{F}(s_{t+1}|s_{t})-\log P_{B}(s_{t}|s_{t+1})\right],where the flow \(F(s)\) for the terminal state \(s_{T}=x\) is defined to be identical to the exponent of the negative energy \(\exp\left(-\mathcal{E}(x)\right)\).
ICLR_2024_oral_38,3,L_{TB}=(\log Z+\sum_{t=0}^{T-1}\log P_{F}(s_{t+1}|s_{t} )-E(x)-\sum_{t=0}^{T-1}\log P_{B}(s_{t}|s_{t+1}))^{2},,"L_{TB}(s,s^{\prime})=(\log F(s)+\log P_{F}(s^{\prime}|s)-\log F(s^{\prime})-\log P_{B}(s|s^{\prime})+Z)^{2}",where \(Z\) is a learnable scalar that aids in minimizing the loss function.
ICLR_2024_oral_38,4,L_{subTB}=(\log F(s_{U})+\sum_{t=U}^{U+L-1}\log P_{F}(s_ {t+1}|s_{t})-\log F(s_{U+L})-\sum_{t=U}^{U+L-1}\log P_{B}(s_{t}|s_{t+1})) ^{2},,L_{subTB}=(\log Z+\sum_{t=U}^{U+L-1}\log P_{F}(s_{t+1}|s_{t})-E(x)-\sum_{t=U}^{U+L-1}\log P_{B}(s_{t}|s_{t+1}))^{2},where \(s_{U}\) to \(s_{U+L}\) represents the sub-trajectory being considered.
ICLR_2024_oral_38,5,"L_{FL}(s,s^{\prime})=(\log\tilde{F}(s)+\log P_{F}(s^{\prime}| s)-E(s)+E(s^{\prime})-\log\tilde{F}(s^{\prime})-\log P _{B}(s|s^{\prime}))^{2}",where \(\tilde{F}(s)=F(s)\exp\left(\mathcal{E}(s)\right)\) is the re-parameterized flow function and \(\mathcal{E}(s^{\prime})-\mathcal{E}(s)\) is the energy gain associated with the transition from \(s\) to \(s^{\prime}\),"L_{FL-GFN}(s,s^{\prime})=(\log\tilde{F}(s)+\log P_{F}(s^{\prime}|s)-\log\tilde{F}(s^{\prime})-\log P_{B}(s|s^{\prime}))^{2}",where \(\tilde{F}(s)=F(s)\exp\left(\mathcal{E}(s)\right)\) is the re-parameterized flow function and \(\mathcal{E}(s^{\prime})-\mathcal{E}(s)\) is the energy gain associated with the transition from \(s\) to \(s^{\prime}\).
ICLR_2024_oral_38,6,E(x)\approx\Phi_{\theta}(\tau)=\sum_{t=0}^{T-1}\phi_{\theta}(s_{t}\to s_{t+1}),"where \(\tau=(s_{0},s_{1},\ldots,s_{T})\), \(x=s_{T}\), and the potential functions are defined on state transition \(s_{t}\to s_{t+1}\)",E(x)=\sum_{t=0}^{T-1}\phi_{\theta}(s_{t}\to s_{t+1}),"where \(\tau=(s_{0},s_{1},\ldots,s_{T})\), \(x=s_{T}\), and the potential functions are defined on state transition \(s_{t}\to s_{t+1}\)."
ICLR_2024_oral_38,7,"L_{LED}(s,s^{\prime})=(\log\tilde{F}(s)+\log P_{F}(s^{\prime} |s)+\phi_{\theta}(s\to s^{\prime})-\log\tilde{F}(s^{\prime})-\log P_{B}(s|s^{\prime}))^{2}",,"L_{LED}(s,s^{\prime})=(\log F(s)+\log P_{F}(s^{\prime}|s)-\Phi_{\theta}(s)-\log F(s^{\prime})-\log P_{B}(s|s^{\prime}))^{2}",where \(\Phi_{\theta}(s)\) is the potential function evaluated at state \(s\).
ICLR_2024_oral_38,8,\ell_{LS}(\tau)=E_{z\simBern(\gamma)} [(\frac{1}{T}E(s_{T})-\frac{1}{C}\sum_{t=0}^{T-1}z_{t}\phi_{\theta}(s_{t}\to s_{t+1}))^{2}],,L_{train}(\tau)=\left(E(x)-\Phi_{\theta}(\tau)\right)^{2}+\lambda\sum_{t=0}^{T-1}\phi_{\theta}(s_{t}\to s_{t+1})^{2},"where \(\mathcal{L}_{\text{train}}\) is the training loss for the potential functions, \(\lambda\) is a regularization parameter, and \(\tau=(s_{0},\ldots,s_{T}=x)\) is the trajectory."
ICLR_2024_oral_39,1,"\epsilon_{\theta}(o_{t}^{(k)},k|h_{t-1},a_{t-1})=(1+\eta)\epsilon_{\theta}(o_{ t}^{(k)},k|h_{t-1},a_{t-1})-\eta\epsilon_{\theta}(o_{t},k|h_{t-1})",where \(\eta\) controls action conditioning strength,"\overline{T}(o_{t}|h_{t-1},a_{t-1})=\epsilon_{\theta}(o_{t}^{(k)},k|h_{t-1},a_{t-1})+\eta\cdot\nabla_{o_{t}^{(k)}}\log p(o_{t}|h_{t-1},a_{t-1})",where \(\eta\) controls action conditioning strength
ICLR_2024_oral_39,2,"L_{MSE}=\|\epsilon-\epsilon_{\theta}\Big{(}\sqrt{1-\beta^{(k)}}o_{t}+\sqrt{\beta^{(k)}}\epsilon,\,k\Big{|}h_{t-1},a_{t-1}\Big{)}\|^{2}","where \(\epsilon\sim\mathcal{N}(0,I)\), and \(\beta^{(k)}\in\mathbb{R}\) are a set of \(K\) different noise levels for each \(k\in[1,K]\)","L(\theta)=E_{o_{t},\epsilon,k}\left[\|\epsilon-\epsilon_{\theta}(o_{t}^{(k)},k|h_{t-1},a_{t-1})\|^{2}\right]","where \(\epsilon\sim\mathcal{N}(0,I)\), and \(\beta^{(k)}\in\mathbb{R}\) are a set of \(K\) different noise levels for each \(k\in[1,K]\)"
ICLR_2024_oral_39,3,"o_{t}^{(k-1)}=\alpha^{(k)}(o_{t}^{(k)}-\gamma^{(k)}\epsilon_{\theta}(o_{t}^{( k)},k|h_{t-1},a_{t-1}))+\xi,\quad\xi\simN\big{(}0,\sigma_{k}^{2}I\big{)}","where \(\gamma^{(k)}\) is the denoising step size, \(\alpha^{(k)}\) is a linear decay on the current denoised sample, and \(\sigma_{k}\) is a time varying noise level that depends on \(\alpha^{(k)}\) and \(\beta^{(k)}\)","o_{t}^{(k-1)}=\frac{1}{\sqrt{\alpha^{(k)}}}\left(o_{t}^{(k)}-\frac{1-\alpha^{(k)}}{\sqrt{1-\beta^{(k)}}}\epsilon_{\theta}(o_{t}^{(k)},k|h_{t-1},a_{t-1})\right)+\sigma_{k}\epsilon,\) is the denoising step size,\(\alpha^{(k)}\) is a linear decay on the current denoised sample, and\(\sigma_{k}\) is a time varying noise level that depends on\(\alpha^{(k)}\) and\(\beta^{(k)}\)</description>","where \(\gamma^{(k)}\) is the denoising step size, \(\alpha^{(k)}\) is a linear decay on the current denoised sample, and \(\sigma_{k}\) is a time varying noise level that depends on \(\alpha^{(k)}\) and \(\beta^{(k)}\)"
ICLR_2024_oral_39,4,RDG=\frac{\|s_{0}-s_{goal}\|_{2}-\|s_{T}-s_{goal}\|_{2}}{\|s_{0}-s_{goal}\|_{2}},"where \(s_{T}\) represents the underlying block locations after executing the policy, \(s_{0}\) and \(s_{\text{goal}}\) represents the initial and goal block locations",RDG=\frac{\|s_{0}-s_{T}\|-\|s_{goal}-s_{T}\|}{\|s_{0}-s_{goal}\|},"where \(s_{T}\) represents the underlying block locations after executing the policy, \(s_{0}\) and \(s_{\text{goal}}\) represents the initial and goal block locations."
ICLR_2024_oral_4,1,p(Y\mid X)=\sum_{Z}p_{LM}(ZY\mid X)=\sum_{Z}p_{LM}(Y\mid XZ)p_{LM}(Z\mid X),where \(p_{\text{LM}}\) denotes the likelihood assigned to a sequence by a language model and apposition of variables (_e,"p(Z | X, Y)=\frac{p_{LM}(X, Z, Y)}{p_{LM}(X, Y)}","where \(p(Z | X, Y)\) is the conditional probability of the latent chain of thought \(Z\) given the question \(X\) and answer \(Y\), and \(p_{\text{LM}}(X, Z, Y)\) is the joint likelihood of the sequence \(X\), \(Z\), and \(Y\)."
ICLR_2024_oral_4,2,"p_{LM}(Z\mid X,Y)=\frac{p_{LM}(XZY)}{\sum_{Z^{\prime}}p_{ LM}(XZY^{\prime}Y)}\propto p_{LM}(XZY)",,"p_{LM}(Z\mid X,Y)=\frac{p_{LM}(XZY)}{\sum_{Z^{\prime}}p_{LM}(XZY^{\prime})}",where \(p_{\text{LM}}(XZY)\) is the likelihood of the concatenated sequence and the denominator normalizes over all possible sequences \(Z'\).
ICLR_2024_oral_4,3,L(Z;\theta)=\sum_{0\leq i<j\leq n}(\log\frac{R(z_{1:i}\top)\prod_{k=i+1}^{j}q_{GFN}(z_{k}\mid z_{1:k-1})q_{GFN}(\top\mid z_ {1:j})}{R(z_{1:j}\top)q_{GFN}(\top\mid z_{1:i})})^{2},,J(Z)=E_{Z\sim q_{GFN}}\left[\sum_{i=1}^{n}R(z_{1:i})\right]-E_{Z\sim q_{GFN}}\left[\sum_{i=1}^{n}R(z_{1:i-1})\right],"where \(J(Z)\) is the learning objective for the GFlowNet, \(R(z_{1:i})\) is the reward for the sequence up to the \(i\)-th token, and \(q_{\text{GFN}}\) is the GFlowNet policy."
ICLR_2024_oral_41,1,"q_{0t}(x_{t}|x_{0})=N(x_{ t}|\alpha_{t}x_{0},\sigma_{t}^{2}I)","where \(\alpha_{t}\) and \(\sigma_{t}\) are referred to as the noise schedule, satisfying \(\alpha_{t}^{2}+\sigma_{t}^{2}=1\)","q_{0t}(x_{t}|x_{0})=N(x_{t};\sqrt{\alpha_{t}}x_{0}, (1-\alpha_{t})I)","where \(\alpha_{t}\) and \(\sigma_{t}\) are referred to as the noise schedule, satisfying \(\alpha_{t}^{2}+\sigma_{t}^{2}=1\)"
ICLR_2024_oral_41,2,"dx_{t}=f(t)x_{t}dt+g(t )dw_{t},\quadx_{0}\sim q_{0}(x_{0})","where \(\mathbf{w}_{t}\) is the standard Wiener process, \(f(t)=\frac{\mathrm{d}\log\alpha_{t}}{\mathrm{d}t}\) and \(g(t)=2\sigma_{t}^{2}\frac{\mathrm{d}\log(\sigma_{t}/\alpha_{t})}{\mathrm{d}t}\)",dx_{t}=f(t)x_{t}dt+g(t)dw_{t},"where \(\mathbf{w}_{t}\) is the standard Wiener process, \(f(t)=\frac{\mathrm{d}\log\alpha_{t}}{\mathrm{d}t}\) and \(g(t)=2\sigma_{t}^{2}\frac{\mathrm{d}\log(\sigma_{t}/\alpha_{t})}{\mathrm{d}t}\)"
ICLR_2024_oral_41,3,"dx_{t}=[f(t)x_{t}-g(t)^{2}\nabla_{x_{t}}\log q_{t}(x_{t})]dt+g (t)d\bar{w}_{t},\quadx_{T}\sim q_{T} (x_{T})",where \(\bar{\mathbf{w}}_{t}\) is a standard Wiener process in the reverse time,"dx_{t}=-f(t)x_{t}dt+g(t)d\bar{w}_{t},\quadx_{T}\sim q_{T}(x_{T})",where \(\bar{\mathbf{w}}_{t}\) is a standard Wiener process in the reverse time.
ICLR_2024_oral_41,4,"L(\theta):=E_{t\simU(0,T),x_{0}\sim q_{0}(x_{0}),\epsilon\simN (0,I)}[\|\epsilon_{\theta}(\alpha_{t}x_{0}+\sigma_{t}\epsilon,t)-\epsilon\|_{2}^{2}]",,"L(\theta)=E_{t,x_{0},\epsilon}\left[\left\|\epsilon_{\theta}(x_{t},t)+\sigma_{t}\nabla_{x_{t}}\log q_{t}(x_{t})\right\|^{2}\right]+\lambdaR(\theta)","where \(\lambda\) is a weighting factor, and \(\mathbf{x}_{0}\) is the original data sample."
ICLR_2024_oral_41,5,"\frac{\partial\epsilon_{\theta}(x,t)}{\partial t}=\frac{\alpha_{t}}{\sqrt{1-\alpha_{t}^{2}}}\frac{d\alpha_{t}}{dt}\nabla_{x}\log q_{t}(x)-\frac{\partial\nabla_{x }\log q_{t}(x)}{\partial t}\sigma_{t}",,"\limsup_{t\to 0+}\left\|\frac{\partial\epsilon_{\theta}(x,t)}{\partial t}\right\|\to\infty",The limit of the supremum of the norm of the partial derivative of the noise prediction model with respect to time as \(t\) approaches zero.
ICLR_2024_oral_41,6,"\lim\sup_{t\to 0+}\|\frac{\partial\epsilon_{\theta}(x,t )}{\partial t}\|\to\infty;\quad\lim\sup_{t\to 0+}\|\frac{\partial\nabla_{x}\log q_{t}(x)}{\partial t}\sigma_{t}\|\to\infty",,"\limsup_{t\to 0+}\left\|\frac{\partial\epsilon_{\theta}(x,t)}{\partial t}\right\|\to\infty\quador\quad\sigma_{t}\to 0",where \(\sigma_{t}\) is the standard deviation of the forward transition distribution.
ICLR_2024_oral_41,7,\nabla_{x}\log q_{t}(x)=\nabla_{x}\log (\frac{1}{\sqrt{2\pi}}\exp(-\frac{\|x\|_{2}^{2}}{2}) )=-x,,"q_{t}(x)=N(x;\alpha_{t}x_{0},\sigma_{t}^{2}I)","where \(q_{t}(\mathbf{x})\) is the marginal distribution of the data at time \(t\), \(\alpha_{t}\) is the noise schedule, and \(\sigma_{t}\) is the standard deviation of the noise added to the data."
ICLR_2024_oral_41,8,"K(t,t^{\prime})=\frac{E_{x_{t}}[\|\epsilon_{\theta} (x_{t},t)-\epsilon_{\theta}(x_{t},t^{\prime})\|_{2}]}{\Delta t}",where \(\Delta t=|t-t^{\prime}|\),"\hat{L}(\epsilon_{\theta}):=E_{t,t^{\prime}\simU(0,T)}\left[\frac{\|\epsilon_{\theta}(x,t)-\epsilon_{\theta}(x,t^{\prime})\|_{2}}{\Delta t}\right]",where \(\Delta t=|t-t^{\prime}|\)
ICLR_2024_oral_41,9,"L(\epsilon_{\theta}):=E_{t\simU(0,T),x_{0}\sim q(x_{0}),\epsilon\simN(0,I)} [\|\epsilon_{\theta}(\alpha_{t}x_{0}+\sigma_{t}\epsilon,f_{T}(t))-\epsilon\|_{2}^{2}]","where \(f_{\mathbb{T}}(t)=\max_{1\leq i\leq n}\{t_{i-1}\in\mathbb{T}:t_{i-1}\leq t\}\) for \(t<\tilde{t}\), while \(f_{\mathbb{T}}(t)=t\) for \(t\geq\tilde{t}\)","L_{E-TSDM}(\theta):=E_{t\simU(0,\tilde{t}),x_{0}\sim q_{0}(x_{0}),\epsilon\simN(0,I)}[\|\epsilon_{\theta}(\alpha_{f_{T}(t)}x_{0}+\sigma_{f_{T}(t)}\epsilon,f_{T}(t))-\epsilon\|_{2}^{2}]","where \(f_{\mathbb{T}}(t)=\max_{1\leq i\leq n}\{t_{i-1}\in\mathbb{T}:t_{i-1}\leq t\}\) for \(t<\tilde{t}\), while \(f_{\mathbb{T}}(t)=t\) for \(t\geq\tilde{t}\)"
ICLR_2024_oral_41,10,"p_{\theta}(x_{t-1}|x_{t})=N(x_{t-1};\frac{\alpha_{t-1}}{\alpha_{t}}(x_{t}-\frac{\beta_{t}}{\sigma_{t}}\epsilon_{\theta}(x_{t},f_{T} (t))),\eta_{t}^{2}I)","where \(\beta_{t}=1-\frac{\alpha_{t}}{\alpha_{t-1}}\), and \(\eta_{t}^{2}=\beta_{t}\)","dx_{t}=\beta_{t}x_{t-1}+\eta_{t}\epsilon_{\theta}(x_{t-1},f_{T}(t))","where \(\beta_{t}=1-\frac{\alpha_{t}}{\alpha_{t-1}}\), and \(\eta_{t}^{2}=\beta_{t}\)"
ICLR_2024_oral_41,11,"\|\epsilon^{*}(x,f_{T}(t)-\epsilon(x,t)\|\leq\sigma_{\tilde{t}}K (x)\Delta t+B(x)\Delta\sigma_{\max}",,"\|\epsilon^{*}(x,f_{T}(t))-\epsilon(x,t)\|_{2}\leq C\Delta t,\end{latex> <description>where\(C\) is a constant and\(\Delta t\) is the time difference.</description>",where \(C\) is a constant and \(\Delta t\) is the time difference.
ICLR_2024_oral_41,12,"K(x)=\sup_{t\neq\tau}\frac{\|\nabla_{x}\log q_{t }(x)-\nabla_{x}\log q_{\tau}(x )\|}{|t-\tau|},\quad B(x)=\sup_{t}\|\nabla_{x}\log q_{t}(x)\|",,"\sigma_{\tilde{t}}=\max_{t\in[0,\tilde{t})}\sigma_{t},\quad B(x)=\max_{t\in[0,\tilde{t})}\left\|\frac{\partial\nabla_{x}\log q_{t}(x)}{\partial t}\right\|","where \(\sigma_{\tilde{t}}\) is the maximum standard deviation in the interval \([0,\tilde{t})\) and \(B(\mathbf{x})\) is the maximum norm of the partial derivative of the score function with respect to time in the same interval."
ICLR_2024_oral_44,1,"I(S;Z)=D_{KL}(p(s,z)\|p(s)p(z))",,"I=E_{s\simS, k\simK}\left[\log\frac{p(k | s)}{p(k)}\right]","where \(I\) is the mutual information, \(s\) represents states from the state space \(\mathcal{S}\), and \(k\) denotes skills from the skill space \(\mathcal{K}\)."
ICLR_2024_oral_44,2,"I_{W}(S;Z)=W(p(s,z),p(s)p(z))",where \(I_{\mathcal{W}}(S;Z)\) is the Wasserstein dependency measure (WDM) (Ozair et al,"I_{W}(S;Z)=\inf_{\gamma\in\Gamma(p(s),p(z))}E_{(s,z)\sim\gamma}[d(s,z)]","where \(I_{\mathcal{W}}(S;Z)\) is the Wasserstein dependency measure, \(\Gamma(p(s),p(z))\) denotes the set of joint distributions \(\gamma\) with marginals \(p(s)\) and \(p(z)\), and \(d(s,z)\) is the chosen distance metric, specifically the temporal distance."
ICLR_2024_oral_44,3,"I_{W}(S;Z)=\sup_{\|f\|_{L}\leq 1}E_{p(s,z)}[f(s,z)]-E_{p(s)p(z)}[f(s,z)]","where \(\|f\|_{L}\) denotes the Lipschitz constant for the function \(f:\mathcal{S}\times\mathcal{Z}\rightarrow\mathbb{R}\) under the given distance metric \(d\), _i","I_{W}(S;Z)=\sup_{f:S\timesZ\rightarrowR}\left\{E_{(s,z)\sim p(s,z)}[f(s,z)]-E_{s\sim p(s),z\sim p(z)}[f(s,z)]\right\},\) denotes the Lipschitz constant for the function\(f:S\timesZ\rightarrowR\) under the given distance metric\(d\).</description>",where \(\|f\|_{L}\) denotes the Lipschitz constant for the function \(f:\mathcal{S}\times\mathcal{Z}\rightarrow\mathbb{R}\) under the given distance metric \(d\).
ICLR_2024_oral_44,4,"I_{W}(S;Z)\approx\sup_{\|\phi\|_{L}\leq 1,\|\psi\|_{L}\leq 1}E_{p( s,z)}[\phi(s)^{\top}\psi(z)]-E_{p(s)}[\phi(s)]^{\top}E_{p(z)}[\psi(z)]",,"I_{W}(S;Z)=\sup_{\|\phi\|_{L}\leq 1,\|\psi\|_{L}\leq 1}E_{p(s,z)}[\phi(s)^{\top}\psi(z)]-E_{p(s)p(z)}[\phi(s)^{\top}\psi(z)]",where \(\phi:\mathcal{S}\rightarrow\mathbb{R}^{D}\) and \(\psi:\mathcal{Z}\rightarrow\mathbb{R}^{D}\) are functions parameterizing the score function under independent \(1\)-Lipschitz constraints.
ICLR_2024_oral_44,5,"I_{W}(S_{T};Z)\approx\sup_{\|\phi\|_{L}\leq 1,\|\psi\|_{L}\leq 1}E_{p(\tau,z)}[\phi(s_{T})^{\top}\psi(z)]-E_{p(\tau)}[\phi(s_{T})]^{\top}E_{p(z)}[\psi(z)] || =\sup_{\phi,\psi}\sum_{t=0}^{T-1}(E_{p(\tau,z)}[(\phi(s_{t+1})-\phi(s_{t}))^{\top}\psi(z)]-E_{p(\tau)}[\phi(s_{t+1})-\phi(s_{t})]^{\top}E_{p(z)}[\psi(z)])",,"I_{W}(S_{T};Z)\approx\sup_{\|\phi\|_{L}\leq 1,\|\psi\|_{L}\leq 1}E_{p(s_{T},z)}[\phi(s_{T})^{\top}\psi(z)]-E_{p(s_{T})}[\phi(s_{T})]^{\top}E_{p(z)}[\psi(z)]","where \(I_{\mathcal{W}}(S_{T};Z)\) is the Wasserstein dependency measure based on the last state \(S_{T}\), and \(\phi\) and \(\psi\) are functions parameterizing the score function under the given Lipschitz constraints."
ICLR_2024_oral_44,6,"I_{W}(S_{T};Z)\approx\sup_{\|\phi\|_{L}\leq 1}E_{p(\tau,z)}[\sum_{t=0}^{T-1}(\phi(s_{t+1})-\phi(s_{t}))^{\top}(z-\bar{z}) ]",where \(\bar{z}=\mathbb{E}_{p(z)}[z]\),I_{W}(S_{T};Z)\approx\sup_{\|\phi\|_{L}\leq 1}E_{p(\tau)}[\phi(s_{T})^{\top}z]-E_{p(\tau)}[\phi(s_{T})]^{\top}\bar{z},where \(\bar{z}=\mathbb{E}_{p(z)}[z]\).
ICLR_2024_oral_44,7,"\sup_{\pi,\phi}\E_{p(\tau,z)}[\sum_{t=0}^{T-1}(\phi(s_{t+1})-\phi(s_{t}))^{\top}z]\\s.t.\\\|\phi(s)-\phi(s^{\prime})\|_{2}\leq 1,\\\forall(s,s^{\prime})\inS_{adj}",where \(\mathcal{S}_{\mathrm{adj}}\) denotes the set of adjacent state pairs in the MDP,"I_{W}(S_{T};Z)\approx\sup_{\|\phi\|_{L}\leq 1}E_{p(\tau,z)}[\sum_{(s_{1},s_{2})\inS_{adj}}(\phi(s_{2})-\phi(s_{1}))^{\top}(z-\bar{z}) ]",where \(\mathcal{S}_{\mathrm{adj}}\) denotes the set of adjacent state pairs in the MDP.
ICLR_2024_oral_45,1,"x^{\prime}(t)=Ax(t)+Bu(t),\quad y(t)=Cx(t)+Du(t)",,x(t+1) &=A x(t)+B u(t)\\y(t) &=C x(t)+D u(t),"State equations of the linear dynamical system, where \(x(t)\) is the state vector, \(u(t)\) is the control input, and \(y(t)\) is the output."
ICLR_2024_oral_45,2,"x_{n}=\bar{A}x_{n-1}+\bar{B}u_{n},\quad y_{n}=\bar{ C}x_{n}+\bar{D}u_{n}","where \(\bar{\mathbf{A}},\bar{\mathbf{B}},\bar{\mathbf{C}}\), and \(\bar{\mathbf{D}}\) are discrete-time parameters obtained from the continuous-time parameters and \(\Delta\) using methods like zero-order hold and bilinear technique (Smith et al","\bar{x}(k+1)=\bar{A}\bar{x}(k)+\bar{B}u(k),\quad y(k)=\bar{C}\bar{x}(k)+\bar{D}u(k)","where \(\bar{\mathbf{A}},\bar{\mathbf{B}},\bar{\mathbf{C}},\) and \(\bar{\mathbf{D}}\) are discrete-time parameters obtained from the continuous-time parameters and \(\Delta\)."
ICLR_2024_oral_45,3,"L(\theta)=\operatorname*{E}_{z_{1:T}\sim q_{\theta}}\sum_{ t=1}^{T}L^{pred}(\theta,h_{t},o_{t},r_{t},c_{t},z_{t})+L^{rep}(\theta,h_{t},o_{t})+L^{dyn}(\theta,h _{t},o_{t})",,"L_{R2I}=E_{(o_{t},r_{t},c_{t})\sim p(o_{t},r_{t},c_{t}\mid o_{<t},a_{<t})}\left[-\log p_{\theta}(\hat{o}_{t}\mid z_{t},h_{t})-\beta\log p_{\theta}(\hat{r}_{t}\mid z_{t},h_{t})-\lambda\log p_{\theta}(\hat{c}_{t}\mid z_{t},h_{t})\right]","where \(\mathcal{L}_{\text{R2I}}\) is the loss function for R2I, \(\beta\) and \(\lambda\) are weighting factors for the reward and continuation predictions, respectively."
ICLR_2024_oral_45,4,"L^{pred}(\theta,h_{t},o_{t},r_{t},c_{t},z_{t})=-\beta_{pred}(\ln p_{\theta}(o_{t}\mid z_{t},h_{t})+\ln p_{\theta}(r_{t}\mid z_{t},h_{t})+\ln p_{\theta}(c_{t}\mid z_{t},h_{t})) || L^{dyn}(\theta,h_{t},o_{t})=\beta_{dyn}\max(1,KL[\lessdot\circ g(q_{\theta}(z_ {t}\mid o_{t}))\parallel p(z_{t}\mid h_{t})\mid)]",,"L^{pred}(\theta,h_{t},o_{t},r_{t},c_{t},z_{t})=-E_{\hat{o}_{t}\sim p_{\theta}(\hat{o}_{t}\mid z_{t},h_{t})}\left[\log p(o_{t}\mid\hat{o}_{t})\right]-E_{\hat{r}_{t}\sim p_{\theta}(\hat{r}_{t}\mid z_{t},h_{t})}\left[\log p(r_{t}\mid\hat{r}_{t})\right]-E_{\hat{c}_{t}\sim p_{\theta}(\hat{c}_{t}\mid z_{t},h_{t})}\left[\log p(c_{t}\mid\hat{c}_{t})\right]","where \(\mathcal{L}^{\text{pred}}\) is the prediction loss, \(p_{\theta}(\hat{o}_{t}\mid z_{t},h_{t})\) is the observation predictor, \(p_{\theta}(\hat{r}_{t}\mid z_{t},h_{t})\) is the reward predictor, and \(p_{\theta}(\hat{c}_{t}\mid z_{t},h_{t})\) is the episode continuation predictor."
ICLR_2024_oral_45,5,"L^{rep}(\theta,h_{t},o_{t})=\beta_{rep}\max(1,KL[\quad q_{\theta}(z_{t}\mid o_ {t})\parallel\lessdot\circ g(p(z_{t}\mid h_{t}))\,])",,"L^{rep}(\theta,h_{t},o_{t})=-\beta_{rep}E_{z_{t}\sim q_{\theta}(z_{t}\mid o_{t})}[\ln p_{\theta}(z_{t}\mid h_{t})]","where \(\mathcal{L}^{\text{rep}}\) is the representation loss, \(\beta_{\text{rep}}\) is a weighting factor, and \(p_{\theta}(z_{t}\mid h_{t})\) is the predicted distribution of the latent state given the deterministic state."
ICLR_2024_oral_45,6,"h_{1:T},x_{1:T}=f_{\theta}((a_{1:T},z_{1:T}),x_{0})",,"L(\theta)=L^{pred}(\theta,h_{t},o_{t},r_{t},c_{t},z_{t})+L^{rep}(\theta,h_{t},o_{t})+L^{dyn}(\theta,h_{t},o_{t})","where \(\mathcal{L}^{\text{pred}}\), \(\mathcal{L}^{\text{rep}}\), and \(\mathcal{L}^{\text{dyn}}\) represent the prediction, representation, and dynamics losses, respectively."
ICLR_2024_oral_49,1,"e_{i}^{num}=x_{i}^{num}\cdotw_{i}^{num}+b_{i}^{num},\\e_{i}^{cat}=x_{i}^{oh}\cdotW_{i}^{cat }+b_{i}^{cat}","where \(\mathbf{w}_{i}^{\mathrm{num}},\mathbf{b}_{i}^{\mathrm{num}},\mathbf{b}_{i}^{\mathrm{cat}} \in\mathbb{R}^{1\times d}\), \(\mathbf{W}_{i}^{\mathrm{cat}}\in\mathbb{R}^{C_{i}\times d}\) are learnable parameters of the tokenizer, \(\mathbf{e}_{i}^{\mathrm{num}},\mathbf{e}_{i}^{\mathrm{cat}}\in\mathbb{R}^{1\times d}\)","e_{i}^{num}=x_{i}^{num}\cdotw_{i}^{num}+b_{i}^{num},\quade_{i}^{cat}=x_{i}^{cat}\cdotW_{i}^{cat}+b_{i}^{cat}","where \(\mathbf{w}_{i}^{\mathrm{num}},\mathbf{b}_{i}^{\mathrm{num}},\mathbf{b}_{i}^{\mathrm{cat}} \in\mathbb{R}^{1\times d}\), \(\mathbf{W}_{i}^{\mathrm{cat}}\in\mathbb{R}^{C_{i}\times d}\) are learnable parameters of the tokenizer, \(\mathbf{e}_{i}^{\mathrm{num}},\mathbf{e}_{i}^{\mathrm{cat}}\in\mathbb{R}^{1\times d}\)"
ICLR_2024_oral_49,2,"E=[e_{1}^{num},\cdots,e_{M_{num}}^{ num},e_{1}^{cat},\cdots,e_{M_{cat}}^{cat}]\inR^{M\times d}",,"e=\left[e_{1}^{num},e_{1}^{cat},\ldots,e_{M_{cat}}^{cat}\right]\inR^{(M_{num}+\sum_{i=1}^{M_{cat}} d)\times 1}","where \(\mathbf{e}\) is the combined embedding vector for all columns, \(M_{\mathrm{num}}\) is the number of numerical columns, and \(d\) is the dimensionality of the embeddings."
ICLR_2024_oral_49,3,"&\hat{x}_{i}^{num}=\hat{e}_{i}^{num}\cdot\hat{w}_{i}^{num}+\hat{b}_{i}^{num},\\\hat{e}_{i}^{oh}=Softmax(\hat{e}_{i}^{cat}\cdot\hat{W}_{i}^{cat}+\hat{b}_{i}^{cat}),\\&\hat{x}=[\hat{x}_{1}^{num},\cdots,\hat{x}_{M_{num}}^{num},\hat{x}_{1}^{oh},\cdots,\hat{x }_{M_{cat}}^{oh}]","where \(\hat{\mathbf{w}}_{i}^{\mathrm{num}}\in\mathbb{R}^{d\times 1},\hat{b}_{i}^{ \mathrm{num}}\in\mathbb{R}^{1\times 1}\), \(\mathbf{W}_{i}^{\mathrm{cat}}\in\mathbb{R}^{d\times C_{i}},\hat{\mathbf{b}}_{i}^{ \mathrm{cat}}\in\mathbb{R}^{1\times C_{i}}\) are detokenizer's parameters","\hat{x}_{i}^{num}=\hat{e}_{i}^{num}\cdot\hat{w}_{i}^{num}+\hat{b}_{i}^{num},\\\hat{x}_{i}^{cat}=\hat{e}_{i}^{cat}\cdotW_{i}^{cat}+\hat{b}_{i}^{cat}","where \(\hat{\mathbf{w}}_{i}^{\mathrm{num}}\in\mathbb{R}^{d\times 1},\hat{b}_{i}^{\mathrm{num}}\in\mathbb{R}^{1\times 1},\mathbf{W}_{i}^{\mathrm{cat}}\in\mathbb{R}^{d\times C_{i}},\hat{\mathbf{b}}_{i}^{\mathrm{cat}}\in\mathbb{R}^{1\times C_{i}}\) are detokenizer's parameters."
ICLR_2024_oral_49,4,"L=\ell_{recon}(x,\hat{x})+\beta\ell_{kl}",,L=E_{q(z|x)}[\log p(x|z)]-\beta\cdot D_{KL}(q(z|x)\| p(z)),"where \(\mathcal{L}\) is the loss function, \(q(\mathbf{z}|\mathbf{x})\) is the approximate posterior, \(p(\mathbf{x}|\mathbf{z})\) is the likelihood of the data given the latent variable, \(D_{KL}\) is the Kullback-Leibler divergence, and \(\beta\) is the weight coefficient."
ICLR_2024_oral_49,5,"z_{t}=z_{0}+\sigma(t)\varepsilon,\\varepsilon\simN(0,I), (Forward Process) || dz_{t}=-2\hat{\sigma}(t)\sigma(t)\nabla_{z_{t}}\log p(z_{t})dt+\sqrt{2\hat{\sigma}(t)\sigma(t)}d\omega_{t}, (Reverse Process)",,z_{t}=\sqrt{\alpha_{t}}z_{0}+\sqrt{1-\alpha_{t}}\epsilon,"where \(\mathbf{z}_{t}\) is the noisy embedding at time \(t\), \(\mathbf{z}_{0}\) is the original embedding, \(\alpha_{t}\) is the noise schedule, and \(\mathbf{\epsilon}\) is Gaussian noise."
ICLR_2024_oral_49,6,"L=E_{z_{0}\sim p(z_{0})}E_{t\sim p (t)}E_{\varepsilon\simN(0,I)}\|\varepsilon_{\theta}(z_{t},t)-\varepsilon)\|_{2}^{2},\\where\z_{t}=z_{0}+\sigma(t)\varepsilon",where \(\mathbf{\epsilon}_{\theta}\) is a neural network (named denoising function) to approximate the Gaussian noise using the perturbed data \(\mathbf{x}_{t}\) and the time \(t\),"L_{denoise}=E_{t,x_{0},\epsilon}\left[\left\|\epsilon-\epsilon_{\theta}(x_{t}, t)\right\|^{2}\right]",where \(\mathbf{\epsilon}_{\theta}\) is a neural network (named denoising function) to approximate the Gaussian noise using the perturbed data \(\mathbf{x}_{t}\) and the time \(t\)
ICLR_2024_oral_52,1,"p(x(t)\midx(0))=N(x(t);x(0),\sigma^{2}(t)I)\",,"x_{t}=x_{0}+\sqrt{\beta_{t}}z,\quadz\simN(0,I)","where \(\mathbf{x}_{t}\) is the perturbed data, \(\mathbf{x}_{0}\) is the original data, \(\beta_{t}\) is the noise schedule, and \(\mathbf{z}\) is Gaussian noise."
ICLR_2024_oral_52,2,"dx(t)=-\sigma(t)\nabla_{x(t)}\log p(x(t))\,dt\",,\frac{dx(t)}{dt}=\frac{1}{\sigma(t)}\nabla_{x}\log p(x(t))\,The probability flow ODE describing the forward evolution of a data point \(\mathbf{x}(t)\) in time.
ICLR_2024_oral_52,3,"dx(t)=\sigma(t)\nabla_{x(t)}\log p(x(t))\,dt\",,"dx(t)=-\nabla_{x(t)}\log p(x(t))\,dt\",Backward ODE describing the denoising process.
ICLR_2024_oral_52,4,"E_{t\simU([0,T])}E_{x(0)\sim p(x (0))}E_{x(t)\sim p(x(t)|x(0))}\|S^{\theta}(x(t),\sigma(t))-\nabla_{x(t)}\log p (x(t)\midx(0))\|_{2}^{2}",,"L_{score}=E_{x(t),y}\left[\left\| S^{\theta}(x(t),\sigma(t))-\nabla_{x(t)}\log p(y)\right\|^{2}\right]\",Score-matching loss function for approximating the score function using a neural network.
ICLR_2024_oral_52,5,"E_{t\simU([0,T])}E_{x(0)\sim p(x (0))}E_{x\simN(0,\sigma^{2}(t)I )}\|D^{\theta}(x(0)+\epsilon,\sigma(t))-x(0)\|_ {2}^{2}\","where we define \(S^{\theta}(\mathbf{x}(t),\sigma(t))=:(D^{\theta}(\mathbf{x}(t),\sigma(t))- \mathbf{x}(t))/\sigma^{2}(t)\)","E_{t\simU([0,T])}E_{x(0)\sim p(x (0))}E_{x(t)\sim p(x(t)|x(0))}\| D^{\theta}(x(t),\sigma(t))-x(t)\|_{2}^{2}","where we define \(S^{\theta}(\mathbf{x}(t),\sigma(t))=:(D^{\theta}(\mathbf{x}(t),\sigma(t))- \mathbf{x}(t))/\sigma^{2}(t)\)"
ICLR_2024_oral_52,6,"\nabla_{x(t)}\log p(x(t))=\nabla_{(x_{1}(t),\ldots,x_{N}(t))}\log p(x_{1}(t),\ldots,x_{N}(t))\",,\nabla_{x}\log p(y\midx)\approx\nabla_{x}\log p(y\midx(t))\,where \(\mathbf{y}\) is the observed mixture and \(\mathbf{x}\) represents the latent sources.
ICLR_2024_oral_52,7,"S^{\theta}(x(t),\sigma(t)):R^{N\times D}\timesR arrowR^{N\times D}\","where \(\mathbf{x}(t)=(\mathbf{x}_{1}(t),\ldots,\mathbf{x}_{N}(t))\) denotes the sources perturbed with the Gaussian kernel in Eq","\nabla_{x(t)}\log p(x(t))\approx S^{\theta}(x(t),\sigma(t))\","where \(\mathbf{x}(t)=(\mathbf{x}_{1}(t),\ldots,\mathbf{x}_{N}(t))\) denotes the sources perturbed with the Gaussian kernel in Eq. (1)."
ICLR_2024_oral_52,8,\nabla_{x_{\overline{I}}(t)}\log p(x_{\overline{I}}(t)\midx_{I}(t)),,\nabla_{x_{\overline{I}}}\log p(x_{\overline{I}}\midx_{I})\,where \(\mathbf{x}_{\mathcal{\overline{I}}}\) denotes the sources to be generated and \(\mathbf{x}_{\mathcal{I}}\) are the fixed sources.
ICLR_2024_oral_52,9,"\nabla_{x_{\overline{I}}(t)}\log p([x_{\overline{I}}(t),\hat{x}_{I}(t)])\","where \(\hat{\mathbf{x}}_{\mathcal{I}}\) is a sample from the forward process: \(\hat{\mathbf{x}}_{\mathcal{I}}(t)\sim\mathcal{N}(\mathbf{x}_{\mathcal{I}}(t) ;\mathbf{x}_{\mathcal{I}}(0),\sigma(t)^{2}\mathbf{I})\)",\nabla_{x_{\overline{I}}(t)}\log p(x_{\overline{I}}(t)\mid\hat{x}_{I}(t))\,"where \(\hat{\mathbf{x}}_{\mathcal{I}}\) is a sample from the forward process: \(\hat{\mathbf{x}}_{\mathcal{I}}(t)\sim\mathcal{N}(\mathbf{x}_{\mathcal{I}}(t) ;\mathbf{x}_{\mathcal{I}}(0),\sigma(t)^{2}\mathbf{I})\)"
ICLR_2024_oral_52,10,"\nabla_{x_{\overline{I}}(t)}\log p(x_{\overline{I}}(t)\midx_{I}(t))\approx S_{\overline{I}}^{\theta}([x_{\overline{I}}(t),\hat{x }_{I}(t)],\sigma(t))\",where \(S_{\overline{\mathcal{I}}}^{\theta}\) denotes the entries of the score network corresponding to the sources indexed by \(\overline{\mathcal{I}}\),"\nabla_{x_{\overline{I}}(t)}\log p([x_{\overline{I}}(t),\hat{x}_{I}(t)])\approx S_{\overline{I}}^{\theta}([x_{\overline{I}}(t),\hat{x}_{I}(t)],\sigma(t))\",where \(S_{\overline{\mathcal{I}}}^{\theta}\) denotes the entries of the score network corresponding to the sources indexed by \(\overline{\mathcal{I}}\)
ICLR_2024_oral_52,11,\nabla_{x(t)}\log p(x(t)\midy(0))\,,\nabla_{x(t)}\log p(x(t)\midy)\,"where \(\mathbf{x}(t)=(\mathbf{x}_{1}(t),\ldots,\mathbf{x}_{N}(t))\) denotes the sources at time \(t\) conditioned on the mixture \(\mathbf{y}\)."
ICLR_2024_oral_52,12,"p(y(t)\midx(t))=N(y(t)\mid\sum_{n=1}^{N}x_{n}(t),\gamma^{2}(t)I)",,"p(y(0)\midx(t))=N(y(0);\sum_{n=1}^{N}x_{n}(t),\sigma^{2}I)\",where \(\sigma\) is the noise level associated with the likelihood function.
ICLR_2024_oral_52,13,p(y(t)\midx(t))=1_{y(t)=\sum_{n=1}^{N}x_{ n}(t)}\,,p(y(t)\midx(t))\approx\delta\left(y(t)-\sum_{n=1}^{N}x_{n}(t)\right)\,where the Dirac delta function indicates that the mixture \(\mathbf{y}(t)\) is exactly equal to the sum of the individual sources \(\sum_{n=1}^{N}\mathbf{x}_{n}(t)\).
ICLR_2024_oral_52,14,"\nabla_{x_{m}(t)}\log p(x(t)\midy(0))\approx S_{m}^{\theta}((x_{1}(t),\dots,x_{N-1}(t),y(0)-\sum_{n=1}^{N-1}x_{n}(t)),\sigma(t)) || -S_{N}^{\theta}((x_{1}(t),\dots,x_{N-1}(t),y(0)-\sum_{n=1}^{N-1}x_{n}(t)),\sigma(t))\",,x_{N}(t)=y(0)-\sum_{n=1}^{N-1}x_{n}(t)\,where \(\mathbf{x}_{N}(t)\) is the estimated source corresponding to the fixed mixture \(\mathbf{y}(0)\).
ICLR_2024_oral_53,1,L=L_{clip}+\lambdaL_{video},where video-paragraph contrastive loss \(\mathcal{L}_{\text{video}}\) explores the temporal correlations between the long video \(\mathbf{V}_{i}\) and its corresponding paragraph \(\mathbf{T}_{i}\) through a novel noise robust temporal optimal transport distance,"L_{video}=\sum_{i=1}^{N}D_{OT}\left(\{v_{a}\}_{a=1}^{n},\{t_{b}\}_{b=1}^{m}\right)",video-paragraph contrastive loss exploring temporal correlations between video and paragraph
ICLR_2024_oral_53,2,"\max_{Q\inQ}&\langleQ,\S\rangle+\varepsilon H(Q)\\s.t.&Q=\{Q\inR _{+}^{n\times m}\midQ1_{m}=\mu,Q^{\top}1_{n}=\nu\}","where \(\mathbf{1}_{m}\) represents the vector of ones in dimension \(m\), \(\boldsymbol{\mu}\in\mathbb{R}^{n}\) and \(\boldsymbol{\nu}\in\mathbb{R}^{m}\) indicate the relative importance of each clip or caption","L_{OT}=\max_{Q\inP(1_{m},\nu)}\langleQ,S\rangle","where \(\mathcal{P}(\mathbf{1}_{m},\boldsymbol{\nu})\) denotes the set of transport plans that satisfy the marginal constraints defined by \(\mathbf{1}_{m}\) and \(\boldsymbol{\nu}\)."
ICLR_2024_oral_53,3,"Q^{*}&=Diag(\kappa_{1})\exp(S/\varepsilon)Diag(\kappa_{2}),\\with iteratively updated \kappa_{1}& arrow\mu./(\exp(S/\varepsilon)\kappa_{2}),\\kappa_{2}arrow\nu_{.}/(\exp(S^{\top}/\varepsilon)\kappa_{1})","where \(\boldsymbol{\kappa}_{1}\in\mathbb{R}^{n}\), \(\boldsymbol{\kappa}_{2}\in\mathbb{R}^{m}\) are the non-negative left and right scaling vectors","Q^{*}=Sinkhorn(S,\mu,\nu,\varepsilon)","where \(\boldsymbol{\kappa}_{1}\in\mathbb{R}^{n}\), \(\boldsymbol{\kappa}_{2}\in\mathbb{R}^{m}\) are the non-negative left and right scaling vectors"
ICLR_2024_oral_53,4,"L_{video}=-\sum_{i=1}^{N}(\log\frac{\exp((Q_{ii},\S_{ii})/\tau)}{\sum_{j=1}^{N}\exp((Q_{ ij},\S_{ij})/\tau)}+\log\frac{\exp((Q_{ii},\S_{ii})/\tau)}{\sum_{j=1}^{N}\exp((Q_{ ji},\S_{ji})/\tau)})","where \(\mathbf{S}_{ij}\in\mathbb{R}^{n\times m}\) is the clip-caption similarity matrix between video \(\mathbf{V}_{i}\) and paragraph \(\mathbf{T}_{j}\), \(\mathbf{Q}_{ij}\) is the corresponding transport assignment of \(\mathbf{S}_{ij}\), and \(\tau\) is a learnable temperature initialized as 0","L_{video}=\sum_{i=1}^{N}\sum_{j=1}^{N}\langleQ_{ij},S_{ij}\rangle/\tau","where \(\mathbf{S}_{ij}\in\mathbb{R}^{n\times m}\) is the clip-caption similarity matrix between video \(\mathbf{V}_{i}\) and paragraph \(\mathbf{T}_{j}\), \(\mathbf{Q}_{ij}\) is the corresponding transport assignment of \(\mathbf{S}_{ij}\), and \(\tau\) is a learnable temperature initialized as 0."
ICLR_2024_oral_53,5,"[S]_{a,b}=\frac{1}{2}(\frac{1}{f}\sum_{i=1}^{f}\alpha\log(\sum_{j=1}^{w}\exp(\frac{v_{a}^{i}\cdott_{b}^{j}}{\alpha}) )+\frac{1}{w}\sum_{i=1}^{w}\alpha\log(\sum_{j=1}^{f}\exp(\frac{t_{b}^{i}\cdotv_{a}^{j}}{\alpha})))",,A_{ab}=\frac{v_{a}^{\top}t_{b}}{\|v_{a}\|\|t_{b}\|},"where \(\mathbf{A}\in\mathbb{R}^{n\times m}\) represents the fine-grained similarity matrix, capturing the interactions between each frame and word."
ICLR_2024_oral_53,6,"[\bar{S}]_{a,m+1}=[\bar{S}]_{n+1,b}=[\bar{S}]_{n+1, m+1}=p,\,[\bar{S}]_{a,b}=[S]_{a,b},\\forall a\in[1,n],\b\in[1,m]",,"[S]_{a,b}=\frac{1}{2}(\frac{1}{f}\sum_{i=1}^{f}\alpha\log(\sum_{j=1}^{w}\exp(\frac{v_{a}^{i}\cdott_{b}^{j}}{\alpha}) )+\frac{1}{w}\sum_{i=1}^{w}\alpha\log(\sum_{j=1}^{f}\exp(\frac{t_{b}^{i}\cdotv_{a}^{j}}{\alpha})))","where \(\alpha\) is a parameter that magnifies the importance of the most relevant words or frames, and \(f\) and \(w\) represent the number of frames and words, respectively."
ICLR_2024_oral_53,7,"\max_{\hat{Q}\in\hat{Q}}\\\\langle\hat{Q},\\hat{S}\rangle+\varepsilon H(\hat{Q})\\\\s.t.\\\\hat{Q}=\{\hat{Q}\inR^{B\times B}_{+}\mid\hat{Q}1_{B}=\frac{1}{B}1_{B},\hat{Q}^{\top}1_{B}=\frac{1}{B}1_{B}\}",where the transport assignment \(\hat{\mathbf{Q}}\) attempts to realign the clips with similar captions (_i,"\hat{Q}^{*}&=Diag(\hat{\kappa}_{1})\exp(\hat{S}/\varepsilon)Diag(\hat{\kappa}_{2}),\\with iteratively updated \hat{\kappa}_{1}&\leftarrow1_{B}/(\exp(\hat{S}/\varepsilon)\hat{\kappa}_{2}),\\hat{\kappa}_{2}\leftarrow1_{B}/(\exp(\hat{S}^{\top}/\varepsilon)\hat{\kappa}_{1})","where \(\hat{\boldsymbol{\kappa}}_{1}\in\mathbb{R}^{B}\), \(\hat{\boldsymbol{\kappa}}_{2}\in\mathbb{R}^{B}\) are the non-negative left and right scaling vectors for the within-batch similarity matrix."
ICLR_2024_oral_53,8,"L_{clip}=-\sum\limits_{i=1}^{B}\sum\limits_{j=1}^{B}[ T]_{i,j}(\log\frac{\exp([\hat{S}]_{i,j}/\tau)}{\sum_{k=1}^{B}\exp([\hat{S}]_{i,k}/\tau)}+\log\frac{\exp([\hat{S}]_{i,j}/\tau)}{\sum_{k=1}^{B}\exp([\hat{S}]_{i,j}/\tau)}),T=(1-\beta )\,I_{B}+\beta\hat{Q}^{*}",where \(\beta\) is a weighted parameter that balances the identity target \(\mathbf{I}_{B}\) and realigned targets \(\hat{\mathbf{Q}}^{*}\),"L_{contrastive}=-\sum_{b=1}^{B}\left(\beta\log\frac{\exp((\hat{Q}^{*}_{bb},\\hat{S}_{bb})/\tau)}{\sum_{j=1}^{B}\exp((\hat{Q}^{*}_{bj},\\hat{S}_{bj})/\tau)}+(1-\beta)\log\frac{\exp((\hat{Q}^{*}_{bb},\\hat{S}_{bb})/\tau)}{\sum_{j=1}^{B}\exp((I_{B},\\hat{S}_{bj})/\tau)}\right)","where \(\mathcal{L}_{\text{contrastive}}\) is the contrastive loss, \(\hat{\mathbf{S}}_{bb}\) is the similarity of the \(b\)-th clip-caption pair, and \(\tau\) is a learnable temperature parameter."
ICLR_2024_oral_54,1,"\min_{S\inR^{N\times d}}D(\nabla_{\theta}\ell(S;\theta),\nabla_{\theta}\ell(B;\theta))=D( S,B;\theta)","where the function \(D(\cdot)\) is defined as a distance metric such as MSE, \(\theta\) represents the model parameters, and \(\nabla_{\theta}\ell(\cdot)\) denotes the gradient, utilizing either the big dataset \(\mathcal{B}\) or its synthetic version \(\mathcal{S}\)","\min_{S} D(B,S)\quadsubject to\quadS\inR^{N\times d}","where \(D(\mathcal{B}, \mathcal{S})\) is the distance metric between the big dataset \(\mathcal{B}\) and the synthetic dataset \(\mathcal{S}\)."
ICLR_2024_oral_54,2,"SarrowS-\lambda\nabla_{S}D( S,B;\theta),\quad\thetaarrow\theta-\eta\nabla_{\theta}\ell(\theta;S)","where \(\lambda\) and \(\eta\) are learning rates designated for \(\mathcal{S}\) and \(\theta\), respectively","S &\leftarrowS-\lambda\nabla_{S} D(\nabla_{\theta}\ell(S;\theta),\nabla_{\theta}\ell(B;\theta)),\\\theta &\leftarrow\theta-\eta\nabla_{\theta} D(\nabla_{\theta}\ell(S;\theta),\nabla_{\theta}\ell(B;\theta))","\(\mathcal{S}\) is the synthetic dataset, \(\theta\) represents the model parameters, \(\lambda\) is the learning rate for \(\mathcal{S}\), and \(\eta\) is the learning rate for \(\theta\)."
ICLR_2024_oral_54,3,"\min_{S\inR^{N\times d}}D(\nabla_{\theta }\ell(S_{[1]},S_{[2]},\ldotsS_{[N]};\theta ),\nabla_{\theta}\ell(B;\theta))","where \(\mathcal{S}_{[n]}=\mathcal{S}_{\{1,2,\ldots,n\}}\subset\mathcal{S}=\mathcal{S}_ {[N]}\) represents \(n_{th}\) subset of the synthetic dataset \(\mathcal{S}\in\mathbb{R}^{N\times d}\)","\min_{S_{[n]}\inR^{n\times d}}D(\nabla_{\theta}\ell(S_{[n]};\theta),\nabla_{\theta}\ell(B;\theta))=D( S_{[n]},B;\theta)","where \(\mathcal{S}_{[n]}=\mathcal{S}_{\{1,2,\ldots,n\}}\subset\mathcal{S}=\mathcal{S}_{[N]}\) represents \(n_{th}\) subset of the synthetic dataset \(\mathcal{S}\in\mathbb{R}^{N\times d}\)"
ICLR_2024_oral_54,4,"SarrowS-\lambda(\nabla_{S}D(S,B;\theta)+\nabla_{S_{[n]}}D(S_{[n]},B;\theta)),\hskip 14.226378ptn\in[1,N-1]","where \(\mathcal{S}=\mathcal{S}_{[N]}\) represents the condensed dataset of \(N\) images and is associated with the ""base loss""","S\leftarrowS-\lambda\nabla_{S}\left( D(\nabla_{\theta}\ell(S;\theta),\nabla_{\theta}\ell(B;\theta))+\sum_{i=1}^{N-1} D(\nabla_{\theta}\ell(S_{[i]};\theta),\nabla_{\theta}\ell(B;\theta))\right)","where \(\lambda\) is the learning rate for the synthetic dataset \(\mathcal{S}\), and the summation accounts for the ""subset loss"" from the previous subsets."
ICLR_2024_oral_54,5,"F_{t}(S_{[n]},B)=D(f_{t}(S _{[n]}),f_{t}(B))","where \(f_{t}(\cdot)\) is the feature extraction function for \(t_{th}\) condensation iteration, and \(D(\cdot)\) is a distance metric like MSE","d_{t}(S_{[n]},B)=D(f_{t}(S_{[n]}),f_{t}(B))","where \(f_{t}(\cdot)\) is the feature extraction function for the \(t_{th}\) condensation iteration, and \(D(\cdot)\) is a distance metric like MSE."
ICLR_2024_oral_54,6,"F_{t}(S_{[p]},B)>F_{t}(S_{[q]},B),\;\;if\;\;\;1<p<q<N",,"F_{t}(S_{[p]},B)\leq F_{t}(S_{[q]},B),\quadfor  1<p<q<N","where \(F_{t}(\mathcal{S}_{[p]},\mathcal{B})\) and \(F_{t}(\mathcal{S}_{[q]},\mathcal{B})\) are the feature distances for subsets \(\mathcal{S}_{[p]}\) and \(\mathcal{S}_{[q]}\), respectively"
ICLR_2024_oral_54,7,"\underbrace{F_{t-\Delta t}(S_{[p]},B)>F_{t} (S_{[p]},B)}_{p},\;\;\underbrace{F_{t-\Delta t }(S_{[q]},B)>F_{t}(S_{[q]},B)}_{q}",where \(t-\Delta t\) and \(t\) are two different time points for the condensation process,"F_{t-\Delta t}(S_{[1]},B)>F_{t}(S_{[1]},B)",where \(t-\Delta t\) and \(t\) are two different time points for the condensation process
ICLR_2024_oral_54,8,"R(S_{[n]},t)=\frac{\Delta F_{S_{[n]}}}{\Delta t}=\frac{ |F_{t}(S_{[n]},B)-F_{t-\Delta t}(S_{[n]},B)|}{\Delta t}","where \(R(\mathcal{S}_{[n]},t)\) represents the rate of change of feature distance for subset \(\mathcal{S}_{[n]}\) at the time point \(t\), and \(\Delta F_{\mathcal{S}_{[n]}}\) denotes the change in feature distance of subset \(\mathcal{S}_{[n]}\) from time \(t-\Delta t\) to \(t\)","R(S_{[n]},t)=\frac{\Delta F_{S_{[n]}}}{\Delta t}=\frac{F_{t-\Delta t}(S_{[n]},B)-F_{t}(S_{[n]},B)}{\Delta t}","where \(R(\mathcal{S}_{[n]},t)\) represents the rate of change of feature distance for subset \(\mathcal{S}_{[n]}\) at the time point \(t\), and \(\Delta F_{\mathcal{S}_{[n]}}\) denotes the change in feature distance of subset \(\mathcal{S}_{[n]}\) from time \(t-\Delta t\) to \(t\)."
ICLR_2024_oral_54,9,"S_{MLS}(t)=S_{[n_{t}^{*}]}=\operatorname*{arg\, max}_{S_{[n]}}(R(S_{[n]},t))\;\; where\;\;n\in[1,N-1]",,"S[n^{*}]=\arg\max_{n\in[1,N-1]}R(S_{[n]},t)","where \(\mathcal{S}[n^{*}]\) is the Most Learnable Subset (MLS) selected at time \(t\) based on the highest feature distance reduction rate \(R(\mathcal{S}_{[n]},t)\)."
ICLR_2024_oral_54,10,{l}Using Eq.~{\ref{eq:mLS} to~{}}Update&S\\Update&S\setminusS_{MLS}(t-\Delta t)if\;S_{MLS}(t)\supsetS_{MLS}(t-\Delta t),where \(\setminus\) is the symbol for set minus,"SarrowS-\lambda(\nabla_{S}D(S,B;\theta)+\nabla_{S_{MLS}}D(S_{MLS},B;\theta)),\hskip 14.226378ptif\;\;|S_{MLS}|\neq|S_{MLS,prev}|","where \mathcal{S}_{\mathrm{MLS}} represents the most learnable subset at the current iteration, and \mathcal{S}_{\mathrm{MLS,prev}} is the most learnable subset from the previous iteration."
ICLR_2024_oral_55,1,"\operatorname*{arg\,max}_{\theta}\prod_{\bar{D}}p(Y_{Q}|f_ {\theta}(S,X_{Q}))",,"L(\theta)=\sum_{(x,y)\inS}L_{class}(f_{\theta}(x), y)","Loss function \(\mathcal{L}(\theta)\) for few-shot classification, where \(\mathcal{L}_{\text{class}}(f_{\theta}(x), y)\) is the classification loss for a sample \(x\) with true label \(y\)."
ICLR_2024_oral_55,2,"\operatorname*{arg\,max}_{\alpha,\phi}\prod_{\bar{D}_{test}} p(Y_{Q}|f_{\alpha,\phi}(S,X_{Q}))",,"\operatorname*{arg\,max}_{\theta}\prod_{\bar{D}_{train}}p(Y_{Q}|f_{\theta}(S,X_{Q}))+\operatorname*{arg\,max}_{\phi}\prod_{\bar{D}_{test}}p(Y_{Q}|f_{\phi}(S,X_{Q};\alpha))","Objective function for knowledge transfer from meta-train to meta-test, incorporating fine-tuning of parameters and additional adapter parameters."
ICLR_2024_oral_55,3,"g_{\phi,\phi^{\prime},\alpha}(x)=g_{\phi,\phi^{\prime}}(x)+h_{\alpha}(x)","where \(x\in\mathbb{R}^{W,H,C}\)",h_{\alpha}(g_{\phi}(x)),"where \(x\in\mathbb{R}^{W,H,C}\)"
ICLR_2024_oral_55,4,"g_{\phi,\phi^{\prime},\alpha}(x)=z(A_{qkv}[q\;;\;g_{\phi,\phi^{\prime}}(x)]+h _{\alpha 1})+h_{\alpha 2}",where \(x\in\mathbb{R}^{D}\) and \([\cdot\;;\;\cdot]\) denotes the concatenation operation,"A_{qkv}(x)=A_{qkv}(x)+h_{\alpha}(x),\quad z(x)=z(x)+h_{\alpha}(x),^{D}\) and\([\cdot\;;\;\cdot]\) denotes the concatenation operation</description>",where \(x\in\mathbb{R}^{D}\) and \([\cdot\;;\;\cdot]\) denotes the concatenation operation
ICLR_2024_oral_55,5,"L(f,S,Q)=\frac{1}{|Q|}\sum_{i=1}^{|Q|}\log\frac{e^{-d_{cos}(C_{Q_{i}},f(Q_{i}))}}{\sum_{j=1}^{|C|}e^{-d_{cos}(C_{j},f(Q_{cj}))}}","where \(C_{\mathcal{Q}_{i}}\) denotes the embedding of the class centroid that corresponds to the true class of \(\mathcal{Q}_{i}\), and \(d_{cos}\) denotes the cosine distance","L(f,S,Q)=\sum_{i=1}^{n} d_{cos}(f_{\theta,\alpha,\phi^{\prime}}(X_{Q_{i}}), C_{Q_{i}})","where \(C_{\mathcal{Q}_{i}}\) denotes the embedding of the class centroid that corresponds to the true class of \(\mathcal{Q}_{i}\), and \(d_{cos}\) denotes the cosine distance."
ICLR_2024_oral_55,6,"\operatorname*{arg\,min}_{\alpha,\phi^{\prime}}\E_{p\sim P}E_{S,Q}\L(f^{p}_{\theta,\alpha,\phi^{\prime}},S,Q)",,"\operatorname*{arg\,min}_{\theta,\alpha,\phi^{\prime}}\sum_{p\in P}L(f^{p}_{\theta,\alpha,\phi^{\prime}},S,Q)",where \(P\) is the set of all possible paths sampled from the search space
ICLR_2024_oral_55,7,"p_{k}=\operatorname*{arg\,max}_{p\in P}E_{S,Q}A(f^{p}_{\theta,\alpha^{\prime},\phi^{\prime\prime}},S,Q),\quads.t || \alpha^{*},\phi^{\prime*}=\operatorname*{arg\,min}_{\alpha,\phi^{\prime}}L(f^{p} _{\theta,\alpha,\phi^{\prime}},S,S)",,"(p_{1},p_{2},...,p_{N})\in\arg\max_{p\in P}E_{S,Q}L(f^{p}_{\theta,\alpha,\phi^{\prime}},S,Q)",where \(N\) denotes the number of selected models
ICLR_2024_oral_55,8,"\quad\forall_{j=1,\dots,k-1}\\d_{cos}(p_{k},p_{j})\geq T",,"p_{k}=\operatorname*{arg\,max}_{p\in P}E_{S,Q}A(f^{p}_{\theta,\alpha^{\prime},\phi^{\prime\prime}},S,Q),\quads.t.\\alpha^{*},\phi^{\prime*}=\operatorname*{arg\,min}_{\alpha,\phi^{\prime}}L(f^{p}_{\theta,\alpha,\phi^{\prime}},S,S)","where \(A(f^{p}_{\theta,\alpha^{\prime},\phi^{\prime\prime}},\mathcal{S}, \mathcal{Q})\) denotes the accuracy of the model on the support and query sets."
ICLR_2024_oral_55,9,"A(f,S,Q)=\frac{1}{|Q|}\sum_{i=1}^{|Q|}[\operatorname*{arg\,min}_{j}d_{cos}(C_{Q_{j}},f(Q_{i}))=Y_ {Q_{i}}]",,"p_{k}=\operatorname*{arg\,max}_{p\in P}E_{S,Q}A(f^{p}_{\theta,\alpha^{\prime},\phi^{\prime\prime}},S,Q),\quads.t.\quad\alpha^{*},\phi^{\prime*}=\operatorname*{arg\,min}_{\alpha,\phi^{\prime}}L(f^{p}_{\theta,\alpha,\phi^{\prime}},S,S)\quad\forall_{j=1,\dots,k-1}\\d_{cos}(p_{k},p_{j})\geq T","where \(T\) denotes a scalar threshold for the cosine distance between paths \(p_{k}\) and \(p_{j}\), and \(A\) is the classification accuracy of a nearest centroid classifier (NCC) (Snell et al., 2017)"
ICLR_2024_oral_55,10,"p^{*}=\operatorname*{arg\,min}_{p\in\{p_{1},...,p_{N}\}}L(f^{p}_{O_{\alpha^{*}},\phi^{\prime\prime*}},S,S),\quads.t || \alpha^{*},\phi^{\prime*}=\operatorname*{arg\,min}_{\alpha,\phi^{\prime}}L(f^{p}_{O,\alpha,\phi^{\prime}},S,S)",,"p^{*}=\operatorname*{arg\,max}_{p_{k}} A(f^{p_{k}}_{\theta,\alpha^{\prime},\phi^{\prime\prime}},S,Q)",where \(p^{*}\) is the selected path for adaptation based on the support set data
ICLR_2024_oral_56,1,"\vec{x}_{n}&=A\vec{x} _{n-1}+Bu_{n}&A\inC^{N\times N},B\inC^{N\times 1}\\y_{n}&=C\vec{x}_{n}&C\inC^{1\times N}",,y_n=\vec{W}\cdot\vec{x}_n,Output sequence \(y_n\) is generated from the hidden state vector \(\vec{\mathbf{x}}_n\) using weight matrix \(\vec{\mathbf{W}}\).
ICLR_2024_oral_56,2,A=A-PQ^{\star} || \bar{A}=(I-\Delta/2\cdotA)^{-1}(I+\Delta/2\cdotA),,K_{k}=C^{T}A^{k}B,"Kernel \(K_{k}\) defined for the convolution operation, where \(\mathbf{C}\) is the output matrix, \(\mathbf{A}\) is the state transition matrix raised to the power \(k\), and \(\mathbf{B}\) is the input matrix."
ICLR_2024_oral_56,3,\bar{B}=(I-\Delta/2\cdotA)^{-1}\DeltaB\quad\bar{C}=C,,K_{k}=C^{T}A^{k}B,"Kernel defined for the convolution operation, where \(K_{k}\) represents the kernel at timestep \(k\), \(\mathbf{C}\) is the output matrix, \(\mathbf{A}\) is the state transition matrix raised to the power of \(k\), and \(\mathbf{B}\) is the input matrix."
ICLR_2024_oral_56,4,K_{k}=\bar{C}^{T}\bar{A}^{k}\bar{B},"where \(\mathbf{A},\mathbf{P},\mathbf{Q},\mathbf{B},\mathbf{C},\Delta\) are learnable parameters and \(\mathbf{A}\in\text{{Diag}}(\mathbb{C}^{N\times N}),\mathbf{P},\mathbf{Q}\in\mathbb{C}^{N \times 1}\)",\bar{A}=A-PQ^{\star},"where \(\mathbf{A},\mathbf{P},\mathbf{Q},\mathbf{B},\mathbf{C},\Delta\) are learnable parameters and \(\mathbf{A}\in\text{{Diag}}(\mathbb{C}^{N\times N}),\mathbf{P},\mathbf{Q}\in\mathbb{C}^{N \times 1}\)"
ICLR_2024_oral_56,5,"\vec{x}_{n}&=A\vec{x} _{n-1}+I\,u_{n}&A\in{diag}(C^{N\times N })\\y_{n}&=C\vec{x}_{n}&C\inC^{1\times N}",where \(\mathbf{I}\) is the all-ones vector,K_{k}=C^{T}A^{k}B,where \(\mathbf{I}\) is the all-ones vector
ICLR_2024_oral_58,1,"\mu_{m,k}\triangleqE_{(x,y)\simG}[\upsilon_{m,k}(x) ],\\sigma_{m,k}^{2}\triangleqE_{(x,y)\simG}[(\upsilon_{m,k}(x)-\mu_{m,k})^{2}]",,"\mu_{m,k}=\frac{1}{N}\sum_{i=1}^{N} f_{m}(x_{i})\quadand\quad\sigma_{m,k}^{2}=\frac{1}{N}\sum_{i=1}^{N} (f_{m}(x_{i})-\mu_{m,k})^{2}",Mean and variance of the \(k^{\text{th}}\) feature for model \(f_{m}\).
ICLR_2024_oral_58,2,"\rho_{(i,j),(a,b)}\triangleqE_{(x,y)\simG}[(\upsilon_{i,a}(x)-\mu_{i,a})(\upsilon_{j,b}(x)-\mu_{j,b}) ](\sigma_{i,a}\\sigma_{j,b})^{-1}",,"\rho_{i,j}^{a,b}\triangleq\frac{E_{(x,y)\simG}[\upsilon_{i,a}(x)\upsilon_{j,b}(x)]-\mu_{i,a}\mu_{j,b}}{\sigma_{i,a}\sigma_{j,b}}",Correlation between models \(f_{i}\) and \(f_{j}\) for features \(a\) and \(b\).
ICLR_2024_oral_58,3,Acc=p_{d}(1-\frac{1}{2}\binom{t_{d}-c_{d}}{a_{d}}\\\binom{t_{d}}{a_{d}}^{+}p_{r}(1-\frac{1}{2}\binom{t_{r}-c_{r}}{c_{r}}\\\binom{t_{r}}{c_{r}}),,"Acc\triangleqE_{(x,y)\simG}\left[1\left\{ |\Psi(f)\cap\Psi(x)|>0\right\}\right]",The expected accuracy over the model distribution and data distribution.
ICLR_2024_oral_59,1,\Pr[A(D)\in R]\leq e^{\epsilon}\Pr[A(D^{\prime})\in R]+\delta,,\Pr[A(D)\in R]\leq e^{\epsilon}\Pr[A(D')\in R]+\delta,"The probability that the output of the algorithm \(A\) on dataset \(D\) falls within region \(R\) is bounded by a function of the output on dataset \(D'\), controlled by the privacy parameters \(\epsilon\) and \(\delta\)."
ICLR_2024_oral_59,2,"\hat{\epsilon}=\max\{\log\frac{1-\delta-FPR}{FNR},\log\frac{1-\delta-FNR}{FPR}\}",,\epsilon=\ln\left(\frac{1-FPR}{FNR}\right),Estimation of the privacy parameter \(\epsilon\) in terms of the false positive rate (FPR) and false negative rate (FNR).
ICLR_2024_oral_59,3,f_{d}(t)=\frac{\Gamma(\frac{d}{2})}{\Gamma(\frac{d-1}{2})\sqrt{\pi}}(1-t^{2})^{\frac{d-3}{2}},,"f_{\tau_{d}}(t)=\frac{d-1}{2}\left(1-t^{2}\right)^{\frac{d-3}{2}},\quad t\in[-1,1]",Probability density function of the cosine similarity between a uniformly sampled vector from the unit sphere and an arbitrary independent nonzero vector.
ICLR_2024_oral_6,1,"\epsilon_{k}(x)\mathop{=}^{def}u_{k}(\texttt{BR}_{k},x_{-k})-u_{k}(x) where \texttt{BR}_{k}\in\operatorname*{arg\,max}_{z}u_{k}(z,x_{-k})",,"f(x)=\sum_{i=1}^{n}\max_{x_i'\in X_i}\left( u_i(x_i', x_{-i})-u_i(x_i, x_{-i})\right)","where \( f(x) \) is the loss function, \( n \) is the number of players, \( u_i \) is the utility function for player \( i \), \( x_i \) is the strategy of player \( i \), \( x_{-i} \) represents the strategies of all other players, and \( X_i \) is the strategy space for player \( i \)."
ICLR_2024_oral_6,2,L(x)=\sum_{k}\eta_{k} || \Pi_{T\Delta}(\nabla_{x_{k}}^{k}) || ^{2},"where each \(\eta_{k}>0\) represents a scalar weight, or equivalently, a step size to be explained next",L(x)=\sum_{k}\eta_{k}\|\nabla_{x_{k}} u_{k}(x)\|_{T\Delta}^{2},"where each \(\eta_{k}>0\) represents a scalar weight, or equivalently, a step size to be explained next."
ICLR_2024_oral_6,3,\epsilon_{k}(x)\leq\sqrt{2} || \Pi_{T\Delta}(\nabla_{x_{k}}^{k}) || ,,\epsilon_{k}(x)\leq C || \Pi_{T\Delta}(\nabla_{x_{k}}^{k}) || ,where \(C\) is a constant that depends on the specific utility functions of the players.
ICLR_2024_oral_6,4,\epsilon\leq\sqrt{\frac{2n}{\min_{k}\eta_{k}}}\sqrt{L(x)}\stackrel{{\tiny def}}{{=}}f(L),,\epsilon_{k}(x)\leq\sqrt{2L(x)},The exploitability of a joint strategy \(\mathbf{x}\) is upper bounded by a function of the loss \(\mathcal{L}(\mathbf{x})\).
ICLR_2024_oral_6,5,L(x)=E[\sum_{k}\eta_{k}\underbrace{(\hat{\nabla}_{x_{ k}}^{k(1)}-\frac{1}{m_{k}}(1^{\top}\hat{\nabla}_{x_{k}}^{k(1)}))}_{ projected-gradient 1})^{\top}(\underbrace{\hat{\nabla}_{x_{k}}^{k(2)}-\frac{1}{m_{k}}(1^{\top}\hat{\nabla}_{x_{k}}^{k(2)})1)}_{projected-gradient 2}),where \(\hat{\nabla}^{k(p)}_{x_{k}}\) is an unbiased estimator of player \(k\)'s gradient,L(x)=\frac{1}{2}E\left[ || \hat{\nabla}^{k(1)}_{x_{k}} || ^{2}+ || \hat{\nabla}^{k(2)}_{x_{k}} || ^{2}\right],where \(\hat{\nabla}^{k(p)}_{x_{k}}\) is an unbiased estimator of player \(k\)'s gradient
ICLR_2024_oral_6,6,L^{\tau}(x)=\sum_{k}\eta_{k} || \Pi_{T\Delta}(\nabla^{k\tau}_{x_ {k}}) || ^{2},,L^{\tau}(x)=E[\sum_{k}\eta_{k}\underbrace{(\hat{\nabla}^{k\tau(1)}_{x_{k}}-\frac{1}{m_{k}}(1^{\top}\hat{\nabla}^{k\tau(1)}_{x_{k}}))}_{ projected-gradient 1})^{\top}(\underbrace{\hat{\nabla}^{k\tau(2)}_{x_{k}}-\frac{1}{m_{k}}(1^{\top}\hat{\nabla}^{k\tau(2)}_{x_{k}})1)}_{projected-gradient 2}),where \(\hat{\nabla}^{k\tau(p)}_{x_{k}}\) is an unbiased estimator of player \(k\)'s gradient with the entropy term included.
ICLR_2024_oral_6,7,\epsilon_{QRE}\leq\sqrt{\frac{2n}{\min_{k}\eta_{k}}}\sqrt{L^{\tau}(x)}\overset{\tiny{def}}{=}f(L^{\tau}),,\epsilon_{QRE}\leq\sqrt{\frac{2n}{\min_{k}\eta_{k}}}\sqrt{L^{\tau}(x)}\stackrel{{\tiny def}}{{=}}f(L^{\tau}),where \(\mathcal{L}^{\tau}(\mathbf{x})\) is the refined loss function with entropy bonuses
ICLR_2024_oral_6,8,\epsilon\leq\tau\log\Big{(}\prod_{k}m_{k}\Big{)}+\sqrt{\frac{2n}{\min_{k}\eta_ {k}}}\sqrt{L^{\tau}(x)}\overset{\tiny{def}}{=} f_{\tau}(L^{\tau}),,L^{\tau}(x)=0 if and only if \epsilon_{QRE}=0,where \(\mathcal{L}^{\tau}(\mathbf{x})\) is the entropy regularized loss function and \(\epsilon_{QRE}\) is the exploitability of the approximate quantal response equilibrium.
ICLR_2024_oral_6,9,\nabla_{x_{l}}L^{\tau}(x)=2\sum_{k}\eta_{k}B_{kl}^{\top}\Pi_{T\Delta}(\nabla_{x_{k}}^{kT}),,\nabla_{x_{l}}L^{\tau}(x)=2\sum_{k}\eta_{k}\left(\Pi_{T\Delta}(\nabla^{k\tau}_{x_{k}})\right)^{\top}\nabla_{x_{l}}(\Pi_{T\Delta}(\nabla^{k\tau}_{x_{k}})),the gradient of the proposed loss function with respect to player \(l\)'s strategy \(x_{l}\)
ICLR_2024_oral_6,10,\textsl{Hess}(L^{\tau})=2\big{[}\tilde{B}^{\top}\tilde{B}+T\Pi_{T\Delta}(\tilde{\nabla}^{\tau})\big{]},,H_{kl}=\sum_{k}\eta_{k}\Big{(}B_{kl}^{\top}\Pi_{T\Delta}(\nabla_{x_{k}}^{kT})\Big{)},where \(H_{kl}^{k}\) captures the second-order behavior of the loss with respect to player \(k\)'s strategy \(x_{k}\).
ICLR_2024_oral_6,11,"M(x)=-\sqrt{\eta_{1}}\Pi_{T\Delta}(\frac{1}{x_{1}})&\sqrt{\eta_{1}}\Pi_{T\Delta}(H^{1}_{12})&\ldots&\sqrt{\eta_{1}}\Pi_{T\Delta}(H^{1}_{ 1n})\\\vdots&\vdots&\vdots&\vdots\\\sqrt{\eta_{n}}\Pi_{T\Delta}(H^{n}_{n1})&\ldots&\sqrt{\eta_{n}}\Pi_{T\Delta}( H^{n}_{n,n-1})&-\tau\sqrt{\eta_{n}}\Pi_{T\Delta}(\frac{1}{x_{n}})\\1_{1}^{\top}&0&\ldots&0\\\vdots&\vdots&\vdots&\vdots\\0&\ldots&0&1_{n}^{\top}",where \(\Pi_{T\Delta}(z\in\mathbb{R}^{n\times b})=[I_{a}-\frac{1}{a}\mathbf{_{1}}\mathbf{_{ 1}}^{\top}]z\) subtracts the mean from each column of \(z\) and \(\frac{1}{x_{k}}\) is shorthand for \(\texttt{diag}\big{(}\frac{1}{x_{k}}\big{)}\),\Pi_{T\Delta}(z\inR^{n\times b})=[I_{a}-\frac{1}{a}_{1}_{ 1}^{\top}]z where \frac{1}{x_{k}}=\texttt{diag}\big{(}\frac{1}{x_{k}}\big{)},where \(\Pi_{T\Delta}(z\in\mathbb{R}^{n\times b})\) projects onto the tangent space of the simplex and \(\frac{1}{x_{k}}\) denotes a diagonal matrix with entries corresponding to the inverse of player \(k\)'s strategy.
ICLR_2024_oral_60,1,y_{i}^{\prime}=I_{\tau}(h)(x_{i}),,"y_{i}^{\prime}=I_{\tau}(h)(x_{i})\quadfor  i=1, 2,\ldots, n",Outputs \(y_{i}^{\prime}\) derived from applying the interpreter \(I_{\tau}\) to the input \(x_{i}\) using the induced rule \(h\).
ICLR_2024_oral_60,2,"a_{\tau}=\frac{1}{|D_{\tau}^{u}|}\sum_{(x,y)\inD_{\tau}}\mathbbm{1}\big{[}I_{\tau}(h)(x)=y\big{]}",,Acc_{\tau}=\frac{1}{n}\sum_{i=1}^{n}I(y_{i}^{\prime}=y_{i}),Accuracy \(Acc_{\tau}\) for task \(\tau\) based on the predictions \(y_{i}^{\prime}\) and true outputs \(y_{i}\) for \(n\) unseen examples.
ICLR_2024_oral_60,3,c=\frac{1}{|T|}\sum_{\tau\inT}a_{\tau}\hskip 28.452756ptc_{t}=\frac{1}{|T|}\sum_{\tau\inT}\mathbbm{1}\big{[}a_{\tau}=1\big{]},,"c=\frac{1}{|T|}\sum_{\tau\inT} a_{\tau},\quad c_{t}=\frac{1}{|D_{\tau}^{u}|}\sum_{(x,y)\inD_{\tau}^{u}}\mathbbm{1}\big{[}I_{\tau}(h)(x)=y\big{]}",Definition of raw accuracy \(c\) and task accuracy \(c_{t}\) for evaluating the performance across all tasks and specific tasks respectively.
ICLR_2024_oral_60,4,"h^{t}\sim P_{LM}\big{(}\cdot\,|d^{t-1},x_{1},y_{1},...,x_{k},y_{k})",where \(d^{t-1}\) is the feedback from previous iterations and which is set to be an empty string at the initial iteration,"H^{t}=\{h_{1}^{t},...,h_{N}^{t}\}.\",where \(d^{t-1}\) is the feedback from previous iterations and which is set to be an empty string at the initial iteration.
ICLR_2024_oral_60,5,"s(h,D_{\tau}^{s})=\frac{1}{|D_{\tau}^{s}|}\sum_{(x,y)\inD_{\tau}^{s}}\mathbbm{1}\big{[}I_{\tau}(h)(x)=y\big{]}",,"s(h,D_{\tau}^{s})=a_{\tau}^{s}.\",where \(a_{\tau}^{s}\) is the accuracy of the rule \(h\) on the seen examples \(\mathcal{D}_{\tau}^{s}\).
ICLR_2024_oral_60,6,"h^{t^{*}}=\operatorname*{arg\,max}_{h^{\prime}\in H^{t}}s(h^{\prime}, D_{\tau}^{s})",,"h^{*}=\arg\max_{h\in H^{t}} s(h,D_{\tau}^{s})","The best hypothesis \(h^{*}\) is selected by maximizing the scoring function \(s(h,\mathcal{D}_{\tau}^{s})\) over the set of hypotheses \(H^{t}\)."
ICLR_2024_oral_61,1,"P(a_{t:t+k}|s_{t},s^{g})=\int_{s_{t+1},\ldots,s_{t+k}}ds_{t+1}\ldots ds_{t+k}\prod_{i=t}^{t+k}P_{\phi}(a_{i}|s_{i},s^{g})P(s_{i+1}|s_{i},a_{i})",where \(s^{g}\in S\) is the goal state,P_{\phi}(a | s^{g}),"where \(P_{\phi}\) is the goal-conditioned policy, \(a\) is the action, and \(s^{g}\) is the goal state in the state space \(S\)."
ICLR_2024_oral_61,2,"L(\phi)=E_{D}[-\log P_{\phi}(a_{i}|s_{i},s^{g})]",,"L(\phi)=-\sum_{i=t}^{t+k}\log P_{\phi}(a_{i}|s_{i},s^{g})","where \(\mathcal{L}(\phi)\) is the loss function to minimize, and \(P_{\phi}(a_{i}|s_{i},s^{g})\) is the predicted probability of action \(a_{i}\) given state \(s_{i}\) and goal state \(s^{g}\)."
ICLR_2024_oral_61,3,L(\psi)=E_{D}[-\log\pi^{p}_{\psi}(a^{h}|s_{t})],,L(\psi)=E_{D}[-\log\pi^{p}_{\psi}(a^{h}|s_{t})],"The training objective for the goal prior model, where \(\mathcal{L}(\psi)\) is the loss function, \(\pi^{p}_{\psi}(a^{h}|s_{t})\) is the predicted distribution of future goals given the current state \(s_{t}\)."
ICLR_2024_oral_61,4,"J(\theta)=E\pi_{\theta}[\sum_{t=0}^{\infty}\gamma^{t}(\sum _{i=kt}^{(k+1)t}R(s_{i},a_{i})-\alpha D_{KL}(\pi^{p}_{\psi}(a^{h }|s_{kt})\|\pi_{\theta}(a^{h}|s_{kt})))]",where \(t\) represents the number of steps for the high-level policy and \(\alpha\) is a hyperparameter balancing the environmental rewards and the intrinsic rewards,L(\theta)=E_{\tau}\left[\sum_{t=0}^{T}\left( R_{t}+\alpha\cdot r^{p}_{t}\right)\right],"where \(R_{t}\) is the environmental reward at time step \(t\), \(r^{p}_{t}\) is the intrinsic reward from the goal prior model, and \(T\) is the total number of steps."
ICLR_2024_oral_63,1,p_{\theta}(x)\propto e^{-f_{\theta}(x)},,p(x)=\frac{e^{-f_{\theta}(x)}}{Z(\theta)},where \(Z(\theta) = \int e^{-f_{\theta}(x)} dx\) is the partition function.
ICLR_2024_oral_63,2,"x_{k+1}=x_{k}-\delta\,\nabla f_{\theta}(x_{k})+\sqrt{2\delta}\,\varepsilon_{k },\,\varepsilon_{k}\simN(0,I_{d})","where \(\nabla\) denotes the gradient of the energy function with respect to inputs, \(k\) is the sampling step, \(\delta\) is the (discretization) step size, and the noise \(\varepsilon_{k}\) is drawn from the normal distribution at each step",x_{k+1}=x_k-\frac{\delta}{2}\nabla f_{\theta}(x_k)+\sqrt{\delta}\varepsilon_k,"where \(x_k\) is the current sample, \(\nabla\) denotes the gradient of the energy function with respect to inputs, \(k\) is the sampling step, \(\delta\) is the (discretization) step size, and the noise \(\varepsilon_{k}\) is drawn from the normal distribution at each step."
ICLR_2024_oral_63,3,\hat{x}(y)=y+\sigma^{2}\nabla\log p(y),where \(p(y)=\int p(y|x)p(x)dx\) is the probability distribution function of the smoothed density,\hat{x}=\arg\min_{x}\|y-x\|^{2}+\frac{1}{\sigma^{2}}\int p(y|x)p(x)dx,"where \(\hat{x}\) is the estimated value of \(X\), \(y\) is the observed value with noise, and \(\sigma^{2}\) is the variance of the Gaussian noise."
ICLR_2024_oral_63,4,\hat{x}_{\phi}(y)=y+\sigma^{2}g_{\phi}(y),,\hat{x}(y)=y+\sigma^{2}g_{\phi}(y),where \(g_{\phi}(y)\) is the score function parameterized by a neural network.
ICLR_2024_oral_63,5,"L(\phi)=E_{x\sim p(x),y\sim p(y|x)}\|x-\hat{x}_{\phi}(y)\|^ {2}",,"L(\theta,\phi)=E_{Y\sim p(Y)}\left[\frac{1}{2\sigma^{2}}\|Y-\hat{x}_{\phi}(Y)\|^{2}\right]-E_{X\sim p(X)}\left[f_{\theta}(X)\right]","The learning objective \(\mathcal{L}(\theta, \phi)\) combines the reconstruction loss and the energy term, where \(Y\) is the observed data, \(\hat{x}_{\phi}(Y)\) is the estimated latent variable, and \(f_{\theta}(X)\) is the energy function for the latent variable \(X\)."
ICLR_2024_oral_63,6,"\operatorname*{arg\,max}_{\theta}E_{y\sim p_{Y}}[\log p_{\theta}( y)]=\operatorname*{arg\,max}_{\theta}(E_{y^{-}\sim p_{Y}(y)}[f_{\theta}(y^{-})]-E_{y^{+}\sim p_{Y}}[f_{\theta}(y^{+})])",where \(y^{+}\) are noisy training data and \(y^{-}\) are noisy data sampled from the model,L_{EBM}(\theta)=E_{y^{+}\sim p(y^{+})}\left[\log p_{\theta}(y^{+})\right]-E_{y^{-}\sim p(y^{-})}\left[\log p_{\theta}(y^{-})\right],where \(y^{+}\) are noisy training data and \(y^{-}\) are noisy data sampled from the model.
ICLR_2024_oral_63,7,\nabla_{\theta}\log p_{\theta}(y)=-\nabla_{\theta}f_{\theta}(y)-\nabla_{\theta}\log Z(\theta) || =-\nabla_{\theta}f_{\theta}(y)+\frac{\int\nabla_{\theta}f_{\theta}(y)e^{-f_{\theta}(y)}dy}{Z(\theta)} || =-\nabla_{\theta}f_{\theta}(y)+\int\nabla_{\theta}f_{\theta}(y)\cdot p_{\theta}(y)dy || =-\nabla_{\theta}f_{\theta}(y)+E_{y\sim p_{\theta}(y)}[\nabla_{\theta}f_{\theta}(y)],,E_{y^{+}\sim p_{Y}}[f_{\theta}(y^{+})]-E_{y^{-}\sim p_{Y}(y)}[f_{\theta}(y^{-})]=0,where the left-hand side represents the difference in expected energy between positive and negative samples.
ICLR_2024_oral_63,8,"P^{\top}argmax\,\hat{x}(y,t)=P^{\top}s",,"P=1 & 0 &\cdots & 0\\0 & 1 &\cdots & 0\\\vdots &\vdots &\ddots &\vdots\\0 & 0 &\cdots & 1\\0 & 0 &\cdots & 0 ,\) is the binary projection matrix that enforces constraints on the generated sequences.</description>",where \(\mathbf{P}\) is the binary projection matrix that enforces constraints on the generated sequences.
ICLR_2024_oral_63,9,\chi_{ii^{\prime}}=\frac{ || X_{i}-X_{i^{\prime}} || }{2\sqrt{d}},where \(d\) is the dimension of the data and the \(\frac{1}{2\sqrt{d}}\) scaling comes from the concentration of isotropic Gaussians in high dimensions,"\chi_{ij}=\frac{1}{2\sqrt{d}}e^{-\frac{\|e_{i}-e_{j}\|^{2}}{2\sigma^{2}}},\quad i,j=1,\ldots,L",where \(d\) is the dimension of the data and the \(\frac{1}{2\sqrt{d}}\) scaling comes from the concentration of isotropic Gaussians in high dimensions.
ICLR_2024_oral_63,10,\sigma_{c}=\max_{ii^{\prime}}\chi_{ii^{\prime}},,"\sigma_{c}=\frac{1}{\sqrt{d}}\), is defined as the threshold for effective sampling in high-dimensional spaces.</description>","The critical noise level, \(\sigma_{c}\), is defined as the threshold for effective sampling in high-dimensional spaces."
ICLR_2024_oral_64,1,Z_{k}\subseteqR^{M}\quadand\quadZ=Z_{1}\times\dots\timesZ_{K}\subseteqR^{KM},,"Z=\bigoplus_{k=1}^{K}Z_{k},\quadwhere \dim(Z_{k})=M for all  k","The latent space \(\mathcal{Z}\) is factorized into \(K\) slots \(\mathcal{Z}_{k}\), each with \(M\) dimensions."
ICLR_2024_oral_64,2,"x=f(z),\quadz\sim p_{z},\quadsupp(p_{z })=Z^{S}",,X^{S}=f(Z^{S})\subseteqX,Generative process for observations from a training space based on a slot-supported subset.
ICLR_2024_oral_64,3,"L_{rec}(X^{S})=L_{rec}\big{(}\hat {g},\hat{f},X^{S}\big{)}:=E_{x\sim p_{x }}\big{[}\big{\|}\hat{f}\big{(}\hat{g}(x)\big{)}-x\big{\|} _{2}^{2}\big{]},\quadsupp(p_{x})=X^{S}",,L_{reconstruction}=\|x-\hat{f}(\hat{g}(x))\|^2,"The reconstruction objective for the autoencoder, measuring the difference between the original data \(\mathbf{x}\) and the reconstructed data \(\hat{\mathbf{f}}(\hat{\mathbf{g}}(\mathbf{x}))\)."
ICLR_2024_oral_64,4,"\frac{\partialf_{n}}{\partialz_{k}}(z)\neq 0\implies\frac{\partialf_{n}}{\partialz_{j}}(z)=0,\quadfor any $k,j\in[K]$, $k\neq j$ and any $n\in[N]$.",,"For all  i\in [N],\quad\frac{\partialf_{i}(z)}{\partialz_{k}}=0\quadfor all  k\neq\pi(i), where \pi is a permutation of  [K]","Definition of compositionality for the generator function, indicating that each output pixel is influenced by at most one latent slot."
ICLR_2024_oral_64,5,"z^{\prime}=\big{(}h_{1}(z_{\pi(1)}),\ldots,h_{K}(z_{\pi(K)})\big{)},\quadZ^{\prime}=h_{1}(Z_{\pi(1)})\times\cdots\timesh_{K}(Z_{\pi(K)})",,"\hat{z}=\big{(}h_{1}(z_{\pi(1)}),\ldots,h_{K}(z_{\pi(K)})\big{)}",Representation of the inferred latent slots that slot-identifies the ground-truth latent slots for any combination of slots in the latent space.
ICLR_2024_oral_64,6,\hat{g}\big{(}f(z)\big{)}=z^{\prime}\quadand\quad\hat{f}(z^{\prime})=f(z),,"\hat{g}(\hat{f}(z))=z^{\prime},\quad\forallz\inZ",Representation matching condition for an autoencoder to generalize slot identifiability from the slot-supported subset \(\mathcal{Z}^{S}\) to the full latent space \(\mathcal{Z}\).
ICLR_2024_oral_64,7,"\hat{f}(z)=\sum_{k=1}^{K}\varphi_{k}(\hat{z}_{k}),\quadwhere \varphi_{k}:R^{M}\toR^{N} for any k\in[K] and \hat{z}\inR^{KM}",,"\hat{f}(z)=\sum_{k=1}^{K}\hat{f}_{k}(z_{k}),\quadfor any z=\big{(}z_{1},\ldots,z_{K}\big{)}\inZ","Definition of an additive decoder in the context of an autoencoder, where the decoder is expressed as a sum of individual slot functions."
ICLR_2024_oral_64,8,"L_{cons}\big{(}\hat{g},\hat{f},Z^{\prime}\big{)}=E_{z^{\prime}\sim q_{z^{\prime}}}\Big{[}\big{\|}\hat{g}\big{(}\hat{f}(z^{\prime})\big{)}-z^{\prime}\big{\|} _{2}^{2}\Big{]}",,"L_{cons}(X)=E_{x\sim p_{x}}\left[\left\|\hat{g}(x)-z^{\prime}\right\|_{2}^{2}\right],\quadsupp(p_{x})=X","Compositional consistency loss for an autoencoder, ensuring that the encoder inverts the decoder on the entire data space."
ICLR_2024_oral_64,9,"L_{rec}\big{(}\hat{g},\hat{f},X^{S}\big{)}+\lambdaL_{cons}\big{(}\hat{g},\hat{f},Z ^{\prime}\big{)}=0,\qquadfor some \lambda>0",,"L_{rec}(X^{S})+L_{cons}\big{(}\hat{g},\hat{f},Z^{\prime}\big{)}\to 0,\end{latex> <description>The total loss for the autoencoder, combining the reconstruction loss on the training space and the compositional consistency loss on the inferred OOD slots.</description>","The total loss for the autoencoder, combining the reconstruction loss on the training space and the compositional consistency loss on the inferred OOD slots."
ICLR_2024_oral_64,10,"\hat{f}(z)=\sum_{k=1}^{K}\tilde{m}_{k}\odotx_{k},\qquad\tilde{m}_{k}=\sigma(m)_{k},\qquad(m_{k},x_{k})=\varphi _{k}(z_{k})",where \(\odot\) is an element-wise multiplication and \(\sigma(\cdot)\) denotes the softmax function,"L_{rec}\big{(}\hat{g},\hat{f},X^{S}\big{)}+\lambdaL_{cons}\big{(}\hat{g},\hat{f},Z ^{\prime}\big{)}=0,\qquadfor some \lambda>0","The combined loss function for the autoencoder, consisting of the reconstruction loss on the training space and the compositional consistency loss, which is minimized to achieve compositional generalization."
ICLR_2024_oral_64,11,h_{k}(z_{\pi(k)})=\hat{g}_{k}\big{(}f(z)\big{)}\quadand\quadZ^{\prime}=\hat{g}_{1}(X^{S})\times\cdots\times\hat{g}_{K}(X^{S}),,"Z^{\prime}=\big{\{}z^{\prime}=\big{(}h_{1}(z_{\pi(1)}),\ldots,h_{K}(z_{\pi(K)})\big{)}\big{\}}",Definition of the set \(\mathcal{Z}^{\prime}\) as the collection of all possible combinations of inferred slots transformed by the diffeomorphisms \(\mathbf{h}_{k}\).
ICLR_2024_oral_64,12,"z^{\prime}=\big{(}\hat{z}^{(\rho_{1})}_{1},\ldots,\hat{z}^{(\rho _{K})}_{K}\big{)},\quadwhere for i\in\{1,2\}\quad\hat{z}^{(i)}=\hat{g}\big{(}x^{(i)}\big{)},\,x^{(i)}\sim p_{z}",,"z^{\prime}=\big{(}\hat{z}^{(\rho_{1})}_{1},\hat{z}^{(\rho_{2})}_{2},\ldots,\hat{z}^{(\rho_{K})}_{K}\big{)},\quadwhere \rho_{k}\simU\{1,2\} for  k\in[K]",Construction of OOD samples by shuffling slots of inferred ID latents.
ICLR_2024_oral_66,1,"\theta^{*}\:=\\operatorname*{arg\,min}_{\theta}\E_{(x,t,\ell(x,t),\cdot)\simH}\\Big{[}-\log p(\ell(x,t)\mid x,t,\hat{\ell}(x,t;\theta))\Big{]}",,"\hat{\ell}\left(x,t;\theta\right)=f\left(H,\theta\right)",The probabilistic performance estimator for the pipeline \(x\) at step \(t\) parameterized by \(\theta\).
ICLR_2024_oral_66,2,"\gamma^{*}\:=\\operatorname*{arg\,min}_{\gamma}\E_{(x,t,\cdot,c(x,t))\simH}\\Big{[}c(x,t)-\hat{c}(x,t;\gamma)\Big{]}^{2}",,"\gamma^{*}\:=\\operatorname*{arg\,min}_{\gamma}\E_{(x,t,c(x,t),\cdot)\simH}\\Big{[}-\log p(c(x,t)\mid x,t,\hat{c}(x,t;\gamma))\Big{]}",Optimal parameters for the cost estimator \(\hat{c}\) based on the observed cost data.
ICLR_2024_oral_66,3,"x^{*}:=\operatorname*{arg\,max}_{x\inX}\frac{EI(x,H,\hat{\ell}(x,\tau(x)))}{\hat{c}\Big{(}x,\tau(x)\Big{)}{-}c\Big{(}x,\tau(x)-\Delta t\Big{)}}=\operatorname*{arg\,max}_{x\inX}\frac{E_{\hat{\ell}(x,\tau(x))}[\max(\ell_{\tau(x)}^{\min}-\hat{\ell}(x,\tau(x)),0)]}{\hat{c}\Big{(}x,\tau(x){-}c\Big{(}x,\tau(x){-}\Delta t\Big{)}}",,"EI_{cost}(x,t)=E\left[\max\left(0,\hat{\ell}(x',t')-\hat{\ell}(x,t)-\xi\right)\right]-\lambda\hat{c}(x,t;\gamma)",Cost-sensitive Expected Improvement acquisition function for selecting the next pipeline to evaluate.
ICLR_2024_oral_66,4,"\theta^{(M)} :=\\operatorname*{arg\,min}_{\theta}\E_{(x,t,\ell(x,t,d),c(x,t,d))\simH^{(M)}}\\[-\log p(\ell(x,t,d)\mid x,t,d,\hat{\ell}(x,t,d;\theta))] || \gamma^{(M)} :=\\operatorname*{arg\,min}_{\gamma}\E_{(x,t,\ell(x,t,d),c(x,t,d))\simH^{(M)}}\\(c(x,t,d)-\hat{c}(x,t,d;\gamma))^{2}",,"\theta^{*}\:=\\operatorname*{arg\,min}_{\theta}\E_{(x,t,d,\ell(x,t,d),\cdot)\simH^{\left(M\right)}}\\Big{[}-\log p(\ell(x,t,d)\mid x,t,d,\hat{\ell}(x,t,d;\theta))\Big{]}",Meta-learned probabilistic validation error estimator for the meta-dataset.
ICLR_2024_oral_66,5,"M=\{m^{*}\,|\,m^{*}\in\operatorname*{arg\,max}_{m\inM _{Timm}}[f_{ImageNet}(m),\-S(m)]\}",,"m^{*} :=\operatorname*{arg\,max}_{m\inM_{Timm}}\left( f_{ImageNet}(m),-S(m)\right)",Optimal model selection based on Top-1 ImageNet accuracy and number of parameters.
ICLR_2024_oral_68,1,P(v\middo(x))=\prod_{i:v_{i}\notinx}P(v_{i }\midpa_{v_{i}})&if $v$ consistent with $x$\\0&otherwise.,,"P(V_{x})=\prod_{i=1}^{n}P(V_{i}\midPa_{V_{i}},X=x)",Truncated factorisation formula for causal Bayesian networks under intervention.
ICLR_2024_oral_68,2,"P(v_{i}\,|pa_{i};\sigma)=\sum_{v_{i}^{\prime}:f(v_{i}^{\prime})=v_{i}}P(v_ {i}^{\prime}\,|pa_{i})",,"P(v;\sigma)=\prod_{i}P(v_{i}\midpa_{v_{i}})\cdot\delta(v_{i},f(v_{i}))","where \(\delta(v_{i},f(v_{i}))\) is the Dirac delta function that enforces the transformation of \(V_{i}\) to the state \(f(v_{i})\)."
ICLR_2024_oral_7,1,"y_{i}=M(x_{i}|\Delta W,W_{0},\theta)",,y_{i}=W_{0}x_{i}+BAx_{i},The output \(\mathbf{y}_{i}\) for a given input \(\mathbf{x}_{i}\) using the adapted weight matrix.
ICLR_2024_oral_7,2,y_{i}=\phi(W_{i}^{T}x_{i}) || =\phi\big{(}(W_{0}^{T}\circ\Delta W_{i}^{T})x_{i}\big{)},,"y_{i}=M(x_{i}|\Delta W_{i},W_{0},\theta)=M(x_{i}|B_{i}A_{i}\circ W_{0},W_{0},\theta)","Where \(\mathbf{y}_{i}\) is the output for input \(\mathbf{x}_{i}\), \(\Delta W_{i}\) is the example-specific adapter matrix, \(W_{0}\) is the pre-trained weight matrix, and \(\circ\) denotes element-wise multiplication."
ICLR_2024_oral_7,3,=\phi\big{(}(W_{0}^{T}\circ(B_{i}A_{i})^{T})x_{i}\big{)},,y_{i}=\phi(W_{i}^{T}x_{i})=\phi\big{(}(W_{0}^{T}\circ\Delta W_{i}^{T})x_{i}\big{)},The activations of the next layer in a neural network for input example \(\mathbf{x}_{i}\) using the weight matrix \(W_{i}\) and the activation function \(\phi\).
ICLR_2024_oral_7,4,=\phi\Big{(}A_{i}\circ\big{(}W_{0}^{T}(B_{i}\circ x_{i})\big{)}\Big{)},,y_{i}=\phi\big{(}(W_{0}^{T}\circ(B_{i}A_{i})^{T})x_{i}+b\big{)},"The output activations \(y_{i}\) for the next layer, including a bias term \(b\)."
ICLR_2024_oral_7,5,Y=\phi\Big{(}A\circ\big{(}(B\circX)W_{0}\big{)}\Big{)},,y=\phi\Big{(}A\circ\big{(}W_{0}^{T}(B\circx)\big{)}\Big{)},Vectorized output activations for the entire minibatch using matrices \(\mathbf{A}\) and \(\mathbf{B}\).
ICLR_2024_oral_7,6,\frac{2c_{1}}{dc_{2}}+\frac{1}{r}\geq 1,,c_{2}(rbld^{2}) < 2c_{1}(dblr)+c_{2}(bld^{2}),"Comparison of computational costs between fLoRA and LoRA, where \(c_{1}\) and \(c_{2}\) are computational coefficients for BMM and matrix multiplication, respectively."
ICLR_2024_oral_73,1,\tilde{x}_{i}=\frac{\mu_{k}+\varepsilon\eta}{\sqrt{1+\varepsilon^{2}}},where \(\eta\) is drawn from the same distribution as the \(\mu_{k}\)'s and \(\varepsilon\) sets the within-class variability,\tilde{x}_{i}=\mu_{k}+\eta+\varepsilon,where \(\eta\) is drawn from the same distribution as the \(\mu_{k}\)'s and \(\varepsilon\) sets the within-class variability
ICLR_2024_oral_73,2,"v_{i}=u_{i}+V_{1}\sum_{j\leq i}p_{ij}^{(1)}u_{j},\quad w_{i}=v_{i}+V_{2}\sum_{ j\leq i}p_{ij}^{(2)}v_{j} || p_{ij}^{(\mu)}=\frac{e^{(K_{\mu}u_{j})^{T}(Q_{\mu}u_{i})}}{\sum_{k\leq i}e^{(K _{\mu}u_{k})^{T}(Q_{\mu}u_{i})}}","where

\[p_{ij}^{(\mu)}=\frac{e^{(K_{\mu}u_{j})^{T}(Q_{\mu}u_{i})}}{\sum_{k\leq i}e^{(K _{\mu}u_{k})^{T}(Q_{\mu}u_{i})}} \tag{3}\]

is the attention paid by query \(i\) on key \(j\) in the \(\mu\)th layer",v_{i}=\sum_{j=1}^{n}p_{ij}^{(\mu)}u_{j},where \(v_{i}\) is the output of the first attention layer and \(p_{ij}^{(\mu)}\) is the attention paid by query \(i\) on key \(j\) in the \(\mu\)th layer.
ICLR_2024_oral_73,3,"v_{i}^{(b)}=\sum_{j\leq i}q_{ij}^{(1)}u_{j}^{(c)},\quad v_{i}^{(c )}=u_{i}^{(c)} || w_{i}^{(b)}=\sum_{j\leq i}q_{ij}^{(2)}v_{j}^{(c)},\quad w_{i}^{( c)}=v_{i}^{(c)}",,"u_{i}^{(c)},u_{i}^{(b)}=u_{i}",where \(u_{i}^{(c)}\) is the content representation and \(u_{i}^{(b)}\) is the buffer representation for token \(u_{i}\).
ICLR_2024_oral_73,4,"q_{ij}^{(1)}=\frac{e^{\beta_{1}\delta_{i-1,j}}}{\sum_{k\leq i}e^{\beta_{1}\delta_ {i-1,k}}},\quad q_{ij}^{(2)}=\frac{e^{\alpha v_{j}^{(k)},v_{i}^{(c)}+\beta_{2}\Delta_{i,j}}}{\sum_{k\leq i}e^{\alpha v_{k}^{(k)},v_{i}^{(c)}+\beta_{2}\Delta_ {i,k}}}",,"v_{i}^{(b)}=\sum_{j\leq i}q_{ij}^{(1)}u_{j}^{(c)},\quad v_{i}^{(c)}=u_{i}^{(c)}","where \(v_{i}^{(b)}\) and \(w_{i}^{(b)}\) are the buffer representations, \(v_{i}^{(c)}\) and \(w_{i}^{(c)}\) are the content representations, and \(q_{ij}^{(1)}\) and \(q_{ij}^{(2)}\) are the attention weights for the first and second layers, respectively."
ICLR_2024_oral_74,1,\partial_{k}|s_{k}\rangle=\sum_{l=0}^{k-1}(-1)^{l}|s_{k-1}(l)\rangle,where \(\left|s_{k-1}(l)\right\rangle\) is the _lower_ simplex obtained by leaving out vertex \(l\) (i,\partial_{k}\left|s_{k}\right\rangle=\sum_{l=0}^{k} (-1)^{l}\left|s_{k-1}(l)\right\rangle,where \(\left|s_{k-1}(l)\right\rangle\) is the _lower_ simplex obtained by leaving out vertex \(l\).
ICLR_2024_oral_74,2,\beta_{k}:=\dim\ker(\Delta_{k}),,\beta_{k}=\dim\ker(\partial_{k}),where \(\ker(\partial_{k})\) is the kernel of the boundary operator \(\partial_{k}\).
ICLR_2024_oral_74,3,|\chi_{k}-\frac{\beta_{k}}{|S_{k}|}|\leq\epsilon,"where \(|S_{k}|\) is the the number of \(k\)-simplices \(S_{k}\in\Gamma\) or \(\dim\tilde{\mathcal{H}}_{k}\), the dimension of the Hilbert space spanned by the set of \(k\)-simplices in the complex",\chi_{k}=\frac{\beta_{k}}{|S_{k}|},"where \(|S_{k}|\) is the number of \(k\)-simplices \(S_{k}\in\Gamma\) or \(\dim\tilde{\mathcal{H}}_{k}\), the dimension of the Hilbert space spanned by the set of \(k\)-simplices in the complex."
ICLR_2024_oral_74,4,"\rank(\Delta_{k})\stackrel{{\tiny{def}}}{{=}}\trace(h(\tilde{\Delta}_{k})), where h(x)=\{{ll}1& if \x>\delta\\0& otherwise",,\delta_{k}\geq\delta,where \(\delta_{k}\) is the smallest nonzero eigenvalue of the data-defined Laplacian \(\Delta_{k}\).
ICLR_2024_oral_74,5,n_{v}=O(\frac{\log(1/\eta)}{\epsilon^{2}})\qquad\qquadand\qquad\qquad m>\frac{\log(1/\epsilon)}{\sqrt{\delta}},,"n_{v}=O(\epsilon^{-2}),\m=O(\log(1/\epsilon))",where \(\operatorname{n_{v}}\) is the number of random vectors used for stochastic trace estimation and \(m\) is the degree of the Chebyshev polynomial used for approximating the step function.
ICLR_2024_oral_74,6,|\chi_{k}-\frac{\beta_{k}}{|S_{k}|}|\leq\epsilon,,|\chi_{k}-\frac{\beta_{k}}{|S_{k}|}|\leq\epsilon,where \(|S_{k}|\) is the number of \(k\)-simplices in the given simplicial complex \(\Gamma\).
ICLR_2024_oral_74,7,"O(\frac{1}{\epsilon^{2}}\max\{\frac{n\log(1/\epsilon)}{\sqrt{\delta} },\frac{n}{\zeta_{k}}\})",,T(n)=O\left(n^{2}+n\log^{2}n+n\right)=O(n^{2}+n\log^{2}n).\,"The total time complexity of the NISQ-TDA algorithm, accounting for the complexities of the boundary operator and projectors."
ICLR_2024_oral_82,1,LayerNorm(x)=\frac{x-E[x]}{\sqrt {Var[x]+\epsilon}}\odot\gamma+\beta,,y=\frac{x-\mu}{\sigma+\epsilon}\cdot\gamma+\beta,"where \(\mu\) is the mean of \(\mathbf{x}\), \(\sigma\) is the standard deviation of \(\mathbf{x}\), \(\gamma\) and \(\beta\) are learnable parameters, and \(\epsilon\) is a small constant for numerical stability."
ICLR_2024_oral_82,2,{BiasNorm}(x)=\frac{x}{RMS[x-b]}\cdot\exp(\gamma),"where \(\mathbf{b}\) is the learnable channel-wise bias, \(\mathrm{RMS}[\mathbf{x}-\mathbf{b}]\) is the root-mean-square value taken over channels, and \(\gamma\) is a scalar",BiasNorm(x)=\frac{x-b}{RMS[x-b]+\epsilon}\odot\gamma,"where \(\mathbf{b}\) is the learnable channel-wise bias, \(\mathrm{RMS}[\mathbf{x}-\mathbf{b}]\) is the root-mean-square value taken over channels, and \(\gamma\) is a scalar."
ICLR_2024_oral_82,3,Swish(x)=x\cdot(1+\exp(-x))^{-1},,Swish(x)=x\cdot\sigma(x),where \(\sigma(\mathbf{x})\) is the sigmoid function defined as \(\sigma(\mathbf{x})=\frac{1}{1+\exp(-\mathbf{x})}\).
ICLR_2024_oral_82,4,"\textit{SwooshR}(x)&=\log(1+\exp(x-1))-0.08x-0.313261687,\\\textit{SwooshL}(x)&=\log(1+\exp(x-4))-0.08x-0.035",,SwooshR(x)=x\cdot(1+\exp(-x))^{-1}+\alpha\cdotReLU(x),where \(\alpha\) is a learnable parameter and \(\mathrm{ReLU}(x)\) is the rectified linear unit function.
ICLR_2024_oral_82,5,\Delta_{t}=-\alpha_{t}\cdot\frac{\sqrt{1-\beta_{t}^{2}}}{1-\beta_{1}^{t}}\cdot\frac{m_{t}}{\sqrt{v_{t}}+\epsilon},"where \(\alpha_{t}\) is the learning rate typically specified by an external schedule, \(\frac{\sqrt{1-\beta_{t}^{2}}}{1-\beta_{1}^{2}}\) is the bias-correction term, and \(\epsilon=10^{-8}\)",\Delta_{t}=-\alpha_{t}\cdot\frac{m_{t}}{\sqrt{v_{t}}+\epsilon},"where \(\alpha_{t}\) is the learning rate typically specified by an external schedule, \(\frac{\sqrt{1-\beta_{t}^{2}}}{1-\beta_{1}^{2}}\) is the bias-correction term, and \(\epsilon=10^{-8}\)."
ICLR_2024_oral_82,6,\Delta^{\prime}_{t}=-\alpha_{t}\cdot r_{t-1}\cdot\frac{\sqrt{1-\beta_{t}^ {2}}}{1-\beta_{1}^{t}}\cdot\frac{m_{t}}{\sqrt{v_{t}}+\epsilon},,\Delta_{t}=-\alpha_{t}\cdot\frac{\sqrt{1-\beta_{t}^{2}}}{1-\beta_{1}^{t}}\cdot\frac{m_{t}}{\sqrt{v_{t}}+\epsilon}\cdot r_{t-1},where \(r_{t-1}\) is the parameter scale at step \(t-1\).
ICLR_2024_oral_82,7,"\Delta^{\prime}_{t,r}&=-\eta\cdot\alpha_{t}\cdot r_{t-1}\cdot\frac{\sqrt{1-\beta_{t}^{2}}}{1-\beta_{1}^{2}}\cdot\frac{n_{t}}{\sqrt{w_{t}}+\epsilon}\odot\theta^{\prime}_{t-1}\\&=-\eta\cdot\alpha_{t}\cdot\frac{\sqrt{1-\beta_{t}^{2}}}{1-\beta _{1}^{t}}\cdot\frac{n_{t}}{\sqrt{w_{t}}+\epsilon}\odot\theta_{t-1}","where \(\eta\) is a scaling factor on learning rate \(\alpha_{t}\), and we found that setting \(\eta=0","\Delta^{\prime}_{t,r}=-\eta\cdot\alpha_{t}\cdot\frac{\sqrt{1-\beta_{t}^{2}}}{1-\beta_{1}^{t}}\cdot r_{t-1}\cdot\frac{n_{t}}{\sqrt{w_{t}}+\epsilon}",where \(\eta\) is a scaling factor on learning rate \(\alpha_{t}\).
ICLR_2024_oral_82,8,"\alpha_{t}=\alpha_{base}\cdot(\frac{t^{2}+\alpha_{step }^{2}}{\alpha_{step}^{2}})^{-0.25}\cdot(\frac{e^{2}+\alpha_{epoch}^{2}}{\alpha_{epoch}^{2}})^{-0.25}\cdotlinear(\alpha_{start},t_{warmup},t)",,{Eden}(\alpha_{t})=\alpha_{0}\cdot\frac{1}{1+\beta\cdot t},where \(\alpha_{0}\) is the initial learning rate and \(\beta\) is a decay factor.
ICML_2024_oral_1,1,"J(\pi)=\sum_{t=0}^{\infty}E_{(s_{t},a_{t})\sim\rho (\pi)}[\gamma^{t}(r(s_{t},a_{t})+\alphaH(\pi(\cdot|s_{t})))]",,J(\pi)=E_{\tau\sim\pi}\left[\sum_{t=0}^{\infty}\gamma^{t}\left(r_{t}+\alpha H(\pi(\cdot|s_{t}))\right)\right],"Where \(J(\pi)\) is the objective function, \(\tau\) represents the trajectory, \(H(\pi(\cdot|s_{t}))\) is the entropy of the policy at state \(s_{t}\), and \(\alpha\) is the temperature parameter controlling the trade-off between reward and entropy."
ICML_2024_oral_1,2,"r_{t}=r_{M}(B_{s\to r|a}\odots_{t},B_{a\to r|s}\odota_{t},\epsilon_{t})","where \(\mathbf{B}_{\mathbf{s}\to r|\mathbf{a}}\in\mathbb{R}^{\mathrm{dim} \mathcal{S}\times 1}\) and \(\mathbf{B}_{\mathbf{a}\to r|\mathbf{s}}\in\mathbb{R}^{\mathrm{dim}\mathcal{A} \times 1}\) are vectors that represent the graph structure 1 from \(\mathbf{s}_{t}\) to \(r_{t}\) given \(\mathbf{a}_{t}\) and from \(\mathbf{a}_{t}\) to \(r_{t}\) given \(\mathbf{s}_{t}\), respectively",r_{t}=B_{s\to r|a}^{T}s_{t}+B_{a\to r|s}^{T}a_{t},"where \(\mathbf{B}_{\mathbf{s}\to r|\mathbf{a}}\in\mathbb{R}^{\mathrm{dim} \mathcal{S}\times 1}\) and \(\mathbf{B}_{\mathbf{a}\to r|\mathbf{s}}\in\mathbb{R}^{\mathrm{dim}\mathcal{A} \times 1}\) are vectors that represent the graph structure from \(\mathbf{s}_{t}\) to \(r_{t}\) given \(\mathbf{a}_{t}\) and from \(\mathbf{a}_{t}\) to \(r_{t}\) given \(\mathbf{s}_{t}\), respectively."
ICML_2024_oral_1,3,"H_{c}(\pi(\cdot|s))&=-E_{a\inA}[\sum_{i=1}^{\dimA}B_{a_{i}\to r|s}\pi(a_{i}|s)\log\pi(a_{i}|s)],\\&a=(a_{1},\ldots,a_{\dimA})",,"H_{c}(\pi)=E_{s_{t}\sim\rho(s),a_{t}\sim\pi(\cdot|s_{t})}\left[\sum_{i=1}^{dimA} B_{a_{i}\to r|s}\log\pi(a_{i}|s_{t})\right]","where \(\mathcal{H}_{c}(\pi)\) is the causality-aware entropy, \(\rho(\mathbf{s})\) is the state distribution, and \(B_{a_{i}\to r|\mathbf{s}}\) are the causal weights associated with actions."
ICML_2024_oral_1,4,"T_{c}^{\pi}Q(s_{t},a_{t })\triangleq& r(s_{t},a_{t})+\gammaE_{s_{t+1}\sim P}[E_{a_{t}\sim\pi}[Q(s_{t+1},a_{t+1})\\&+\alphaH_{c}(\pi(a_{t+1}|s_{t+1}))]]",,"Q^{\pi}(s,a)=T_{c}^{\pi}Q^{\pi}(s,a)=E_{s'\simP(\cdot|s,a)}\left[r(s,a)+\gamma Q^{\pi}(s',a')+\alphaH_{c}(\pi(\cdot|s'))\right]","where \(Q^{\pi}(\mathbf{s},\mathbf{a})\) is the action-value function, \(\mathcal{T}_{c}^{\pi}\) is the modified Bellman operator, \(\mathcal{P}(\cdot|\mathbf{s},\mathbf{a})\) represents the transition dynamics, \(\gamma\) is the discount factor, and \(\alpha\) is a coefficient for the causality-aware entropy term."
ICML_2024_oral_1,5,\frac{n_{i}^{l}(x)}{\frac{1}{N^{l}}\sum_{k\in l}n_{k}^{l}}\leq\tau,where \(\tau\) is a constant serving as a threshold to determine the gradient dormancy of neurons in each layer,n_{i}^{l} <\tau,"where \(n_{i}^{l}\) is the L2 norm of gradients of the weights for neuron \(i\) in layer \(l\), and \(\tau\) is a constant serving as a threshold to determine the gradient dormancy of neurons in each layer."
ICML_2024_oral_1,6,\alpha_{\tau}=\frac{\sum_{l\in\phi}N_{\tau}^{l}}{\sum_{l\in\phi}N^{l}},,\alpha_{\tau}=\frac{N_{\tau}^{l}}{\sum_{l}N^{l}},where \(N_{\tau}^{l}\) is the number of gradient-dormant neurons and \(\sum_{l}N^{l}\) is the total number of neurons in the neural network.
ICML_2024_oral_1,7,"\theta_{t}=(1-\eta)\theta_{t-1}+\eta\phi_{i},\\phi_{i}\siminitializer",,\alpha_{\tau}=\frac{\sum_{l\in\phi}N_{\tau}^{l}}{\sum_{l\in\phi}N^{l}},The \(\tau\)-dormancy degree \(\alpha_{\tau}\) indicates the percentage of gradient-dormant neurons with the \(\tau\) threshold in the fully-connected neural network.
ICML_2024_oral_10,1,"attention(Q,K,V)=softmax(QK)^{T}/\sqrt{d})V",,"Attention(Q, K, V)=softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V","Attention mechanism formula using query (Q), key (K), and value (V) matrices, where \(d_k\) is the dimension of the key vectors."
ICML_2024_oral_10,2,"d\hat{x}_{t}=f(\hat{x}_{t},t)dt+g(t)dw",,"\frac{dx(t)}{dt}=f(x(t), t)+g(t)z(t)","where \(\mathbf{x}(t)\) represents the state of the system at time \(t\), \(f\) is a deterministic function, \(g(t)\) is a noise scaling function, and \(\mathbf{z}(t)\) is a standard Wiener process."
ICML_2024_oral_10,3,"d\hat{x}_{t}=[f(\hat{x}_{t},t)-g(t)^{2}s(\hat{x}_{t},t) ]dt+g(t)d\tilde{w}",where \(\mathbf{\tilde{w}}\) is a backward-in-time Wiener process,"d\hat{x}_{t}=-f(\hat{x}_{t},t)dt+\sigma(t)d\tilde{w}",where \(\mathbf{\tilde{w}}\) is a backward-in-time Wiener process
ICML_2024_oral_10,4,"L(\phi)=E_{t,\hat{x}_{0},\hat{x}_{t}}[\lambda (t)\|s_{\phi}(\hat{x}_{t},t)-\nabla_{\hat{x}_{t}}\log p_{t}(\hat{x}_{t}|\hat{x}_{0})\|_{2}^{2}]",where \(\lambda\) denotes a positive weighting function,"L(\phi)=E_{\hat{x}_{0},t}\left[\left\| s_{\phi}(\hat{x}_{t},t)-s(\hat{x}_{t},t)\right\|^{2}\right]+\lambdaR(\phi)","where \(\mathcal{L}(\phi)\) is the loss function, \(\mathbb{E}\) denotes the expectation, and \(\mathcal{R}(\phi)\) is a regularization term."
ICML_2024_oral_10,5,"\ell(\phi,M_{C},t,\hat{x}_{0},\hat{x}_{t})=\\(1-M_{C})\cdot(s_{\phi}^{M_{E}}(\hat{x}_{t}^{Mc}, t)-\nabla_{\hat{x}_{t}}\log p_{t}(\hat{x}_{t}|\hat{x}_{0}))",where \(s_{\phi}^{M_{E}}\) denotes the score model equipped with a specific attention mask \(M_{E}\),"L^{M_{E}}(\phi)=E_{t,\hat{x}_{0},\hat{x}_{t}}[\lambda (t)\|s_{\phi}^{M_{E}}(\hat{x}_{t},t)-\nabla_{\hat{x}_{t}}\log p_{t}(\hat{x}_{t}|\hat{x}_{0})\|_{2}^{2}]",where \(s_{\phi}^{M_{E}}\) denotes the score model equipped with a specific attention mask \(M_{E}\)
ICML_2024_oral_10,6,"L(\phi)=E_{M_{C},t,\hat{x}_{0},\hat{x}_{ t}}[\|\ell(\phi,M_{C},t,\hat{x}_{0},\hat{x}_{t})\|_{2}^{2}]",,"E_{t,\hat{x}_{0},\hat{x}_{t}}[\ell(\phi,M_{C},t,\hat{x}_{0},\hat{x}_{t})]",where the expectation is taken over the noise levels and the data samples.
ICML_2024_oral_10,7,"s(\hat{x}_{t},t|y)\approx s_{\phi}(\hat{x}_{t},t)+\nabla_{\hat{x}_{t}}\log p_{t}(y|\hat{x}_{t})",,"s_{guided}(\hat{x}_{t},t,y)=s_{\phi}(\hat{x}_{t},t)+\nabla_{y}\log p(y|\hat{x}_{t})",where \(\nabla_{\mathbf{y}}\) denotes the gradient with respect to the context \(\mathbf{y}\)
ICML_2024_oral_10,8,"s_{\phi}(\hat{x}_{t},t|c)\approx s_{\phi}(\hat{x}_{t},t)+\nabla_{\hat{x}_{t}}\log\sigma(-s(t)c(\hat{x}_{t}))",,"s(\hat{x}_{t},t|y)\approx s_{\phi}(\hat{x}_{t},t)+\nabla_{\hat{x}_{t}}\log p_{t}(y|\hat{x}_{t})","where \(s(\hat{\mathbf{x}}_{t},t|\mathbf{y})\) is the guided score function conditioned on context \(\mathbf{y}\)."
ICML_2024_oral_101,1,"G(z)=\sum_{j_{1}=0}^{d-1}\cdots\sum_{j_{n}=0}^{d-1}p(j_{1},\ldots,j_{n})z_{1}^ {j_{1}}\cdots z_{n}^{j_{n}}",,"P(z_{1},\ldots,z_{n})=\sum_{a_{1}=0}^{d-1}\cdots\sum_{a_{n}=0}^{d-1} p(a_{1},\ldots,a_{n}) z_{1}^{a_{1}}\cdots z_{n}^{a_{n}}","The probability generating function \(P(z_{1},\ldots,z_{n})\) for categorical variables \(X_{1},\ldots,X_{n}\)."
ICML_2024_oral_101,2,"f(V_{1},...,V_{n},E_{1,N(1,1)},...,E_{n,N(n,3)})=\prod_{i=1}^{n}\sum_{j\in N(i )}E_{i,j}V_{j}",,"f=\sum_{i=1}^{n}\sum_{j=1}^{3} V_{i} E_{i,N(i,j)}","Polynomial \(f\) representing the relationship between formal variables \(V_{i}\) and binary random variables \(E_{i,j}\) in the context of counting perfect matchings in a \(3\)-regular bipartite graph."
ICML_2024_oral_101,3,"\Pr[V_{1}=1,\ldots,V_{n}=1]=h(1,\ldots,1)=\frac{\#PM(G)}{3^{n}}",,"\Pr[V_{1}=1,V_{2}=1,\ldots,V_{n}=1]=\frac{h(1,\ldots,1)}{3^{n}}","The probability of all \(V_{i}\) being equal to 1, where \(h\) is the coefficient polynomial derived from the PGC."
ICML_2024_oral_101,4,"f(z_{1},...,z_{n})=\sum_{s=(s_{1},\ldots,s_{n})\in\{0,1,\ldots,k-1\}^{n}}c_{s}\cdot\prod_{i=1}^{n}z_{i}^{s_{i}}",,"f(z_{1},\ldots,z_{n})=\sum_{j_{1}=0}^{k-1}\cdots\sum_{j_{n}=0}^{k-1}p(j_{1},\ldots,j_{n})z_{1}^{j_{1}}\cdots z_{n}^{j_{n}}",Probability generating polynomial corresponding to the selective marginal probability for \(k\)-nary random variables.
ICML_2024_oral_101,5,"g(x_{1},\overline{x_{1}},...,x_{n},\overline{x_{n}})=f(\frac{x_{1}}{\overline{ x_{1}}},\frac{x_{2}}{\overline{x_{2}}},...,\frac{x_{n}}{\overline{x_{n}}})\cdot\prod_{i=1}^{n}\overline{x_{i}}",,"g(z_{1},...,z_{n})=\sum_{s\in\{0,1\}^{n}}c_{s}\cdot\prod_{i=1}^{n}z_{i}^{s_{i}}",Probability generating polynomial for \(n\) binary random variables computed by a nonmonotone PC.
ICML_2024_oral_101,6,m^{\prime}=c_{S}\cdot(\prod_{i\in S}\frac{x_{i}}{\overline{x_{i}}})\cdot(\prod _{j=1}^{n}\overline{x_{j}})=c_{S}(\prod_{i\in S}x_{i})\cdot(\prod_{i\notin S}\overline{x_{i}}),,"g(x_{1},\overline{x_{1}},...,x_{n},\overline{x_{n}})=\sum_{S\subseteq[n]}c_{S}\cdot\prod_{i\in S}x_{i}\cdot\prod_{j\notin S}\overline{x_{j}}",Definition of the polynomial \(g\) that simulates the probability generating function \(f\) for binary random variables using nonmonotone PCs.
ICML_2024_oral_101,7,"& f(x_{1}(1+\overline{x_{1}}),x_{2}(1+\overline{x _{2}}))\cdot(1-\overline{x_{1}})(1-\overline{x_{2}})\\&=(0.6x_{1}x_{2}(1+\overline{x_{1}})(1+\overline{x_{2}})\\&\quad+0.4x_{1}(1+\overline{x_{1}}))(1-\overline{x_{1}})(1-\overline{x_{2}})\\&=0.6x_{1}x_{2}+0.4x_{1}(1-\overline{x_{2}})\+\higher degree terms",,"h(E_{1,N(1,1)},...,E_{n,N(n,3)})=\sum_{s_{1}=0}^{1}\cdots\sum_{s_{n}=0}^{1}c_{s}\cdot\prod_{i=1}^{n}E_{i,s_{i}}","The coefficient of the polynomial \(h\) in terms of the binary random variables \(E_{i,j}\) that represent the edges in the bipartite graph, where \(c_{s}\) are the coefficients corresponding to the perfect matchings."
ICML_2024_oral_101,8,"P=\sum_{j_{1}=0}^{d-1}\cdots\sum_{j_{n}=0}^{d-1}\Pr[X_{1}=j_{1},\ldots,X_{n}=j _{n}]z_{1,j_{1}}\cdots z_{n,j_{n}}",,"\Pr[X_{1}\in A_{1},\ldots,X_{n}\in A_{n}]=\sum_{s_{1}\in A_{1},\ldots,s_{n}\in A_{n}}c_{s}","Probability of random variables \(X_{1},\ldots,X_{n}\) taking values in sets \(A_{1},\ldots,A_{n}\)."
ICML_2024_oral_101,9,"e_{i,j}=1&if $j=a_{i$},\\0&otherwise",,"e=(z_{1,a_{1}},\ldots,z_{n,a_{n}})","Input vector for the polynomial \(P\) corresponding to the elementary event \(X_{1}=a_{1},\ldots,X_{n}=a_{n}\)."
ICML_2024_oral_101,10,"v_{i,j}=1&if $j\in A_{i$},\\0&otherwise",,"e_{i,j}=1&if $j\in A_{i$},\\0&otherwise","Definition of the input vector \(e\) for computing the probability of the event \(X_{1}\in A_{1},\ldots,X_{n}\in A_{n}\)."
ICML_2024_oral_101,11,"P(v)=\sum_{j_{1}\in A_{1}}\cdots\sum_{j_{n}\in A_{n}}\Pr[X_{1}=j_{1},\ldots,X_{n}=j_{n}]\cdot 1 || =\Pr[X_{1}\in A_{1},\ldots,X_{n}\in A_{n}]",,"P(v)=\sum_{j_{1}=0}^{d-1}\cdots\sum_{j_{n}=0}^{d-1}\Pr[X_{1}=j_{1},\ldots,X_{n}=j_{n}]v_{1,j_{1}}\cdots v_{n,j_{n}}","Probability of the event that \(X_{1}\) is in \(A_{1}\), ..., and \(X_{n}\) is in \(A_{n}\)."
ICML_2024_oral_101,12,"f(a)=\sum_{a\in\Delta^{|A|}}\alpha_{a}\prod_{i\in A}z_{i,a_{i}},\quad g(b)=\sum_{b\in\Delta^{|B|}}\beta_{b}\prod_{j\in B}z_{j,b_{j}}",where \(\alpha_{a}=\Pr[X_{A}=a]\) and \(\beta_{b}=\Pr[X_{b}=b]\),"f(z_{1},\ldots,z_{n})=\sum_{a}\sum_{b}\alpha_{a}\beta_{b}z_{a}z_{b}",where \(\alpha_{a}=\Pr[X_{A}=a]\) and \(\beta_{b}=\Pr[X_{B}=b]\)
ICML_2024_oral_101,13,"f(a,b^{\prime})= || (\sum_{a\in\Delta^{|A|}}\alpha_{a}\prod_{i\in A}z_{i,a_{i}} )\prod_{j\in B\setminus A}\frac{1}{d}(z_{j,0}+\cdots+z_{j,d-1})",,"h(a,b)=\sum_{a\in\Delta^{|A|}}\sum_{b\in\Delta^{|B|}}\alpha_{a}\beta_{b}\prod_{i\in A}z_{i,a_{i}}\prod_{j\in B}z_{j,b_{j}}","where \(h(a,b)\) represents the joint probability distribution over the combined random variables \(X_{A}\) and \(X_{B}\)."
ICML_2024_oral_104,1,call-count(u)=\sum_{v\in Q(u)}(1+call-count(v)),,R(u)=1+\sum_{v\in Q(u)} R(v),The recursive call count \(R(u)\) for node \(u\) is defined as one plus the sum of the recursive call counts of all queried neighbors \(v\) of \(u\).
ICML_2024_oral_104,2,\sigma(u_{0})\leq\pi(u_{0})\leq\sigma(u_{1})\leq\pi(u_{1})\leq\ldots || \leq\sigma(u_{L-1})\leq\pi(u_{L-1})\leq\sigma(u_{L})\leq\pi(u_{L}),,"\sigma(u_{i})\geq\pi(u_{i-1}) for all  i\in\{1,\ldots,L\}",Defines the condition for a path to be a query path based on the settling iterations of the nodes.
ICML_2024_oral_104,3,"\sigma(u_{0})\leq\pi(u_{0})\leq\sigma(u_{1})\leq\pi(u_{1})\leq\ldots || \leq\sigma(u_{L-2})\leq\pi(u_{L-2})\leq\min(\sigma(u_{L-1}),\sigma (u_{L}))",,\sigma(u_{0})\leq\pi(u_{0})\leq\sigma(u_{1})\leq\pi(u_{1})\leq\ldots\leq\sigma(u_{L-1})\leq\pi(u_{L-1})\leq\sigma(u_{L})\leq\pi(u_{L}),"A path \((u_{0},u_{1},\ldots,u_{L})\) of length \(L\geq 2\) is an extended query path (EQ-path) if the following conditions hold: (1) \((u_{0},u_{1},\ldots,u_{L-1})\) is a query path; and (2) \(\pi(u_{L-2})\leq\sigma(u_{L})\)."
ICML_2024_oral_104,4,\sigma(u_{0})<\sigma(u_{1}) and\sigma(u_{0})\leq\pi(u_{0})\leq\sigma(u_{1})\leq\pi(u_{1})\leq || \cdots\leq\pi(u_{L-2})\leq\sigma(u_{L-1})=\sigma(u_{L}),,\sigma(u_{0})<\sigma(u_{1})\quadand\quad\sigma(u_{L-1})=\sigma(u_{L}),"An extended query path \((u_{0},u_{1},\ldots,u_{L})\) is expensive if the first node is settled before the second, but the last two nodes are settled at the same iteration."
ICML_2024_oral_104,5,"E_{\pi}|X|\leq 2E\Big{[}\sum_{(u,v)\in E}1 (\sigma(u)\neq\sigma(v))\Big{]}",,\sigma(u_{0})\neq\sigma(u_{1}),The first condition \(\sigma(u_{0})<\sigma(u_{1})\) in (4) can be replaced with \(\sigma(u_{0})\neq\sigma(u_{1})\).
ICML_2024_oral_104,6,"\Phi_{t}(a,b)=2|D_{t}(a,b)|+|Q_{t}(a,b)| || \Psi_{t}(a,b)=2|D_{t}(a,b)|+|X_{t}(a,b)|",,\sigma(u_{0})<\sigma(u_{1}) and \sigma(u_{0})\leq\pi(u_{0})\leq\sigma(u_{1})\leq\pi(u_{1})\leq\cdots\leq\pi(u_{L-2})\leq\sigma(u_{L-1})=\sigma(u_{L}),An expensive extended query path is defined by the conditions on the stopping times \(\sigma\) and the ranks \(\pi\) of the nodes in the path.
ICML_2024_oral_106,1,"p(u)=\prod_{i=1}^{n}p(u_{i}|u_{1},...,u_{i-1},\Theta)",,"P(u_{t} | u_{1}, ..., u_{t-1})","Probability of the next word \(u_{t}\) given the previous words \(u_{1}, ..., u_{t-1}\)."
ICML_2024_oral_106,2,L=-log\;p(u),,"L=-\sum_{i=1}^{n}\log p(u_{i}|u_{1},...,u_{i-1},\Theta)",Negative log-likelihood of the target words.
ICML_2024_oral_106,3,"p(x)=\prod_{i=1}^{n}p(x_{i}|x_{1},...,x_{i-1},\Theta)",,"p(x)=\prod_{i=1}^{n}p(x_{i}|x_{1},...,x_{i-1},\Theta)","Probability of each pixel \(x_{i}\) based on the preceding pixels \(x_{1},...,x_{i-1}\) in the sequence."
ICML_2024_oral_106,4,L=-log\;p(x),,"L=-\sum_{i=1}^{n}\log p(x_{i}|x_{1},...,x_{i-1},\Theta)",Negative log-likelihood of predicting each pixel \(x_{i}\) based on preceding pixels.
ICML_2024_oral_106,5,"p(s)=\prod_{i=1}^{n}p(s_{i}|s_{1},...,s_{i-1},\Theta)",,"p(s)=\prod_{i=1}^{n}p(s_{i}|s_{1},...,s_{i-1},\Theta)","Autoregressive probability of clusters \(s_{i}\) based on preceding clusters \(s_{1},...,s_{i-1}\)."
ICML_2024_oral_106,6,"L_{G}=-\sum_{i=1}^{n}cosine(G(f(x_{s_{1}:s_{i-1}});\theta_{G}),f_{\phi }(x)_{s_{i}})","where \(f(\cdot)\) is the encoder, \(f_{\phi}(x)_{s_{i}}\) is the semantically enriched tokens corresponding to the cluster \(s_{i}\), \(G(\cdot;\ \theta_{G})\) is the generative decoder for autoregressive prediction, and \(cosine\) is the cosine similarity loss",p(s_{i})=G(f_{\phi}(x)_{s_{i}};\\theta_{G}),"where \(f(\cdot)\) is the encoder, \(f_{\phi}(x)_{s_{i}}\) is the semantically enriched tokens corresponding to the cluster \(s_{i}\), \(G(\cdot;\ \theta_{G})\) is the generative decoder for autoregressive prediction, and \(cosine\) is the cosine similarity loss."
ICML_2024_oral_106,7,"L_{D}=-\sum_{i=1}^{n}cosine(D(f(x_{s_{1}:s_{i-1}});\theta_{D}),f_{\phi}(x)_{s_{1}:s_{i-1}})","where \(D(\cdot;\ \theta_{D})\) is the discriminative decoder, tasked with predicting the semantic tokens of visible pixels",L_{D}=-\sum_{i=1}^{n}D(f_{\phi}(x)_{s_{i}};\theta_{D}),"where \(D(\cdot;\ \theta_{D})\) is the discriminative decoder, tasked with predicting the semantic tokens of visible pixels."
ICML_2024_oral_107,1,"L_{eval}=-\log p_{M}(<\)cc\(>\)\(|X_{l},X_{r})",,L_{eval}=-\sum_{i=1}^{N} y_i\log(\hat{y}_i),"Cross-entropy loss for self-assessment, where \(y_i\) is the true label and \(\hat{y}_i\) is the predicted probability for the \(i\)-th instance."
ICML_2024_oral_107,2,"L_{gen}=-\log p_{M}(Y|X_{l},X_{r},CC),& if label\\-\log p_{M}(Y|X_{l},X_{r}),&otherwise",,"L_{gen}=-\log p_{M}(Y\|\<fim\_middle>\, X_{l}, X_{r}, CC)",Cross-entropy loss for code generation based on the tokens following the special token \(<\text{fim\_middle}>\).
ICML_2024_oral_107,3,"ES(\hat{Y},Y)=\frac{1-Lev(\hat{Y},Y)}{\max(|\hat{Y}|,|Y|)}",where \(Lev\) is the Levenshtein distance (Levenshtein et al,L_{final}=\lambdaL_{eval}+L_{gen},"The final training objective, which is a weighted combination of the self-assessment loss \(\mathcal{L}_{eval}\) and the code generation loss \(\mathcal{L}_{gen}\)."
ICML_2024_oral_109,1,"\operatorname*{arg\,max}_{k=1,\dots,K}\,\cos(\phi(x),\psi(t_{k}))",,\hat{y}=\arg\max_{k}\frac{\phi(x)\cdot\psi(t_{k})}{\|\phi(x)\|\|\psi(t_{k})\|},where \(\hat{y}\) is the predicted class label for the image \(x\).
ICML_2024_oral_109,2,"f_{k}(\phi,x)=\cos(\phi(x),\psi(t_{k}))=\langle\frac{\phi(x)}{\|\phi (x)\|_{2}},\frac{\psi(t_{k})}{\|\psi(t_{k})\|_{2}}\rangle",,"f(x)=\cos(\phi(x),\psi(t_{k}))\quadfor  k=1,\dots,K",Logits of the classifier \(f\) defined by the cosine similarity between the image embedding \(\phi(x)\) and the text embeddings \(\psi(t_{k})\).
ICML_2024_oral_109,3,"\operatorname*{arg\,max}_{k=1,\dots,K}\,f_{k}(\phi,z)\neq y,\quad\|z-x\|_{p}\leq\varepsilon,\quad z\in I",where \(\varepsilon\) is the perturbation size,\| z-x\|_{p}\leq\varepsilon,where \(\varepsilon\) is the perturbation size
ICML_2024_oral_109,4,"L_{TeCoA}(y,f(\phi,x))=-\log(\frac{e^{f_{y}(\phi,x)}}{\sum_{k=1}^ {K}e^{f_{k}(\phi,x)}})",,"L_{TeCoA}=-\frac{1}{N}\sum_{i=1}^{N}\sum_{k=1}^{K} y_{i,k}\log\left(\frac{e^{f_{k}(\phi,x_{i})}}{\sum_{j=1}^{K} e^{f_{j}(\phi,x_{i})}}\right)","where \(\mathcal{L}_{\text{TeCoA}}\) is the loss function, \(N\) is the number of training samples, \(y_{i,k}\) is the ground truth label for sample \(i\) and class \(k\), and \(f_{k}(\phi,x_{i})\) are the logits for the \(i\)-th sample."
ICML_2024_oral_109,5,"\phi_{FT}=\operatorname*{arg\,min}_{\phi}\sum_{i=1}^{n}\max_{\|z-x_{i}\|_{\infty}\leq\varepsilon}L_{TeCoA}(y_{i},f(\phi,z))",where the inner problem is approximately solved with projected gradient descent (PGD) during training and \(\phi_{\mathrm{FT}}\) indicates the weights of the robust CLIP vision encoder,"L_{TeCoA}(y,f(\phi_{FT},x))=\frac{1}{n}\sum_{i=1}^{n}L_{TeCoA}(y_{i},f(\phi_{FT},x_{i}))","where \(L_{\text{TeCoA}}\) is the loss function, \(n\) is the number of training samples, and \(\phi_{\mathrm{FT}}\) represents the fine-tuned weights of the CLIP vision encoder."
ICML_2024_oral_109,6,"L_{FARE}(\phi,x)=\max_{\|z-x\|_{\infty}\leq\varepsilon}\|\phi(z)-\phi_{Org}(x)\|_{2}^{2}",,L_{unsupervised}(x)=\|\phi_{Org}(x)-\phi_{FT}(x)\|_{2}^{2}+\betaL_{adv}(x),where \(\beta\) is the weight for the unsupervised loss and \(\mathcal{L}_{\text{adv}}\) is the adversarial loss computed over the perturbed image embeddings.
ICML_2024_oral_109,7,"\phi_{FT}=\operatorname*{arg\,min}_{\phi}\sum_{i=1}^{n}L_{ FARE}(\phi,x_{i})",,"\phi_{FT}=\operatorname*{arg\,min}_{\phi}\sum_{i=1}^{n}L_{FARE}(\phi,x_{i})","where \(L_{\text{FARE}}(\phi,x)\) is the embedding loss that ensures the features of perturbed points remain close to those of the original model."
ICML_2024_oral_109,8,"|\cos(\phi_{FT}(x),\psi(t))-\cos(\phi_{Org},\psi(t))| || \leq\min(\frac{2}{\|\phi_{Org}(x)\|_{2}},\frac{2}{\|\phi_{FT}(x)\|_{2}})\|\phi_{FT}(x)-\phi_{Org}(x)\|_{2}",,"\|\phi_{FT}(x)-\phi_{Org}(x)\|_{2}^{2}\leq\delta\implies\cos(\phi_{FT}(x),\psi(t_{k}))\approx\cos(\phi_{Org}(x),\psi(t_{k})),\quad\forall k=1,\dots,K",where \(\delta\) is a small positive constant that controls the preservation of cosine similarities.
ICML_2024_oral_110,1,F_{iso}=\{f:\Omega\subsetR^{d}\to&R^{D}:\Df^{\top}(s)Df(s)=Id\\&for all s\in\Omega\},,f:\Omega\subsetR^{d}\toR^{D},Function class of local isometries mapping from the support \(\Omega\) in \(\mathbb{R}^{d}\) to \(\mathbb{R}^{D}\).
ICML_2024_oral_110,2,"\Theta_{p}^{p}(f,\Omega)=\int_{\Omega}&dist(Df(s),SO(d))^{p}\\&+dist\big{(}(Df)^{-1}(s),SO(d)\big{)}^{p}\,ds",,\Theta(f)=\sup_{s\in\Omega}\left\| Df^{\top}(s)Df(s)-Id\right\|,Distance measure of a function \(f\) to the space of local isometries \(\mathcal{F}_{\mathrm{iso}}\).
ICML_2024_oral_110,3,"\Theta_{p}^{p}(f,\Omega)=&\int_{\Omega }dist^{p}(Df(z),SO(d,T_{f(z)}M))\\&+dist^{p}\big{(}(Df)^{-1}(z),SO(T_{f(z)}M,d)\big{)}\,dz","where \(T_{f(z)}M\) denotes the tangent space of \(M\) at \(f(z)\) and \(\mathrm{SO}(d,T_{f(z)}M)\) denotes the set of orthogonal matrices \(Q\in\mathbb{R}^{D\times d}\) (i","\Theta_{p}^{p}(f,M)=\int_{M}&dist(Df(s),SO(d,T_{f(s)}M))^{p}\\&+dist\big{(}(Df)^{-1}(s),SO(d,T_{f(s)}M)\big{)}^{p}\,ds","where \(T_{f(z)}M\) denotes the tangent space of \(M\) at \(f(z)\) and \(\mathrm{SO}(d,T_{f(z)}M)\) denotes the set of orthogonal matrices \(Q\in\mathbb{R}^{D\times d}\)."
ICML_2024_oral_110,4,"MCC(\tilde{S},\tilde{S})=\max_{\pi\in S_{d}}d^{-1}\sum_{i=1}^{ d}|\rho(S_{i},\tilde{S}_{\pi(i)})|","where \(\rho(X,Y)=\operatorname{Cov}(X,Y)/(\operatorname{Var}(X)\operatorname{Var}( Y))^{1/2}\) denotes the correlation coefficient","MCC(\tilde{S},S)=\frac{Cov(\tilde{S},S)}{\sqrt{Var(\tilde{S})Var(S)}}","where \(\rho(X,Y)=\operatorname{Cov}(X,Y)/(\operatorname{Var}(X)\operatorname{Var}( Y))^{1/2}\) denotes the correlation coefficient"
ICML_2024_oral_110,5,"MCC(\hat{S},S)\geq 1-C\Theta_{p}^{2}(f)",,"MCC(\tilde{S},S)\geq 1-C\Theta_{p}(f,\Omega)^{p}(\tilde{S},S)\) denotes the mean correlation coefficient between the estimated latent variables\(\tilde{S}\) and the true latent variables\(S\).</description>","where \(\operatorname{MCC}(\tilde{S},S)\) denotes the mean correlation coefficient between the estimated latent variables \(\tilde{S}\) and the true latent variables \(S\)."
ICML_2024_oral_110,6,"\min_{L}&\|u-L\|_{L^{q}(\Omega)}\\&\leq C(\Omega,p)(\int_{\Omega}dist(Du(s),SO (d))^{p}\,ds)^{\frac{1}{p}}",,"MCC(\hat{S},S)\geq 1-C\Theta_{p}^{2}(f)",where \(C\) depends on everything except \(f\).
ICML_2024_oral_110,7,"M(f_{*}P)=\{&(g,Q,\Omega^{\prime}):g\inF(\Omega^{\prime}), where \\g_{*}Q=f_{*}P,\,supp(Q)=\Omega^{\prime}\}",,"\min_{L}&\|u-L\|_{L^{q}(\Omega)}\\&\leq C(\Omega,p)\left(\int_{\Omega}dist(Du(s),SO (d))^{p}\,ds\right)^{\frac{1}{p}}",where the minimum is over functions of the form \(L(s)=As+b\) with \(A\in\mathrm{SO}(d)\).
ICML_2024_oral_110,8,"&(g,Q,\Omega^{\prime})\in\\&\operatorname*{argmin}_{(\bar{g},\bar{Q},\Omega)\inM(f,P)}\int_{\Omega}dist((D\bar{g})^{-1}(g(s)),SO(d))^ {p}\,\bar{Q}(ds)",,"M(f_{*}P)=\{&(g,Q,\Omega^{\prime}):g\inF(\Omega^{\prime}), where \\g_{*}Q=f_{*}P,\,supp(Q)=\Omega^{\prime}\}","where \(g\) is a mixing function in the function class \(\mathcal{F}\), \(\mathbb{Q}\) is a probability measure, and \(\Omega^{\prime}\) is the support of \(\mathbb{Q}\)."
ICML_2024_oral_110,9,"\|h\|_{P,q}\leq C_{1}\Theta_{p}(f,\Omega)",,"g^{-1}\circ f(s)=&As+h(s)+b\\&where  h\in L^{q}(\Omega),\, b\inR^{d}","where \(A\) is an orthogonal matrix, \(h\) is a function in the space \(L^{q}(\Omega)\), and \(b\) is a vector in \(\mathbb{R}^{d}\)."
ICML_2024_oral_110,10,x=f(s)=As+\eta h(s),where \(h:\mathbb{R}^{d}\to\mathbb{R}^{d}\) is a non-linear function and \(\eta\in\mathbb{R}\) is a small constant,X=A S+h(S)+\eta,"where \(A\) is a linear transformation, \(h:\mathbb{R}^{d}\to\mathbb{R}^{d}\) is a non-linear function, and \(\eta\in\mathbb{R}\) is a small constant."
ICML_2024_oral_110,11,H(w)=EG(w^{\top}\Sigma_{X}^{-\frac{1}{2}}X),where \(\Sigma_{X}\) denotes the covariance matrix of \(X\) so that \(\Sigma_{X}^{-\frac{1}{2}}X\) is whitened and \(G\) is the so-called contrast function,x=f(s)=As+\eta h(s),where \(h:\mathbb{R}^{d}\to\mathbb{R}^{d}\) is a non-linear function and \(\eta\in\mathbb{R}\) is a small constant.
ICML_2024_oral_110,12,\bar{w}_{i}=(AA^{\top})^{\frac{1}{2}}A^{-\top}e_{i},,H(w)=EG(w^{\top}\Sigma_{X}^{-\frac{1}{2}}X),where \(\Sigma_{X}\) denotes the covariance matrix of \(X\) so that \(\Sigma_{X}^{-\frac{1}{2}}X\) is whitened and \(G\) is the so-called contrast function.
ICML_2024_oral_110,13,"|G^{(k)}(x)|\leq C_{g}(1+|x|)^{\max(d_{g}-k,0)}",where \(G^{(k)}\) denotes the \(k\)-th derivative of \(G\),H(w)=EG(w^{\top}\Sigma_{X}^{-\frac{1}{2}}X),where \(\Sigma_{X}\) denotes the covariance matrix of \(X\) so that \(\Sigma_{X}^{-\frac{1}{2}}X\) is whitened and \(G\) is the so-called contrast function.
ICML_2024_oral_110,14,E(|S|^{q})=M,,"|S_{i}|^{q}\leq M,\for  i=1,\ldots,d",where \(S_i\) are the components of the latent sources \(S\) and \(q\) is defined as the maximum of \(d_g\) and 3.
ICML_2024_oral_110,15,E(S_{i}g(S_{i})-g^{\prime}(S_{i}))=\alpha_{i},,E(G^{\prime}(w^{\top}\Sigma_{X}^{-\frac{1}{2}}X)S_{i})\neq 0,where \(G^{\prime}\) denotes the derivative of the contrast function \(G\).
ICML_2024_oral_111,1,"L_{sup}=\frac{1}{H\times W}\sum_{j=1}^{H\times W}L_{BCE} (M_{i,j}^{l},Y_{i,j}^{l})","where \(M_{i,j}^{l}\) denotes the prediction mask of Burn-In model for the \(j\)-th pixel of \(i\)-th labeled image, \(Y_{i,j}^{l}\) denotes the corresponding ground truth, \(\mathcal{L}_{BCE}\) denotes binary cross entropy loss (Csiszar, 2008)","L_{BCE}=-\frac{1}{N}\sum_{i=1}^{N}\sum_{j=1}^{H\times W}\left( Y_{i,j}^{l}\log(M_{i,j}^{l})+(1-Y_{i,j}^{l})\log(1-M_{i,j}^{l})\right)","where \(M_{i,j}^{l}\) denotes the prediction mask of Burn-In model for the \(j\)-th pixel of \(i\)-th labeled image, \(Y_{i,j}^{l}\) denotes the corresponding ground truth, \(\mathcal{L}_{BCE}\) denotes binary cross entropy loss (Csiszar, 2008)."
ICML_2024_oral_111,2,"\theta_{t}arrow\theta,\theta_{s}arrow\theta","where \(\theta_{t},\theta_{s},\theta\) denote the parameters of the teacher, student and Burn-In model, respectively","\theta_{t},\theta_{s}\leftarrow\theta","where \(\theta_{t},\theta_{s},\theta\) denote the parameters of the teacher, student and Burn-In model, respectively"
ICML_2024_oral_111,3,"L_{unsup}=\frac{1}{H\times W}\sum_{j=1}^{H\times W}L_{BCE} (M_{i,j}^{u},\hat{M}_{i,j}^{u})","where \(M_{i,j}^{u}\) and \(\hat{M}_{i,j}^{u}\) denote the predicted mask for \(j\)-th pixel of \(i\)-th unlabeled image by student and teacher, respectively","M_{i,j}^{u}=\hat{M}_{i,j}^{u}","where \(M_{i,j}^{u}\) and \(\hat{M}_{i,j}^{u}\) denote the predicted mask for \(j\)-th pixel of \(i\)-th unlabeled image by student and teacher, respectively."
ICML_2024_oral_111,4,L=\lambda_{sup}L_{sup}+\lambda_{unsup}L_{unsup},where \(\lambda_{sup}\) and \(\lambda_{unsup}\) is the hyperparameter of supervised loss \(\mathcal{L}_{sup}\) and unsupervised loss \(\mathcal{L}_{unsup}\),L_{total}=\lambda_{sup}L_{sup}+\lambda_{unsup}L_{unsup},where \(\mathcal{L}_{total}\) is the total loss function combining supervised and unsupervised components.
ICML_2024_oral_111,5,\theta_{t}arrow\alpha\theta_{t}+(1-\alpha)\theta_{s},"where \(\alpha\) is the decay coefficient of EMA, typically set within the small range of 0",\theta_{t}=\alpha\theta_{t-1}+(1-\alpha)\theta_{s},"where \(\theta_{t}\) denotes the updated parameters of the teacher model, \(\theta_{t-1}\) denotes the previous parameters of the teacher model, and \(\theta_{s}\) denotes the parameters of the student model."
ICML_2024_oral_111,6,"s^{k}=\frac{\sum_{j=1}^{H\times W}(\hat{M}_{i,j}^{u}\cap\widetilde{M}_{i, j,k}^{u})}{\sum_{j=1}^{H\times W}(\hat{M}_{i,j}^{u}\cup\widetilde{M}_{i, j,k}^{u})}","where \(\hat{M}_{i,j}^{u}\) and \(\widetilde{M}_{i,j,k}^{u}\) denote the pseudo-labels and \(k\)-th segment mask generated by SAM for \(j\)-th pixel of \(i\)-th unlabeled image \(\mathcal{I}_{i}^{u}\), respectively","\hat{M}_{i,j}^{u}=\arg\max_{k}IoU(\hat{M}_{i,j}^{u},\widetilde{M}_{i,j,k}^{u})","where \(\hat{M}_{i,j}^{u}\) and \(\widetilde{M}_{i,j,k}^{u}\) denote the pseudo-labels and \(k\)-th segment mask generated by SAM for \(j\)-th pixel of \(i\)-th unlabeled image \(\mathcal{I}_{i}^{u}\), respectively."
ICML_2024_oral_111,7,"s_{1}^{k}=\frac{\sum_{j=1}^{H\times W}(\hat{M}_{i,j}^{u}\cap\widetilde{M}_{ i,j,k}^{u})}{\sum_{j=1}^{H\times W}(\hat{M}_{i,j}^{u})+\epsilon}",where \(\epsilon\) is the smoothing factor to prevent a denominator of zero,"\epsilon_{i,j}=\sum_{k=1}^{K}\widetilde{M}_{i,j,k}^{u}",where \(\epsilon\) is the smoothing factor to prevent a denominator of zero.
ICML_2024_oral_111,8,"s_{2}^{k}=\frac{\sum_{j=1}^{H\times W}(\hat{M}_{i,j}^{u}\cap\widetilde{M} _{i,j,k}^{u})}{\sum_{j=1}^{H\times W}(\widetilde{M}_{i,j,k}^{u} )}",,"s_{2}^{k}=\frac{\sum_{j=1}^{H\times W}(\hat{M}_{i,j}^{u}\cap\widetilde{M}_{i,j,k}^{u})}{\sum_{j=1}^{H\times W}(\widetilde{M}_{i,j,k}^{u})+\epsilon}",where \(\epsilon\) is the smoothing factor to prevent a denominator of zero
ICML_2024_oral_111,9,"\Psi(\hat{M}^{u}_{i,j})=\gamma-\frac{1}{\sqrt{2\pi\sigma}}\exp(-\frac{(\hat{M}^{u}_{i,j}-\mu)^{2}}{2\sigma^{2}})","where \(\gamma,\sigma^{2},\mu\) are hyperparameters, which are set to 1",\Psi(x)=\frac{1}{\gamma}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right),"where \(\gamma,\sigma^{2},\mu\) are hyperparameters, which are set to 1."
ICML_2024_oral_111,10,"L_{unsup}=\frac{1}{H\times W}\sum_{j=1}^{H\times W}\Psi(\hat{M}^{u} _{i,j})*L_{BCE}(M^{u}_{i,j},\hat{M}^{u}_{i,j})",,"L_{PWA}=\frac{1}{H\times W}\sum_{j=1}^{H\times W}\Psi(\hat{M}^{u}_{i,j})L_{BCE}(\hat{M}^{u}_{i,j},Y_{i,j}^{u})","where \(\mathcal{L}_{PWA}\) denotes the loss function incorporating the Pixel-Wise Weighted Adjustment for the \(i\)-th unlabeled image, and \(\Psi(\hat{M}^{u}_{i,j})\) represents the weight assigned to each pixel based on its confidence level."
ICML_2024_oral_113,1,"\nabla_{\theta}J(\pi_{\theta})=\operatorname*{E}_{s\sim\rho_{d},a\sim\pi(\cdot|s)}[\nabla_{\theta}\log(\pi_{\theta}(a))\hat{A}^{\pi_{\theta}}(s,a)]","where \(\hat{A}^{\pi_{\theta}}(s,a)\) is an advantage function that estimates the contribution of the transition to the gradient","J(\pi)=E_{\pi_{0}\sim\rho,a_{t}\sim\pi(\cdot|s_{t})}\left[\sum_{t=0}^{T-1}\gamma^{t}r(s_{t},a_{t})\right]","where \(\hat{A}^{\pi_{\theta}}(s,a)\) is an advantage function that estimates the contribution of the transition to the gradient"
ICML_2024_oral_113,2,"L_{on}(\pi_{\theta})&=\operatorname*{E}_{\pi_{old}}[\min(r_{t}(\pi_{\theta}),.\\&.clip(r_{t}(\pi_{\theta}),1-\epsilon,1+\epsilon))A_{t}^{\pi_{old}}]",,"\hat{A}^{\pi_{\theta}}(s,a)=\hat{Q}^{\pi_{\theta}}(s,a)-\hat{V}^{\pi_{\theta}}(s)","where \(\hat{A}^{\pi_{\theta}}(s,a)\) is the advantage function, \(\hat{Q}^{\pi_{\theta}}(s,a)\) is the estimated action-value function, and \(\hat{V}^{\pi_{\theta}}(s)\) is the estimated state-value function."
ICML_2024_oral_113,3,"& L_{off}(\pi_{i};X)=\frac{1}{|X|}\sum_{j\inX}\operatorname*{E}_{(s,a)\sim\pi_{j}}[\min (r_{\pi_{i}}(s,a),..\\&..clip(r_{\pi_{i}}(s,a),\mu(1-\epsilon),\mu(1+\epsilon)))A^{\pi_{i,old}}(s,a)]","where \(r_{\pi_{i}}(s,a)=\frac{\pi_{i}(s,a)}{\pi_{j}(s,a)}\) and \(\mu\) is an off-policy correction term \(\mu=\frac{\pi_{i,old}(s,a)}{\pi_{j}(s,a)}\)","J(\pi_{i})=\operatorname*{E}_{s,a\sim\pi_{j}}[r_{\pi_{i}}(s,a)\hat{A}^{\pi_{j}}(s,a)]","where \(r_{\pi_{i}}(s,a)=\frac{\pi_{i}(s,a)}{\pi_{j}(s,a)}\) and \(\mu\) is an off-policy correction term \(\mu=\frac{\pi_{i,old}(s,a)}{\pi_{j}(s,a)}\)"
ICML_2024_oral_113,4,L(\pi_{i})=L_{on}(\pi_{i})+\lambda\cdot L_{off}(\pi_{i};X),,L(\pi_{i};X) &=L_{off}(\pi_{i};X)+\alpha L_{on}(\pi_{i}),"where \(L(\pi_{i};\mathcal{X})\) is the combined loss for policy \(\pi_{i}\), \(L_{off}(\pi_{i};\mathcal{X})\) is the off-policy loss, \(L_{on}(\pi_{i})\) is the on-policy loss, and \(\alpha\) is a scaling factor for the on-policy term."
ICML_2024_oral_113,5,"V^{target}_{on,\pi_{j}}(s_{t})=\sum_{k=t}^{t+2}\gamma^{k-t}r_{k}+\gamma^{3}V_{\pi_{j},old}(s_{t+3})",,R_{t}^{(n)}=\sum_{k=0}^{n-1}\gamma^{k} r_{t+k}+\gamma^{n} V(s_{t+n}),"where \(R_{t}^{(n)}\) is the \(n\)-step return, \(r_{t+k}\) is the reward at time \(t+k\), and \(V(s_{t+n})\) is the value function at state \(s_{t+n}\)."
ICML_2024_oral_113,6,"V^{target}_{off,\pi_{j}}(s^{\prime}_{t})=r_{t}+\gamma V_{\pi_{j},old}(s^{\prime}_{t+1})",,"V^{target}_{off,\pi_{j}}(s_{t})=r_{t}+\gamma V_{\pi_{j},old}(s_{t+1})","where \(V^{target}_{off,\pi_{j}}(s_{t})\) is the target value for off-policy updates, approximating the 1-step return using off-policy data."
ICML_2024_oral_113,7,"L^{critic}_{on}(\pi_{i})=\operatorname*{E}_{(s,a)\sim\pi_{i}}[( V_{\pi_{i}}(s)-V^{target}_{on,\pi_{i}}(s))^{2}] || L^{critic}_{off}(\pi_{i};X)=\frac{1}{|X|}\sum_{j\inX}\operatorname*{E}_{(s,a)\sim\pi_{j}}[(V_{\pi_{i}}(s)-V^{target}_{off,\pi_{i}}(s))^{2}] || L^{critic}(\pi_{i})=L^{critic}_{on}(\pi_{i})+\lambda\cdot L^{critic}_{off}(\pi_{i})",,"L_{critic}(\pi_{j})=\operatorname*{E}_{(s,a)\sim\pi_{j}}\left[\left(V^{target}_{on,\pi_{j}}(s)-V_{\pi_{j}}(s)\right)^{2}+\left(V^{target}_{off,\pi_{j}}(s')-V_{\pi_{j}}(s')\right)^{2}\right]",The critic loss combines the squared differences between the target values and the estimated values for both on-policy and off-policy data.
ICML_2024_oral_117,1,"Attention(q,K,V)=softmax(\frac{qK^{T}}{\sqrt{d_{model}}})V",,"v=Attention(q, K, V)=softmax\left(\frac{q K^{\top}}{\sqrt{d_{model}}}\right) V","Output vector \(\mathbf{v}\) generated by the output attention head based on the query vector \(\mathbf{q}\), keys matrix \(K\), and values matrix \(V\)."
ICML_2024_oral_117,2,"FTr_{i}=\frac{AUC_{i}-AUC_{i}^{b}}{1-AUC_{i}^{ b}},\quadAUC_{i}=\frac{1}{\Delta}\int_{(i-1)\cdot\Delta}^{i\cdot\Delta}p_{i}(t)dt || AUC_{i}^{b}=\frac{1}{\Delta}\int_{0}^{\Delta}p_{i}^{b}(t)dt",,FTr_{i}=\frac{\int_{0}^{T} p_{i}(t) dt-\int_{0}^{T} p_{i}^{b}(t) dt}{\int_{0}^{T} p_{i}^{b}(t) dt},"Forward transfer metric for task \(i\), comparing the performance of the proposed method with the baseline."
ICML_2024_oral_117,3,"RT=\frac{1}{N}\sum_{i=2}^{N}\max_{j<i}FTr(j,i)","where \(\text{FTr}(j,i)\) is the forward transfer obtained by training a model from scratch in the \(j\)-th task and fine-tuning it in the \(i\)-th task","RT=\max_{j}FTr(j,i)","where \(\text{FTr}(j,i)\) is the forward transfer obtained by training a model from scratch in the \(j\)-th task and fine-tuning it in the \(i\)-th task."
ICML_2024_oral_121,1,"p(x;p^{m})=\sum_{i}^{N}k(x,d_{i})p_{i}^{m}",,p(x)=\sum_{i=1}^{N} p_{i}^{m}\cdot\frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{(x-d_{i})^{2}}{2\sigma^{2}}},The probability density function of disparity \(x\) computed using a Laplacian kernel interpolation of the discrete distribution.
ICML_2024_oral_121,2,"\texttt{argmin}_{y}F(y,p^{m})=\texttt{argmin}_{y}\intL(y,x )p(x;p^{m})dx","where \(F(y,\mathbf{p}^{m})\) is called as the risk at \(y\), and \(\mathcal{L}(y,x)\) is the error function between \(y\) and \(x\)","F(y,p^{m})=\intL(y,x)p(x;p^{m})dx","where \(F(y,\mathbf{p}^{m})\) is the risk at \(y\), and \(\mathcal{L}(y,x)\) is the error function between \(y\) and \(x\)."
ICML_2024_oral_121,3,y=\int xp(x;p^{m})dx,,"\texttt{argmin}_{y}F(y,p^{m})=\texttt{argmin}_{y}\intL(y,x )p(x;p^{m})dx","where \(F(y,\mathbf{p}^{m})\) is the risk at \(y\), and \(\mathcal{L}(y,x)\) is the error function between \(y\) and \(x\)."
ICML_2024_oral_121,4,"\texttt{argmin}_{y}F(y,p^{m})=\texttt{argmin}_{y}\int|y-x|p(x;p^{m})dx",,"L(y,x)=|y-x|","where \(\mathcal{L}(y,x)\) is the loss function used to measure the error between the predicted disparity \(y\) and the ground truth disparity \(x\)."
ICML_2024_oral_121,5,"G(y,p^{m})\triangleq\frac{\partial F(y,p^{m})} {\partial y} || =\sum_{i}p_{i}^{m}\texttt{Sign}(y-d_{i})(1-\exp-\frac{|y-d_{i}|}{\sigma})=0",,"\frac{\partial F(y,p^{m})}{\partial y}=\intsign(y-x)p(x;p^{m})dx=0",where \(\text{sign}(y-x)\) is the sign function that indicates the direction of the error between the predicted disparity \(y\) and the ground truth disparity \(x\).
ICML_2024_oral_121,6,"dG(y,p^{m})=\frac{\partial G}{\partial y}dy+\frac{\partial G}{\partialp^{m}}dp^{m}=0",,"\frac{dy}{dp^{m}}=-\frac{\partial G(y,p^{m})/\partialp^{m}}{\partial^{2}G(y,p^{m})/\partial y\partialp^{m}}","where \(G(y,\mathbf{p}^{m})\) is defined as the first derivative of the risk function with respect to the predicted disparity \(y\) and \(\mathbf{p}^{m}\) is the discrete distribution of disparities."
ICML_2024_oral_121,7,"\frac{dy}{dp^{m}}=-\frac{\partial G/\partialp^{m}}{\partial G/\partial y} || =[\dots,\frac{\sigma\texttt{Sign}(d_{i}-y)(1-\exp-\frac{|y-d_{i} |}{\sigma})}{\sum_{j}p_{j}^{m}\exp-\frac{|y-d_{i}|}{\sigma}},\dots]^{T}",,\frac{dy}{dp^{m}}=-\frac{\frac{\partial G}{\partialp^{m}}}{\frac{\partial G}{\partial y}},where \(\frac{dy}{d\mathbf{p}^{m}}\) represents the gradient of the optimal disparity with respect to the discrete distribution \(\mathbf{p}^{m}\).
ICML_2024_oral_121,8,"L(x^{\texttt{gt}},x^{\texttt{pred}})=0.5(x^{\texttt{gt}}-x^{\texttt{pred}})^{2},&if |x^{\texttt{gt}}-x^{\texttt{pred}}|<1.0\\|x^{\texttt{gt}}-x^{\texttt{pred}}|-0.5,&otherwise",,dy/dp^{m}=-\frac{\partial G/\partialp^{m}}{\partial G/\partial y},where \(\mathbf{y}\) represents the optimal disparity predictions and \(\mathbf{p}^{m}\) is the discrete distribution of disparity hypotheses.
ICML_2024_oral_122,1,"\min_{\theta}\,\sum_{i=1}^{N}(\hat{y}(x;\theta)-y_{i})^{2}",,MSE(\theta)=\frac{1}{N}\sum_{i=1}^{N} (y_{i}-\hat{y}(x_{i};\theta))^{2},Mean-squared error objective for the maximum likelihood estimator.
ICML_2024_oral_122,2,"\min_{\theta}\,\sum_{i=1}^{N}\int_{Y}p(y\,|\,x_{i})\log(\hat{p}(y\,|\,x_ {i};\theta))dy",,"\min_{\theta}\,D_{KL}\left(\hat{p}(y\,|\,x;\theta)\,\|\,p(y\,|\,x)\right)","where \(\mathbb{D}_{KL}\) denotes the Kullback-Leibler divergence between the learned distribution \(\hat{p}(y\,|\,x;\theta)\) and the target distribution \(p(y\,|\,x)\)."
ICML_2024_oral_122,3,"Z=\{\sum_{i=1}^{m}p_{i}\,\delta_{z_{i}}\,:\,p_{i}\geq 0,\sum_{i=1} ^{m}p_{i}=1\}",where \(p_{i}\) is the probability associated with location \(z_{i}\) and \(\delta_{z_{i}}\) is the Dirac delta function at location \(z_{i}\),"p(y\,|\,x;\theta)=\sum_{i=1}^{m}p_{i}\delta_{z_{i}}(y)",where \(p_{i}\) is the probability associated with location \(z_{i}\) and \(\delta_{z_{i}}\) is the Dirac delta function at location \(z_{i}\)
ICML_2024_oral_122,4,"\boxed{TD_{MSE}(\theta)=E_{D}[((\tilde{T}Q)(S,A;\tilde{\theta})-\,Q(S,A;\theta))^{2}]} || (\tilde{T}Q)(s,a;\tilde{\theta})=R+\gamma\max_{a^{\prime}}Q(S^{\prime },a^{\prime};\tilde{\theta})\,\big{|}\,S=s,A=a\","where \(\tilde{\theta}\) is a slow moving copy of the parameters \(\theta\) that parameterize the ""target network"" and

\[(\tilde{\mathcal{T}}Q)(s,a;\tilde{\theta})=R+\gamma\max_{a^{\prime}}Q(S^{\prime },a^{\prime};\tilde{\theta})\,\big{|}\,S=s,A=a\,,\]

is the sample version of the Bellman optimality operator which defines our scalar regression target","\min_{\theta}\,\sum_{(S,A,R,S^{\prime})\inD}\left(Q(S,A;\theta)-(\tilde{T}Q)(S,A;\tilde{\theta})\right)^{2}","where \(\tilde{\theta}\) is a slow moving copy of the parameters \(\theta\) that parameterize the ""target network""."
ICML_2024_oral_122,5,"\alpha\,E_{D}[\log\big{(}\sum_{a^{\prime}}\exp(Q(S^{\prime},a^{\prime};\theta))\big{)}-Q(S,A;\theta)]",,"L_{CQL}(\theta)=TD_{MSE}(\theta)+\alphaE_{D}\left[\log\left(\frac{Q(S,A;\theta)}{\pi(A|S)}\right)\right]","where \(\mathcal{L}_{\text{CQL}}(\theta)\) is the combined loss function, \(\pi(A|S)\) is the behavior policy, and \(\alpha\) is a scaling factor for the regularization term."
ICML_2024_oral_122,6,"Q(s,a;\theta)=E[\,Z(s,a;\theta)\,],\Z(s,a;\theta)=\sum_{i=1}^{m}\hat{p}_{i}(s,a;\theta)\cdot\delta_{z_{i}} || \hat{p}_{i}(s,a;\theta)=\frac{\exp(l_{i}(s,a;\theta))}{\sum_{j=1} ^{m}\exp(l_{j}(s,a;\theta))}\",,"\hat{p}_{i}(s,a;\theta)=\frac{\exp(l_{i}(s,a;\theta))}{\sum_{j=1}^{m}\exp(l_{j}(s,a;\theta))}","where \(l_{i}(s,a;\theta)\) are the logits for each class \(z_{i}\) and \(\hat{p}_{i}(s,a;\theta)\) is the probability associated with class \(z_{i}\)."
ICML_2024_oral_122,7,"\boxed{TD_{CE}(\theta)=E_{D}[\sum_{i=1 }^{m}p_{i}(S,A;\tilde{\theta})\log\hat{p}_{i}(S,A;\theta)]}",,"L_{CE}(\theta)=-E_{D}\left[\sum_{i=1}^{m}p_{i}(S,A;\tilde{\theta})\log(\hat{p}_{i}(S,A;\theta))\right]","where \(\mathcal{L}_{\text{CE}}(\theta)\) is the cross-entropy loss, \(p_{i}(S,A;\tilde{\theta})\) are the target probabilities, and \(\hat{p}_{i}(S,A;\theta)\) are the predicted probabilities from the categorical representation of \(Q\)."
ICML_2024_oral_122,8,"p_{i}(S,A;\tilde{\theta})=\frac{y-z_{i}}{z_{i+1}-z_{i}},\p_{i+1}(S,A;\tilde{\theta})=\frac{z_{i+1}-y}{z_{i+1}-z_{i}}\",,"p_{i}=\frac{z_{i+1}-y}{z_{i+1}-z_{i}},\p_{i+1}=\frac{y-z_{i}}{z_{i+1}-z_{i}}","where \(p_{i}\) and \(p_{i+1}\) are the probabilities assigned to the locations \(z_{i}\) and \(z_{i+1}\) respectively, based on the scalar target \(y\)."
ICML_2024_oral_122,9,"p_{i}(S,A;\tilde{\theta})=\int_{z_{i}-\nicefrac{{\varsigma}}{{2}}}^{z_{i}+\nicefrac{{\varsigma}}{{2}}}f_{Y|S,A}(y|S,A)dy || =F_{Y|S,A}(z_{i}+\nicefrac{{\varsigma}}{{2}}|S,A)-F_{Y|S,A}(z_{i}-\nicefrac{{\varsigma}}{{2}}|S,A)",,"p_{i}(S,A;\tilde{\theta})=\int_{z_{i}-\nicefrac{{\varsigma}}{{2}}}^{z_{i}+\nicefrac{{\varsigma}}{{2}}}f_{Y|S,A}(y)dy\","where \(f_{Y|S,A}(y)\) is the probability density function of the random variable \(Y\,|\,S,A\)."
ICML_2024_oral_122,10,"(\widehat{T}Z)(s,a;\tilde{\theta})\overset{D}{=}\sum_{i=1}^{m}\hat{p}_{i}(S^{\prime},A^{\prime};\tilde{\theta})\cdot\delta_{R+\gamma z_{i}}\bigm{|}S=s,\,A=a\","where \(A^{\prime}=\operatorname*{arg\,max}_{a^{\prime}}Q(S^{\prime},a^{\prime}; \tilde{\theta})\)","T_{Z}(Z)(S,A;\tilde{\theta})=R+\gamma Z(S^{\prime},A^{\prime};\tilde{\theta})\,|\,S=S,A=A","where \(A^{\prime}=\operatorname*{arg\,max}_{a^{\prime}}Q(S^{\prime},a^{\prime}; \tilde{\theta})\)"
ICML_2024_oral_122,11,"p_{i}(S,A;\tilde{\theta})=\sum_{j=1}^{m}\hat{p}_{j}(S^{\prime},A ^{\prime};\tilde{\theta})\cdot\xi_{j}(R+\gamma z_{i}) || \xi_{j}(x)=\frac{x-z_{j}}{z_{j+1}-z_{j}}\mathds{1}\{\lfloor x\rfloor=z_{j}\}+\frac{z_{j+1}-x}{z_{j+1}-z_{j}}\mathds{1}\{\lceil x\rceil=z_ {j}\}\",,"(\widehat{T}Z)(s,a;\tilde{\theta})\overset{D}{=}\sum_{i=1}^{m}\hat{p}_{i}(S^{\prime},A^{\prime};\tilde{\theta})\cdot\delta_{R+\gamma z_{i}}\bigm{|}S=s,\,A=a\","where \(A^{\prime}=\operatorname*{arg\,max}_{a^{\prime}}Q(S^{\prime},a^{\prime}; \tilde{\theta})\)"
ICML_2024_oral_125,1,"E_{i}=L_{i}(E_{i-1}),\\i=1,...,N || \dot{y}=\texttt{Head}(e_{N}^{0})",,E_{i+1}=f_{\Theta}(E_{i}),Equation representing the output patch embeddings \(\mathbf{E}_{i+1}\) of the \((i+1)\)-th layer as a function of the input patch embeddings \(\mathbf{E}_{i}\) processed by the ViT model \(f_{\Theta}(\cdot)\).
ICML_2024_oral_125,2,"\min_{\tilde{\Theta}}L(x;\Theta),\x\sim Q(x)",where \(\tilde{\Theta}\subseteq\Theta\) denotes the model parameters involved for updating,"\tilde{\Theta}=\arg\min_{\Theta}L(D_{test}, f_{\Theta}(\cdot))",where \(\tilde{\Theta}\subseteq\Theta\) denotes the model parameters involved for updating
ICML_2024_oral_125,3,"p^{*}=\operatorname*{arg\,min}_{p}L(f_{\Theta}(p;x))","where \(\mathcal{L}(\cdot)\) is a fitness function and \(\mathbf{p}\in\mathbb{R}^{d\times N_{p}}\) consists of \(N_{p}\) prompt embeddings, each of dimension \(d\)","p^{*}=\arg\min_{p}L(x;p),\x\sim Q(x),\p\inR^{d\times N_{p}}","where \(\mathcal{L}(\cdot)\) is a fitness function and \(\mathbf{p}\in\mathbb{R}^{d\times N_{p}}\) consists of \(N_{p}\) prompt embeddings, each of dimension \(d\)"
ICML_2024_oral_125,4,L(f_{\Theta}(p;X_{t}))=\sum_{ x\inX_{t}}\sum_{c\inC}-\hat{y}_{c}\log\hat{y}_{c} || \qquad+\lambda\sum_{i=1}^{N}\lvert\lvert\mu_{i}(X_ {t})-\mu_{i}^{S}\rvert\rvert_{2}+\lVert\sigma_{i}(X_{t})-\sigma_{i}^{S}\rvert\rvert_{2},,L(p)=\sum_{i=0}^{N}\left\|\mu_{i}(X_{t})-\mu_{i}^{S}\right\|^2+\sum_{i=0}^{N}\left\|\sigma_{i}(X_{t})-\sigma_{i}^{S}\right\|^2,"where \(\mathbf{\mu}_{i}(\mathcal{X}_{t})\) and \(\mathbf{\sigma}_{i}(\mathcal{X}_{t})\) are the mean and standard deviation of the CLS tokens for the current batch of testing samples, respectively"
ICML_2024_oral_125,5,"p_{k}^{(t)}\simm^{(t)}+\tau^{(t)}N(0,\bm {\Sigma}^{(t)})",,"p_{t}^{(j)}\simN(\mu_{t},\Sigma_{t}),\j=1,\ldots,\lambda","where \(\mathbf{p}_{t}^{(j)}\) represents the \(j\)-th candidate prompt sampled at iteration \(t\), and \(\mathbf{\mu}_{t}\) and \(\mathbf{\Sigma}_{t}\) denote the mean and covariance matrix of the distribution at iteration \(t\)"
ICML_2024_oral_125,6,e_{N}^{0}arrowe_{N}^{0}+\gammad,where \(\mathbf{d}\) is a shifting direction and \(\gamma\) is a step size,e_{N}^{0}\leftarrowe_{N}^{0}+\gammad,where \(\mathbf{d}\) is a shifting direction and \(\gamma\) is a step size.
ICML_2024_oral_125,7,d_{t}=\mu_{N}^{S}-\mu_{N}(t),where \(\boldsymbol{\mu}_{N}^{S}\) is the mean of the \(N\)-th final layer CLS feature \(\mathbf{e}_{N}^{0}\) and calculated over source in-distribution samples \(\mathcal{D}_{S}\) (the same one used in Eqn,"d=\mu_{N}^{S}-\frac{1}{M}\sum_{j=1}^{M}e_{N}^{0}(X_{t,j})","where \(\boldsymbol{\mu}_{N}^{S}\) is the mean of the \(N\)-th final layer CLS feature calculated over source in-distribution samples and \(\mathbf{e}_{N}^{0}(\mathcal{X}_{t,j})\) are the CLS features from the current batch of testing samples."
ICML_2024_oral_125,8,\mu_{N}(t)=\alpha\mu_{N}(X_{t})+(1-\alpha )\mu_{N}(t-1),where \(\boldsymbol{\mu}_{N}(\mathcal{X}_{t})\) is the mean of the \(N\)-th layer's CLS feature and calculated over the \(t\)-th test batch \(\mathcal{X}_{t}\),\mu_{N}(t)=\alpha\mu_{N}(t-1)+(1-\alpha)\mu_{N}(X_{t}),where \(\alpha\) is the decay rate for the exponential moving average.
ICML_2024_oral_127,1,\sup_{x}\mathds{E}_{D^{j}\sim\chi}[\lfloor\frac{j}{i}\hat{f}(x )-f_{D^{j}}(x)\rceil]\geq\frac{\sqrt{k}}{4},,\sup_{x}\mathds{E}_{D\sim\chi}\left[\left\lfloor\frac{j}{i}\hat{f}(x)-f_{D^{j}}(x)\right\rfloor\right]\geq\frac{1}{2}\left(1-\frac{i}{j}\right),Lower bound on the expected maximum generalization error of the model to dataset \(\mathbf{D}^{j}\).
ICML_2024_oral_127,2,\mathds{P}_{D\sim\chi}[\|f_{\chi}-\hat{f}\|_{\infty}\geq\epsilon]\leq\varkappa_ {1}e^{-\varkappa_{2}(\frac{\epsilon}{\sqrt{n}}-1)^{2}},,"\mathds{P}(\lvert\hat{f}(x)-f_{D}(x)\rvert\leq\epsilon)\geq 1-\delta,}\) with confidence\(1-\delta\).</description>",Probability that the model's output is within an error margin \(\epsilon\) of the true function \(f_{\mathbf{D}}\) with confidence \(1 - \delta\).
ICML_2024_oral_13,1,"&P\{X_{1}=x_{1}\}\\&P\{X_{2}=x_{2}|X_{1}=x_{1}\}\\&\quad\vdots\\&P\{X_{n}=x_{n}|X_{1}=x_{1},\cdots,X_{n-1}=x_{n-1 }\}",,"P(X_{1},\cdots,X_{n})=P(X_{1}=x_{1})\cdot P(X_{2}=x_{2} | X_{1}=x_{1})\cdots P(X_{n}=x_{n} | X_{1}=x_{1},\cdots, X_{n-1}=x_{n-1})","The joint probability of observing a sequence of \(n\) tokens \(X_{1},\cdots,X_{n}\) given the previous tokens in the sequence."
ICML_2024_oral_13,2,"&P\{X_{n}=x_{n}\}\\&P\{X_{n-1}=x_{n-1}|X_{n}=x_{n}\}\\&\quad\vdots\\&P\{X_{1}=x_{1}|X_{n}=x_{n},\cdots,X_{2}=x_{2}\}",,"&P\{X_{n}=x_{n}\}\\&P\{X_{n-1}=x_{n-1}|X_{n}=x_{n}\}\\&\quad\vdots\\&P\{X_{1}=x_{1}|X_{2}=x_{2},\cdots,X_{n}=x_{n}\}",Product of the estimations of the probabilities of the tokens in reverse order.
ICML_2024_oral_13,3,"\sum_{i=1}^{n}\ell_{i}^{arrow}=-\lnP_{n}^{arrow}\{X_ {1}=x_{1},\cdots,X_{n}=x_{n}\}",,\ell_{i}^{\leftarrow}=-\lnp_{i}^{\leftarrow}\left( x_{i}\right),"Cross-entropy loss for the backward model, where \(\ell_{i}^{\leftarrow}\) is the loss for the \(i\)-th token prediction from the backward model."
ICML_2024_oral_13,4,L_{n}^{arrow}=D_{KL}(P_{n}\big{|}\big{|}P_{n}^{arrow})+H(P_{n}),where \(H\) denotes the entropy and \(\mathrm{D}_{\mathrm{KL}}\) the Kullback-Leibler divergence,L_{n}^{\leftarrow}=H(X)+D_{KL}\left(P_{n}^{\leftarrow}\|P_{n}\right),where \(H\) denotes the entropy and \(\mathrm{D}_{\mathrm{KL}}\) the Kullback-Leibler divergence
ICML_2024_oral_132,1,P(A(D)\in S)\leq e^{\epsilon}P(A(D^{\prime})\in S)+\delta,,\Pr[A(D)\in S]\leq e^{\epsilon}\cdot\Pr[A(D^{\prime})\in S]+\delta,Probability that the output of the algorithm \(\mathcal{A}\) on dataset \(\mathcal{D}\) falls within a set \(S\) is bounded by the probabilities on adjacent dataset \(\mathcal{D}^{\prime}\) with a privacy loss defined by \(\epsilon\) and \(\delta\).
ICML_2024_oral_132,2,\epsilon=\epsilon_{\alpha}+\log(\frac{\alpha-1}{\alpha})-\frac{\log\delta+\log\alpha}{\alpha-1},,\epsilon=\epsilon_{\alpha}+\delta,where \(\epsilon_{\alpha}\) is the privacy parameter for Renyi differential privacy and \(\delta\) is a small positive value representing the probability of failure.
ICML_2024_oral_132,3,"\widetilde{g}_{t}=\frac{1}{B}(\sum_{x\inB_{t}}clip_{C}(\nabla_{\theta}\ell(x;\theta_{t})+N(0,\sigma^{2}C^{2}I))","where \(\eta_{t}\) is the learning rate, \(\mathcal{B}_{t}\) is the sampled batch, \(B\) is the average batch size, \(\sigma>0\) is the noise multiplier, and \(\mathsf{clip}_{C}\) is the operation that clips the per-sample gradient norm to at most \(C>0\)","\theta_{t+1}=\theta_{t}-\eta_{t}clip_{C}(\widetilde{g}_{t})+N(0,\sigma^2I)","\(\mathbf{\theta}_{t}\): model parameters at iteration \(t\), \(\eta_{t}\): learning rate at iteration \(t\), \(\mathcal{B}_{t}\): sampled batch, \(B\): average batch size, \(\sigma\): noise multiplier, \(C\): clipping threshold, \(\mathsf{clip}_{C}(\cdot)\): clipping operation, \(\mathcal{N}(0, \sigma^2 \mathbf{I})\): Gaussian noise with mean 0 and variance \(\sigma^2\)."
ICML_2024_oral_132,4,"L_{MAE}(\theta):=\frac{1}{n}\sum_{i=1}^{n}\underbrace{\xi_{ MSE}(g\circ\psi(mask(x_{i});\theta),x_{i})}_{\ell(x_{i};\theta)}","where \(n\) is the number of training samples, \(\mathbf{x}_{i}\in\mathbb{R}^{C\times H\times W}\) is the input of the \(i\)-th training image (\(C\)-number of channels, \(H\)-height, \(W\)-width), mask\((\cdot)\) is a function that mask out a fraction of the image, \(\psi:\mathbb{R}^{C\times H\times W}\rightarrow\mathbb{R}^{d}\) is the encoder and \(g:\mathbb{R}^{d}\rightarrow\mathbb{R}^{C\times H\times W}\) is the decoder","L_{MAE}(\theta)=\frac{1}{n}\sum_{i=1}^{n}\ell(x_{i},g(\psi(mask(x_{i}))))","where \(n\) is the number of training samples, \(\mathbf{x}_{i}\in\mathbb{R}^{C\times H\times W}\) is the input of the \(i\)-th training image (\(C\)-number of channels, \(H\)-height, \(W\)-width), mask\((\cdot)\) is a function that masks out a fraction of the image, \(\psi:\mathbb{R}^{C\times H\times W}\rightarrow\mathbb{R}^{d}\) is the encoder and \(g:\mathbb{R}^{d}\rightarrow\mathbb{R}^{C\times H\times W}\) is the decoder."
ICML_2024_oral_15,1,\min_{p(T|X)}\quad I(X;T),where \(I(X;T)\) denotes the mutual information of \(X\) and \(T\),"I(T;X)=\min_{p(t|x)}E_{X} [d(X,T)]","where \(I(T;X)\) denotes the mutual information of \(T\) and \(X\), \(p(t|x)\) is the conditional probability distribution of \(T\) given \(X\), and \(d(X,T)\) is the distortion measure between \(X\) and \(T\)."
ICML_2024_oral_15,2,\min_{p(T|X)} I(X;T),,"\min_{p(T|X,Q)}\quad I(X;T|Q)",where \(I(X;T|Q)\) denotes the conditional mutual information of \(X\) and \(T\) given \(Q\).
ICML_2024_oral_15,3,I(T;Q)\geq\varepsilon,where \(\varepsilon>0\) is a threshold,\varepsilon > 0,where \(\varepsilon\) is a threshold
ICML_2024_oral_15,4,L(p(T|X))=I(X;T)-\beta I(T;Q),where \(\beta\) is the Lagrange multiplier,"L(p(T|X),\beta)=I(X;T)-\beta (I(T;Q)-\varepsilon)",where \(\beta\) is the Lagrange multiplier
ICML_2024_oral_15,5,L(p(T|X))=I(X;T)+\beta I(X;Q|T)+constant,,L(p(T|X))=I(X;T)-\beta I(T;Q),where \(\beta\) is the Lagrange multiplier
ICML_2024_oral_15,6,"p^{*}(T|X)=\frac{p^{*}(T)}{Z(X,\beta)}\exp\Bigl{(}-\betaKL[p(Q|X)\\big{\|}\p(Q|T)]\Bigr{)}","where \(Z(X,\beta)\) is a probability normalization term, \(p^{*}(T)=\mathbb{E}_{X}[p^{*}(T|X)]\)","\nablaL(p(T|X))=Z(X,\beta)p^{*}(T)","where \(Z(X,\beta)\) is a probability normalization term, \(p^{*}(T)=\mathbb{E}_{X}[p^{*}(T|X)]\)"
ICML_2024_oral_15,7,"p(X,Q|f)\equiv\prod_{x\inX}p^{*}\big{(}X=x\\big{|}\T=f(x)\big{)}",where \(p^{*}\) is given by Formula (5),"f^{*}=\arg\max_{f:X\toT} p(X,Q|f)",where \(p^{*}\) is given by Formula (5)
ICML_2024_oral_15,8,"I(X;T)=E_{X,T}\log\frac{p(X|T)}{p(X)} || I(X;Q|T)=E_{X,T,Q}\log\frac{p(X,Q|T)}{p(X|T)p(Q|T)}",,"I(X;T)=\sum_{x\inX}p(X=x)\sum_{t\inT}p(T=t|X=x)\log\frac{p(T=t|X=x)}{p(T=t)},\quad I(X;Q|T)=\sum_{t\inT}p(T=t)\sum_{x\inX}p(X=x|T=t)\sum_{q\inQ}p(Q=q|X=x,T=t)\log\frac{p(Q=q|X=x,T=t)}{p(Q=q|T=t)}","where \(I(X;T)\) is the mutual information between \(X\) and \(T\), and \(I(X;Q|T)\) is the conditional mutual information between \(X\) and \(Q\) given \(T\)."
ICML_2024_oral_15,9,"=E_{X,T,Q}\log\frac{p(X|Q)}{p(T|Q)}\frac{p(T)}{p(X)}",,"I(X;T)=E_{X,T}\log\frac{p(X|T)}{p(X)}",where \(I(X;T)\) denotes the mutual information between \(X\) and \(T\).
ICML_2024_oral_22,1,"x_{t-1}=\frac{1}{\sqrt{\alpha_{t}}}\Big{(}x_{t}-\frac{1-\alpha_{t}} {\sqrt{1-\bar{\alpha}_{t}}}\epsilon_{\theta}(x_{t},t,c)\Big{)}+\sigma_{t}\epsilon","where \(\mathbf{\epsilon}\sim\mathcal{N}(0,I)\), \(\alpha_{0}:=1\), \(\alpha_{t}\) and \(\bar{\alpha}_{t}\) define the noise schedule, \(\sigma_{t}\) is the sampling standard deviation",x_{t}=\sqrt{\bar{\alpha}_{t}}x_{0}+\sqrt{1-\bar{\alpha}_{t}}\epsilon,"where \(\mathbf{x}_{t}\) is the noisy data at time step \(t\), \(\mathbf{x}_{0}\) is the original clean data, and \(\mathbf{\epsilon}\) is the noise sampled from a standard normal distribution."
ICML_2024_oral_22,2,"x_{T}^{*}=\arg\min_{x_{T}}L(f(x_{0}),y) || x_{0}=\texttt{Sampler}_{T}(\epsilon_{\theta},x_{T},c)",,L=\| f(x_{0})-y\|^2,"where \(\mathcal{L}\) is the loss function, \(f(\cdot)\) is the target feature extractor, and \(\mathbf{y}\) is the target control feature."
ICML_2024_oral_24,1,"\omega_{1}(D_{1},D_{2},J)=\frac{1}{N}\sum_{i=1}^{N}\mathds{1}\{J(q_{i},a_{i1}, a_{i2})=a_{i1}\}",,\omega_{1}=\frac{N_{1}}{N},"Where \(\omega_{1}\) is the win rate for Debater \(D_{1}\), \(N_{1}\) is the number of times Judge \(J\) selects \(D_{1}\)'s answer, and \(N\) is the total number of matches."
ICML_2024_oral_24,2,"\bar{\omega}_{1}(D_{1},D_{2},J)>\frac{1}{2}",,"\bar{\omega}_{1}(D_{1},D_{2},J)=\frac{1}{2}\left(\omega_{1}(D_{1},D_{2},J)+\omega_{1}(D_{2},D_{1},J)\right)",Average win rate for Debater \(D_{1}\) considering flipped assignments.
ICML_2024_oral_24,3,"\bar{\omega}_{1}(D_{1},D_{2},J)=\frac{1}{1+10^{(E_{2}-E_{1})/400}}",,"E_{1}=E_{2}+\log\left(\frac{1-\bar{\omega}_{1}(D_{1},D_{2},J)}{\bar{\omega}_{1}(D_{1},D_{2},J)}\right)",Aggregate rating for debater \(D_{1}\) in relation to debater \(D_{2}\) and judge \(J\).
ICML_2024_oral_24,4,"\omega_{C}(D_{1},D_{2},J)=\frac{1}{1+10^{(E^{I}_{2}-E^{C}_{1})/400}}",,E^{C}=\frac{1}{1+\sum_{j=1}^{M}10^{(E_{j}-E^{C})/400}},Correct rating \(E^{C}\) for a debater based on their performance against multiple opponents \(E_{j}\).
ICML_2024_oral_25,1,"\min_{\theta}KL(p(Y|X)\|q(Y|X,\theta))",,\theta^{(k)}=\arg\min_{\theta}\sum_{i=1}^{N}-y_i\log(f(x_i;\theta))+(1-y_i)\log(1-f(x_i;\theta)),Optimization problem for minimizing the cross-entropy loss for each model in the Deep Ensemble.
ICML_2024_oral_25,2,"H(q(Y|X))=\underbrace{I(Y;\theta| X)}_{D}+\underbrace{E_{q(\theta|D)}H(q(Y|X,\theta))}_{D}",where \(\mathcal{I}\) denotes the mutual information under the \(q\) distribution,"U=E_{q(\theta|D)}[KL(p(Y|X)\|q(Y|X,\theta))]+I(q(Y|X), p(Y|X))",where \(\mathcal{I}\) denotes the mutual information under the \(q\) distribution
ICML_2024_oral_25,3,H(q(Y|X))=\underbrace{I(Y;C|X)}_ {Q^{\prime}}+\underbrace{E_{q(C|X)}H( q(Y|X\oplusC))}_{Q^{\prime}},,H(q(Y|X))=\underbrace{I(Y;C|X)}_{D}+\underbrace{E_{q(C|X)}H(q(Y|X\oplusC))}_{D},where \(\mathcal{I}\) denotes the mutual information under the \(q\) distribution and \(\mathbf{C}\) represents the set of clarifications.
ICML_2024_oral_26,1,"&Attention(Q,K,V)=softmax(QK^{\intercal})V,\\&where Q,K,V\inR^{n\times d}",where \(n\) denotes the number of tokens and \(d\) denotes the hidden dimension of the attention,A=\frac{QK^T}{\sqrt{d}},"where \(A\) is the attention score matrix, \(Q\) is the matrix of queries, \(K\) is the matrix of keys, and \(d\) is the hidden dimension of the attention."
ICML_2024_oral_26,2,"& o_{i}=\sum_{j=1}^{n}\frac{e^{q_{i}k_{j}^{\intercal}} }{\sum_{j^{\prime}=1}^{n}e^{q_{i}k_{j^{\prime}}^{\intercal}}v_{j}},\\&where q_{i},k_{i},v_{i}\inR^{1\times d},i=\{1,2,...,n\}",,"O=Attention(Q,K,V)=softmax(QK^{\intercal})V","where \(O\) represents the output of the self-attention mechanism, and \(o_{i}\) denotes the individual output for each token."
ICML_2024_oral_26,3,"o_{i}=\sum_{j=1}^{n}\frac{K(q_{i},k_{j})}{\sum_{j^{\prime}=1}^{n}K( q_{i},k_{j^{\prime}})}v_{j}",,"&Attention(Q,K,V)=\sum_{i=1}^{n}\sum_{j=1}^{n} K(q_{i}, k_{j}) v_{j},\\&where  K(\cdot,\cdot) is a positive-definite kernel.","where \(K(q_{i}, k_{j})\) denotes the kernel function measuring similarity between query \(q_{i}\) and key \(k_{j}\)."
ICML_2024_oral_26,4,o_{i}=\sum_{j=1}^{n}\frac{\phi(q_{i})\phi(k_{j})^{\intercal}}{\sum_{j^{\prime }=1}^{n}\phi(q_{i})\phi(k_{j^{\prime}})^{\intercal}}v_{j},where \(\phi(\cdot):\mathbb{R}^{d}\rightarrow\mathbb{R}^{m}\) is a projection to map the inputs into \(m\) dimension features,"o_{i}=\sum_{j=1}^{n}\frac{K(\phi(q_{i}),\phi(k_{j}))}{\sum_{j^{\prime}=1}^{n}K(\phi(q_{i}),\phi(k_{j^{\prime}}))}v_{j}",where \(\phi(\cdot):\mathbb{R}^{d}\rightarrow\mathbb{R}^{m}\) is a projection to map the inputs into \(m\) dimension features.
ICML_2024_oral_26,5,\phi_{PRF}(x)=e^{\omega x^{\intercal}-\frac{1+x^{2}}{2}},where \(\omega\in\mathbb{R}^{m\times d}\),o_{i}=\sum_{j=1}^{n}\frac{\phi(q_{i})\phi(k_{j})^{\intercal}}{\sum_{j^{\prime }=1}^{n}\phi(q_{i})\phi(k_{j^{\prime}})^{\intercal}}v_{j}+\omega v_{j},where \(\omega\in\mathbb{R}^{m\times d}\)
ICML_2024_oral_26,6,"K(x,z)=\int_{R^{d}}e^{i(x-z)^{\intercal}w}p(w)dw=E_{w\sim p(\cdot)}[ e^{iw^{\intercal}x}(e^{iw^{\intercal}z})^{\intercal}]",,"e^{qk^{\intercal}}=E_{\omega\simN(0,I)}\left[e^{\omega q^{\intercal}-\frac{1+x^{2}}{2}}e^{\omega k^{\intercal}-\frac{1+x^{2}}{2}}\right]","where \(q\) and \(k\) are the queries and keys, respectively, and \(\omega\) is a random variable drawn from a normal distribution."
ICML_2024_oral_26,7,"K(x,z)=E_{w\sim p(\cdot)}[e^{iw^{\intercal}x}(e^{iw^{\intercal}z})^{\intercal }]\approx\varphi_{p}(x)^{\intercal}\varphi_{p}^{*}(z)",where \(w_{i}\sim p(\cdot)\) are samples constructed by Monte Carlo methods,\varphi_{p}(x)=\frac{1}{\sqrt{m}} e^{-iw_{1}^{\intercal}x}\\\vdots\\e^{-iw_{m}^{\intercal}x},where \(w_{i}\sim p(\cdot)\) are samples constructed by Monte Carlo methods.
ICML_2024_oral_26,8,"K_{G}(x,y):=e^{-\frac{\|x-y\|^{2}}{2}}=e^{-\frac{\|x\|^{2}+\|y\|^{2}}{2}}e^{x ^{\intercal}y}",,"K(x,z)=\int_{R^{d}}e^{i(x-z)^{\intercal}w}p(w)dw,\end{latex> <description>where\(p(w)\) is the probability measure associated with the Gaussian kernel.</description>",where \(p(w)\) is the probability measure associated with the Gaussian kernel.
ICML_2024_oral_26,9,"\varphi_{PFF}(x):=\frac{e^{-\|x\|^{2}}}{\sqrt{m}}[e^{\Phi^{-1}(t_{1}) x^{\intercal}v_{1}},...,e^{\Phi^{-1}(t_{m})x^{\intercal}v_{m}}]^{\intercal}",,"PFF(x,y)=\frac{1}{\sqrt{m}}[e^{\omega_{1}x^{\intercal}}e^{\omega_{1}y^{\intercal}},...,e^{\omega_{m}x^{\intercal}}e^{\omega_{m}y^{\intercal}}]^{\intercal}",where \(\omega\in\mathbb{R}^{m\times d}\) are the fixed feature weights.
ICML_2024_oral_26,10,"\varphi_{WPFF}(x):=\frac{De^{-\|x\|^{2}}}{\sqrt{m}}[e^{\Phi^{-1}(t_{1 })x^{\intercal}v_{1}},...,e^{\Phi^{-1}(t_{m})x^{\intercal}v_{m}}]^{\intercal}",,"\varphi_{WPFF}(x):=\frac{e^{-\|x\|^{2}}}{\sqrt{m}}[w_{1}e^{\Phi^{-1}(t_{1}) x^{\intercal}v_{1}},...,w_{m}e^{\Phi^{-1}(t_{m})x^{\intercal}v_{m}}]^{\intercal}","where \(w_{i}\) are weights assigned to each feature and \(V=[v_{1},...,v_{m}]\in\mathbb{S}^{d\times m}\) is asymptotically uniformly distributed and \(t_{i}\sim U(0,1)\)."
ICML_2024_oral_26,11,\smallC_{j_{1}j_{2}}=s_{j_{1}}s_{j_{2}}\sum_{i_{1}=0}^{n-1}\sum_{i_{2}=0 }^{d-1}\cos(\frac{\pi(2i_{1}+1)j_{1}}{2d})\cos(\frac{\pi(2i_{2}+1)j_{2}}{2d}),where \(s_{j}=\sqrt{1/d}\) if \(j=0\) and \(s_{j}=\sqrt{2/d}\) otherwise,"C_{jk}=s_{0}\sum_{i=0}^{d-1}x_{i} &if  j=0,\\s_{j}\sum_{i=0}^{d-1}x_{i}\cos\left(\frac{\pi}{d}(i+\frac{1}{2})j\right) &if  j>0",where \(s_{j}=\sqrt{1/d}\) if \(j=0\) and \(s_{j}=\sqrt{2/d}\) otherwise.
ICML_2024_oral_26,12,\small\phi_{WDCF}(x)=De^{TCx^{\intercal}},"where \(\mathcal{C}\in\mathbb{R}^{m\times d}\) is the DCT coefficient, \(D\in\mathbb{R}^{m}\) is a learnable weight, and \(T=\text{diag}(t_{1},\dots,t_{m})\) is a random diagonal matrix following the inverse cumulative distribution",\smallW_{DCT}(x)=DCT\varphi_{p}(x),"where \(\mathcal{C}\in\mathbb{R}^{m\times d}\) is the DCT coefficient, \(D\in\mathbb{R}^{m}\) is a learnable weight, and \(T=\text{diag}(t_{1},\dots,t_{m})\) is a random diagonal matrix following the inverse cumulative distribution."
ICML_2024_oral_26,13,"\smallFKA(Q,K,V)=\phi_{WDCF}(Q)\phi_{WDCF}(K)^{\intercal}V,\\where Q,K,V\inR^{n\times d}",,o_{i}=\sum_{j=1}^{n}\frac{\phi_{WDCF}(q_{i})\phi_{WDCF}(k_{j})^{\intercal}}{\sum_{j^{\prime }=1}^{n}\phi_{WDCF}(q_{i})\phi_{WDCF}(k_{j^{\prime}})^{\intercal}}v_{j},where \(\phi_{\text{WDCF}}(x)\) represents the Weighted Discrete Cosine Features mapping.
ICML_2024_oral_28,1,f\in\operatorname*{argmin}_{f}\sup_{e\inE}R^{e}(\tilde{f}),"where the risk \(R^{e}(f)=\mathbb{E}_{(x,y)\sim P^{e}}[\ell(f(x),y)]\) measures the average loss \(\ell\) incurred by the predictor \(f\) across examples from environment \(e\), all of them drawn iid from \(P^{e}\)","R^{e}(f)=E_{(x,y)\sim P^{e}}[\ell(f(x),y)]","where \(R^{e}(f)\) is the risk for environment \(e\), \(P^{e}\) is the distribution of examples in environment \(e\), and \(\ell\) is the loss function."
ICML_2024_oral_28,2,(p_{y_{i}^{out}}^{out}-1/n_{classes})\cdot n_{ classes}/(n_{classes}-1),,p_{flip}=\frac{1}{1+e^{-\alpha (p_{i}^{out}-\tau)}},"where \(p_{flip}\) is the probability of flipping the label, \(\alpha\) is a scaling factor, \(p_{i}^{\text{out}}\) is the held-out softmax prediction, and \(\tau\) is a threshold for determining when to flip the label."
ICML_2024_oral_28,3,"\llbracket(y\notin\operatorname*{argmax}_{j}f^{a}(x)_{j})\,\vee\,(y\notin\operatorname*{argmax}_{j}f^{b}(x)_{j})\rrbracket","where ""\(\vee\)"" denotes logical-OR, and ""\(\llbracket\)"" is the Iverson bracket","e(x,y)=\llbracket y\neq y_{i}^{out}\vee y_{i}^{out}\neq y\rrbracket","where ""\(\vee\)"" denotes logical-OR, and ""\(\llbracket\)"" is the Iverson bracket."
ICML_2024_oral_28,4,Y\perp E\mid X_{inv},,"e(x,y)=\llbracket(y\notin\operatorname*{argmax}_{j}f^{a}(x)_{j})\,\vee\,(y\notin\operatorname*{argmax}_{j}f^{b}(x)_{j})\rrbracket","where ""\(\vee\)"" denotes logical-OR, and ""\(\llbracket\)"" is the Iverson bracket"
ICML_2024_oral_3,1,"\operatorname*{argmin}_{\Theta_{T},M_{T}}\frac{1}{|D|}\sum_{x,y\inD}L(x,y|\Theta_{T},M_{T})",,"\min_{M_{t},R_{t}}L\quads.t.\quad\Theta\leq\gamma_{T}\cdot\Theta_{0},\quadand\quad\sum_{i=1}^{t}M_{i}\leq\Delta_{t}",Objective function to minimize task loss under constraints on parameter size and tuning parameters.
ICML_2024_oral_3,2,"1-\frac{C(\Theta_{t},M_{t})}{C(\Theta_{0},M_{0})}\geq\gamma_{t} || \delta(\Theta_{t},M_{t},R_{t})\leq\Delta_{t} || \forall t\in\{0,1,\dots,T\}",,"s.t. \sum_{i=1}^{N}M_{t,i}\leq\Delta_{t},\quadand\quad\frac{|\Theta|-|\Theta_{T}|}{|\Theta|}\geq\gamma_{t}","Subject to the constraints that the sum of the pruning masks \(\mathcal{M}_{t,i}\) does not exceed the maximum number of tuning parameters \(\Delta_{t}\), and the sparsity ratio after \(T\) training steps meets or exceeds the target sparsity \(\gamma_{t}\)."
ICML_2024_oral_3,3,H_{apt}(X)=m_{o}\circ(W+s\cdot W_{B}W_{A})X\circ m_{i},"where \(s\) is the constant scaling factor following LoRA's implementation, and \(\circ\) denotes the Hadamard product between the masks and their corresponding matrices",H_{apt}(X)=X\cdot m_{i}\cdot W_{A}\cdot W_{B}\cdot m_{o}\cdot s,"where \(s\) is the constant scaling factor following LoRA's implementation, and \(\circ\) denotes the Hadamard product between the masks and their corresponding matrices."
ICML_2024_oral_3,4,"S(W_{i,j})=|W_{i,j}\cdot\frac{\partialL}{\partial W_{i,j}}|",,"s(\Theta_{t},M_{t})=\frac{1}{|D|}\sum_{x,y\inD}\left|\nabla_{\Theta}L(x,y|\Theta_{t},M_{t})\right|","where \(s(\Theta_{t},\mathcal{M}_{t})\) represents the salience score of the LM parameters at training step \(t\), and \(\nabla_{\Theta} \mathcal{L}(x,y| \Theta_{t},\mathcal{M}_{t})\) is the gradient of the loss with respect to the parameters."
ICML_2024_oral_3,5,"\widetilde{S}_{t}(W_{:,j})=\sum_{(x,y)\inD_{t}}\sum_{i}|\frac{\partialL (x,y|\Theta_{t},M_{t})}{\partial H_{j,i}}| || \sum_{(x,y)\inD_{t}}\sum_{i}|H_{j,i}| || \hat{S}((W_{:,j})=\widetilde{S}(W_{:,j})+(Kurt(O_{j:}))^{\frac{1}{2}}",,"\hat{S}(W_{i,j})=S(W_{i,j})+\kappa(activation)","where \(\hat{S}\) is the outlier-aware salience score, \(\mathcal{D}_{t}\) is the supervised finetuning dataset, and \kappa is the kurtosis of the activation."
ICML_2024_oral_3,6,C(\Theta_{t};M_{t})\approx d_{m}\sum_{i=1}^{n_{L}}(4n_{h} ^{i}\cdot d_{h}+2n_{f}^{i}),where \(d_{h}\) is the dimension per MHA head,N\approx n_{L}\cdot (n_{h}^{i}\cdot d_{h}+n_{f}^{i}\cdot d_{m}),"where \(N\) is the approximated number of LM parameters, \(n_{L}\) is the number of transformer layers, \(n_{h}^{i}\) is the number of MHA heads in layer \(i\), \(n_{f}^{i}\) is the number of FFN neurons in layer \(i\), \(d_{h}\) is the dimension per MHA head, and \(d_{m}\) is the hidden dimension size of the transformer layers."
ICML_2024_oral_3,7,"L&=\muL_{ distill}+(1-\mu)L_{ft}\\L_{layer}&=\sum_{i=1}^{T}MSE(Tr(H_{s}^{\phi(i)}),H_{t}^{i})","where \(\mu\) is a moving term linearly scales from 0 to 1 during distillation to encourage the pre-pruned model vastly fit to the training data, \(\mathcal{L}_{distill}\) is the distillation objective from CoFi, and \(\mathcal{L}_{ft}\) is the supervised fine-tuning objective",L_{total}=(1-\mu)L_{ft}+\muL_{distill},"where \(\mu\) is a moving term linearly scales from 0 to 1 during distillation to encourage the pre-pruned model vastly fit to the training data, \(\mathcal{L}_{distill}\) is the distillation objective from CoFi, and \(\mathcal{L}_{ft}\) is the supervised fine-tuning objective."
ICML_2024_oral_30,1,W^{\prime}=W_{0}+\Delta W=W_{0}+\underline{BA},"where \(W_{0}\) remains static during the fine-tuning process, and the underlined parameters are being trained",W^{\prime}=W_{0}+BA,"where \(W_{0}\) remains static during the fine-tuning process, and the underlined parameters are being trained"
ICML_2024_oral_30,2,W=m\frac{V}{ || V || _{c}}= || W || _{c}\frac{W}{ || W || _{c}},"where \(m\in\mathbb{R}^{1\times k}\) is the magnitude vector, \(V\in\mathbb{R}^{d\times k}\) is the directional matrix, with \(||\cdot||_{c}\) being the vector-wise norm of a matrix across each column",W=m\odot V,"where \(m\in\mathbb{R}^{1\times k}\) is the magnitude vector, \(V\in\mathbb{R}^{d\times k}\) is the directional matrix, with \(||\cdot||_{c}\) being the vector-wise norm of a matrix across each column"
ICML_2024_oral_30,3,"\Delta M_{FT}^{t}=\frac{\sum_{n=1}^{k}|m_{FT}^{n,t}-m_{0}^{n}|}{ k} || \Delta D_{FT}^{t}=\frac{\sum_{n=1}^{k}(1-\bf cos(V_{FT}^{ n,t},W_{0}^{n}))}{k}",,"m_{FT}= || W_{FT}-W_{0} || _{c},\quad V_{FT}=\frac{W_{FT}-W_{0}}{ || W_{FT}-W_{0} || _{c}}","where \(m_{\text{FT}}\) is the magnitude of the fine-tuned weight update, and \(V_{\text{FT}}\) is the directional matrix representing the normalized update direction from the pre-trained weights to the fine-tuned weights."
ICML_2024_oral_30,4,W^{\prime}=\underline{m}\frac{V+\Delta V}{ || V+\Delta V || _{c}}=\underline{m}\frac{W_{0}+\underline{BA}}{ || W_{0}+\underline{BA} || _{c}},"where \(\Delta V\) is the incremental directional update learned by multiplying two low-rank matrices \(B\) and \(A\), and the underlined parameters denote the trainable parameters",W^{\prime}=W_{0}+\underline{m}\frac{\Delta V}{ || \Delta V || _{c}}=W_{0}+\underline{m}\frac{BA}{ || BA || _{c}},"where \(\Delta V\) is the incremental directional update learned by multiplying two low-rank matrices \(B\) and \(A\), and the underlined parameters denote the trainable parameters"
ICML_2024_oral_30,5,\nabla_{V^{\prime}}L=\frac{m}{ || V^{\prime} || _{c}}(I-\frac{V^{\prime}V^{\primeT}}{ || V^{\prime} || _{c}^{2}})\nabla_{W^{\prime}}L || \nabla_{m}L=\frac{\nabla_{W^{\prime}}L\cdot V^{\prime}}{ || V^{\prime} || _{c}},,"\nablaL=\left(\frac{\partialL}{\partial m},\frac{\partialL}{\partial V^{\prime}}\right)","where \(\nabla\mathcal{L}\) is the gradient of the loss function with respect to the parameters, and \(V^{\prime}\) represents the updated directional component."
ICML_2024_oral_30,6,"cos(\nabla_{w^{\prime}}L,v^{\prime})=cos(\nabla_{w^{\prime}}L,v)=\frac{\nabla_{w^{\prime}}L\cdot v}{ || \nabla_{w^{\prime}}L ||  || v || }",,w^{\prime\prime}=w^{\prime}+\Delta w,"where \(w^{\prime\prime}\) is the updated weight vector, \(\Delta w\) is the weight update proportional to the gradient, and \(\nabla_{w^{\prime}}\mathcal{L}\) is the gradient of the loss with respect to the weight \(w^{\prime}\)."
ICML_2024_oral_30,7,"\nabla_{m_{*}}L=\frac{\nabla_{w^{\prime}}L\cdot v^{\prime }}{ || v^{\prime} || }= || \nabla_{w^{\prime}}L || \cdot cos(\nabla_{w^{\prime}}L,v)",,"cos(\nabla_{w^{\prime}}L,v^{\prime})=\frac{m_{*}\nabla_{w^{\prime}}L\cdot v}{ || \nabla_{w^{\prime}}L ||  || v || }","where \(m_{*}\) is the magnitude scalar of the weight vector \(w^{\prime}\), and \(\Delta w\) represents the parameter update."
ICML_2024_oral_30,8," || \nabla_{w^{\prime}}^{S1}L || \cdot|cos(\nabla_{w^{\prime}}^{S1}L,v)|> || \nabla_{w^{\prime}}^{S2}L || \cdot|cos(\nabla_{w^{\prime}}^{S2}L,v)|",,"cos(\nabla_{w^{\prime}}^{S1}L,w^{\prime})=cos(\nabla_{w^{\prime}}^{S2}L,w^{\prime})","the cosine similarity between the gradients and the weight vector can be expressed as follows, highlighting the relationship between the updates and the gradients."
ICML_2024_oral_30,9,\nabla_{V^{\prime}}L=\frac{m}{C}\nabla_{W^{\prime}}L\text { where }C= || V^{\prime} || _{c},,\nabla_{V^{\prime}}L=m\cdot\left(I-\frac{V^{\prime}V^{\primeT}}{ || V+\Delta V || _{c}^{2}}\right)\nabla_{W^{\prime}}L,"where the term \(||V+\Delta V||_{c}\) is treated as a constant during backpropagation, simplifying the gradient computation."
ICML_2024_oral_37,1,"P_{t}=\{W_{t},O_{t}\}",,"P_{t}=\{W_{t},O_{t}\}","Checkpoint of the neural network at the \(t^{th}\) training iteration, containing model weights \(\mathcal{W}_{t}\) and optimizer momentum parameters \(\mathcal{O}_{t}\)."
ICML_2024_oral_37,2,"P=\{P_{1},P_{2},\cdots,P_{t}\,\cdots,P_{T}\}",,"P=\{P_{1},P_{2},\ldots,P_{T}\}",Series of checkpoints saved during training.
ICML_2024_oral_37,3,"O_{t}=\{v_{t},m_{t}\}",,"v_{t}, m_{t}\inR^{d}","where \(v_{t}\) is the second moment and \(m_{t}\) is the first moment, both of which are vectors in \(d\)-dimensional space."
ICML_2024_oral_37,4,"\DeltaP_{t}=\{\DeltaW_{t},O_{t}\}=\{W_ {t}-W_{t-1},O_{t}\}",,\DeltaP_{t}=P_{t}-P_{t-1},"The residual checkpoint at the \(t^{th}\) iteration, defined as the difference between the current checkpoint and the previous checkpoint."
ICML_2024_oral_37,5,"r_{w}=\frac{\alpha}{\sqrt{m_{t}}}\timesmedian(W),M _{w}(i)=\mathds{1}_{w_{t}(i)>r_{w}}",,"\tau_{w}=\frac{1}{N}\sum_{i=1}^{N} |g_{i}^{(2)}|, where\(g_{i}^{(2)}\) is the second-order moment of gradients for the\(i^{th}\) weight and\(N\) is the total number of weights",Pruning threshold for model weights based on the second-order moment of gradients.
ICML_2024_oral_37,6,"r_{o}=\beta\timesmean(v_{t}),M_{o}(i)=\mathds{1}_{v_{t}(i)> r_{o} and M_{w}(i)=1}",,"M_{o}(i)=\mathds{1}_{v_{t}(i)>r_{o}},\quad r_{o}=\beta\timesmedian(v_{t})","\(\mathcal{M}_{o}(i)\) is the pruning mask for momentum states, \(v_{t}\) is the first-order moment, \(r_{o}\) is the pruning threshold for momentum states, and \(\beta\) is a hyperparameter."
ICML_2024_oral_37,7,"\tilde{R}(T)\leq\frac{D^{2}}{2\alpha(1-\beta_{1})}\sum_{i=1}^{d}\sqrt {T\widehat{v}_{T,i}} || +\frac{\alpha(1+\beta_{1})G_{\infty}}{(1-\beta_{ 1})\sqrt{1-\beta_{2}}(1-\gamma)^{2}}\sum_{i=1}^{d}\|g_{1,\tau,i}\|_{2} || +\frac{D_{\infty}^{2}G_{\infty}\sqrt{1-\beta_{2}}}{2\alpha}\sum_ {i=1}^{d}\sum_{t=1}^{t}\frac{\beta_{1,t}}{(1-\beta_{1,t})}\sqrt{t} || +\frac{D^{2}}{2\alpha(1-\beta_{1})}\sum_{i=1}^{d}(\sqrt{T\widehat{v}_{\tau,i}}-\sqrt{T\widehat{v}_{\tau,i}M_{o}})",,"C_{t}=Quantize(\DeltaW_{t}),I_{t}=Quantize(O_{t})","Where \(\mathcal{C}_{t}\) is the cluster center after quantization of the residual weights \(\Delta\mathcal{W}_{t}\), and \(\mathcal{I}_{t}\) is the cluster index after quantization of the optimizer states \(\mathcal{O}_{t}\)."
ICML_2024_oral_37,8,"\Delta\tilde{R}(T)&=\frac{D^{2}}{2\alpha(1-\beta_{1})}\sum_{i=1}^{d}(\sqrt{T\widehat{v}_{\tau,i}}-\sqrt{T\widehat{v}_{\tau,i}M_{o}})\\&=\frac{D^{2}}{2\alpha(1-\beta_{1})}\sum_{i=1}^{d}(\sqrt{T\widehat{v}_{\tau,i}(1-M_{o})})",,"\tilde{R}(T)\leq\frac{D^{2}}{2\alpha(1-\beta_{1})}\sum_{i=1}^{d}\sqrt {T\widehat{v}_{T,i}}+\frac{\alpha(1+\beta_{1})G_{\infty}}{(1-\beta_{1})\sqrt{1-\beta_{2}}(1-\gamma)^{2}}\sum_{i=1}^{d}\|g_{1,\tau,i}\|_{2}+\frac{D_{\infty}^{2}G_{\infty}\sqrt{1-\beta_{2}}}{2\alpha}\sum_{i=1}^{d}\sum_{t=1}^{t}\frac{\beta_{1,t}}{(1-\beta_{1,t})}\sqrt{t}+\frac{D^{2}}{2\alpha(1-\beta_{1})}\sum_{i=1}^{d}(\sqrt{T\widehat{v}_{\tau,i}}-\sqrt{T\widehat{v}_{\tau,i}M_{o}})",Regret bound for the checkpoint compression method with an additional term compared to the original convergence analysis of Adam.
ICML_2024_oral_37,9,\lim_{Tarrow\infty}\frac{\tilde{R}(T)}{T}\leq\lim_{Tarrow\infty}\frac{R(T)+\Delta\tilde{R}(T)}{T}=0,,\tilde{R}(T)\leq R(T)+\Delta\tilde{R}(T),"Where \(\tilde{R}(T)\) is the regret bound of the checkpoint compression method, and \(R(T)\) is the regret bound of the original Adam optimization process."
ICML_2024_oral_37,10,\frac{R(T)}{T}=O(\frac{1}{\sqrt{T}}),,"\tilde{R}_{o}(T)\leq\frac{D^{2}}{2\alpha(1-\beta_{1})}\sum_{i=1}^{d}\sqrt{T\widehat{v}_{T,i}M_{o}}","Where \(\tilde{R}_{o}(T)\) is the regret bound for the momentum pruning method, \(\mathcal{M}_{o}\) is the pruning mask for momentum states, and \(\widehat{v}_{T,i}\) represents the second-order moment at iteration \(T\)."
ICML_2024_oral_4,1,S_{opt_{t}}=\arg\min_{S\subseteq V_{t}}\{\textsc{Cost}(S):f(S)=f(V_{t})\}\enspace,where \(V_{t}\) denotes the set of the currently present elements after the first \(t\) updates,\textsc{Cost}(S)=\sum_{v\in S} w(v),where \(V_{t}\) denotes the set of the currently present elements after the first \(t\) updates.
ICML_2024_oral_4,2,"f(S)\geq(1-\epsilon)f(V),\quadand\quad\textsc{Cost}(S)\leq c\textsc{ Cost}(S_{opt})",where \(S_{opt}\) denotes the optimal solution,S_{opt}\subseteq S\quadand\quad\textsc{Cost}(S)\leq (1+\epsilon)\textsc{OPT}_{\textsc{cost}}\enspace,where \(S_{opt}\) denotes the optimal solution
ICML_2024_oral_4,3,"E[\,X(r)\,]\geq 1-2\epsilon for all r\in[1,m^{*}] and || E[\,X(m^{*}+1)\,]\leq 1-\frac{\epsilon}{2}",,"m^{*}\leq |L^{\prime}|\quadand\quadE\left[\,\frac{|G^{\prime}|}{m^{*}}\,\right]\geq(1-\epsilon)\quadand\quadE\left[\,\frac{|B^{\prime}|}{m^{*}}\,\right]\geq\epsilon\enspace","where \(L^{\prime}\) is the set of elements being considered, \(G^{\prime}\) is the set of elements added to the solution, and \(B^{\prime}\) is the set of elements in the bucket."
ICML_2024_oral_41,1,"\min_{\theta_{1},\theta_{2},...,\theta_{N}}[\frac{1}{N}\sum_{i=1}^{N}L_{i}(\theta_{i})]+R(\theta_{1},\theta_{2},...,\theta_{N})",,"L_{global}=\sum_{i=1}^{N}E_{(x_{ij},y_{ij})\simD_{i}}\left[L_{i}(f_{A_{i}}(x_{ij};\theta_{i}), y_{ij})\right]","The global objective function of Multimodal Federated Learning, where \(\mathcal{L}_{\text{global}}\) is the overall loss, \(\mathcal{L}_{i}\) is the local loss for client \(i\), and the expectation is taken over the local dataset \(\mathcal{D}_{i}\)."
ICML_2024_oral_41,2,A_{i}\capA_{i^{\prime}}=\{B_{enc}^{(m)}|\forall m\inI_{i}\capI_{i^{\prime}}\}\cup\{B_{share}\},,"A_{i}:=\{B_{enc}^{(m)}|\forall m\inI_{i}\}\cup\{B_{share}\}\cup\{B_{dec,i}\}","Definition of the architecture-compositional model architecture for client \(i\), consisting of modality-specific encoders, shared blocks, and a client-specific decoder."
ICML_2024_oral_41,3,A_{i}\capA_{i^{\prime}}=\emptyset,,"R(\theta_{1:N})=\sum_{i=1}^{N}\sum_{j=1}^{N}R_{ij}(\theta_{i},\theta_{j})","where \(\mathcal{R}_{ij}(\theta_{i},\theta_{j})\) represents the sharing mechanism between clients \(i\) and \(j\) based on their respective model parameters \(\theta_{i}\) and \(\theta_{j}\)."
ICML_2024_oral_41,4,"\theta_{i}:=h(A_{i},c_{i};\phi),\\\\forall i\in[N]","where the first generative factor \(\mathcal{A}_{i}\in\mathcal{G}\) is the local neural architecture from a globally-shared _latent topology space_\(\mathcal{G}\) and the second generative factor \(\mathbf{c}_{i}\in\mathcal{T}\) represents the lo

Figure 1: (a) Local mapping functions per client in Multimodal Federated Learning (MFL)","h(A_{i},c_{i};\phi)=\theta_{i},\quad\forall i\in\{1,2,...,N\}","where \(h(\cdot,\cdot;\phi)\) is the bridge function that re-parameterizes the locally-trained weights \(\theta_{i}\) based on the local architecture \(\mathcal{A}_{i}\) and the generative factor \(\mathbf{c}_{i}\)."
ICML_2024_oral_41,5,"A_{i}:=(V_{i},E_{i},Z_{i}^{(0)})",,"\theta_{i}:=h(A_{i},c_{i};\phi),\\\\forall i\in[N]","where \(h(\cdot,\cdot;\phi)\) is the bridge function that re-parameterizes the locally-trained weights of client \(i\) based on its architecture \(\mathcal{A}_{i}\) and generative factor \(\mathbf{c}_{i}\)."
ICML_2024_oral_41,6,"h(A_{i},c_{i};\phi)=\texttt{Comb}(c_{i},\texttt{Role}(A_{i};\phi_{1});\phi_{2})","where the first stage \(\texttt{Role}(\cdot;\phi_{1})\) parameterized by \(\phi_{1}\) learns the implicit roles of layers such that layers across clients share a unified _layer-role embedding_ space, and the second stage \(\texttt{Comb}(\cdot,\cdot;\phi_{2})\) parameterized by \(\phi_{2}\) aims to combine the two heterogeneity patterns and directly generates the weights","A_{i}:=(V_{i},E_{i},Z_{i}^{(0)})","The multimodal neural architecture at each client, represented as a directed acyclic graph structure, where \(\mathcal{V}_{i}\) is the set of nodes, \(\mathcal{E}_{i}\) is the set of edges, and \(\mathbf{Z}_{i}^{(0)}\) is the node feature matrix containing configuration information for each operator."
ICML_2024_oral_41,7,"Z_{i}^{(L)}=\texttt{Role}(A_{i};\phi_{1}) || :=g_{L}\circ g_{L-1}\circ...\circ g_{1}(Z_{i}^{(0)}; V_{i},E_{i})",,"h(A_{i},c_{i};\phi)=\texttt{Comb}(c_{i},\texttt{Role}(A_{i};\phi_{1});\phi_{2})","where \(\phi=\{\phi_{1},\phi_{2}\}\)."
ICML_2024_oral_41,8,"z_{i,v}^{(l)}=\sigma(W_{self}^{(l)}z_{i,v}^{(l-1)}+W_{in}^{(l)}\sum_{(v^{\prime},v)\inE_{i}}z_{i,v^{\prime}}^ {(l-1)} || \qquad+W_{out}^{(l)}\sum_{(v,v^{\prime})\in E_{i}}z_{i,v^{\prime}}^{(l-1)}+b^{(l)})",,"z_{i,v}^{(l)}=\sum_{u\inN(v)}\texttt{AGGREGATE}(z_{i,u}^{(l-1)},z_{i,v}^{(l-1)}),\\forall v\inV_{i}, l=1,2,...,L","where \(\mathcal{N}(v)\) denotes the set of neighboring nodes of node \(v\) in the graph, and \texttt{AGGREGATE} is a function that combines the features of the neighboring nodes and the current node."
ICML_2024_oral_41,9,"\theta_{i,v}:=g_{node}(c_{i}\oplusz_{i,v}^{(L)};\phi_{ 2}),\;\forall v\inV_{i}",where \(\oplus\) denotes an operation (e,"Z_{i}^{(L)}=\texttt{Role}(A_{i};\phi_{1}) :=g_{L}\circ g_{L-1}\circ...\circ g_{1}(Z_{i}^{(0)};V_{i},E_{i})",where \(\mathbf{Z}_{i}^{(L)}\) is the output of the final GNN layer representing layer-role embeddings for all parametric computational operators in \(\mathcal{A}_{i}\).
ICML_2024_oral_41,10,\Deltac_{i}=\nabla_{c_{i}}L_{i}(\theta_{i})=\Delta\theta_{i}\cdot\nabla_{c_{i}}\theta_{i} || \Delta\phi_{2}=\frac{1}{|N_{r}|}\sum_{i\inN_{r}}(\Delta\theta_{i}\cdot\nabla_{\phi_{2}}\theta_{i}) || \Delta\phi_{1}=\frac{1}{|N_{r}|}\sum_{i\inN_{r}}(\Delta\theta_{i}\cdot\nabla_{z_{1}^{(L)}}\theta_{i}\cdot\nabla_{\phi_{i} }Z_{i}^{(L)}),,"\Delta\phi=\sum_{i\inN_{r}}\frac{\partial h(A_{i},c_{i};\phi)}{\partial\phi}\Delta\theta_{i}",where \(\Delta\phi\) represents the update for the TAHN parameters based on the local updates from selected clients.
ICML_2024_oral_44,1,"\operatorname*{arg\,max}_{\theta\in\Theta}E_{G^{\prime}\sim D_{\theta}}[u_{\tau}(G^{\prime})]",where \(D_{\theta}\) is a parameterized distribution and \(\Theta\) represents a feasible set of real-valued parameters,\arg\max_{E}E_{D_{\theta}}[u_{\tau}(G_{E})],where \(D_{\theta}\) is a parameterized distribution and \(\Theta\) represents a feasible set of real-valued parameters.
ICML_2024_oral_44,2,"\prod_{i=1}^{d}\theta_{i}&if\,(N,E\cup(\{e_{j}\}_{j=1}^{i-1}\capE)\cup\{e_{i}\})\,is a DAG,\\0&otherwise",,P(G^{\prime}=G_{E})=\prod_{i=1}^{d}\theta_{i}^{e_{i}} (1-\theta_{i})^{1-e_{i}},The probability of \(G^{\prime}=G_{\mathcal{E}}\) for \(G^{\prime}\sim D_{\theta}\)
ICML_2024_oral_44,3,\nabla_{\theta}E_{G_{E}\sim D_{\theta}} [u_{\tau}(G_{E})]\approx\frac{1}{M}\sum_{i=1}^{M}\hat{u}_ {\tau}(G_{i})\nabla_{\theta}\log(p_{\theta}(G_{i})),"where \(G_{1},G_{2},\ldots,G_{N}\sim D_{\theta}\) are mutually independent and \(\hat{u}_{\tau}(G_{i})\) is an independent unbiased estimate of \(u_{\tau}(G_{i})\) for all \(i\) and some \(M\in\mathbb{N}\)",\nabla_{\theta}E_{G\sim D_{\theta}}[\hat{u}_{\tau}(G)]\approx\frac{1}{M}\sum_{i=1}^{M}\nabla_{\theta}\log P(G_{i})\hat{u}_{\tau}(G_{i}),"where \(G_{1},G_{2},\ldots,G_{N}\sim D_{\theta}\) are mutually independent and \(\hat{u}_{\tau}(G_{i})\) is an independent unbiased estimate of \(u_{\tau}(G_{i})\) for all \(i\) and some \(M\in\mathbb{N}\)"
ICML_2024_oral_5,1,x_{i}^{\ell+1}=x_{i}^{\ell}+\texttt{MLP}^{\ell}(x_{i}^{\ell}+\texttt{Att}^{\ell}(x_{i}^{\ell})),,x_{i}^{(l)}=MLP\left(Attention\left(x_{i}^{(l-1)}\right)\right),The updated residual stream for token \(w_{i}\) at layer \(l\) after applying the attention and MLP operations.
ICML_2024_oral_5,2,\texttt{MLP}^{\ell}(x^{\ell})=\sigma(W_{K}^{\ell}x^{\ell})W_{V}^{\ell},"where \(W_{K}^{\ell},W_{V}^{\ell}\in\mathbb{R}^{d_{mlp}\times d}\)",x^{\ell\textit{mid}}=x_{i}^{\ell}+\sigma(W_{K}^{\ell}x_{i}^{\ell}+b_{K}^{\ell})+\sigma(W_{V}^{\ell}x_{i}^{\ell}+b_{V}^{\ell}),"where \(W_{K}^{\ell},W_{V}^{\ell}\in\mathbb{R}^{d_{mlp}\times d}\)"
ICML_2024_oral_5,3,\texttt{MLP}^{\ell}(x^{\ell})=\sum_{i=1}^{d_{mlp}}\sigma(x^{\ell}\cdotk_{i}^{\ell})v_{i}^{\ell}=\sum_{i=1}^{d_{mlp}}m_{i} ^{\ell}v_{i}^{\ell},,y^{\ell}=\sum_{i} m_{i}^{\ell}v_{i}^{\ell},where \(m_{i}^{\ell}\) is the coefficient value for the \(i\)-th value vector \(\mathbf{v}_{i}^{\ell}\).
ICML_2024_oral_5,4,"p\big{(}w\midx^{\ell}+m_{i}^{\ell}v_{i}^{\ell},E\big{)}\propto\exp\big{(}e_{w}\cdotx^{\ell}\big{)}\cdot\exp\big{(}e_{w}\cdot m_{i}^{\ell}v_{i}^{\ell}\big{)}",where \(\mathbf{e}_{w}\) is the embedding of \(w\),\texttt{MLP}^{\ell}(x^{\ell})=\sum_{i=1}^{d_{mlp}}m_{i}^{\ell}e_{w},where \(\mathbf{e}_{w}\) is the embedding of \(w\)
ICML_2024_oral_5,5,\texttt{GLU}^{\ell}(x^{\ell})=(\sigma(W_{1}x^{\ell})\odot W_{ 2}x^{\ell})W_{V}^{\ell},"where \(W_{1}^{\ell},W_{2}^{\ell},W_{V}^{\ell}\in R^{d_{mlp}\times d}\)",\texttt{GLU}^{\ell}(x^{\ell})=W_{V}^{\ell}\sigma(W_{1}^{\ell}x^{\ell})\odot W_{2}^{\ell}x^{\ell},"where \(W_{1}^{\ell},W_{2}^{\ell},W_{V}^{\ell}\in \mathbb{R}^{d_{mlp}\times d}\)"
ICML_2024_oral_5,6,"P(Toxic|\bar{x}^{L-1})=softmax(W_{Toxic}\bar{x}^{L-1}),W_{Toxic}\inR^{d}",,\bar{x}^{L-1}=W_{Toxic}^{\top}x^{L-1},where \(W_{\text{Toxic}}\in\mathbb{R}^{d\times 1}\)
ICML_2024_oral_5,7,x^{L-1}=x^{L-1}-\alpha*W,where \(a\) is a heuristic scale value and \(W\) is one of our toxicity vectors,x^{L-1}_{new}=x^{L-1}-aW_{Toxic},where \(a\) is a heuristic scale value and \(W\) is one of our toxicity vectors
ICML_2024_oral_5,8,"L_{DPO}=-E[\log\sigma(\beta\log P-\beta\log N)] || P=\frac{\pi_{\theta}(y_{+}\midw)}{\pi_{ref}(y_{+}\midw)},N=\frac{\pi_{\theta}(y_{-}\midw)}{\pi_{ref}(y_{-}\midw)}",,L_{DPO}=-\log\left(\frac{p_{+}}{p_{+}+p_{-}}\right),"where \(p_{+}\) and \(p_{-}\) are the probabilities of the positive and negative samples, respectively"
ICML_2024_oral_5,9,p(y\mid a)\propto p(y)p(a\mid y),,x^{L-1}=x^{L-1}-\alpha W,where \(a\) is a heuristic scale value and \(W\) is one of our toxicity vectors
ICML_2024_oral_5,10,"\gamma(k_{i}^{\ell}):=\{g|g\inR^{d},\sigma(k_{i}^{\ell}\cdotg)>0\}",where \(\sigma\) is a non-linear activation,\sigma(x^{\ell}\cdotMLP.k_{i}^{\ell}),where \(\sigma\) is a non-linear activation
ICML_2024_oral_5,11,"\forall j<\ell,\forall i<d_{mlp}:cos(\delta_{x}^{\ell\_midmid},\delta_{MLP,v_{i}}^{j})",,"\delta_{x}^{\ell\_mid}=\sum_{j=1}^{\ell-1}\delta_{MLP,v}^{j}","where \(\delta_{\text{MLP},\mathbf{v}}^{j}\) represents the shift in value vectors in the preceding layers."
ICML_2024_oral_53,1,generalization gap\leq\sqrt{\frac{CMI_{D}(A_{n})}{n}},,Generalization Error\leqCMI+Other Terms\end{latex> <description>Generalization error bound for any learner in terms of CMI and other terms.</description>,Generalization error bound for any learner in terms of CMI and other terms.
ICML_2024_oral_53,2,F_{D}(A_{n}(S_{n}))-\min_{\theta\in\Theta}F_{D}(\theta)\leq\varepsilon,,CMI_{D}(A_{n})\geq\Omega\left(\frac{1}{\varepsilon^{2}}\right) &for Lipschitz bounded SCO\\\Omega\left(\frac{1}{\varepsilon}\right) &for Strong Convexity,CMI lower bounds for Lipschitz bounded SCO and Strong Convexity.
ICML_2024_oral_53,3,CMI_{D}(A_{n})\triangleq I(A_{n}(S_{n}) ;U|\tilde{Z}),,"CMI_{D}(A_{n})=I(\tilde{Z}; U | S_{n})=H(\tilde{Z} | S_{n})-H(\tilde{Z} | U, S_{n})",Conditional mutual information (CMI) of the learning algorithm \(\mathcal{A}_{n}\) with respect to the data distribution \(\mathcal{D}\).
ICML_2024_oral_53,4,E[F_{D}(A_{n}(S_{n}) )]-\min_{\theta\in\Theta}F_{D}(\theta) || \leqEGE_{D}(A_{n})+E\big{[}\hat{F}_{S_{n}}(A_{n}(S_{n}))-\min_{\theta\in\Theta}\hat{F}_{S_{n}}(\theta)\big{]},,EGE_{D}(A_{n})\leqgeneralization gap+optimization error,"Expected generalization error of the learning algorithm under the data distribution, bounded by the sum of the generalization gap and the optimization error."
ICML_2024_oral_53,5,E[F_{D}(A_{n}(S_{n}) )]-\min_{\theta\in\Theta}F_{D}(\theta) || \leq LR\sqrt{\frac{8CMI_{D}(A_{n})}{ n}}+E\big{[}\hat{F}_{S_{n}}(A_{n}(S_{n}))-\min_{\theta\in\Theta}\hat{F}_{S_{n}}(\theta)\big{]},,E[F_{D}(A_{n}(S_{n}) )]-\min_{\theta\in\Theta}F_{D}(\theta)\leq LR\sqrt{\frac{8CMI_{D}(A_{n})}{n}}+E\big{[}\hat{F}_{S_{n}}(A_{n}(S_{n}))-\min_{\theta\in\Theta}\hat{F}_{S_{n}}(\theta)\big{]},Excess population error upper bound using CMI and generalization error.
ICML_2024_oral_53,6,"ISCMI_{D}(A_{n})\triangleq\sum_{i=1}^{n}I(A_{n}(S);U_{i}|Z_{0,i},Z_{1,i})",,"ISCMI_{D}(A_{n})\triangleq I(A_{n}(S_{n}) ;U|Z_{1},\ldots,Z_{n})",Definition of the individual sample variant of conditional mutual information (ISCMI) for a learning algorithm \(\mathcal{A}_{n}\) with respect to a data distribution \(\mathcal{D}\).
ICML_2024_oral_53,7,E[\sum_{i=1}^{n}\sum_{k=1}^{d}(\frac{144\varepsilon^{2}-d(\mu^{(k)})^{2}}{1-d(\mu^{(k)})^{2}})(\hat{\theta} ^{(k)})(Z_{i}^{(k)}-\mu^{(k)})] || \quad\geq 2\varepsilon-4\delta,,ISCMI_{D}(A_{n})\geq\Omega\left(\frac{1}{\varepsilon^{2}}\right),Individual Sample Conditional Mutual Information (ISCMI) of the learning algorithm with respect to the data distribution.
ICML_2024_oral_55,1,"D_{LCA}(y^{\prime},y):=f(y)-f(N_{LCA}(y,y^{\prime}))","where \(f(y)\geq f(N_{LCA}(y,y^{\prime}))\) and \(N_{LCA}(y^{\prime},y)\) denotes the lowest common ancestor class node for classes \(y\) and \(y^{\prime}\) within the hierarchy, and \(f(\cdot)\) represents a function of a node, such as the tree depth or entropy","d_{LCA}(y, y^{\prime})=f(y)-f(N_{LCA}(y, y^{\prime}))","where \(f(y)\) is the function value for the ground truth class \(y\), and \(N_{LCA}(y, y^{\prime})\) is the lowest common ancestor of classes \(y\) and \(y^{\prime}\)."
ICML_2024_oral_55,2,"D_{LCA}(model,M):=\frac{1}{n}\sum_{i=1}^{n}D_{LCA}(\widehat{y}_{i},y _{i})\iff y_{i}\neq\widehat{y}_{i}","where \(\widehat{y}_{i}\) is the predicted class for sample \(X_{i}\) using the model, \(y_{i}\) is the ground truth class for sample \(X_{i}\), and \(y_{i}\neq\widehat{y}_{i}\)",I_{IC}(X_{i}):=-\log P(y_{i} |\widehat{y}_{i})\,"where \(I_{IC}(X_{i})\) is the information content for sample \(X_{i}\), \(P(y_{i} | \widehat{y}_{i})\) is the probability of the ground truth class \(y_{i}\) given the predicted class \(\widehat{y}_{i}\)."
ICML_2024_oral_58,1,"Sequence model:\hat{z}_{t},\,h_{t}=seq(z_{t-1},\,h_{t-1},\,a_{t-1}) || Multimodal encoder: z_{t}\simenc(x_{t},\,l_{t},\,h_{t}) || Multimodal decoder:\hat{x}_{t},\,\hat{l}_{t},\,\hat{r}_{t},\,\hat{c}_{t}=dec(z_{t},\,h_{t})",,"\hat{z}_{t+1}=f(z_{t}, a_{t}, h_{t})","Predicted next representation based on current representation, action, and recurrent state."
ICML_2024_oral_58,2,"Image loss:\qquad\quadL_{x}=\|\hat{x}_{t}-x_{t}\|_{2}^{2} || Language loss:\qquad\quadL_{t}=catext(\hat {t}_{t},l_{t}) || Reward loss:\qquad\quadL_{r}=catext(\hat {r}_{t},twoot(r_{t})) || Continue loss:\qquad\quadL_{c}=binkent(\hat{c}_{t},c_{t}) || Regularizer:\qquad\quadL_{reg}=\beta_{reg}\max(1,KL\big{[}z_{t}\bigm{\|}\\sg(\hat{z}_{t})\big{]})",,"L_{repr}=catxent(\hat{z}_{t},\,z_{t})+\beta_{reg}\cdotbinxent(sg(\hat{z}_{t}),\,\hat{z}_{t})","The representation learning loss \(\mathcal{L}_{\mathrm{repr}}\) is the sum of the categorical cross entropy loss between the predicted and actual latent representations, and a regularization term that uses binary cross entropy to encourage predictability of the latent representations."
ICML_2024_oral_58,3,"Prediction loss:\quadL_{pred}=\beta_{pred}\\max(1,KL\big{[}\,sg(z_{t})\bigm{\|}\hat{z}_{t}\big{]})",where the gradient around the target distribution for \(z_{t}\) is stopped since it is also a learned representation and \(\beta_{\mathrm{pred}}=0,L_{pred}=\|\hat{z}_{t}-z_{t}\|_{2}^{2}+\beta_{pred}sg\big{[}KL\big{[}z_{t}\bigm{\|}\hat{z}_{t}\big{]}\big{]},"Future prediction loss, where \(\hat{z}_{t}\) is the predicted representation, \(z_{t}\) is the actual representation, and \(\beta_{\mathrm{pred}}\) is a hyperparameter."
ICML_2024_oral_58,4,"Actor net:\quad\pi(a_{t}|h_{t},z_{t})\qquadCritic net:\quadV(h_{t},z_{t})",,L=L_{repr}+L_{pred},"Overall loss function for training the world model, combining representation learning loss and future prediction loss."
ICML_2024_oral_60,1,"\min_{M}\alphaL_{in}(f(M\odot X),y) || -\betaL_{out}(f((1-M)\odot X),y)+R(M)",,L_{mask}=-\frac{1}{N}\sum_{i=1}^{N}\left( y_i\log(p_i)+(1-y_i)\log(1-p_i)\right),"The masking loss function, where \( L_{\text{mask}} \) is the loss, \( N \) is the number of samples, \( y_i \) is the true label, and \( p_i \) is the predicted probability for each sample."
ICML_2024_oral_60,2,"\min_{\theta}\lambda_{in}L_{in}(\log f(M_{\theta}(h)\odot X),y) || -\lambda_{out}L_{out}(\log f((1-M_{\theta}(h))\odot X),y)+R(M_{\theta}(h))",,"\min_{\theta}\alphaL_{in}(f(M_{\theta}(h)\odot X),y)-\betaL_{out}(f((1-M_{\theta}(h))\odot X),y)+R(M_{\theta}(h))",Where \(\theta\) represents the parameters of the neural decoder used to estimate the binary mask \(M\).
ICML_2024_oral_60,3,R(M_{\theta}(h))=\lambda_{g}\|M_{\theta}(h)\odot X-X\|+\lambda_{s}\|M_{\theta} (h)\|_{1},"where \(\lambda_{g}\) and \(\lambda_{s}\) are regularization coefficients, and \(X\) represents the spectrogram of the original signal",\min_{\theta}\lambda_{g}R_{g}(M_{\theta}(h))+\lambda_{s}R_{s}(M_{\theta}(h)),"where \(\lambda_{g}\) and \(\lambda_{s}\) are regularization coefficients, and \(X\) represents the spectrogram of the original signal."
ICML_2024_oral_60,4,x_{interpretation}=ISTFT((M_{\theta}(h)\odot X)e^{jX_{ phase}}),,x=I^{-1}(M_{\theta}(h)\odot X),"where \(x\) is the reconstructed audio waveform, \(\mathcal{I}^{-1}\) denotes the inverse operation to convert the masked linear spectrogram back to the audio domain."
ICML_2024_oral_60,5,FF_{n}:=f(X_{n})_{c}-f(X_{n}\odot(1-M_{\theta}(h)))_{c},,FF=f_{c}(X)-f_{c}((1-M)\odot X),"where \(FF\) represents the faithfulness metric, \(f_{c}(.)\) is the classifier output for class \(c\), and \(M\) is the interpretation mask."
ICML_2024_oral_60,6,AI:=\frac{1}{N}\sum_{n=1}^{N}1_{[f(X_{n}\odot M_{\theta}(h))>f (X_{n})_{c}]}\cdot 100,where \(\mathbf{1}_{[,AI:=\frac{1}{N}\sum_{n}(f(X_{n}\odot M_{\theta}(h)))_{c}-f(X_{n})_{c},where \(\mathbf{1}_{[n]}\) is the indicator function for the masked-in portion of the interpretation.
ICML_2024_oral_60,7,"AD:=\frac{1}{N}\sum_{n=1}^{N}\frac{\max(0,f(X_{n})_{c}-f(X_{n}\odot M_ {\theta}(h))_{c})}{f(X_{n})_{c}}\cdot 100",,AD:=\frac{1}{N}\sum_{n=1}^{N}1_{[f(X_{n}\odot(1-M_{\theta}(h)))<f(X_{n})_{c}]}\cdot 100,"where \(\mathbf{1}_{[.]}\) is an indicator function which returns one if the argument is true, and zero otherwise. For this metric, larger is better."
ICML_2024_oral_60,8,"AG:=\frac{1}{N}\sum_{n=1}^{N}\frac{\max(0,f(X_{n}\odot M_{\theta}(h))_ {c}-f(X_{n})_{c})}{1-f(X_{n})_{c}}\cdot 100",,AG:=\frac{1}{N}\sum_{n=1}^{N}1_{[f(X_{n}\odot M_{\theta}(h))<f(X_{n})_{c}]}\cdot 100,"where \(\mathbf{1}_{[f(X_{n}\odot M_{\theta}(h))<f(X_{n})_{c}]}\) is an indicator function which returns one if the argument is true, and zero otherwise."
ICML_2024_oral_60,9,Fid-In=\frac{1}{N}\sum_{n=1}^{N}1_{[\arg\max_{c}f(X_{n})_{c}=\arg\max_{c}f_{c}(X_{n}\odot M_{\theta}(h))]},,Fid-In:=\frac{1}{N}\sum_{n=1}^{N}1_{[f(X_{n}\odot M_{\theta}(h))_{c}=f(X_{n})_{c}]}\cdot 100,"where \(\mathbf{1}_{[.]}\) is an indicator function that returns one if the classifier's prediction on the masked-in portion matches the original prediction, and zero otherwise."
ICML_2024_oral_66,1,"cost(P,S)=\sum_{p\in P}\min_{s\in S}dist(p,s)^{ z}",,"C(S)=\sum_{p\in P}\min_{s\in S} d(p, s)","C(S): Cost function for the clustering problem; \(P\): Set of input points; \(S\): Set of centers; \(d(p, s)\): Distance between point \(p\) and center \(s\)."
ICML_2024_oral_66,2,P(M(P)\in S)\leq\exp(\varepsilon)\cdotP( M(P^{\prime})\in S)+\delta,,"\textsc{opt}_{k,z}=\min_{S}cost(P,S)","The optimal cost for the \((k,z)\)-clustering problem."
ICML_2024_oral_66,3,\sum_{p\in P}\|p-\mu\|^{z}\leq 2^{z}\sum_{p\in P}\|p-\mu_{z}\|^{z},,\mu_{z}(P)=\frac{1}{|P|}\sum_{p\in P}|p|^{z-2}p.\,"Optimal center for the \((1,z)\)-clustering of a multiset \(P\) in \(\mathbb{R}^{d}\)."
ICML_2024_oral_66,4,"cost(E,\mu(E))=\sum_{p\in E}\|p\|_{2}^{2}-\frac{\|\sum_{p\in E }p\|_{2}^{2}}{|E|}",,"cost(E,\mu_{2}(E))=\sum_{p\in E}\|p-\mu_{2}(E)\|^{2}",Cost of a multiset \(E\) with respect to its \(k\)-means center \(\mu_{2}(E)\).
ICML_2024_oral_72,1,"L_{mask}=\underset{X\inD,t\in[0,T]}{ E}-\sum_{i=1}^{N}m_{t,i}\cdot\log(p_{\theta}(x_{i}|X_{t},X^{p},C))",,"L(\theta)=-E_{t,X,M_{t}}\left[\log p_{\theta}(X_{0}|X_{t},X^{p},C)\right]","Loss function for optimizing the parameters \(\theta\) of the diffusion model, where \(\mathcal{L}(\theta)\) is the negative log-likelihood of the masked tokens, and the expectation is taken over the time step \(t\), the original token sequence \(\mathbf{X}\), and the binary mask \(\mathbf{M}_{t}\)."
ICML_2024_oral_72,2,"p(X_{t-\Delta t}|X_{t},X^{p},C)=\underset {X_{0}\sim p_{\theta}(X_{0}|X_{t},X^{p},C)}{E}q(X_{t-\Delta t}|\hat{X}_{0},X_{t})",,"q(X_{t-\Delta t}|X_{t})=N(X_{t-\Delta t};\mu_{\theta}(X_{t}, t),\Sigma_{\theta}(X_{t}, t))","Reverse transition distribution of the diffusion process, where \(\mu_{\theta}\) and \(\Sigma_{\theta}\) are the mean and covariance predicted by the model."
ICML_2024_oral_73,1,"\theta_{t+1}=\theta_{t}-\eta_{t}(\sum_{i=1}^{n}\nabla\ell(\theta_{t},z_{i})+N(0,\frac{G^{2}}{2\rho}I_{p}))",,"L(\theta)=\sum_{i=1}^{n}\ell(\theta, z_{i})","Loss function for the Noisy Gradient Descent algorithm, where \(\theta\) represents the model parameters, \(n\) is the number of data points, and \(\ell\) is the loss function evaluated at each data point \(z_{i}\)."
ICML_2024_oral_73,2,M=\sqrt{\frac{K}{K-1}}P(I_{K}-\frac{1}{K}1_{K}1_{K}^{T })\inR^{p\times K},"where \(P=[P_{1},\cdots,P_{K}]\in\mathbb{R}^{p\times K}\) is a partial orthogonal matrix such that \(P^{T}P=I_{K}\)",P^{T}P=I_{K},"where \(P=[P_{1},\cdots,P_{K}]\in\mathbb{R}^{p\times K}\) is a partial orthogonal matrix such that \(P^{T}P=I_{K}\)"
ICML_2024_oral_73,3,\Pr[y\widehat{\theta}_{NoisyGD}^{T}x<0]\leq\exp(-\frac{n^{2}\rho}{2(1+\beta^{2}p)^{2}}) || +\exp(-\frac{n}{8(\beta^{4}p^{2}+\frac{1}{3}\beta^{2} p)}),,\Pr[y\widehat{\theta}_{NoisyGD}^{T}x<0]\leq\exp\left(-\frac{n}{2\left(\beta^{4}p^{2}+\frac{1}{3}\beta^{2}p+\frac{1}{3}\sigma^{2}\right)}\right),Misclassification error for NoisyGD.
ICML_2024_oral_73,4,\Pr[y\widehat{\theta}_{NoisyGD}^{T}x<0]\leq\exp(-\frac{n^{2}\rho}{2(1+\beta^{2}p)^{2}}) || +\exp(-\frac{n}{8(\beta^{4}p+\frac{1}{3}\beta^{2} )}),,O\left(\frac{(1+\beta^{2}p)^{2}\sqrt{\log\frac{1}{\gamma}}}{2\rho}+p\beta^{4}\log(1/\gamma)\right),Sample complexity for achieving a misclassification error \(\gamma\) under the assumption of independence among components of \(v\).
ICML_2024_oral_73,5,\nablaL(\theta)=\frac{n}{2}\cdot 0.5\cdot-(-e_{1}+v)+\frac{n}{2}\cdot 0.5\cdot(e_{1}+v)=\frac{n}{2}e_{1},,O\left(\frac{(1+\beta^{2}p)^{2}\sqrt{\log\frac{1}{\gamma}}}{2\rho}+4p\beta^{4}\log\frac{1}{\gamma}\right),Sample complexity to achieve a misclassification error \(\gamma\).
ICML_2024_oral_75,1,"J(\pi)=E_{(s_{t},\tau)\inD}[E_{a_{t}=\pi} [Q^{\pi}(s_{t},a_{t},\tau)]-\etaD_{KL}[\pi,\tilde{\pi}|s_{t},\tau]]",where \(\eta\) is a hyperparameter determining the strength of the regularization towards the reference policy \(\tilde{\pi}\),"J(\pi)=E_{s\sim\rho_{\tilde{\pi}}}\left[E_{a\sim\pi}\left[ r(s, a)-\eta D_{KL}(\pi(\cdot | s) || \tilde{\pi}(\cdot | s))\right]\right]","where \(r(s, a)\) is the reward function, \(D_{KL}\) is the Kullback-Leibler divergence, and \(\rho_{\tilde{\pi}}\) is the state distribution under the reference policy \(\tilde{\pi}\)."
ICML_2024_oral_75,2,"\pi_{imp}(a_{t}|s_{t},\tau)\propto\exp(Q^{\pi_{imp}(s_{t},a_{t},\tau)}/\eta)\tilde{\pi}(a_{t}|s_{t},\tau) || \propto\exp(A^{\pi_{imp}(s_{t},a_{t},\tau)}/\eta)\tilde{\pi}(a_{t}|s_{t},\tau)",,\pi_{imp}=\arg\max_{\pi} J(\pi),where \(\pi_{\text{imp}}\) is the improved policy derived from the maximization of the objective \(J(\pi)\).
ICML_2024_oral_75,3,"L^{Q}(\theta)=E\Big{[} (1-\alpha)D_{KL}[\pi_{imp},\pi_{\theta}|s_ {t},\tau,\tilde{\pi}=\pi_{\theta^{\prime}}] || +\alphaD_{KL}[b,\pi_{\theta}|s_{t},\tau] || +\betaD_{KL}[\Gamma_{\theta^{\prime}}(q|s_{t},a_{t },\tau),p_{\theta}(q|s_{t},a_{t},\tau)]\Big{]} || =-E\Big{[} (1-\alpha)\operatorname*{E}_{d^{\prime}\sim\pi_{\theta^{\prime}}}[w(a^{\prime},s_{t},\tau)\log\pi_{\theta}(a^{\prime}|s_{t},\tau)] || +\alpha\log\pi_{\theta}(a_{t}|s_{t},\tau) || +\beta\operatorname*{E}_{q\sim\Gamma_{\theta^{\prime}}}\log p_{\theta}(q|s_{t},a_{t},\tau)\Big{]}+K_{H}",,L_{total}=L_{policy}+L_{BC}+L_{KL},"where \(\mathcal{L}_{\text{policy}}\) is the policy loss, \(\mathcal{L}_{\text{BC}}\) is the behavior cloning loss, and \(\mathcal{L}_{\text{KL}}\) is the KL divergence loss between the policies."
ICML_2024_oral_75,4,"N(C)=N_{0}*C^{a},\;\;\;D(C)=D_{0}*C^{b}",,C\propto D^{\alpha} N^{\beta},"where \(C\) is the number of compute operations, \(D\) is the number of tokens, \(N\) is the number of parameters, and \(\alpha\) and \(\beta\) are scaling exponents."
ICML_2024_oral_79,1,"L_{dyn}[\theta_{F,G,P}](o_{t,T},a_{t})= || -\bf cos\_sim\big{(}Q(P(T(z_{t},e_{t}))), stopgrad(P(z_{t+1}))\big{)}",,"L_{BYOL}=E_{(z, a)\simD}\left[\left\|T(z, a)-z'\right\|^2\right]","Loss function for the BYOL-like objective, where \(\mathcal{L}_{\text{BYOL}}\) is the loss, \(z\) is the current latent state, \(a\) is the action, and \(z'\) is the predicted next latent state."
ICML_2024_oral_79,2,"L[\theta_{F,G,G,P,\psi}]=L_{dyn}[\theta_{F,G,P}]+\betaL_{act\_decode}[\theta_{F,G,\psi}]",,"L_{pretrain}[\theta_{F,G,\psi}](o_{t,T},a_{t})=L_{dyn}[\theta_{F,G,P}](o_{t,T},a_{t})+L_{act\_decode}[\theta_{F,G,\psi}](o_{t,T},a_{t})",Combined pretraining objective for the state encoder and action encoder/quantization module.
ICML_2024_oral_79,3,"L_{\textbf{CE}}(\pi(stopgrad(z_{t})),\xi_{t})[\theta_{\pi}]",,L_{policy}[\theta_{\pi}]=-\sum_{i=1}^{|D|}\sum_{t=0}^{H_{i}-1}\log\pi(\xi_{t}^{(i)}|z_{t}^{(i)}),Loss function for training the skill-token policy \(\pi\) and the action decoder \(\psi\) based on expert demonstrations.
ICML_2024_oral_79,4,"L_{\textbf{FT\_DECODER}}[\theta_{\pi,\psi}]=E_{\xi_{t}\sim\pi(stopgrad(z_{t}))}\Big{[}L[\theta_{\psi}](\xi_{t})\Big{]} || L[\theta_{\psi}](\xi_{t})=\sum_{i=0}^{\hat{K}}L_{ action}(\psi(stopgrad(z_{t+i}),\xi_{t}[i]),a_{t+i})","where

\[\mathcal{L}[\theta_{\psi}](\xi_{t})=\sum_{i=0}^{\hat{K}}\mathcal{L}_{\text{ action}}(\psi(\text{stopgrad}(z_{t+i}),\xi_{t}[i]),a_{t+i}) \tag{5}\]

In this equation, \(\hat{K}=\min(K,L_{\xi})\), where \(K\) is a hyperparameter, the motivation behind which is explained at the end of Section 3","L[\theta_{\psi}](\xi_{t})=\sum_{i=0}^{\hat{K}}L_{ action}(\psi(stopgrad(z_{t+i}),\xi_{t}[i]),a_{t+i})","where \(\hat{K}=\min(K,L_{\xi})\), with \(K\) as a hyperparameter."
ICML_2024_oral_79,5,"L[\theta_{\pi,\psi}]=L_{\textbf{CE}}(\pi(stopgrad(z _{t})),\xi_{t})+L_{\textbf{FT\_DECODER}}[\theta_{\pi,\psi}]",,"L[\theta_{\pi,\psi}]=L_{\textbf{CE}}(\pi(stopgrad(z_{t})),\xi_{t})+E_{\xi_{t}\sim\pi(stopgrad(z_{t}))}\Big{[}L[\theta_{\psi}](\xi_{t})\Big{]}",Overall objective for optimizing the skill-token policy and decoder during downstream adaptation.
ICML_2024_oral_84,1,\max_{\pi}E_{\hat{e}\sim\hat{p}(e)}J(\pi;\hat{e}),,\hat{p}(e)=\frac{1}{Z} e^{-\beta J},"Oracle distribution of the target environment, normalized by partition function \(Z\) and influenced by objective \(J\) with temperature parameter \(\beta\)."
ICML_2024_oral_84,2,\hat{e}\sim\hat{p}(e),,\hat{e}\sim\hat{p}(e),where \(\hat{e}\) represents a specific sample environment drawn from the oracle distribution \(\hat{p}(e)\).
ICML_2024_oral_84,3,"E^{test}=\{\langle\hat{e}_{sim,i},r\rangle\}_{i=1,\cdots,n}",where the generated behavior \(\pi\) will be evaluated in,"E^{test}=\{(\hat{e}_{sim,i}, r)\mid i=1,\ldots, n\}\",Set of tuples representing the test environment where each tuple consists of a simulated sample environment and a task specification.
ICML_2024_oral_84,4,f:E^{ref}arrowE^{shaped},,E^{shaped}=f(E^{ref}),"A Shaped Environment \(\mathcal{E}^{\text{shaped}}\) is a modification of a reference environment, where the transformation \(f\) incorporates design choices specifically optimized for learning performance."
ICML_2024_oral_84,5,"\max_{\pi}E_{\tau\sim\pi}[\sum_{t=0}^{T}\gamma^{t}r _{t}(s_{t},a_{t}))] || s.t.\\\s_{t+1}\sim p(s_{t},a_{t};E^{shaped })",,\max_{\pi}E_{\hat{e}\simE^{shaped}}J(\pi;\hat{e}),where the behavior \(\pi\) is optimized for performance in the shaped environment \(\mathcal{E}^{\text{shaped}}\).
ICML_2024_oral_84,6,H:f_{k}\times J(\pi^{\star}_{k};E^{test})\to f_{k+1},,"H(\pi^{\star},E^{test})=f^{\star}",where \(f^{\star}\) represents the improved shaping function based on the evaluation of the behavior.
ICML_2024_oral_84,7,"f_{k+1}=H(f_{k},J(\pi^{\star}_{k};E^{test})) || where\\\pi^{\star}_{k}=\underset{\pi}{argmax}\J(\pi;E^{shaped}_{k}) || E^{shaped}_{k}=f_{k}(E^{ref}),\f_{0}=I_{identity}",,H:f_{k}\times J(\pi^{\star}_{k};E^{test})\to f_{k+1},"where \(\mathcal{H}\) represents the reflection process, \(f_k\) is the current shaping function, \(J(\pi^{\star}_k;\mathcal{E}^{\text{test}})\) is the performance of the optimal behavior \(\pi^{\star}_k\) in the test environment \(\mathcal{E}^{\text{test}}\), and \(f_{k+1}\) is the updated shaping function after reflection."
ICML_2024_oral_84,8,"\max_{f\inF} J(\pi^{\star};E^{test}) || s.t.\\\\pi^{\star}\in\arg\max_{\pi}J(\pi;E^{shaped }),\\E^{shaped}=f(E^{ref})",,f^{\star}=\underset{f\inF}{argmax}\J(\pi^{\star};E^{test}),where \(J\) is the performance metric evaluated on the test environment \(\mathcal{E}^{\text{test}}\) using the optimal behavior \(\pi^{\star}\).
ICML_2024_oral_93,1,c(\pi)=\|x_{\pi_{n}}-x_{\pi_{1}}\|_{2}+\sum_{i=1}^{n-1}\| x_{\pi_{i}}-x_{\pi_{i+1}}\|_{2},where \(\|\cdot\|_{2}\) denotes the \(\ell_{2}\) norm,c(\pi)=\sum_{j=1}^{n}\|x_{\pi_{j}}-x_{\pi_{j-1}}\|_{2},where \(\|\cdot\|_{2}\) denotes the \(\ell_{2}\) norm
ICML_2024_oral_93,2,"L(\theta)=E_{\pi\simS}[E_{\Phi\sim f_{\theta}(s)}[E_{\pi\sim g(s,\Phi)}[c(\bm {\pi})]]]","where \(s\) represents an instance from distribution \(\mathcal{S}\), \(\theta\) is the trainable parameters of model \(f\), \(\mathbf{\pi}\) is the solution outputed by post-hoc search algorithm \(g\) given \(\Phi\), and \(c(\mathbf{\pi})\) is calculated based on Equation 1","\min_{\pi}E_{s\simS}\left[ c(\pi)+\lambda\sum_{i=1}^{n}\sum_{j=1}^{n}\Phi_{i,j}\cdotI(\pi)\right]","where \(s\) represents an instance from distribution \(\mathcal{S}\), \(\theta\) is the trainable parameters of model \(f\), \(\mathbf{\pi}\) is the solution outputted by post-hoc search algorithm \(g\) given \(\Phi\), and \(c(\mathbf{\pi})\) is calculated based on Equation 1."
ICML_2024_oral_93,3,"L_{\textit{surrogate}}(\theta)=E_{s\simS}[E_{\Phi\sim f_{\theta}(s)}[\ell(s,\Phi)]]",,"L_{surrogate}(\theta)=E_{s\simS}[\ell(s,\Phi)]","where \(\ell(s,\Phi)\) is the surrogate loss function based on the heatmap \(\Phi\) and the TSP instance \(s\)."
ICML_2024_oral_93,4,"p(\pi_{i}|\pi_{i-1})=\frac{Z_{\pi_{i-1},\pi_{i}}}{\sum_{l\inX_{\pi_{ i-1}}}Z_{\pi_{i-1},l}}",,"p(\pi_{i}|\pi_{i-1})=\frac{Z_{i,\pi_{i}}}{\sum_{j\neq\pi_{i-1}}Z_{i,j}}","where \(Z_{i,j}\) is the edge potential for edge \((i,j)\), \(\Omega_{i}\) is the average weight of edges connected to vertex \(i\), \(\alpha\) is a balancing parameter, and \(M\) is the total number of actions sampled."
ICML_2024_oral_93,5,"\Phi_{i,j}=\frac{e^{-d_{i,j}/\tau}}{\sum_{k\neq i}e^{-d_{i,k}/\tau}}","where \(d_{i,j}=\left\|x_{i}-x_{j}\right\|_{2}\), and \(\tau\) is a parameter controlling the smoothness of the score distribution in \(\Phi\)","\Phi_{i,j}=\frac{\exp\left(-\frac{d_{i,j}}{\tau}\right)}{\sum_{l\neq i}\exp\left(-\frac{d_{i,l}}{\tau}\right)}","where \(d_{i,j}=\left\|x_{i}-x_{j}\right\|_{2}\), and \(\tau\) is a parameter controlling the smoothness of the score distribution in \(\Phi\)"
ICML_2024_oral_93,6,\textit{Score}=\frac{\textit{Gap}_{LKH-3}}{\textit{Gap}_{MCTS}},where \(\textit{Gap}_{\text{LKH-3}}=\frac{L_{\text{LKH-3}}}{L^{*}}-1\) and \(\textit{Gap}_{\text{MCTS}}=\frac{L_{\text{MCTS}}}{L^{*}}-1\),\textit{Score}=\frac{\textit{Gap}_{LKH-3}}{\textit{Gap}_{MCTS}},where \(\textit{Gap}_{\text{LKH-3}}=\frac{L_{\text{LKH-3}}}{L^{*}}-1\) and \(\textit{Gap}_{\text{MCTS}}=\frac{L_{\text{MCTS}}}{L^{*}}-1\)
ICML_2024_oral_95,1,"K_{img}(i,j)=\langle f_{img}(x_{i}),f_{img}(x_{j})\rangle || K_{text}(i,j)=\langle f_{text}(y_{i}),f_{text}(y_{j})\rangle",,"K_{text}(x_i, x_j) &=f_{text}(x_i)^\top f_{text}(x_j)\\K_{vision}(y_i, y_j) &=f_{vision}(y_i)^\top f_{vision}(y_j)","Alignment kernels for text and vision models, where \(K_{\text{text}}\) is the kernel for the language model and \(K_{\text{vision}}\) is the kernel for the vision model."
ICML_2024_oral_95,2,"P_{\texttt{coor}}(x_{a},x_{b})\propto\sum_{(t,t^{\prime})\colon|t-t^{\prime}|\leq T_{\texttt{ window}}}P(X_{t}=x_{a},X_{t^{\prime}}=x_{b})",,"P_{\texttt{coor}}(x_{a}, x_{b})=\frac{1}{|D|}\sum_{(x_{i}, x_{j})\in D}I(x_{a}=x_{i}\land x_{b}=x_{j})",Cooccurrence probability of two observations \(x_{a}\) and \(x_{b}\) occurring within a window \(T_{\texttt{window}}\) over dataset \(D\).
ICML_2024_oral_95,3,"\langle f_{X}(x_{a}),f_{X}(x_{b})\rangle\approx\log\frac{P(\texttt{pos}\mid x_{a},x_{b})}{ P(\texttt{neg}\mid x_{a},x_{b})}+\tilde{c}_{X}(x_{a}) || =\log\frac{P_{\texttt{coor}}(x_{a}\mid x_{b})}{P_{\texttt{coor}} (x_{a})}+c_{X}(x_{a})",,"K_{coor}(x_{a},x_{b})=\langle f_{X}(x_{a}), f_{X}(x_{b})\rangle+b","where \(K_{\text{coor}}\) is the dot-product kernel for the cooccurrence probability, \(f_{X}\) is the learned representation function, and \(b\) is an offset."
ICML_2024_oral_95,4,"=K_{\texttt{PMI}}(x_{a},x_{b})+c_{X}(x_{a})","where \(K_{\texttt{PMI}}\) is the pointwise mutual information (PMI) kernel, and \(c_{X}(x_{a})\) is constant in \(x_{b}\)","K_{PMI}(x_{a},x_{b})\approx\log\frac{P_{\texttt{coor}}(x_{a}\mid x_{b})}{P_{\texttt{coor}}(x_{a})}+c_{X}(x_{a})","where \(K_{\text{PMI}}\) is the pointwise mutual information (PMI) kernel, and \(c_{X}(x_{a})\) is constant in \(x_{b}\)"
ICML_2024_oral_95,5,"\langle f_{X}(x_{a}),f_{X}(x_{b})\rangle=K_{\texttt{PMI}}(x_{a},x_{b})+c_{X}",where we observed that \(c_{X}(x_{a})\) from Eq,"c_{X}(x_{a})=K_{PMI}(x_{a},x_{b})-K_{PMI}(x_{a},x_{b})",where \(c_{X}(x_{a})\) is a constant function in \(x_{b}\).
ICML_2024_oral_95,6,"P_{\texttt{coor}}(x_{a},x_{b})=P_{\texttt{coor}}(x_{a},x_{b}),\,K_{\texttt{PMI }}(x_{a},x_{b})=K_{\texttt{PMI}}(z_{a},z_{b})",where we use \(P_{\texttt{coor}}\) and \(K_{\texttt{PMI}}\) modality-agnostically to emphasize that different modalities share these same quantities,"K_{\texttt{PMI}}(x_{a},x_{b})=P_{\texttt{coor}}(x_{a},x_{b})+c_{X}","where \(K_{\texttt{PMI}}\) is the pointwise mutual information (PMI) kernel, and \(c_{X}\) is a constant."
ICML_2024_oral_95,7,"K_{\texttt{PMI}}(z_{a},z_{b})=\langle f_{X}(x_{a}),f_{X}(x_{b})\rangle-c_{X} || =\langle f_{Y}(y_{a}),f_{Y}(y_{b})\rangle-c_{Y}",,"P_{\texttt{coor}}(y_{a},y_{b})=P_{\texttt{coor}}(y_{a},y_{b}),\,K_{\texttt{PMI}}(y_{a},y_{b})=K_{\texttt{PMI}}(z_{a},z_{b})","where \(P_{\texttt{coor}}\) and \(K_{\texttt{PMI}}\) are defined for observations \(y_{a}\) and \(y_{b}\) in the modality of interest, maintaining the same relationships as for \(X\) and \(Z\)."
ICML_2024_oral_99,1,"\min_{(\pi,\hat{\Delta})}\max_{\nu\inE_{0}}(R_{\nu}(n,\pi),\max_{1\leq j\leq M}e_{\nu}(n,\hat{\Delta}(X_{j})))",where we use the subscript \(\nu\) to denote the contextual bandit instance,"R(n,\pi)=E^{\pi}\left[\sum_{i=1}^{n}\left[r_{i}(a^{*}(x_{i}) |x_{i})-r_{i}\left(a_{i}|x_{i}\right)\right]\right]","Accumulative regret \(\mathcal{R}(n,\pi)\) defined as the expected difference between the reward under the optimal policy and the policy \(\pi\)."
ICML_2024_oral_99,2,"\inf_{(\pi,\hat{\Delta}_{n})}\max_{\nu\inE_{0}}[e_{\nu}(n,\hat{\Delta}_{n})R_{\nu}(n,\pi)]\geq\Omega (M)",,"e_{\nu}\left(n,\hat{\Delta}_{n}\right)\geq\Omega\left(\frac{M}{R_{\nu}(n,\pi)}\right)","where \(e_{\nu}(n,\hat{\Delta}_{n})\) is the estimation error for the CATE estimator \(\hat{\Delta}_{n}\) under instance \(\nu\), \(\mathcal{R}_{\nu}(n,\pi)\) is the regret of policy \(\pi\) under instance \(\nu\), and \(M\) is the number of features."
ICML_2024_oral_99,3,"e_{\nu}(n,\hat{\Delta}_{n})=\max_{1\leq j\leq M}E[(\hat{\Delta}_{n}(X_{j})-\Delta(X_{j}))^{2}]=O(\frac{1}{ f_{\min}(n)})",,"R_{\nu}(n,\pi)=O(M)","where \(\mathcal{R}_{\nu}(n,\pi)\) denotes the regret of the policy \(\pi\) under the instance \(\nu\)."
ICML_2024_oral_99,4,"R_{\nu}(n,\pi)\leqO(M\max\{f_{min}(n)^{1-\alpha},\log n\}) || e_{\nu}(n,\hat{\Delta}_{n})\leqO(\frac{1}{\max\{f_{min}(n)^{1-\alpha},\log n\}})",,"R_{\nu}(n,\pi)\leqO(n^{\alpha}),\quad e_{\nu}(n,\hat{\Delta})\leqO(n^{1-\alpha})","where \(\mathcal{R}_{\nu}(n,\pi)\) is the regret and \(e_{\nu}(n,\hat{\Delta})\) is the estimation error for the given instance \(\nu\)."
ICML_2024_oral_99,5,"e_{\nu}(n,\hat{\Delta}_{n})R_{\nu}(n,\pi)\leqO(M)",,"e_{\nu}(n,\hat{\Delta}_{n})R_{\nu}(n,\pi)\leqO(M)",The product of estimation error and regret is bounded by a constant multiple of the number of features.
ICML_2024_oral_99,6,O((\log n_{j}+\log\log(1/\Delta(X_{j})))(\frac{ 1}{\Delta(X_{j})^{2}}+\frac{1}{\varepsilon\Delta(X_{j})})),,O\left(\frac{M\log n}{\varepsilon^{2}}\right),Expected number of times the suboptimal arm is pulled for any feature \(X_{j}\) in the first half periods of the DP-ConSE algorithm.
ICML_2024_oral_99,7,"e(n,\hat{\Delta})=O(\frac{1}{\max\{f_{min}(n)^{1-\alpha},\frac{\log n}{\varepsilon}\}})",,"e_{\nu}(n,\hat{\Delta}_{n})\leqO\left(\frac{1}{\max\{f_{min}(n)^{1-\alpha},\log n\}}+\frac{M}{\varepsilon}\right)","The expected estimation error of the CATE in the DP-ConSE algorithm, considering the privacy parameter \(\varepsilon\) and the frequency of occurrences of features."
naacl_2024_short_14,1,p_{lm}(x_{t}|c)=f\circg\circenc(c),"where \(f\) is the last linear layer with softmax activation, \(g\) is the two-layer MLP network with a residual connection in the last Transformer layer, and \(\mathrm{enc}\) includes the earlier layers of the model",p(x_t | c)=f(g(enc(c))),"where \(p(x_t | c)\) is the probability of the next token \(x_t\) given the context \(c\), \(f\) is the last linear layer with softmax activation, \(g\) is the two-layer MLP network with a residual connection in the last Transformer layer, and \(\mathrm{enc}\) includes the earlier layers of the model."
naacl_2024_short_14,2,p_{knnlm}(x_{t}|c)=\lambda p_{lm}(x_{t}|c)+(1-\lambda)p_{\text {knn}}(x_{t}|c),where \(\lambda\) is a hyperparameter for interpolation,p_{knn}(x^{\prime}_{t}|c)=\lambda\cdot p_{lm}(x^{\prime}_{t}|c)+(1-\lambda)\cdotretrieval(enc(c)),where \(\lambda\) is a hyperparameter for interpolation
naacl_2024_short_14,3,"z^{*}\in\operatorname*{arg\,min}_{z\inR^{d}}KL[f(z)\|p_{knnlm}]",where \(f\) is the last layer of the model with its trained parameters fixed (definition in Eq 1),\hat{p}_{lm}(x_{t}|c)=f^{*}\circg\circenc(c),where \(f^{*}\) is the projection of the last layer's parameters onto the probability space.
naacl_2024_short_16,1,"g(x)=\operatorname*{arg\,max}_{c\inY}P_{x\sim\phi(x,m)}(f(M(x,s))=c)",,"g(x)=E_{s\sim\phi(x, m)}[f(M(x,s))]","where \(g(\mathbf{x})\) is the smoothed model, \(\mathbb{E}\) denotes the expectation, and \(\mathcal{M}(\mathbf{x}, \mathbf{s})\) is the masked input generated by applying the binary mask \(\mathbf{s}\) to the input \(\mathbf{x}\)."
naacl_2024_short_16,2,"g^{\prime}(x)=\operatorname*{arg\,max}_{e\inY}P_{s \sim\phi(x,m)}(f(D(M(x,s)))=c)",,"f(M(x,s))=f(M(x,s)\cup D(M(x,s)))",where \(D(\cdot)\) is the denoiser function that fills in the masked tokens in the input.
naacl_2024_short_22,1,"R_{LM}(X,\tau,a^{*})=LM(a^{*}|X,\tau)",,R_{LM}(X)=1 &if  X leads to successful task completion\\0 &otherwise,where \(R_{LM}(X)\) is the reward function for the state description \(X\).
naacl_2024_short_22,2,"L(X,S,\tau,a^{*})=\\\sum_{t=0}^{|X|}(V_{\theta}(X_{:t},\tau)-\gamma^{|X|-t}R_{LM}( X,\tau,a^{*}))^{2}",,"L(X,\tau, a^{*})=-R_{LM}(X,\tau, a^{*})",The loss function \(L\) for a single state description \(X\) given the task description \(\tau\) and target action \(a^{*}\).
naacl_2024_short_22,3,"L_{V_{\theta}}=\underset{{c}S,\tau,a^{*}\sim D\\X\sim\pi|S,\tau}{E}[L(X,S,\tau,a^{*})+\phi]","where \(\gamma\) is a discount factor and \(\phi\) is a Kullback-Leibler penalty for normalizing \(V_{\theta}\), common when finetuning LMs with RL (Stiennon et al","L_{total}(D,V_{\theta})=\sum_{(S,\tau,a^{*})\in D}L(X,S,\tau,a^{*})+\phi D_{KL}(V_{\theta} || V_{prior})","where \(\gamma\) is a discount factor and \(\phi\) is a Kullback-Leibler penalty for normalizing \(V_{\theta}\), common when finetuning LMs with RL."
naacl_2024_short_23,1,"\hat{C}_{m},\hat{S}_{m},\hat{F}_{m}=\textsc{attributePredictor}(m,M) || \hat{E}=\textsc{CandidateGenerator}(m,E) || f(m|T,M,E)=\textsc{Constrainer}(\hat{E},\hat{C}_{m},\hat{S}_{m},\hat{F}_{ m})",,"f(m|T,M,E)=\hat{e}",f: function for geocoding; m: mention under consideration; T: text of the document; M: location mentions; E: geographical database entries; \hat{e}: predicted entry
naacl_2024_short_23,2,"Z=\textsc{transformer}(\textsc{Toinput}(m,M)) || \hat{C}_{m}=softmax(Z_{c}W_{c}) || \hat{S}_{m}=softmax(Z_{s}W_{s}) || \hat{F}_{m}=softmax(Z_{f}W_{f})",,\hat{C}_{m}=\textsc{predictCountry}(m),"Predictions for the country, state, and feature class of the mention \(m\) using respective prediction functions."
naacl_2024_short_23,3,L=C_{m}log(\hat{C}_{m})+S_{m}log(\hat{S}_{m})+F_{m}log(\hat{F}_{m}),"where \(C_{m}\), \(S_{m}\), and \(F_{m}\) are one-hot vectors of size \(N\) representing the true country, state, and feature class for mention \(m\)",L=-\frac{1}{|M|}\sum_{m\in M}\left(C_{m}\log(\hat{C}_{m})+S_{m}\log(\hat{S}_{m})+F_{m}\log(\hat{F}_{m})\right),"where \(C_{m}\), \(S_{m}\), and \(F_{m}\) are one-hot vectors of size \(N\) representing the true country, state, and feature class for mention \(m\)."
naacl_2024_short_25,1,"f(q,d_{i})=\frac{\exp(s_{i,1})}{\exp(s_{i,1})+\exp(s_{i,0})}",,"f(q,d_{i})=\frac{e^{s_{i,1}}}{e^{s_{i,1}}+e^{s_{i,0}}}",Ranking score for the document \(d_{i}\) given query \(q\).
naacl_2024_short_25,2,"s_{i,k}=LLM(l_{k}|q,d_{i})",,"p(l_{j}|q,d_{i})=LLM(l_{j}|q,d_{i})\quadfor  j\in\{0, 1, 2\}",Log-likelihood of the LLM generating each relevance label \(l_{j}\) given a query \(q\) and a document \(d_{i}\).
naacl_2024_short_25,3,"f(q,d_{i})=\sum p_{i,k}\cdot y_{k} || where p_{i,k}=\frac{\exp(s_{i,k})}{\sum_{k^{\prime}}\exp(s_{i,k^{\prime}})}",,"ER(q,d_{i})=\sum_{k=0}^{2} y_{k}\cdot\frac{\exp(s_{i,k})}{\sum_{j=0}^{2}\exp(s_{i,j})}","Expected relevance value for a query-document pair, calculated using relevance values assigned to labels and their corresponding log-likelihoods."
naacl_2024_short_25,4,"f(q,d_{i})=s_{i,k^{*}}",,"f(q,d_{i})=s_{i,k^{*}}",Ranking score based on the log-likelihood of the peak relevance label.
naacl_2024_short_26,1,"f(x,t,k;\theta)=y",,"y=f(x, t;\theta, k)",y: stance expressed by text x towards target t; f: detection function; θ: model parameters; k: knowledge involved
naacl_2024_short_26,2,"L_{gen}=-\sum_{i=1}^{|u|}\log p(u_{i}|,u_{< i},h(x,t,k);\theta)","where \(p(u_{i}|,\boldsymbol{u}_{<i},h(x,t,k);\theta)\) is the probability to select a token \(u_{i}\) at step \(i\) given the input \(h(x,t,k)\) and previously generated tokens \(\boldsymbol{u}_{<i}\)","p(u_{i}|u_{<i},h(x,t,k);\theta)","where \(p(u_{i}|\boldsymbol{u}_{<i},h(x,t,k);\theta)\) is the probability to select a token \(u_{i}\) at step \(i\) given the input \(h(x,t,k)\) and previously generated tokens \(\boldsymbol{u}_{<i}\)"
naacl_2024_short_26,3,v_{c}arrowNormalize(\betav_{c}+(1-\beta)v_{c}^{\prime}),"where \(\beta\) is a momentum coefficient, \(\text{Normalize}(\cdot)\) is the normalization function, and \(\boldsymbol{v}_{c}^{\prime}\) is the centroid of embeddings belonging to class \(c\) in the batch",v_{c}\leftarrow (1-\beta)v_{c}+\beta\cdotNormalize(v_{c}^{\prime}),"where \(\beta\) is a momentum coefficient, \(\text{Normalize}(\cdot)\) is the normalization function, and \(\boldsymbol{v}_{c}^{\prime}\) is the centroid of embeddings belonging to class \(c\) in the batch"
naacl_2024_short_26,4,L_{con}=-\sum_{c=1}^{C}y_{c}\log\frac{exp(\frac{s_{c}}{\gamma})}{\sum_{j=1}^{C}exp(\frac{s_{j}}{\gamma})},where \(\gamma\) is a scalar temperature parameter and \(\boldsymbol{y}\) is the one-hot label for the current sample,L_{proto}=-\sum_{j=1}^{C}y_{j}\log\frac{\exp(s_{j}/\gamma)}{\sum_{k=1}^{C}\exp(s_{k}/\gamma)},"where \(C\) is the number of stance classes, \(s_{j}\) is the cosine similarity score between the projected stance embedding \(\boldsymbol{\hat{z}}\) and the prototype \(\boldsymbol{v}_{j}\), \(\gamma\) is a scalar temperature parameter, and \(\boldsymbol{y}\) is the one-hot label for the current sample."
naacl_2024_short_26,5,L=\lambda_{l}\cdotL_{gen}+(1-\lambda_{l})\cdotL _{con},where \(\lambda_{l}\) is involved to balance the optimization,L=L_{gen}+\lambda_{l}L_{con},"where \(\mathcal{L}_{gen}\) is the generation loss, \(\mathcal{L}_{con}\) is the contrastive loss, and \(\lambda_{l}\) is a balancing parameter."
naacl_2024_short_27,1,"[sim_{A}^{j},sim_{B}^{j}]=X_{i}\cdot[X_{A}^{j},X_{B}^{j}]",,"S_{i,j}=X_{i}\cdot A_{j}","Where \(S_{i,j}\) is the similarity score between example \(X_{i}\) and attribute \(A_{j}\)."
naacl_2024_short_27,2,"d_{sim}^{j}=\sigma(sim_{A}^{j},sim_{B}^{j})",,"\sigma(sim_{A}^{j}, sim_{B}^{j})=\frac{e^{sim_{A}^{j}}}{e^{sim_{A}^{j}}+e^{sim_{B}^{j}}}",Softmax function applied to the similarity scores \(sim_{A}^{j}\) and \(sim_{B}^{j}\).
naacl_2024_short_27,3,"L_{kl}=\sum_{j=1}^{K}D_{KL}(d_{sim}^{j},d_{uni})",,KL(d_{sim}^{j}\| d_{uni})=\sum_{k} d_{sim}^{j}(k)\log\frac{d_{sim}^{j}(k)}{d_{uni}(k)},KL divergence between the similarity distribution \(d_{sim}^{j}\) and the uniform distribution \(d_{uni}\).
naacl_2024_short_27,4,L_{total}=L_{ce}+\lambda L_{kl},where \(L_{ce}\) is the usual cross-entropy loss,L=L_{kl}+L_{ce},where \(L_{ce}\) is the usual cross-entropy loss
naacl_2024_short_28,1,"S(y)=-\frac{1}{|H(x)|}\sum_{y^{\prime}\in H(x)}L(y^{\prime},y)",,"S(y^{\prime})=E_{y^{\prime}\sim P(\cdot|x)}[L(y^{\prime},y^{*})]",Score of each translation based on the expected loss with respect to the true translation \(\mathbf{y}^{*}\).
naacl_2024_short_28,2,"y^{*}=\operatorname*{arg\,max}_{y\in H(x)}S(y)",,y_{MBR}=\arg\min_{y\in H(x)} S(y),"The MBR hypothesis, which minimizes the score \(S(\mathbf{y})\) over the set of sampled translations \(H(\mathbf{x})\)."
naacl_2024_short_28,3,"\max_{\pi_{\theta}}E_{x\sim D,y\sim\pi_{\theta}( y|x)}[r_{\phi}(x,y)] || -\betaD_{KL}[\pi_{\theta}(y|x)\parallel\pi_{ref}(y|x)]",,"R(y)=E_{y^{\prime}\sim P(\cdot|x)}[L(y^{\prime},y)]",where \(R(\mathbf{y})\) is the expected reward for a translation \(\mathbf{y}\) based on the loss function \(L\) and the distribution of sampled translations \(P(\cdot|\mathbf{x})\).
naacl_2024_short_28,4,"L_{DPO}=-E_{(x,y_{w},y_{l})\sim D}[log\sigma(M(y_{w},y_{l},x,\theta))] || \beta(log\frac{\pi_{\theta}(y_{w}|x)}{\pi_{ref}(y_{w}|x)}-log\frac{\pi_{\theta}( y_{l}|x)}{\pi_{ref}(y_{l}|x)})","where the reward margin \(M(\mathbf{y}_{w},\mathbf{y}_{l},\mathbf{x},\theta)\) is

\[\beta\left(\text{log}\frac{\pi_{\theta}(\mathbf{y}_{w}|\mathbf{x})}{\pi_{ \text{ref}}(\mathbf{y}_{w}|\mathbf{x})}-\text{log}\frac{\pi_{\theta}(\mathbf{ y}_{l}|\mathbf{x})}{\pi_{\text{ref}}(\mathbf{y}_{l}|\mathbf{x})}\right) \tag{5}\]

### Related Work in Translation

Previous work has explored the effectiveness of enhancing the translation performance of LLMs via Reinforcement Learning (RL) algorithms or supervised fine-tuning","M(y_{w},y_{l},x,\theta)=\beta\left(log\frac{\pi_{\theta}(y_{w}|x)}{\pi_{ref}(y_{w}|x)}-log\frac{\pi_{\theta}(y_{l}|x)}{\pi_{ref}(y_{l}|x)}\right)","the reward margin \(M(\mathbf{y}_{w},\mathbf{y}_{l},\mathbf{x},\theta)\)"
naacl_2024_short_32,1,"y^{*}=\operatorname*{arg\,max}_{y\inY}E_{r\sim P_{ human}(\cdot|x)}[u(y,r)]",,"u(y,r)=\arg\max_{y\inY}E[u(y,r)]","Utility function \(u(y,r)\) that maximizes the expected utility of candidate translations \(y\) given reference translation \(r\)."
naacl_2024_short_32,2,"y^{*}=\operatorname*{arg\,max}_{y\inY}\frac{1}{|R^{\prime}|}\sum_{r^{\prime}\inR^{\prime}}u(y,r^{\prime})",,"y^{*}=\operatorname*{arg\,max}_{y\inY}\frac{1}{N}\sum_{i=1}^{N}u(y,r^{\prime}_{i}),_{i}\) represents the\(i\)-th pseudo-reference drawn from the model\(P_{model}(\cdot|x)\).</description>",where \(N\) is the number of pseudo-references and \(r^{\prime}_{i}\) represents the \(i\)-th pseudo-reference drawn from the model \(P_{\mathrm{model}}(\cdot|x)\).
naacl_2024_short_33,1,"\min_{P_{n},v_{n}}-\sum_{x_{n},y_{n}}\log p(y_{n}|x_{n},P^{\prime}_{n},\theta)-\sum_{x_{n}}\cos(x_{n},v_{n})",,"P_{n}, v_{n}=\arg\min_{P_{n}, v_{n}}\left(L_{CE}(P_{n})-\lambda\cdot\cos(v_{n}, x_{n})\right)",PEFT module \(P_{n}\) and task feature vector \(v_{n}\) that minimize the cross-entropy loss \(\mathcal{L}_{\text{CE}}(P_{n})\) while maximizing the cosine similarity between \(v_{n}\) and the input embeddings \(x_{n}\).
naacl_2024_short_33,2,"FWT=\frac{1}{N-1}\sum_{j=2}^{N}(a_{i,i}-\tilde{a}_{i})","where \(N\) is the number of tasks in the continual learning sequence, \(a_{i,i}\) denotes the performance evaluated on the \(i\)-th task after incremental learning on the first \(i\) tasks, \(\tilde{a}_{i}\) is the task performance of a randomly initialized reference model trained with dataset \(D_{i}\)","FWT=\frac{1}{N}\sum_{i=1}^{N}\left(a_{i,i}-\tilde{a}_{i}\right)","where \(N\) is the number of tasks in the continual learning sequence, \(a_{i,i}\) denotes the performance evaluated on the \(i\)-th task after incremental learning on the first \(i\) tasks, \(\tilde{a}_{i}\) is the task performance of a randomly initialized reference model trained with dataset \(D_{i}\)"
naacl_2024_short_34,1,x_{k}&=A_{k}x_{k-1}+Bu_{k}\\y_{k}&=h(x_{k}),,h_t=W\cdoth_{t-1}+U\cdotx_t,"Where \(\mathbf{h}_t\) is the hidden state at time \(t\), \(\mathbf{W}\) is the transition matrix, \(\mathbf{h}_{t-1}\) is the hidden state from the previous time step, \(\mathbf{U}\) is the input matrix, and \(\mathbf{x}_t\) is the input at time \(t\)."
naacl_2024_short_34,2,x_{k}=Ax_{k-1}+Bu_{k},,A_{k}=A <description>A_{k} is the input-independent transition matrix in the LRNN recurrence relation.</description>,A_{k} is the input-independent transition matrix in the LRNN recurrence relation.
naacl_2024_short_34,3,"x_{0-1}&=A^{3}z+A^{2}u_{0}+Au_{-}+u_{ 1},\\for ""0-1""\\x_{1-0}&=A^{3}z+A^{2}u_{1}+Au_{-}+u_{0},\\for ""1-0""",,"z_{0}=A z+B u_{0},\quad z_{1}=A z_{0}+B u_{-},\quad z_{2}=A z_{1}+B u_{1}","Denote \(z_{0}\), \(z_{1}\), and \(z_{2}\) as the state vectors representing the sequences ""0-1"" and ""1-0"" in the LRNN framework."
naacl_2024_short_34,4,A^{2}u_{0}+Au_{-}+u_{1}\neq A^{2}u_{1}+Au_{-}+u_{0},,x_{0-1}\neq x_{1-0},"Indicates that the outputs for the sequences ""0-1"" and ""1-0"" are not equal, highlighting the inconsistency of an input-independent LRNN in representing subtraction."
naacl_2024_short_34,5,x_{0-0-1}&=A^{3}x_{0-}+A^{2}u_{0}+Au_ {-}+u_{1}\\x_{0-1-0}&=A^{3}x_{0-}+A^{2}u_{1}+Au_{-}+u_{0},,"x_{0-0-1}&=A^{4}z+A^{3}u_{0}+A^{2}u_{-}+Au_{0}+u_{1},\\for ""0-0-1""\\x_{0-1-0}&=A^{4}z+A^{3}u_{0}+A^{2}u_{-}+Au_{1}+u_{0},\\for ""0-1-0""","The vector representations for the sequences ""0-0-1"" and ""0-1-0""."
naacl_2024_short_34,6,A^{2}u_{0}+Au_{-}+u_{1}=A^{2}u_{1}+Au_{-}+u_{0},,A^{2}u_{0}+Au_{-}+u_{1}=A^{2}u_{1}+Au_{-}+u_{0},"Equation representing the equality condition for the sequences ""0-0-1"" and ""0-1-0"" in the context of input-independent LRNNs."
naacl_2024_short_34,7,x_{k}=diag(v_{k})x_{k-1}+Bu_{k},where \(v_{k}=f(u_{k})\) is a vector that depends on \(u_{k}\),v_{k}=f(u_{k}),where \(v_{k}\) is a vector that depends on \(u_{k}\).
naacl_2024_short_34,8,x_{k}&=Ax_{k-1}+(Bu_{k})\odot x_{k-1}+Bu_{k}\\&=(A+diag(Bu_{k}))x_{k-1}+Bu_{k},where \(\odot\) denotes the Hadamard product and \(\text{diag}(w)\) constructs a diagonal matrix from \(w\),x_{k}=diag(w_{k})\odot x_{k-1}+Bu_{k},where \(w_{k}\) is a vector that depends on the input \(u_{k}\).
naacl_2024_short_34,9,x_{k}=A_{k}x_{k-1}+Bu_{k},where \(A_{k}=g(u_{k})\) is a block diagonal matrix in practice for the sake of efficiency,x_{k}=g(u_{k})x_{k-1}+Bu_{k},where \(A_{k}=g(u_{k})\) is a block diagonal matrix in practice for the sake of efficiency.
naacl_2024_short_34,10,"& A_{k}=diag(A_{k}^{(1)},...,A_{k}^{(h)} )\inR^{bh\timesbh}\\& A_{k}^{(i)}=[v_{k}^{(i,1)}\quad\ldots\quad v_{k}^{(i,b)} ]\inR^{b\times b}\\&\|v_{k}^{(i,j)}\|_{p}\leq 1,\\i\in[1,...,h],\\j\in[1,...,b]","where \(\|\cdot\|_{p}\) denotes the vector p-norm and \(v_{k}^{(i,j)}\) is a column vector that depends on \(u_{k}\)",\|A_{k}\|_{p}\leq C,"where \(\|\cdot\|_{p}\) denotes the vector p-norm and \(v_{k}^{(i,j)}\) is a column vector that depends on \(u_{k}\)."
naacl_2024_short_34,11,&[\|v^{(1)}\|_{1}\quad\ldots\quad\|v^{(b)}\|_{1} ]=1^{\top}|A_{k+1}^{(i)}A_{k}^{(i)}|\\&\leq1^{\top}|A_{k+1}^{(i)} || A_{k}^{(i)} |\leq1^{\top}|A_{k}^{(i)}|\leq1^{\top},,"& v^{(j)}=A_{k+1}^{(i)}v_{k}^{(i,j)}\inR^{b},\\j\in[1,...,b]\\&\|v^{(j)}\|_{1}\leq\|A_{k+1}^{(i)}\|_{1}\|v_{k}^{(i,j)}\|_{1}\leq 1,\\j\in[1,...,b]","For any vector \(v\), we can derive another vector \(v^{\prime}\) to satisfy the p-norm constraint through \(v^{\prime}=v/\max(1,\|v\|_{p})\)."
naacl_2024_short_38,1,"f(C,P,N_{1},N_{2})=1&if s(C,P)>s(C,N_{1} )\\&and s(C,P)>s(C,N_{2} )\\0&otherwise",where \(s(,"f\left(C,P,N_{1},N_{2}\right)=\frac{1}{1+e^{-s\left(C,P\right)+s\left(C,N_{1}\right)+s\left(C,N_{2}\right)}}",where \(s\) denotes the similarity score between the prompt and the respective images.
naacl_2024_short_38,2,"Mean Similarity=\frac{1}{n}\sum_{i=1}^{5}s(c,p_{i})","where \(p_{i}\in P\) denotes the generated prompts, \(s(","\mu(c)=\frac{1}{|P|}\sum_{i=1}^{|P|} s(c, p_{i})","where \(\mu(c)\) is the mean similarity of the image \(c\) with the generated text prompts, \(P\) is the set of generated prompts, and \(s(c, p_{i})\) is the cosine similarity between the image \(c\) and the prompt \(p_{i}\)."
naacl_2024_short_39,1,"\underset{\theta_{P}}{\max}\underset{i}{\sum}\log p_{\theta,\theta_{P}}(y_{i}|[P;x_{i}])",,\max_{\Theta}\sum_{i}^{N}\log p_{\Theta}(y_{i}|x_{i}),Maximize the log-likelihood of the output sequence given the input text.
naacl_2024_short_39,2,\frac{(Zero-shot\correct)\cap(PoT\incorrect)}{Zero-shot\correct},"where _Zero-shot correct_ is the case of correct responses in a zero-shot setting, and _PoT incorrect_ is the case of incorrect answers after prompt transfer in the target task",Catastrophic Forgetting=\frac{Zero-shot correct-PoT incorrect}{Zero-shot correct},"where _Zero-shot correct_ is the case of correct responses in a zero-shot setting, and _PoT incorrect_ is the case of incorrect answers after prompt transfer in the target task."
naacl_2024_short_43,1,"I_{c,i}\sim f(c_{\ell})",,I_i=f(c_{\ell}),\(I_i\): the \(i\)-th image produced by the T2I model; \(f\): the T2I model function; \(c_{\ell}\): the tangible concept written in language \(\ell\).
naacl_2024_short_43,2,"X_{c}=\frac{1}{n^{2}}\sum_{i=0}^{n}\sum_{j=0}^{n}SIM_{F}(I_{c_{\ell}, i},I_{c_{\ell_{s}},j})","where we sample \(n\) images per-concept per-language (we use 9), and \(\mathrm{SIM}_{F}(\cdot,\cdot)\) measures the cosine similarity in feature space by image feature extractor \(F\)","X_{c}(f,c_{\ell},c_{\ell_{s}})=\frac{1}{n}\sum_{i=1}^{n}SIM_{F}(I_{c,i}, I_{c_{s},i})","where \(n\) is the number of images sampled per-concept per-language, and \(\mathrm{SIM}_{F}(\cdot,\cdot)\) measures the cosine similarity in feature space by image feature extractor \(F\)."
naacl_2024_short_43,3,"\Delta X_{c}(c,\ell)=X_{c}(f,c^{\prime}_{\ell},c_{\ell_{s}})-X_{c}(f,c_{\ell}, c_{\ell_{s}})",,"\Delta X_{c}=X_{c}(f,c^{\prime}_{\ell},c_{\ell_{s}})-X_{c}(f,c_{\ell},c_{\ell_{s}})",where \(c^{\prime}_{\ell}\) is the corrected translation of the concept \(c\).
naacl_2024_short_43,4,"\DeltaSEM=SIM_{Ft}(c_{\ell_{s}},c^{\prime}_{\ell})- SIM_{Ft}(c_{\ell_{s}},c_{\ell})",,"\DeltaSEM(c_{\ell_{s}},c_{\ell},c^{\prime}_{\ell})=SIM_{F_{t}}(c_{\ell_{s}},c^{\prime}_{\ell})-SIM_{F_{t}}(c_{\ell_{s}},c_{\ell})","where \(c_{\ell_{s}}\) is the source language concept, \(c_{\ell}\) is the original translation, and \(c^{\prime}_{\ell}\) is the corrected translation."
naacl_2024_short_45,1,\ell=softmax(affine(ReLU(u_{s}))) || \ell_{i}=softmax(affine(ReLU(u_{i}))) || \nabla_{i}=\ell-\ell_{i},,R_{i}=P(\hat{y} |u_{s})-P(\hat{y} |u_{i}),"Where \(\mathbf{R}_{i}\) is the relevance score for feature \(x_{i}\), \(P(\mathbf{\hat{y}} | \mathbf{u}_{s})\) is the probability of the predicted label with the full input, and \(P(\mathbf{\hat{y}} | \mathbf{u}_{i})\) is the probability of the predicted label with the feature \(x_{i}\) removed."
naacl_2024_short_45,2,L_{dialect-classifier}=-\sum_{i}\log\ell[y_{i}^{*}],where \(y_{i}^{*}\) is the correct label for instance \(i\),L=-\sum_{i} y_{i}^{*}\log(\ell),where \(y_{i}^{*}\) is the correct label for instance \(i\)
naacl_2024_short_45,3,s_{j}=softmax(affine(ReLU(u_{s})-ReLU(u_{j}))) || loss=L_{{{dialect-classifier}}}+\alpha_{1}L_{{{LII}}},,\ell_{j}=softmax(affine(ReLU(u_{s}-u_{j}))),where \(\mathbf{u}_{j}\) is the representation of feature \(x_{i}\)
naacl_2024_short_45,4,r_{j}=[\ell]_{y_{i}^{*}}-[s_{j}]_{y_{i}^{*}},"where higher \(r_{j}\) signifies more relevant features to the prediction, serving as better explanations",r_{i}=\ell[y_{i}^{*}]-s_{i},"where higher \(r_{i}\) signifies more relevant features to the prediction, serving as better explanations"
naacl_2024_short_45,5,E^{\prime}=\{e\in E\midisCorrect(e)\landisUnique(e)\},,E^{\prime}=\{e_{i}\in E\midprediction(e_{i})=correct\landunique(e_{i})\},where \(E^{\prime}\) is the set of filtered explanations
naacl_2024_short_45,6,"TF-IDF(t,d,D)=TF(t,d)\timesIDF(t,D)",,"TFIDF(t, d)=f(t, d)\cdot\log\left(\frac{N}{df(t)}\right)","where \(f(t, d)\) is the term frequency of term \(t\) in document \(d\), and \(N\) is the total number of documents in the corpus."
naacl_2024_short_45,7,"F=\{TF-IDF(t,d,E^{\prime})\mid t\in d,d\in E^{\prime}\}",,"F=\{f_{1}, f_{2},\ldots, f_{m}\} where  f_{j}=TF-IDF(e_{j}, d, D) for  e_{j}\in E^{\prime}",where \(F\) is the set of features extracted from the filtered explanations \(E^{\prime}\)
naacl_2024_short_46,1,"[x^{i},Q^{k}]=BERT([x^{i},Q^{k}])","where \(\mathbf{x}^{i}\) and \(\mathbf{Q}^{k}\) are the representations of \(x^{i}\) and \(\mathcal{Q}^{k}\), respectively","x^{i},Q^{k}=BERT(x^{i},Q^{k})","where \(\mathbf{x}^{i}\) and \(\mathbf{Q}^{k}\) are the representations of \(x^{i}\) and \(\mathcal{Q}^{k}\), respectively"
naacl_2024_short_46,2,"Z^{i}_{t}=Linear(FFN([x^{i}_{m},x^{i}_{n}]))","where \(\overline{\mathbf{x}}^{i}_{t}=\text{FFN}([\mathbf{x}^{i}_{m},\mathbf{x}^{i}_{ n}])\) is the span representation, \(m\) and \(n\) denote the start and end index of the span, respectively","\overline{x}^{i}_{t}=FFN([x^{i}_{m},x^{i}_{n}])","where \(\overline{\mathbf{x}}^{i}_{t}\) is the span representation, \(m\) and \(n\) denote the start and end index of the span, respectively"
naacl_2024_short_46,3,"L_{new}=-\sum_{(\overline{x}^{i}_{t},y^{i}_{t})\in D^{k}_{train}}CE(Z^{i}_{t}+Z_{q},y^{i}_{t})",,"L=-\sum_{j} Z^{i}_{t,j}\log(Z_{q,j})","where \(Z_{q}\) is the probability distribution over all prompts, and \(Z^{i}_{t}\) is the logits for the predicted event type"
naacl_2024_short_46,4,"L_{mem}=-\sum_{(\overline{x}^{i}_{t},y^{i}_{t})\in M}CE(Z^{i}_{t}+Z_{q},y^{i}_{t})",,"L_{replay}=-\sum_{(\overline{x}^{i}_{t},y^{i}_{t})\inD^{k-1}_{replay}}CE(Z^{i}_{t},y^{i}_{t})","where \(\mathcal{D}^{k-1}_{\text{replay}}\) is the set of selected training examples from the previous task, and \(\mathcal{L}_{\text{replay}}\) is the loss for the memory replay objective."
naacl_2024_short_46,5,"L_{sim}=\sum_{(\overline{x}^{i}_{t},y^{i}_{t})\inD^ {k}_{train}}\sum_{e_{t}\inE^{k-1}}\max(0,g(\overline {x}^{i}_{t},e_{t})-m_{1})","where \(\mathcal{E}^{k-1}\) is the prototype set of previous \(k-1\) tasks, \(g(,)\) is the similarity function (cosine similarity) and \(m_{1}\) is the margin for \(\mathcal{L}_{\text{sim}}\)","L_{sim}=\sum_{E^{k-1}}g(\overline{x}^{i}_{t},E^{k-1})-m_{1}","where \(\mathcal{E}^{k-1}\) is the prototype set of previous \(k-1\) tasks, \(g(,)\) is the similarity function (cosine similarity) and \(m_{1}\) is the margin for \(\mathcal{L}_{\text{sim}}\)"
naacl_2024_short_46,6,"L_{cal}=-\sum_{(\overline{x}_{t}^{i},\overline{x}_{t}^{i})\inM}\log\frac{\exp g(\overline{x}_{t}^{i},e_{t}) }{\sum_{j=1}^{|e^{k-1}|}\exp g(\overline{x}_{t}^{i}, e_{j})}",where \(\mathbf{e}_{t}\) is the prototype of \(y_{t}^{i}\),"L_{cal}=\sum_{(\overline{x}^{i}_{t},y^{i}_{t})\inM}\|\overline{x}^{i}_{t}-e_{t}\|^{2}",where \(\mathbf{e}_{t}\) is the prototype of \(y_{t}^{i}\)
naacl_2024_short_46,7,L_{total}=L_{new}+\lambda_{1}L_{sim}+\lambda_{2}(L_{mem}+L_{cal}),where \(\lambda_{1}\) and \(\lambda_{2}\) are loss weights,L_{total}=\lambda_{1}L_{new}+\lambda_{2}L_{mem}+L_{sim}+L_{cal},where \(\lambda_{1}\) and \(\lambda_{2}\) are loss weights
naacl_2024_short_46,8,"e_{j}=\frac{1}{|M_{e_{j}}|}\sum_{(\overline{x}_{t}^{i},\overline{x}_{t}^{i})\inM_{e_{j}}}\overline{x}_{t}^{i}","where \(\mathcal{M}_{e_{j}}=\{(\overline{x}_{t}^{i},y_{t}^{i})|(\overline{x}_{t}^{i}, y_{t}^{i})\in\mathcal{M},y_{t}^{i}=e_{j}\}\)","e_{j}=\frac{1}{|M_{e_{j}}|}\sum_{(\overline{x}_{t}^{i},y_{t}^{i})\inM_{e_{j}}}\overline{x}_{t}^{i}","where \(\mathcal{M}_{e_{j}}=\{(\overline{x}_{t}^{i},y_{t}^{i})|(\overline{x}_{t}^{i}, y_{t}^{i})\in\mathcal{M},y_{t}^{i}=e_{j}\}\)"
naacl_2024_short_5,1,"L^{unsup}_{simce}=-\log\frac{e^{sim(h_{i},h_{i}^{\prime})/\tau}}{\sum_{j=1}^{N}e^{ sim(h_{i},h_{j}^{\prime})/\tau}} || L^{unsup}_{skice}=-\log\frac{e^{sim(h_{i},h_{i}^{ski})/\tau}}{\sum_{j=1}^{N}e^{sim(h_{i},h_{j}^{ski})/\tau}}",,L_{SKICSE}=-\log\frac{\exp(h\cdoth^{ski}/\tau)}{\sum_{j=1}^{N}\exp(h\cdoth^{\prime}_{j}/\tau)+\exp(h\cdoth^{ski}/\tau)},"Loss function for the SKICSE objective, where \(\tau\) is the temperature parameter, \(\mathbf{h}\) is the embedding of the original sentence, and \(\mathbf{h}^{\text{ski}}\) is the embedding of the SKI sentence."
naacl_2024_short_5,2,L^{unsup}=(1-\lambda)L^{unsup}_{ simce}+\lambdaL^{unsup}_{skice},where the weight \(\lambda\) is a balanced hyperparameter and reflects the importance of SKI,L^{unsup}_{final}=\lambdaL^{unsup}_{simce}+(1-\lambda)L^{unsup}_{skice},where the weight \(\lambda\) is a balanced hyperparameter and reflects the importance of SKI.
naacl_2024_short_5,3,"L^{sup}_{simce}=-\log\frac{e^{sim(h_{i},h_{i}^{+})/\tau}}{\sum_{j=1}^{N}(e^{ sim(h_{i},h_{j}^{+})/\tau}+e^{sim(h_{i},h_{j}^{-})/\tau})} || L^{sup}_{skice_{1}}=-\log\frac{e^{sim(h_{i}^{ski},h_{i}^{+})/\tau}}{\sum_{j=1}^{N} (e^{sim(h_{i}^{ski},h_{j}^{+})/\tau}+e^{sim(h_{i}^{ski},h_{j}^{-})/\tau})} || L^{sup}_{skice_{2}}=-\log\frac{e^{sim(h_{i},h_{i}^{ski})/\tau}}{\sum_{j=1}^{N}(e ^{sim(h_{i},h_{j}^{+})/\tau}+e^{sim (h_{i},h_{j}^{-})/\tau})}",,"L^{sup}_{simce}=-\log\frac{e^{sim(h_{i},h_{i}^{+})/\tau}}{\sum_{j=1}^{N}e^{sim(h_{i},h_{j}^{+})/\tau}}",where \(\mathcal{L}^{\text{sup}}_{\text{simce}}\) and \(\mathcal{L}^{\text{sup}}_{\text{skice}}\) are the objective functions for supervised SKICSE.
naacl_2024_short_5,4,L^{sup}=(1-\lambda_{1}-\lambda_{2})L^{sup}_{simce}+\lambda_{1}L^{sup}_{skice_{1}}+\lambda_ {2}L^{sup}_{skice_{2}},,L^{sup}=(1-\lambda_1)L^{sup}_{simce}+\lambda_1L^{sup}_{skice_{1}}+\lambda_2L^{sup}_{skice_{2}},where \(\lambda_1\) and \(\lambda_2\) are balanced hyperparameters reflecting the importance of the respective objectives.
naacl_2024_short_51,1,"R_{d}(w)=\frac{tf(w,d)}{\sum_{d^{\prime}}tf(w,d ^{\prime})}\cdot\log\frac{N}{df(w)}","where \(\operatorname{tf}(w,d)\) is the number of times the word \(w\) occurs in the day \(d\), \(\operatorname{df}(w)\) is the number of days in which the word \(w\) occurs, and \(N\) is the total number of days in the dataset","R_{d}=\frac{tf(w,d)}{df(w)\cdot N}","where \(R_{d}\) is the relevance of token \(w\) on day \(d\), \(\operatorname{tf}(w,d)\) is the number of times the word \(w\) occurs in the day \(d\), \(\operatorname{df}(w)\) is the number of days in which the word \(w\) occurs, and \(N\) is the total number of days in the dataset."
naacl_2024_short_51,2,"sim(x,x^{\prime})=\frac{\sum_{w\in x^{\prime}x^{\prime}}R_{d}(w)}{max(\hat{R}_{d}(x),\hat{R}_{d}(x^{\prime}))}",,\hat{R}_{d}(x)=\sum_{w\in x}R_{d}(w),"where \(\hat{R}_{d}(x)\) is the relevance score for article \(x\) on day \(d\), and \(R_{d}(w)\) is the relevance of word \(w\) on day \(d\)."
naacl_2024_short_52,1,"& L_{discrete}(y_{i}=t;y_{<i},x)=-\log p(y_{i}=t\midy_{<i},x)\\&\quad=-\langleE(t),h\rangle+\log\sum_{t^{\prime}\in V }\exp\langleE(t^{\prime}),h\rangle","where \(t\) is a token index, \(V\) is the vocabulary, \(\mathbf{E}:V\rightarrow\mathbb{R}^{d}\) is an embedding lookup, and \(\mathbf{h}\) is a transformer hidden state calculated in terms of \(\mathbf{x}\) and the output prefix \(\mathbf{y}_{<i}\)",L=-\sum_{t=1}^{n}\log\frac{\exp(E(y_t)^\toph_t)}{\sum_{y'\in V}\exp(E(y')^\toph_t)},"where \(\mathcal{L}\) is the loss function, \(t\) is a token index, \(n\) is the number of output tokens, \(V\) is the vocabulary, \(\mathbf{E}:V\rightarrow\mathbb{R}^{d}\) is an embedding lookup, and \(\mathbf{h}_t\) is the transformer hidden state at time step \(t\)."
naacl_2024_short_52,2,"L_{cos}(y_{i}=t;y_{<i},x)=1-\cos(E(t),h)",,"L_{continuous}(y;x)=-\langley,h\rangle+\log\sum_{y^{\prime}\inR^{d}}\exp\langley^{\prime},h\rangle","where \(L_{\text{continuous}}\) is the continuous output loss, \(\mathbf{y}\) is the output vector, and \(\mathbf{h}\) is the transformer hidden state."
naacl_2024_short_52,3,"E(y_{i})=u_{i}/\|u_{i}\|;\quadu_{i}\simNormal( 0,I_{d})",,"E(y_{i})=\frac{z}{\|z\|},\quadz\simN(0,I_{d}),(y_{i})\) is the embedding for token\(y_{i}\),\(z\) is a standard normal random vector in\(R^{d}\), and\(\|z\|\) is the Euclidean norm of\(z\).</description>","where \(\mathbf{E}(y_{i})\) is the embedding for token \(y_{i}\), \(\mathbf{z}\) is a standard normal random vector in \(\mathbb{R}^{d}\), and \(\|\mathbf{z}\|\) is the Euclidean norm of \(\mathbf{z}\)."
naacl_2024_short_52,4,E(y_{i})=r_{i}/\sqrt{d};\quadr_{i}\simRademacher(d),,"E(y_{i})=\sigma\cdotr_{i};\quadr_{i}\simRademacher(\{-1,1\}^{d})",The scaled Rademacher distribution for generating embeddings.
naacl_2024_short_52,5,E_{cmb}(y_{i})=\frac{\alphaE_{pre}(y_{i})+(1-\alpha)E_{rand}(y_{i})}{\|\alphaE_{pre}(y_{i})+(1-\alpha)\bm {E}_{rand}(y_{i})\|},,E(y_{i})=\alphaE_{pre-trained}(y_{i})+(1-\alpha)E_{random}(y_{i}),where \(\alpha\) is a hyperparameter controlling the trade-off between pre-trained and random embeddings.
naacl_2024_short_53,1,"S(y,\bar{y};x)>S(y,\hat{y};x)",where \(\mathbf{y}\) is the reference sequence,\bar{y}=f(e_{1:L}+\delta_{1:L};\phi_{d}),"where \(\bar{\mathbf{y}}\) is the perturbed output sequence, \(\mathbf{y}\) is the reference sequence, and \(\mathcal{S}\) is the scoring function."
naacl_2024_short_53,2,\delta_{i}=\alpha|e_{i}|\bigg{|}\frac{\partial f(e_{1:L};\phi_{nap})}{\partiale_{i}}\bigg{|}^{-1}\frac{\partial f(e_{1:L};\phi_{nap})}{\partiale_{i}},"where \(i=1,\ldots,L\)","f(e;\phi_{nap})\approxS(y,\hat{y};x)","where \(i=1,\ldots,L\)"
naacl_2024_short_56,1,X_{all}=L\[SEP]\X_{ori}\[SEP]\P_{C}\[SEP]\P_{L},,"P_{L}=""relation is not  S_{i1},\ldots, S_{ij}, or  N_{random}""",Label prompt \(P_{L}\) consisting of similar relations \(S_{ij}\) and a randomly selected non-similar relation \(N_{random}\).
naacl_2024_short_56,2,L_{MLM}=-{\sum}_{n=1}^{M}\log P(x_{n}),where \(M\) is the number of masked tokens and \(P(x_{n})\) is the predicted probability of token \(x_{n}\) over the vocabulary size,L_{MLM}=-\frac{1}{M}\sum_{n=1}^{M}\log P(x_{n}),where \(M\) is the number of masked tokens and \(P(x_{n})\) is the predicted probability of token \(x_{n}\) over the vocabulary size
naacl_2024_short_56,3,"L_{s}=\frac{1}{N}\sum_{p=1}^{N}-\frac{1}{N_{y_{p}}-1}\sum_{q=1}^{N_ {y_{p}}}\log\frac{e^{sim(h_{p},h_{q}^{+})/\tau}}{\sum_{k=1}^{N}1_{p\neq k}e^{sim(h_{p},h_{k})/\tau}}",where \(N\) is the total number of examples in the batch and \(N_{y_{p}}\) is the number of positive pairs in the batch,"L_{SCL}=-\frac{1}{N}\sum_{i=1}^{N}\log\frac{\exp(sim(h_{p_{i}}, h_{p_{i}^{+}})/\tau)}{\sum_{j=1}^{N}1_{[y_{p_{i}}=y_{p_{j}}]}\exp(sim(h_{p_{i}}, h_{p_{j}})/\tau)}",where \(N\) is the total number of examples in the batch and \(N_{y_{p}}\) is the number of positive pairs in the batch
naacl_2024_short_56,4,L_{final}=\alphaL_{s}+(1-\alpha)L_{MLM},,L=\alphaL_{MLM}+(1-\alpha)L_{s},"where \(\mathcal{L}\) is the final loss function, \(\alpha\) is the hyperparameter balancing the two objectives, \(\mathcal{L}_{MLM}\) is the loss from the masked language model task, and \(\mathcal{L}_{s}\) is the loss from the supervised contrastive learning task."
naacl_2024_short_62,1,"P_{ATTN}(n_{i}|Q,A)=softmax(\sum_{t=1}^{T}\alpha_{t} || v_{t} || _{2})",where \(T\) represents the total token count in \(n_{i}\),\alpha_{t}=\frac{e^{v_{t}}}{\sum_{j=1}^{T} e^{v_{j}}},where \(T\) represents the total token count in \(n_{i}\)
naacl_2024_short_62,2,"P_{RETR}(n_{i}|Q)=\frac{exp(s(n_{i},Q)/\theta)}{\sum_{k=1}^{K}exp(s(n_{k},Q)/\theta)}","where \(s\) denotes the dot-product between the representation vectors of the input question \(Q\) and document candidate \(n_{i}\), and \(\theta\) is the temperature hyper-parameter","p_{RETR}(n_{i}|Q)=\frac{e^{s(Q,n_{i})/\theta}}{\sum_{j=1}^{k} e^{s(Q,n_{j})/\theta}}","where \(s\) denotes the dot-product between the representation vectors of the input question \(Q\) and document candidate \(n_{i}\), and \(\theta\) is the temperature hyper-parameter."
naacl_2024_short_66,1,"y_{t}\sim p_{\theta}(y_{t}\midc,x,y_{ct}) || \propto\explogit_{\theta}(y_{t}\midc,x,y _{ct})",,"y\sim P(y\midx,c;\theta)",Response \(\mathbf{y}\) sampled from the probability distribution conditioned on input query \(\mathbf{x}\) and context \(\mathbf{c}\) given the model \(\theta\).
naacl_2024_short_66,2,"y_{t}\sim\tilde{p}_{\theta}(y_{t}\midc,x,y_{ct}) || \propto p_{\theta}(y_{t}\midc,x,y_{ct})\bigg{(}\frac{p_{\theta}(y_{t}\midc,x,y_{ct})}{p_{\theta}(y_{t}\mid\bm {x},y_{ct})}\bigg{)}^{\alpha}",,"p_{\theta}(y_{t}\midc,x,y_{ct})\propto\exp\left(logit_{\theta}(y_{t}\midx,y_{ct})+PMI(c, y_{t}\midx,y_{ct})\right)",Adjusted output probability distribution incorporating pointwise mutual information (PMI) between context and generation.
naacl_2024_short_66,3,"y_{t}\simsoftmax[(1+\alpha)\,logit_{\theta}(y_{t}\midc,x,y_{ct}) || \qquad\qquad-\alpha\,logit_{\theta}(y_{t}\midx,y _{ct})]",,"\tilde{p}_{\theta}(y_{t}\midc,x,y_{ct})=\frac{p_{\theta}(y_{t}\midc,x,y_{ct})\bigg{(}\frac{p_{\theta}(y_{t}\midc,x,y_{ct})}{p_{\theta}(y_{t}\midx,y_{ct})}\bigg{)}^{\alpha}}{\sum_{y'\inY} p_{\theta}(y'\midc,x,y_{ct})\bigg{(}\frac{p_{\theta}(y'\midc,x,y_{ct})}{p_{\theta}(y'\midx,y_{ct})}\bigg{)}^{\alpha}}","The normalized context-aware output probability distribution for the generation \(y_{t}\) given the context \(\mathbf{c}\), query \(\mathbf{x}\), and previous outputs \(\mathbf{y}_{ct}\)."
naacl_2024_short_67,1,"h_{0}^{x},h_{1}^{x},...,h_{n}^{x}=\textbf{BERT}(w_{0}^{x},w_{1}^{x},...,w_{n}^{x}) || x^{c}=h_{0}^{x},x^{h}=h_{[E_{h}]}^{x},x^{t}=h_{[E_{t}]}^{x}",,"h_{e}=h([E_{h}]),\quad h_{t}=h([E_{t}]),\quad h_{c}=h([CLS])","Where \(h_{e}\), \(h_{t}\), and \(h_{c}\) represent the hidden states of the head entity, tail entity, and contextual information, respectively, and \(h\) denotes the encoder function applied to the special tokens."
naacl_2024_short_67,2,x^{vec}=x^{c}\oplus x^{h}\oplus x^{t},where \(d\) is the hidden dimension of the encoder and \(\oplus\) denotes the concatenation operator,x^{vec}=x^{c}\oplus x^{h}\oplus x^{t},where \(d\) is the hidden dimension of the encoder and \(\oplus\) denotes the concatenation operator
naacl_2024_short_67,3,"h_{0}^{d},h_{1}^{d},...,h_{n}^{d}=\textbf{BERT}(w_{0}^{d},w_{1}^{d},...,w_{n}^ {d}) || d^{c}=h_{0}^{d} || d^{h}=\textit{WeightPooling}_{1}(h_{1}^{d},...,h_{n}^{d}) || d^{t}=\textit{WeightPooling}_{2}(h_{1}^{d},...,h_{n}^{d}) || d^{vec}=d^{c}\oplus d^{h}\oplus d^{t}",,d^{vec}=d^{c}\oplus d^{h}\oplus d^{t},where \(d\) is the hidden dimension of the encoder and \(\oplus\) denotes the concatenation operator
naacl_2024_short_67,4,"H=(h_{1}^{d},...,h_{n}^{d}) || A=\textit{softmax}(HW+b) || d^{*}=AH",,a_{i}=\frac{\exp(\textbf{w}^{T}h_{i}^{d})}{\sum_{j=1}^{n}\exp(\textbf{w}^{T}h_{j}^{d})},"where \(d^{h}\) and \(d^{t}\) are the representations of the head and tail entities, respectively, and \(a_{i}\) are the attention weights for each hidden state."
naacl_2024_short_67,5,"L_{i}=-\log\frac{e^{sim(x_{i}^{vec},d_{i}^{vec})/\tau}}{\sum_ {j=1}^{N}e^{sim(x_{i}^{vec},d_{j}^{vec})/\tau}}",where \(\tau\) is a temperature hyperparameter and \(\text{sim}(\cdot)\) is the cosine similarity,"L_{contrastive}=-\frac{1}{N}\sum_{i=1}^{N}\log\frac{\exp(sim(x_{i}^{vec},d_{i}^{vec})/\tau)}{\sum_{j=1}^{N}I_{[j\neq i]}\exp(sim(x_{i}^{vec},d_{j}^{vec})/\tau)}",where \(\tau\) is a temperature hyperparameter and \(\text{sim}(\cdot)\) is the cosine similarity
naacl_2024_short_67,6,O_{i}=\textit{Pooling}(BERT(\langle x\oplus d_{j}\rangle)) || \hat{y}=\textit{MLP}(O_{0}\oplus O_{1}\oplus...O_{k}) || L_{c}=-log(\frac{e^{\hat{y}_{+}}}{\sum_{i=1}^{k}e^{\hat{y}_{i}}}),,"y=argmax_{d_{j}\in D}sim(x^{vec}, d_{j}^{vec})","where \(y\) is the predicted relation description, and \(\text{sim}(\cdot)\) is the cosine similarity function."
naacl_2024_short_69,1,ATE=E[Y^{a=1}-Y^{a=0}],,ATE=E[Y^{a=1}]-E[Y^{a=0}],Average Treatment Effect (ATE) of the treatment \(A\) on the outcome \(Y\).
naacl_2024_short_69,2,A=a\\Rightarrow\Y^{a}=Y,,"Y^{a}=Y^{a=1} if  A=1,\quad Y^{a}=Y^{a=0} if  A=0",Counterfactual outcome \(Y^{a}\) based on treatment assignment \(A\).
naacl_2024_short_69,3,"Y^{a}\perp A\mid C\quad\forall a\in\{0,1\}",,Y^{a}\perp\!\!\!\perp A\mid C,"Conditional exchangeability assumption, indicating that the counterfactual outcome \(Y^{a}\) is independent of the observed treatment \(A\) given the confounders \(C\)."
naacl_2024_short_69,4,"E[Y^{a}]=\sum_{C}E[Y^{a}\mid C]P(C) || \overset{\eqref{eq:constraint}}{=}\sum_{C}E[Y^{a}\mid A=a,C]P(C)",,"E[Y^{a}]=E[Y\mid A=a,C]\) given treatment\(A=a\) and confounders\(C\).</description>",The expected value of the counterfactual outcome \(Y^{a}\) given treatment \(A=a\) and confounders \(C\).
naacl_2024_short_69,5,"\overset{\eqref{eq:constraint}}{=}\sum_{C}E[Y\mid A=a,C]P(C)",,"E[Y^{a}]=\sum_{C}E[Y^{a}\mid A=a,C]P(C)","The equation expresses the expected value of the counterfactual outcome \(Y^{a}\) as a sum over all confounders \(C\), where the expectation is conditioned on both the treatment \(A\) and the confounders \(C\)."
naacl_2024_short_69,6,E[Y^{a}]=\frac{1}{N}\sum_{i\in[N]}Y_{i}\frac{\mathds{1}(A_{i}=a)}{P(A_{i}=a\mid T)},,E[Y^{a}]=\frac{1}{N}\sum_{i=1}^{N}\frac{Y_i A_i}{P(A_i\mid T)}+\frac{1}{N}\sum_{i=1}^{N}\frac{Y_i (1-A_i)}{1-P(A_i\mid T)},Estimation of the expected counterfactual outcome \(\mathbb{E}[Y^{a}]\) using Inverse Propensity of Treatment Weighting (IPTW) for a dataset of size \(N\).
naacl_2024_short_7,1,"\underset{\theta}{min}\,E_{S^{i}}[\frac{1}{k+1}\sum_{i=0}^{k}l( M_{\theta}(S^{i}),f(x_{i+1}))]",,"l(M_{\theta}(S^{i}), f(x_{i+1}))=\left(M_{\theta}(S^{i})-f(x_{i+1})\right)^{2}",Squared error loss between the model prediction and the ground-truth value.
naacl_2024_short_7,2,"f(x)=\varphi(\langle x,w\rangle)",,"f(x)=\sum_{j=1}^{p}\beta_{j} h_{j}(x),\quad h_{j}(x)=H_{j}(g(x)),\quad g(x)=\theta^{T} x","where \(f(x)\) is the single-index function, \(\beta_{j}\) are the coefficients, \(h_{j}(x)\) are the Hermite polynomials, \(H_{j}\) are the normalized Hermite polynomials, and \(g(x)\) is a linear transformation of the input \(x\) with parameters \(\theta\)."
naacl_2024_short_7,3,\varphi_{linear}(t)=t || \varphi_{quadratic}(t)=\frac{1}{\sqrt{2}}(t+\frac{1}{\sqrt{2}}(t^{2}-1)) || \varphi_{cubic}(t)=\frac{1}{\sqrt{3}}(t+\frac{1}{\sqrt{ 2}}(t^{2}-1)+\frac{1}{\sqrt{6}}(t^{3}-3t)),,"f_{k}(x)=\frac{1}{\sqrt{n!}}He_{n}(\langle x,w\rangle)",Function derived from normalized probabilist's Hermite polynomial of degree \(n\) for the \(k\)-th function class.
naacl_2024_short_7,4,f\simF_{1}&1\leq t<\frac{T}{3}\\F_{2}&\frac{T}{3}\leq t<\frac{2T}{3}\\F_{3}&\frac{2T}{3}\leq t<T,,"T_{k}=\left\lfloor\frac{T}{K}\right\rfloor for  k=1,2,\ldots,K-1,\quad T_{K}=T-\sum_{k=1}^{K-1} T_{k}","Where \(T_{k}\) is the number of training steps allocated to the \(k\)-th function class, \(T\) is the total number of training steps, and \(K\) is the total number of function classes."
naacl_2024_short_7,5,f\simF_{1}&1\leq t<\frac{T}{3}\\\sum_{s=1}^{2}1(\xi=s)F_{s}&\frac{T}{3}\leq t<\frac{2T}{3}\\\sum_{s=1}^{3}1(\zeta=s)F_{s}&\frac{2T}{3}\leq t<T,,f\simF_{\xi}&1\leq t<\frac{T}{2}\\F_{\zeta}&\frac{T}{2}\leq t<T,"Here, \(\xi\) is drawn from \(\{1,2\}\) to select between the first two function classes, and \(\zeta\) is drawn from \(\{1,2,3\}\) to select among all three function classes."
naacl_2024_short_7,6,"f\sim\sum_{s=1}^{3}1(\zeta=s)F_{s},\quad 1\leq t<T",,"f\simF_{random} for  t=1,2,\ldots,T",Randomly sampled function class from the set of \(K\) function classes at each training step.
naacl_2024_short_8,1,"s_{m}(q,d)=T_{\texttt{[CLS]}}W+b","where \(T_{\texttt{[CLS]}}\in\mathbf{R}^{D}\) is the [CLS] representation on the final layer, and \(W\in\mathbf{R}^{D\times 1}\) and \(b\in\mathbf{R}\) are the weight and bias for classification","s_{q,d}=W^T T_{\texttt{[CLS]}}+b","where \(s_{q,d}\) is the relevance score, \(T_{\texttt{[CLS]}}\in\mathbf{R}^{D}\) is the [CLS] representation on the final layer, \(W\in\mathbf{R}^{D\times 1}\) is the weight for classification, and \(b\in\mathbf{R}\) is the bias for classification."
naacl_2024_short_8,2,"s_{m}(q,d)=\frac{1}{n}\sum_{i}^{n}(T_{tok_{i}}W+b)","where \(T_{tok_{i}}\) is the final-layer representation of the \(i\)-th token, and \(n\) is the total number of tokens in the input sequence","s_{mp}(q,d)=\frac{1}{n}\sum_{i=1}^{n}T_{tok_{i}}","where \(T_{tok_{i}}\) is the final-layer representation of the \(i\)-th token, and \(n\) is the total number of tokens in the input sequence"
naacl_2024_short_8,3,v_{q_{i}}=T_{q_{i}}W+b;\quad v_{d_{j}}=T_{d_{j}}W+b,"where \(q_{i}\) and \(d_{j}\) represent the \(i\)-th token of query \(q\) and the \(j\)-th token of document \(d\), respectively","r_{q,d}=\sum_{i}^{m}\sum_{j}^{n}T_{q_{i}}T_{d_{j}}","where \(T_{q_{i}}\) and \(T_{d_{j}}\) are the representations of the \(i\)-th token of query \(q\) and the \(j\)-th token of document \(d\), respectively, and \(m\) and \(n\) are the total number of tokens in the query and document."
naacl_2024_short_8,4,"s_{l}(q,d)=\sum_{q_{i}}\max_{d_{j}}(v_{q_{i}}^{T}v_{d_{j}})",,"s_{late}(q,d)=\sum_{i}^{n}\max_{j}(v_{q_{i}}\cdot v_{d_{j}})","where \(s_{late}(q,d)\) is the late interaction score, \(v_{q_{i}}\) and \(v_{d_{j}}\) are the representations of the \(i\)-th query token and \(j\)-th document token, respectively."
naacl_2024_short_8,5,"L=lce(s_{m}(q,d^{+}),s_{m}(q,d_{1}^{-}),...,s_{m}(q,d_{n}^ {-})) || +lce(s_{l}(q,d^{+}),s_{l}(q,d_{1}^{-}),...,s_{l}(q,d_{n}^ {-}))",,LCE=-\frac{1}{N}\sum_{k=1}^{N}y_{k}\log(\hat{y}_{k})+(1-y_{k})\log(1-\hat{y}_{k}),"where \(N\) is the number of samples, \(y_{k}\) is the true label for sample \(k\), and \(\hat{y}_{k}\) is the predicted probability for sample \(k\)."
naacl_2024_short_9,1,"L_{ir}(q,p_{i})=-\log(\frac{e^{q\cdot p_{t}}}{\sum_{i=1}^{N}e^{q\cdot p_{ i}}})",,L=-\log\frac{e^{q\cdot p_{t}}}{\sum_{i=1}^{N} e^{q\cdot p_{i}}},"The loss function for training Dense Passage Retrieval (DPR), where \(\mathcal{L}\) is the loss, \(q\) is the query embedding, \(p_{t}\) is the gold passage embedding, and \(p_{i}\) are the passage embeddings in the batch."
naacl_2024_short_9,2,"L_{c}(q_{s},q_{t})=-\log(\frac{e^{q_{s}\cdot q_{t}}}{\sum_{j=1}^{N}e^{q_ {s}\cdot q_{j}}})",,"L_{c}(q_{s},q_{j})=-\log(\frac{e^{q_{s}\cdot q_{t}}}{\sum_{j=1}^{N}e^{q_{s}\cdot q_{j}}})",The contrastive loss term that aligns the English query embedding \(q_{s}\) with its corresponding code-mixed query embedding \(q_{t}\) within a batch of size \(N\).
naacl_2024_short_9,3,L=L_{ir}+wL_{c},where \(w\) is a hyperparameter for weighting the contrastive loss,"L=L_{ir}(q,p_{i})+w\cdot L_{c}(q_{s},q_{t})",where \(w\) is a hyperparameter for weighting the contrastive loss
neurips_2024_oral_10,1,"q(x_{1:T}|x_{0})\coloneqq\prod_{t=1}^{T}q(x_{t}|x_{t-1}) || q(x_{t}|x_{t-1})\coloneqqN(x_{t};\sqrt{1-\beta_{t}}x _{t-1},\beta_{t}I)",,f_t=f_0+\epsilon_t,"Where \( f_t \) is the noisy feature at time step \( t \), \( f_0 \) is the clean feature, and \( \epsilon_t \) is the noise added at time step \( t \)."
neurips_2024_oral_10,2,"p_{\theta}(x_{0:T})\coloneqq p(x_{T})\prod_{t=1}^{T}p_{\theta}(x_{t-1}|x_{t}) || p_{\theta}(x_{t-1}|x_{t})\coloneqqN(x_{t-1};\mu_{\theta}(x_{t },t),\Sigma_{\theta}(x_{t},t))",,x_{t-1}\coloneqq\frac{1}{\sqrt{1-\beta_{t}}}(x_{t}-\sqrt{\beta_{t}}z),Equation for reconstructing the previous feature state from the current noisy feature and a noise term.
neurips_2024_oral_10,3,"X_{t-1}=\frac{1}{\sqrt{a_{t}}}(X_{t}-\frac{1-a_{t}}{\sqrt{1-\tilde{a_{t}}}}D_{\theta}(X_{t},t))+\sigma_{t}z","where \(a_{t}=1-\beta_{t}\), \(D_{\theta}\) are the parameters of the prediction noise network",W_{fused}=W_{embedding}+a_{t}\cdotW_{denoising},"where \(a_{t}=1-\beta_{t}\), \(\mathbf{W}_{\text{fused}}\) are the merged parameters, \(\mathbf{W}_{\text{embedding}}\) are the parameters of the original embedding layer, and \(\mathbf{W}_{\text{denoising}}\) are the parameters of the denoising layer."
neurips_2024_oral_10,4,Y&=WX+b\\\frac{1}{\sqrt{a_{t}}}X_{t}-X_{t-1}&=\frac{1-a_{t}}{\sqrt{a_{t}}\sqrt{1-\tilde{a_{t}}}}D_{\theta}X_{t}-\sigma_{t}z\\\frac{1}{\sqrt{a_{t}}}Y_{t}-Y_{t-1}&=\frac{1-a_{t} }{\sqrt{a_{t}}\sqrt{1-\tilde{a_{t}}}}D_{\theta}Y_{t}-\sigma_{t}z,,"X_{t-1}=\frac{1}{\sqrt{a_{t}}}(X_{t}-\frac{1-a_{t}}{\sqrt{1-\tilde{a_{t}}}}D_{\theta}(X_{t},t))+\sigma_{t}z","where \(X_{t-1}\) is the denoised feature at step \(t-1\), \(X_{t}\) is the noisy feature at step \(t\), \(D_{\theta}\) are the parameters of the prediction noise network, \(a_{t}=1-\beta_{t}\), and \(z\) is a standard normal random variable."
neurips_2024_oral_10,5,"Y_{t-1}=[W-C_{1}(t)WW_{D}]X_{t}+WC_{2}(t)C_{3}+b\\C_{1}(t)=\frac{1-a_{t}}{\sqrt{a_{t}}\sqrt{1-\tilde{a_{t}}}}\qquad C _{2}(t)=\frac{1-a_{t-1}^{-}}{1-\tilde{a_{t}}}\beta_{t}\qquad C_{3}=Z\sim N(0,I)","where \(W_{D}\) denotes the parameters of \(D_{\theta}(X_{t},t)\), \(X_{t}\) denotes the input of this linear layer, \(Y_{t}\) denotes the output of this linear layer, and \(Y_{t-1}\) denotes the result after denoising in one step of \(Y_{t}\)",Y_{t}=WX_{t}+b,"where \(W_{D}\) denotes the parameters of \(D_{\theta}(X_{t},t)\), \(X_{t}\) denotes the input of this linear layer, \(Y_{t}\) denotes the output of this linear layer, and \(Y_{t-1}\) denotes the result after denoising in one step of \(Y_{t}\)"
neurips_2024_oral_10,6,\frac{1}{\sqrt{a_{t}}}Y_{t}-Y_{t-1}=C_{1}(t)D_{\theta}Y_{t}-\sigma_{t}z || \frac{1}{\sqrt{a_{t-1}}}Y_{t-1}-Y_{t-2}=C_{1}(t-1)D_{\theta}Y_{t-1}-\sigma_{t-1}z,,Y_{t-2}=[W-C_{1}(t-1)WW_{D}]Y_{t-1}+WC_{2}(t-1)C_{3}+b,where \(Y_{t-2}\) denotes the result after denoising in two steps of \(Y_{t}\)
neurips_2024_oral_10,7,Y_{t-2}=W^{\prime\prime}X_{t}+C^{\prime\prime}\\W^{\prime\prime}=\frac{1}{\sqrt{a_{t}-1}}\{\frac{W}{\sqrt{a_{t} }}-[C_{1}(t)+C_{1}(t-1)]WW_{D}+\sqrt{a_{t}}C_{1}(t-1)C_{1}(t)WW_{D}W_{D}\}\\C^{\prime\prime}=\frac{1}{\sqrt{a_{t}-1}}[WC_{2}(t)+\sqrt{a_{t}}WC_{ 2}(t-1)-\sqrt{a_{t}}C_{1}(t-1)C_{2}(t)WW_{D}]Z+b,,\frac{1}{\sqrt{a_{t-1}}}(WX_{t}+b)-Y_{t-2}=C_{1}(t-1)D_{\theta}(WX_{t}+b)-\sigma_{t-1}z,"where \(W\) denotes the parameters of the linear layer, \(b\) denotes the bias term, and \(Y_{t-2}\) denotes the result after denoising in two steps of \(Y_{t}\)"
neurips_2024_oral_10,8,"Loss_{p}=\sum_{i=1}^{N}|\epsilon_{i}-D_{\theta_{i}}(X_{t_{i}},t_{i})|","where \(\epsilon\) denotes the sampled noise, \(N\) denotes the number of denoising layers, \(X_{t}\) denotes the noise sample, \(t\) denotes the diffusion step, and \(D_{\theta}(X_{t},t)\) denotes the noise predicted by the denoising layer","Loss_{p}=\frac{1}{N}\sum_{i=1}^{N}E_{X_{t},\epsilon,t}[ || D_{\theta}(X_{t},t)-\epsilon || ^{2}]","where \(\epsilon\) denotes the sampled noise, \(N\) denotes the number of denoising layers, \(X_{t}\) denotes the noise sample, \(t\) denotes the diffusion step, and \(D_{\theta}(X_{t},t)\) denotes the noise predicted by the denoising layer."
neurips_2024_oral_10,9,Loss=(1-\lambda)Loss_{l}+\lambda Loss_{p},,Loss_{total}=Loss_{p}+\lambda Loss_{l},"where \(Loss_{total}\) is the combined loss function, \(Loss_{p}\) is the loss from the denoising layers, \(Loss_{l}\) is the supervised loss with labels, and \(\lambda\) is the trade-off parameter between the two losses."
neurips_2024_oral_11,1,"sim(f,g)=\int_{x\in D(f)}\frac{\mathds{1}[f(x)=g(x)]}{|D(f)|}\\\approx\sum_{x\in X|X\sim D(f)}\frac{\mathds{1}[f(x)=g(x)]}{|X|}",,"sim(f, g)=\frac{|\{x\in D(f)\mid f(x)=g(x)\}|}{|D(f)|}",Similarity between functions \(f\) and \(g\) based on their outputs for the same input values.
neurips_2024_oral_11,2,"f^{*}=\textsc{FunConsensus}(F)=\operatorname*{arg\,max}_{f_{(i)}\in F}\sum_{ f_{(j)}\in F\setminus\{f_{(i)}\}}sim(f_{(i)},f_{(j)})",,"f^{*}=\arg\max_{f_{(i)}\in F} sim(f_{(i)}, g)",The candidate function \(f^{*}\) that achieves maximal similarity with the other sampled functions \(f_{(i)}\) from the set \(F\).
neurips_2024_oral_12,1,"P(X_{1},\ldots,X_{d})=\prod_{i=1}^{d}P(X_{i}\midPA_{i})",,"P(X_{1},\ldots,X_{d})=\prod_{j=1}^{d} P(X_{j}\midPA_{j})","The joint distribution of observable variables \(P(X_{1},\ldots,X_{d})\) expressed as the product of conditional distributions given their parents \(\mathbf{PA}_{j}\)."
neurips_2024_oral_12,2,"P(X_{1},\ldots,X_{d}|do(X=x))=\prod_{i:X_{i}\not\inX }P(X_{i}|PA_{i})\big{|}_{X=x}","where \(|_{\mathbf{X}=\mathbf{x}}\) enforces \(X_{1},\ldots,X_{d}\) to be consistent with realizations of \(\mathbf{X}\) else Eq","P(Y|do(X))=\sum_{Z} P(Y|Z,X) P(Z)|_{X=x}","where \(|_{\mathbf{X}=\mathbf{x}}\) enforces \(X_{1},\ldots,X_{d}\) to be consistent with realizations of \(\mathbf{X}\)."
neurips_2024_oral_12,3,"P(X_{\sigma(1)},\ldots,X_{\sigma(N)})=P(X_{1},\ldots,X_{N})",,"P(X_{\sigma(1)},X_{\sigma(2)},\ldots,X_{\sigma(N)})=P(X_{1},X_{2},\ldots,X_{N}),\forall\sigma\in S_{N}",where \(S_{N}\) is the set of all permutations of \(N\) elements.
neurips_2024_oral_12,4,"P(X_{::[N]}=x_{:,[N]})=\int\int\prod_{n=1}^{N}\prod_{i=1}^{ d}p(x_{i;n}\mida_{i;n}^{G},\theta_{i})d\nu_{1}(\theta_{1})\ldots d\nu_{d}(\theta_{d})",,P(X_{::n})=\int P(X_{::n}|PA_{i})P(PA_{i})dPA_{i},"where \(\mathbf{X}_{::n}\) denotes the sequence of random variable arrays, \(\boldsymbol{P}\boldsymbol{A}_{i}\) represents the parents of node \(X_{i}\), and the integral accounts for the mixture over the parent distributions."
neurips_2024_oral_12,5,"P(Y=y|do(X=x))=p(y|x,\psi_{0})=P(Y=y|x),\psi_{0}\inR",,"P(X_{1},\ldots,X_{N}|do(X=x))=\prod_{i=1}^{N}p(X_{i}|X=x)",where \(do(X=x)\) assigns the sampling density \(p(x|\theta)\) to \(\delta(X=x)\).
neurips_2024_oral_12,6,"P(Y=y|do(X=x))=\int p(y|x,\psi)p(\psi)d\psi=P(Y=y|x)",,"P(Y=y|do(X=x))=\int p(y|x,\theta)p(\theta)d\nu(\theta)",where \(\theta\) represents the parameters of the distribution and \(\nu\) is the corresponding probability measure.
neurips_2024_oral_12,7,"\textbf{i.i.d. generative process}:P(X_{1},Y_{1},\ldots,X_{N},Y_{N})\stackrel{{ ind}}{{=}}\prod_{n=1}^{N}P(X_{n},Y_{n})\stackrel{{ idc}}{{=}}[P(X,Y)]^{N}",,"P(Y=y|do(X=x))=P(Y=y|x,\psi_{0})=\int p(y|x,\psi)p(\psi)d\psi,\psi_{0}\inR","where \(p(y|x,\psi)\) represents the conditional probability of \(Y\) given \(X\) and the causal de Finetti parameter \(\psi\)."
neurips_2024_oral_12,8,"\textbf{ICM gen. process}:P(x_{1},y_{1},\ldots,x_{N},y_{N})=\int\int\prod_{n=1} ^{N}p(y_{n}|x_{n},\psi)p(x_{n}|\theta)d\mu(\theta)d\nu(\psi)",,"P(X,Y)=\int\int p(X,Y|\theta)p(\theta)d\theta","where \(p(X,Y|\theta)\) is the conditional distribution of \(X\) and \(Y\) given the causal de Finetti parameter \(\theta\)."
neurips_2024_oral_12,9,"P(X_{J;n}\mid do(X_{I;n}=x))=P(X_{J;m}\mid do (X_{I;m}=x)),\forall n\neq m",,"P(Y_{n}|do(X_{n}=x))=P(Y_{m}|do(X_{m}=x)),\forall n\neq m",Identical marginal post-interventional distributions for variables in different positions in an ICM generative process.
neurips_2024_oral_12,10,"P(x_{1},y_{1},x_{2},y_{2}|do(X_{1}=\hat{x}))=P(y_{1}|\hat{x})P(y_{2}|x_{2})P(x _{2})1_{x_{1}=\hat{x}}",,P(Y_{1}|do(X_{1}=\hat{x}))=P(Y_{1}|X_{1}=\hat{x}),where the causal effect of \(X_{1}\) on \(Y_{1}\) is expressed under the ICM generative process.
neurips_2024_oral_12,11,"P(x_{1},y_{1},x_{2},y_{2}|do(X_{1}=\hat{x}))=\int p(y_{1}|\hat{x},\psi)p(y_{2 }|x_{2},\psi)p(\psi)d\psi p(x_{2})1_{x_{1}=\hat{x}}",,"P(X_{1},Y_{1},X_{2},Y_{2}|do(X_{1}=\hat{x}))=P(Y_{1}|\hat{x})P(Y_{2}|X_{2})P(X_{2})1_{X_{1}=\hat{x}}+\int P(Y_{1}|X_{1},\psi)P(Y_{2}|X_{2},\psi)P(\psi)d\psi1_{X_{1}=\hat{x}}","where \(\hat{x}\) is the value assigned to \(X_{1}\) during the intervention, and \(\psi\) represents the causal de Finetti parameters influencing the distributions of \(Y_{1}\) and \(Y_{2}\)."
neurips_2024_oral_12,12,"p(x_{:,1},\ldots,x_{:,N}|do(X=\hat{x}))=\prod_{i\in I_{X}}p(x_{i;[-N_{i}]}|pa_{i;[-N_{i}]}^{G})\prod_{i\not\in I_{ X}}p(x_{i;[N]}|pa_{i;[N]}^{G})\big{|}_{X=\hat{x}}",,"P(Y|X=\hat{x})=\int p(Y|\hat{x},\psi)p(\psi)d\psi",The density of the post-interventional distribution after intervening on \(\mathbf{X}=\hat{\mathbf{x}}\) in ICM generative processes.
neurips_2024_oral_12,13,"P(X_{\negI;n}|do(X_{I;n}=\hat{x} ),X_{\negS})=\prod_{i\not\inI}P(X_{i;n}|X_{i;S},PA_{i;S\cup\{n\}})|_{ X_{I;n}=\hat{x}}",,"P(Y_{i}|do(X_{i}=\hat{x}),X_{j},Y_{j})=P(Y_{i}|X_{j},Y_{j}),\forall i\inI, j\notinI","where \(Y_{i}\) is the response variable corresponding to the intervention on \(X_{i}\), and \(X_{j}, Y_{j}\) are other observed variables not in the intervention set."
neurips_2024_oral_12,14,"\int\int\prod_{n}p(y_{n}\mid x_{n},\psi)p(x_{n}\mid\theta)p(\theta)p(\psi)d\theta d\psi","where \(p(\theta),p(\psi)\) are Beta distributions and \(p(y_{n}\mid x_{n},\psi),p(x_{n}\mid\theta)\) are Bernoulli distributions","P(X_{n},Y_{n})=\int\int\prod_{n=1}^{N}p(y_{n}|x_{n},\psi)p(x_{n}|\theta)d\mu(\theta)d\nu(\psi)","where \(p(\theta),p(\psi)\) are Beta distributions and \(p(y_{n}\mid x_{n},\psi),p(x_{n}\mid\theta)\) are Bernoulli distributions."
neurips_2024_oral_13,1,"X_{q}=clamp(\,\lfloor\frac{X}{\Delta}\rceil\!+\!z,0,2^{b}-1),where \Delta=\frac{\max(X)-\min(X)}{2^{b}-1},z=-\lfloor\frac{\min(X)}{\Delta}\rfloor",,X_{q}=round(X\cdot 2^{b-1}),"Here, \(\mathbf{X}_{q}\) is the quantized activation, and \(\text{round}(\cdot)\) denotes the rounding function applied to the scaled input tensor."
neurips_2024_oral_13,2,"R^{1}=E_{d^{(1)}}\hat{R}QE_{d^{(1)}},\qquadQ=1&Q\\O&Q^{\prime}\!",,"\hat{R}=\cos(\theta) &-\sin(\theta)\\\sin(\theta) &\cos(\theta) , where \theta=\tan^{-1}\left(\frac{X_{i,d^{(1)}}}{X_{j,d^{(1)}}}\right) for  i\neq j.}\) is the approximated rotation matrix,\(\theta\) is the angle of rotation calculated based on the elements of\(X\) at the identified feature dimension\(d^{(1)}\).</description>","Here, \(\hat{\mathbf{R}}\) is the approximated rotation matrix, \(\theta\) is the angle of rotation calculated based on the elements of \(\mathbf{X}\) at the identified feature dimension \(d^{(1)}\)."
neurips_2024_oral_13,3,"\hat{R}= BlockDiag(\hat{R}_{b_{1}},...,\hat{R}_{b_{K}})","where \(\hat{\mathbf{R}}_{b_{i}}\in\mathbb{R}^{2^{n}\times 2^{n}}\) denotes a square matrix of the \(i\)-th block, which is constructed following the three steps mentioned above","\hat{R}=\hat{R}_{b_{1}} &O &\cdots &O\\O &\hat{R}_{b_{2}} &\cdots &O\\\vdots &\vdots &\ddots &\vdots\\O &O &\cdots &\hat{R}_{b_{m}} , where  b_{i} denotes the  i-th block.","where \(\hat{\mathbf{R}}_{b_{i}}\in\mathbb{R}^{2^{n}\times 2^{n}}\) denotes a square matrix of the \(i\)-th block, which is constructed following the three steps mentioned above."
neurips_2024_oral_13,4,"Var([M_{b_{1}},M_{b_{2}},...,M_{b_{K}}])",,\sigma^{2}=\frac{1}{K}\sum_{i=1}^{K}(O_{j}-M_{b_{i}})^{2}.\) is the largest outlier in dimension\(d_{j}\) and\(M_{b_{i}}\) is the mean value of all largest outliers in the\(i\)-th block.</description>,where \(O_{j}\) is the largest outlier in dimension \(d_{j}\) and \(M_{b_{i}}\) is the mean value of all largest outliers in the \(i\)-th block.
neurips_2024_oral_13,5,Y=X\cdotW=[(X\cdot\underbrace{A^ {-1}}_{G})\hat{R}_{(1)}\cdotP\cdot\hat{R }_{(2)}]\cdot[\underbrace{\hat{R}_{(2)}^{\top}\cdotP^{\top}\cdot\hat{R}_{(1)}^{\top}(A}_{G^{-1}}\cdotW)],"where the notation \(\mathbf{P}\) denotes the orthogonal permutation matrix learned via the zigzag manner, the \(\hat{\mathbf{R}}_{(1)}\) and \(\hat{\mathbf{R}}_{(2)}\) represent the first and second block-diagonal rotation matrix, respectively",Y=X\cdotW=(XR)(R^{\top}W)\cdotP\cdot\hat{R}_{(1)}\cdot\hat{R}_{(2)},"where the notation \(\mathbf{P}\) denotes the orthogonal permutation matrix learned via the zigzag manner, the \(\hat{\mathbf{R}}_{(1)}\) and \(\hat{\mathbf{R}}_{(2)}\) represent the first and second block-diagonal rotation matrix, respectively."
neurips_2024_oral_13,6,\max_{1\leq j\leq 2^{n}}\O_{j}(X_{b_{i}}\hat{R}_{b_{i}})\leq\max_{1\leq j\leq 2^{n}}\O_{j}(X_{b_{i}}),,"O_{j}(\hat{R}\cdotX)\leq\gamma O_{j}(X), where \gamma < 1.\","where \(O_{j}(\cdot)\) denotes the maximum outlier of the \(j\)-th dimension, and \(\gamma\) is a constant representing the reduction factor."
neurips_2024_oral_13,7,"M_{b_{i}}\leq O^{(1)}+\frac{(2^{n}K-1)(2^{n-1}-1)}{2^{n}}\delta,\qquad i=1,2, 3,...,K",,"M_{b_{i}}\leq M_{b_{i+1}}+\delta,\quad\forall i=1,2,...,K-1","where \(M_{b_{i}}\) is the mean value of the maximum outliers in the \(i\)-th block, and \(\delta\) is the maximum difference between consecutive reordered outliers."
neurips_2024_oral_15,1,"\forall x\inX\colonP[c_{x }=1]=\frac{1}{1+\exp(-2ax^{\top}\theta^{*})},\\E[c_{x}]=\tanh(ax^{\top}\theta^{*})\\V[c_{x}]=1-\tanh^{2}(ax^{\top}\theta^{*}),\\E[t_{x}]=\frac{a}{x^{\top}\theta^{*}}\tanh(ax^ {\top}\theta^{*})&if x^{\top}\theta^{*}\neq 0\\a^{2}&if x^{\top}\theta^{*}=0",,"t_{RT,x}=t_{nondec}+t_{x}","The total response time, where \(t_{\text{nondec}}\) is the non-decision time and \(t_{x}\) is the decision time."
neurips_2024_oral_15,2,\forall x\inX\colon x^{\top}\frac{\theta^{*}}{a}=\frac{E [c_{x}]}{E[t_{x}]},,"\theta^{*}=\arg\max_{\theta\inR^{d}}\sum_{x\inX_{sample}}\sum_{i=1}^{n_{x}}\left[c_{x,s_{x,i}}(x^{\top}\theta)-\log\left(1+\exp(x^{\top}\theta)\right)\right]t_{x,s_{x,i}}","The choice-decision-time estimator for human preference \(\theta^{*}\) based on the relationship between human utilities, choices, and decision times."
neurips_2024_oral_15,3,"\widehat{\theta}_{CH,DT}\coloneqq(\sum_{x\inX_{ sample}}n_{x}\;xx^{\top})^{-1}\sum_{x\inX_{sample}}n_{x}\;x\;\frac{\sum_{i=1}^{n_{x}}c_{x,s_{x,i}}}{\sum_{i=1}^{n_{x}}t_{x,s_{x,i}}}",,"\widehat{\theta}_{CH,DT}=\arg\min_{\theta}\sum_{x\inX_{sample}}\left(\frac{E[c_{x}]}{E[t_{x}]}-x^{\top}\frac{\theta}{a}\right)^{2}.\end{latex>","The estimate of the human preference vector scaled by the decision barrier, calculated using the ordinary least squares method based on empirical means of choices and decision times."
neurips_2024_oral_15,4,"\widehat{\theta}_{CH}\coloneqq\operatorname*{arg\,max}_{\theta\inR^{d}}\sum_{x\inX_{sample}}\sum_{i=1}^{n_{x}}\log\mu(c_{x,s_{x,i}}\,x^{\top}\theta)",where \(\mu(y)\coloneqq 1/[1+\exp(-y)]\) is the standard logistic function,"\widehat{\theta}_{CH}\coloneqq\arg\max_{\theta}\sum_{x\inX_{sample}}\sum_{i=1}^{n_{x}}\left[c_{x,s_{x,i}}\log\mu(2a\theta^{\top}x)+\left(1-c_{x,s_{x,i}}\right)\log(1-\mu(2a\theta^{\top}x))\right]",where \(\mu(y)\coloneqq 1/[1+\exp(-y)]\) is the standard logistic function
neurips_2024_oral_15,5,"\sqrt{n}\;y^{\top}(\widehat{\theta}_{CHD,n}-\theta^{*}/a)\overset{D}{\longrightarrow}N(0,\zeta^{2}/a^{2})",,"\sqrt{n}\left(\widehat{\theta}_{CH,DT}-\frac{\theta^{*}}{a}\right)\xrightarrow{d}N(0,\Sigma)",Asymptotic normality of the choice-decision-time estimator
neurips_2024_oral_15,6,\zeta^{2}\leq\|y\|_{(\sum_{x\inX_{sample}} [\min_{x^{\prime}\inX_{sample}}E[t_{x^{\prime}}]]\cdot xx^{\top})^{-1}}^{-1},,\zeta^{2}\coloneqq\frac{1}{\sum_{x\inX_{sample}}\frac{|E[c_{x}]|^{2}}{E[t_{x}]^{2}}},asymptotic variance constant
neurips_2024_oral_15,7,"\sqrt{n}y^{\top}(\widehat{\theta}_{CH,n}-2a\theta^{*})\overset{D}{\longrightarrow}N(0,4a^{2}\|y\|_{(\sum_{x\inX_{sample}}[a^{2}\,V[c_{x}] ]\cdot xx^{\top})^{-1}}^{-1})",,"\sqrt{n}\;y^{\top}(\widehat{\theta}_{CH,n}-2a\theta^{*})\overset{D}{\longrightarrow}N(0,\sigma^{2})",Asymptotic normality of \(\widehat{\theta}_{\text{CH}}\)
neurips_2024_oral_15,8,"\widehat{u}_{x,CH,DT}\coloneqq\frac{\sum_{i=1}^{n_{x}}c_{x,s_{x,i}}}{\sum_{i=1}^{n_{x}}t_{x,s_{x,i}}}",,"\widehat{u}_{x}\coloneqq a\cdot\frac{\sum_{i=1}^{n_{x}}c_{x,s_{x,i}}}{\sum_{i=1}^{n_{x}}t_{x,s_{x,i}}}",Estimate of the utility difference \(u_{x}\) using the choice-decision-time estimator.
neurips_2024_oral_15,9,"\widehat{u}_{x,CH}\coloneqq\mu^{-1}(\frac{1}{n_{x}}\sum_{i=1}^{n _{x}}\frac{c_{x,s_{x,i}}+1}{2})","where \((c_{x,s_{x,i}}+1)/2\) is the binary choice coded as 0 or 1, and \(\mu^{-1}(p)\coloneqq\log\left(p/(1-p)\right)\) is the logit function (inverse of \(\mu\) introduced in eq","\widehat{u}_{x,CH}\coloneqq\operatorname*{arg\,max}_{\theta\inR^{d}}\sum_{i=1}^{n_{x}}\log\mu\left(\frac{c_{x,s_{x,i}}+1}{2}\right)","where \((c_{x,s_{x,i}}+1)/2\) is the binary choice coded as 0 or 1, and \(\mu^{-1}(p)\coloneqq\log\left(p/(1-p)\right)\) is the logit function (inverse of \(\mu\) introduced in eq."
neurips_2024_oral_15,10,"P(|\widehat{u}_{x,CH,DT}-\frac{u_{x}}{a}|>\epsilon)\leq 4\exp(-[m_{CH,DT}^{non-axym}(x^{\top}\theta^{*})]^{2}\,n_{x}\,[\epsilon\cdot a]^{2} )",,"\left|\widehat{u}_{x,CH,DT}-u_{x}/a\right|\leq\epsilon with probability at least 1-\delta, for \delta\in(0,1).\",Non-asymptotic concentration bound for the choice-decision-time estimator.
neurips_2024_oral_15,11,"P(|\widehat{u}_{x,CH}-2au_{x}|>\epsilon)\leq 6\exp(-[m_{CH}^{non-asym}(x^{\top}\theta^{*} )]^{2}\,n_{x}\,[\epsilon/(2a)]^{2})",,"P(|\widehat{u}_{x,CH}-2au_{x}|>\epsilon)\leq 4\exp(-[m_{CH}^{non-axym}(x^{\top}\theta^{*})]^{2}\,n_{x}\,[\epsilon]^{2} )",where \(\dot{\mu}(y)\) is the derivative of the logistic function \(\mu(y)\) evaluated at \(y=2au_{x}\)
neurips_2024_oral_16,1,"H=GNN(A,X)=\tilde{A}XW^{\prime},h_{i}=\sum_{j\inN_{i}}\alpha_{ij}x_{j}\cdotW^{\prime}=\sum_{j\inN_{i}}\frac{1}{d_{ i}}x_{j}\cdotW^{\prime}","where \(\mathbf{W}^{\prime}=\xi(\mathbf{W}),\mathbf{W}\in\mathbb{R}^{D\times K}\) with \(\xi\) being an activation function like ReLU [22] for ease of understanding, \(\mathcal{N}_{i}\) denotes the set of first-order neighbors of node \(v_{i}\), inclusive of \(v_{i}\) itself",L_{DGI}=-\sum_{v_{i}\inG}JS(p_{i}\parallel p_{data}),"where \(\mathcal{L}_{\text{DGI}}\) is the DGI loss, \(p_{i}\) is the distribution of node \(v_{i}\) and its first-order neighbors, and \(p_{\text{data}}\) is the distribution of all nodes in the original graph."
neurips_2024_oral_16,2,"E[h_{i}]&=E [\sum_{j\inN(i)}\alpha_{ij}x_{j}\cdotW^{\prime}]=\sum_{j\inN(i)}\alpha_{ij}E[x_{j}]\cdotW^{\prime}=\sum_{j\inN(i)}\alpha_{ij}\mu_{i}\cdotW^{\prime},\\Var(h_{i})&=Var(\sum_{ j\inN(i)}\alpha_{ij}x_{j}\cdotW^{\prime})=\sum_{j\inN(i)}\alpha_{ij}Var(x_{j})\cdotW ^{\prime 2}=\sigma_{i}^{2}\sum_{j\inN(i)}\alpha_{ij}^{2}\cdotW^{\prime 2}",,"\mu_{i}=\frac{1}{|N_{i}|}\sum_{j\inN_{i}}x_{j},\quad\sigma_{i}^{2}=\frac{1}{|N_{i}|}\sum_{j\inN_{i}}(x_{j}-\mu_{i})^{2}",where \(\mu_{i}\) is the mean of the aggregated representation for node \(v_{i}\) and \(\sigma_{i}^{2}\) is the variance of the aggregated representation for node \(v_{i}\).
neurips_2024_oral_16,3,"\tilde{h}_{i}=Trans_{R^{d}\toS^{k}}(h_{i})=\frac{h_{i}}{Max(\|h_{i}\|_{2},\varepsilon)},\quadS^{k}=\{\tilde{h}_{i}:\|\tilde{h }_{i}\|_{2}=1\}","where \(\mathbf{h}_{i}\) is representation for node \(v_{i}\in\mathcal{V}\), generated by the target encoder, \(\|\tilde{\mathbf{h}}_{i}\|_{2}=(\sum_{j=1}^{k}\tilde{\mathbf{h}}_{ij}^{2})^{ \frac{1}{2}}\), and \(\epsilon\) is a small value to avoid division by zero",\tilde{h}_{i}=\frac{h_{i}}{\|h_{i}\|_{2}+\epsilon},"where \(\mathbf{h}_{i}\) is representation for node \(v_{i}\in\mathcal{V}\), generated by the target encoder, \(\|\tilde{\mathbf{h}}_{i}\|_{2}=(\sum_{j=1}^{k}\tilde{\mathbf{h}}_{ij}^{2})^{\frac{1}{2}}\), and \(\epsilon\) is a small value to avoid division by zero."
neurips_2024_oral_16,4,"L_{scattering}=-\frac{1}{n}\sum\nolimits_{i=1}^{n}\|\tilde{h}_{i}-c\|_{2}^{2},\quadc=\frac{1}{n}\sum\nolimits _{i=1}^{n}\tilde{h}_{i}",,L_{scattering}=\frac{1}{n}\sum_{i=1}^{n}\|\tilde{h}_{i}-c\|_{2}^{2},"where \(\tilde{\mathbf{h}}_{i}\) is the normalized representation for node \(v_{i}\), and \(\mathbf{c}\) is the scattered center in the subspace \(\mathbb{S}^{k}\)."
neurips_2024_oral_16,5,H_{online}^{topology}=\hat{A}^{k}H_{online}+H_{online},where k represents the order of neighbors and \(\hat{\mathbf{A}}=\mathbf{A}+\mathbf{I}\) is the adjacency matrix with self-loops,H_{target}^{topology}=\hat{A}^{k}H_{online}=AH_{online}+H_{online},where \(k\) represents the order of neighbors and \(\hat{\mathbf{A}}=\mathbf{A}+\mathbf{I}\) is the adjacency matrix with self-loops.
neurips_2024_oral_16,6,"L_{alignment}=-\frac{1}{N}\sum_{i=1}^{N}\frac{Z_{(online,i)}^{\top}H_{(target,i)}}{\|Z_{(\text {online},i)}\|\|H_{(target,i)}\|}","where \(\mathbf{Z}_{\text{online}}\) and \(\mathbf{H}_{\text{target}}\) represent the predicted and scattered representations, respectively",L_{alignment}=\frac{1}{n}\sum\nolimits_{i=1}^{n}\|Z_{online}-H_{target}\|_{2}^{2},"where \(\mathbf{Z}_{\text{online}}\) and \(\mathbf{H}_{\text{target}}\) represent the predicted and scattered representations, respectively"
neurips_2024_oral_16,7,\phiarrow\tau\phi+(1-\tau)\theta,"where \(\tau\) is a target decay rate and \(\tau\in[0,1]\)",\phi\leftarrow\tau\phi+(1-\tau)\theta,"where \(\phi\) represents the parameters of the target encoder, \(\theta\) represents the parameters of the online encoder, and \(\tau\) is a target decay rate with \(\tau\in[0,1]\)."
neurips_2024_oral_17,1,"L_{cons}=\frac{1}{|D_{U}|}\sum_{x\inD_{U}}H\Big{(}p_{m}(y\mid f_{\theta}(\alpha(x))),p_{m}(y\mid f_{\theta}(A(x)))\Big{)}",,"L_{consistency}=E_{(x, y)\sim D}\left[L_{pred}(f(x), y)+\lambdaL_{aug}(f(T(x)), f(T'(x)))\right]","Where \(\mathcal{L}_{\text{consistency}}\) is the overall consistency loss, \(D\) is the dataset containing labeled and unlabeled data, \(f\) is the model function, \(T\) and \(T'\) are different augmentation transformations, \(\mathcal{L}_{\text{pred}}\) is the prediction loss, and \(\lambda\) is a weighting factor for the augmentation loss \(\mathcal{L}_{\text{aug}}\)."
neurips_2024_oral_17,2,"L_{cons}=\frac{1}{|D_{U}|}\sum_{x\inD_{U}}d\Big{(}p_{m}(y\mid f_{\theta}(\alpha(x ))-\Delta\Re),p_{m}(y\mid f_{\theta}(A(x)) )\Big{)}",,L_{align}=\frac{1}{|D_{U}|}\sum_{x\inD_{U}}\| f_{\theta}(\alpha(x))-f_{\theta}(A(x))\|^2,"where \(\mathcal{L}_{\text{align}}\) represents the alignment loss, and \(\|\cdot\|\) denotes the Euclidean norm, facilitating the synchronization of teacher and student representations."
neurips_2024_oral_17,3,"E(\xi;X)=\frac{1}{2}\xi^{\top}\xi-\rm{lse}(X^{\top}\xi,\beta)+c,\quadwith \rm{lse}(v,\beta):=\beta^{-1}\log(\sum_{i=1}^{N}\exp(v_{i}))",,"E(\xi,X)=-\frac{1}{N}\sum_{i=1}^{N}\langle\xi,x_{i}\rangle^2","The energy function \(E(\mathbf{\xi}, \mathbf{X})\) measures the similarity between a query pattern \(\mathbf{\xi}\) and stored patterns \(\mathbf{X}\), where \(N\) is the number of stored patterns and \(\langle \cdot, \cdot \rangle\) denotes the inner product."
neurips_2024_oral_17,4,\xiarrow\xi-\eta\nabla_{\xi}E(\xi;X)=\xi-\rm{sm}(\beta\xi^{\top}X)X^{\top},,\xi_{new}=\xi-\eta\nabla E(\xi;X)=\xi-\nabla E(\xi;X),"The updated state pattern \(\mathbf{\xi}_{\text{new}}\) after minimizing the energy function \(E\) through gradient descent, where \(\eta\) is the step size set to 1."
neurips_2024_oral_17,5,"E(Q_{s};K_{t})=\frac{\alpha}{2}diag(K_{t}K_{t}^{T})-\sum_{i=1}^{N}\rm{lse}(Q_{s}k_{t,i}^{T},\beta)+c || E(K_{t})=\rm{lse}(\frac{1}{2}diag(K_{t}K_{t}^{T}),1)=\log\sum_{i=1}^{N}\exp(\frac{ 1}{2}k_{t,i}k_{t,i}^{T})+c",,"E_{s}(Q_{s},K_{s})=\frac{1}{2}\|Q_{s}\|^{2}-lse(K_{s}^{\top}Q_{s},\beta)+c,\quadwith lse(v,\beta) :=\beta^{-1}\log\left(\sum_{i=1}^{N}\exp(v_{i})\right)","The energy function \(E_{s}(\mathbf{Q}_{s}, \mathbf{K}_{s})\) models the dynamics between student and teacher learning patterns, where \(\mathbf{Q}_{s}\) and \(\mathbf{K}_{s}\) are the query and key matrices derived from student representations, respectively."
neurips_2024_oral_17,6,p(K_{t}|Q_{s})=\frac{p(Q_{s}|K_{t})p(K_{t})}{p(Q_{s})},,K_{t}^{*}=\arg\max_{K_{t}}P(K_{t}\midQ_{s})=\arg\max_{K_{t}}P(Q_{s}\midK_{t})P(K_{t}),"The optimal teacher keys \(\mathbf{K}_{t}^{*}\) are derived by maximizing the posterior probability given the observed student queries \(\mathbf{Q}_{s}\), where \(P(\mathbf{Q}_{s}\mid \mathbf{K}_{t})\) is the likelihood and \(P(\mathbf{K}_{t})\) is the prior distribution of the teacher keys."
neurips_2024_oral_17,7,\nabla_{K_{t}}\log p(K_{t}|Q_{s})&=-(\nabla_{K_{t}}E(Q_{s};K_{t})+\nabla_{K_{t}}E(K_{t}))\\&=sm(\betaQ_{s}K_{t}^{T})Q_ {s}-(\alphaI+D(sm(\frac{1}{2}diag(K_{t}K_{t}^{T}))))K_{t},,\nabla_{K_{t}}\log p(K_{t}|Q_{s})=\nabla_{K_{t}}\log p(Q_{s}|K_{t})+\nabla_{K_{t}}\log p(K_{t})-\nabla_{K_{t}}\log p(Q_{s}),"where the gradients of the log posterior are derived from the energy functions defined in Equations 5a and 5b, facilitating the optimization of the teacher's representation based on the observed student queries."
neurips_2024_oral_17,8,"K_{t}^{update}=K_{t}+\gamma_{update}[\,(sm(\betaKQ^{T})QW_{K}^{T})-\gamma_{reg }(\alphaI+D(sm(\frac{1}{2}diag (KK^{T})))KW_{K}^{T})\, ]",,K_{t}\leftarrowK_{t}-\eta\nabla_{K_{t}}\log p(K_{t}|Q_{s}),where \(\eta\) is the learning rate for updating the teacher keys \(\mathbf{K}_{t}\).
neurips_2024_oral_17,9,L_{rep}^{s}=\frac{1}{|B^{\prime}|}\sum_{i\inB^{\prime}}\frac{1}{|N_{i}|}\sum_{q\inN_{i}}-\log\frac{\exp(z_{i}^{T}z_{q}^{\prime}/\tau_{c})}{\sum_{i^{\prime}\neq i }\exp(z_{i}^{T}z_{i^{\prime}}^{\prime}/\tau_{c})} || L_{rep}^{u}=\frac{1}{|B|}\sum_{i\inB}-\log\frac{\exp(z_{i}^{T}z_{i}^{\prime}/\tau_{u} )}{\sum_{i^{\prime}\neq i}\exp(z_{i}^{T}z_{i^{\prime}}^{\prime}/\tau_ {u})},,p(K_{t}|Q_{s})=\frac{p(Q_{s}|K_{t})p(K_{t})}{p(Q_{s})},where \(p(\mathbf{Q}_{s}|\mathbf{K}_{t})\) and \(p(\mathbf{K}_{t})\) are modeled by energy functions.
neurips_2024_oral_17,10,"L_{cons}=\frac{1}{|B|}\sum_{i\inB }\ell(q^{\prime}_{i},p_{i})-\varepsilon H(\bar{p})&for unlabeled,\\\frac{1}{|B|}\sum_{i\inB}\ell(y_{i},p_{i})&for labeled.",,"L_{rep}=\ell(q^{\prime},p)=(1-\lambda)L_{rep}^{u}+\lambdaL_{rep}^{s}","where \(\mathcal{L}_{\text{rep}}^{s}\) and \(\mathcal{L}_{\text{rep}}^{u}\) are the supervised and unsupervised contrastive losses, respectively, and \(\lambda\) is a balancing hyperparameter."
neurips_2024_oral_18,1,G(x)=\exp(-\frac{1}{2}(x-\mu_{0})^{T}\Sigma_{0}^{-1}(x-\mu_{0})),"where \(\Sigma_{0}\) can be factorized as \(\Sigma_{0}=R_{0}S_{0}S_{0}^{T}R_{0}^{T}\), in which \(R_{0}\) is a rotation matrix represented by a quaternion vector \(r_{0}\in\mathbb{R}^{4}\), and \(S_{0}\) is a a diagonal scaling matrix characterized by a 3D vector \(s_{0}\in\mathbb{R}^{3}\)","p=\left(\mu_{0},\Sigma_{0},\sigma, c\right)","where \(\Sigma_{0}\) can be factorized as \(\Sigma_{0}=R_{0}S_{0}S_{0}^{T}R_{0}^{T}\), in which \(R_{0}\) is a rotation matrix represented by a quaternion vector \(r_{0}\in\mathbb{R}^{4}\), and \(S_{0}\) is a diagonal scaling matrix characterized by a 3D vector \(s_{0}\in\mathbb{R}^{3}\)."
neurips_2024_oral_18,2,"I(u)=\sum_{i\in N}T_{i}\alpha_{i}c_{i},\qquad A(u)=\sum_{i\in N}T_{i}\alpha_{i },\qquad D(u)=\sum_{i\in N}T_{i}\alpha_{i}d_{i}","where \(T_{i}=\prod_{j=1}^{i-1}(1-\alpha_{j})\) is the accumulated transmittance, \(\alpha_{i}\) is the probability of termination at point \(i\), and \(d_{i}\) is the depth of the Gaussian point at the specific view",D(u)=\sum_{i=1}^{N} T_{i}\cdot d_{i}\cdot\alpha_{i},"where \(T_{i}=\prod_{j=1}^{i-1}(1-\alpha_{j})\) is the accumulated transmittance, \(\alpha_{i}\) is the probability of termination at point \(i\), and \(d_{i}\) is the depth of the Gaussian point at the specific view."
neurips_2024_oral_18,3,"\mu(t)=\mu_{0}+\sum_{i=1}^{N_{m}}w_{i}(\mu_{0},t)d\mu_{i}(t),\qquad s(t)=s_{0}+\sum_{i=1}^{N_{m}}w_{i}(\mu_{0},t)ds_{i}(t)",,"d\mu_{i}(t)=\sum_{j=1}^{N_{m}} w_{j}(\mu_{0},t)\cdot\mu_{j},\qquad ds_{i}(t)=\sum_{j=1}^{N_{m}} w_{j}(\mu_{0},t)\cdot s_{j}","where \(d\mu_{i}(t)\) is the deformation at position \(i\) at time \(t\), \(ds_{i}(t)\) is the scale change at position \(i\) at time \(t\), \(w_{j}(\mu_{0},t)\) are the motion coefficients for the \(j\)-th basis, and \(\mu_{j}\) and \(s_{j}\) are the deformation and scale for the \(j\)-th motion basis, respectively."
neurips_2024_oral_18,4,"L_{gs}=L_{1}(I,\tilde{I})+\lambda_{1}L_{ssim}(I,\tilde{I})+\lambda_{2}L_{1}(s(t))",where \(\lambda_{1}\) and \(\lambda_{2}\) are balancing hyperparameters,"L=\lambda_{1}\|I-\tilde{I}\|_{1}+\lambda_{2}SSIM(I,\tilde{I})+\|s-s_{0}\|_{1}","where \(L\) is the overall loss function, \(I\) is the rendered image, \(\tilde{I}\) is the ground truth image, \(\lambda_{1}\) and \(\lambda_{2}\) are balancing hyperparameters, and \(s\) and \(s_{0}\) are the current and initial scale attributes, respectively."
neurips_2024_oral_18,5,"P_{\tilde{P}}=\{(\tilde{p},s_{\Delta x},\sigma_{F})\}","where \(\tilde{p}\in\tilde{P}\), \(s_{\Delta x}=\Delta x/2^{n_{u}}\), and \(\sigma_{F}=F[Discretize(\tilde{p})]\) (we neglect \(t\) in the notation for simplicity)","\tilde{P}(t)=\{(\tilde{p},s_{\Delta x},\sigma_{F})|\tilde{p}\in\tilde{P}\}","where \(\tilde{p}\in\tilde{P}\), \(s_{\Delta x}=\Delta x/2^{n_{u}}\), and \(\sigma_{F}=F[Discretize(\tilde{p})]\) (we neglect \(t\) in the notation for simplicity)"
neurips_2024_oral_18,6,"L_{ppe}=\frac{1}{m}\sum_{i=1}^{m}[L_{CD}(S(t_{i}),\tilde{S} (t_{i}))+\frac{1}{n}\sum_{j=1}^{n}L_{1}(A_{j}(t_{i}),\bar{A}_{j}(t_{ i}))]","where \(\mathcal{L}_{CD}\) and \(\mathcal{L}_{1}\) are chamfer distance and L1 norm respectively, \(S(t_{i})\) denotes the simulated surface at time \(t_{i}\), \(A_{j}(t_{i})\) is the rendered mask at view \(j\), and \(\bar{A}_{j}(t_{i})\) represents the object mask of the image extracted from video \(V_{j}\) at time \(t_{i}\)","L_{rollout}=L_{CD}(S(t),\tilde{S}(t))+\sum_{j=1}^{n}L_{1}(A_{j}(t_{i}),\bar{A}_{j}(t_{i}))","where \(\mathcal{L}_{CD}\) and \(\mathcal{L}_{1}\) are chamfer distance and L1 norm respectively, \(S(t_{i})\) denotes the simulated surface at time \(t_{i}\), \(A_{j}(t_{i})\) is the rendered mask at view \(j\), and \(\bar{A}_{j}(t_{i})\) represents the object mask of the image extracted from video \(V_{j}\) at time \(t_{i}\)"
neurips_2024_oral_2,1,"\operatorname*{minimize}_{\theta}L(\theta;D_{SFT})=-E_{(x,y)\simD_{SFT}}[\log\pi_{\theta}(y|x)]",,L_{SFT}=-\frac{1}{N}\sum_{i=1}^{N}\log\pi_{\theta}^{SFT}(y^{(i)} |x^{(i)}),"Loss function for Supervised Fine-Tuning (SFT), where \(\mathcal{L}_{\text{SFT}}\) is the average negative log-likelihood of the target answers given the inputs."
neurips_2024_oral_2,2,"\pi^{\prime}(y_{c}|x)=\sum_{y_{k}}\mu_{\phi}(y_{c}|y_{k},x)\pi_{\theta}(y_{k}|x)\geqslant\mu_{\phi} (y_{c}|y_{o},x)\pi_{\theta}(y_{o}|x)",where \(\mathbf{y}_{k}\) is a possible answer generated by upstream LLM \(\pi_{\mathbf{\theta}}\),y_{c}=\mu_{\phi}(y_{o}|x)+y_{k},where \(\mathbf{y}_{k}\) is a possible answer generated by upstream LLM \(\pi_{\mathbf{\theta}}\)
neurips_2024_oral_2,3,"-E_{M}[\log\pi^{\prime}(y_{c}|x)]\leqslant-E_{M}[\log\mu_{\phi}(y_{c}|y_{o},x)]-E_{M}[\log\pi_{\theta}(y_{o}|x)]",,"L_{Aligner}(\phi;M)=-E_{(x,y_{o},y_{c})\simM}[\log\mu_{\phi}(y_{c}|y_{o},x)]","where \(\mathcal{L}_{\text{Aligner}}\) is the loss function for the _Aligner_ model, \(\mathbf{\phi}\) are the parameters of the model, and \(\mathcal{M}\) is the dataset containing user queries and corresponding original and corrected answers."
neurips_2024_oral_2,4,"\operatorname*{minimize}_{\phi}L_{Aligner}(\phi,M)=-E_{M}[\log\mu_{\phi}(y_{c} |y_{o},x)]",,"\operatorname*{minimize}_{\phi}L(\phi;M)=-E_{M}[\log\mu_{\phi}(y_{c}|y_{o},x)]","The training objective for the _Aligner_ model, where \(\mathbf{\phi}\) represents the parameters of the _Aligner_ model and \(\mathcal{M}\) is the dataset used for training."
neurips_2024_oral_21,1,"q(X_{V}^{t}\midX_{V}^{t-1})=Cat(X_{V}^{t};p=X_{V}^{t-1}Q_{V}^{t}),\\q(X_{E}^{t}\midX_{E}^{t-1})=Cat(X_{E}^ {t};p=X_{E}^{t-1}Q_{E}^{t})",where \(\mathrm{Cat}(\mathbf{X};\mathbf{p})\) denotes sampling from a categorical distribution with probability \(\mathbf{p}\),q(G^{t}\mid G^{t-1})=Cat(X;p),where \(\mathrm{Cat}(\mathbf{X};\mathbf{p})\) denotes sampling from a categorical distribution with probability \(\mathbf{p}\)
neurips_2024_oral_21,2,p_{\theta}(\tilde{G}^{0}\mid G^{t})=\prod_{v\in V}p_{\theta}(v^{t-1}\mid G^{t})\prod_{e\in E}p_{\theta}(e^{t-1}\mid G^{t}),,p_{\theta}(\tilde{G}^{0}\mid G^{t})=\prod_{i\in V}p_{\theta}(\tilde{X}_{V}^{0}\mid X_{V}^{t})\prod_{j\in E}p_{\theta}(\tilde{X}_{E}^{0}\mid X_{E}^{t}),where \(p_{\theta}(\tilde{G}^{0}\mid G^{t})\) denotes the predicted probability of the initial state given the noisy state at timestep \(t\)
neurips_2024_oral_21,3,"p_{\theta}(v^{t-1}\mid G^{t})=\sum_{\tilde{v}\in\tilde{x}_{v}}q(v^{t-1 }\mid\tilde{v},G^{t})p_{\theta}(\tilde{v}\mid G^{t})",,p_{\theta}(G^{t-1}\mid G^{t})=\prod_{v\in V}\sum_{\tilde{v}\in\tilde{x}_{v}} p_{\theta}(v^{t-1}=\tilde{v}\mid G^{t})\prod_{e\in E}\sum_{\tilde{e}\in\tilde{x}_{e}} p_{\theta}(e^{t-1}=\tilde{e}\mid G^{t}),where \(\tilde{\mathbf{x}}_{v}\) denotes the predicted node types and \(\tilde{\mathbf{x}}_{e}\) denotes the predicted edge types.
neurips_2024_oral_21,4,L=E_{q(G^{0})}E_{q(G^{t}|G^{0})}[-E_{x\in G^{0}}\log p_{\theta}(x\mid G^{t})],,L(\theta)=-E_{G^{t}\sim\pi(G)}\left[\log p_{\theta}(\tilde{G}^{0}\mid G^{t})\right],"The negative log-likelihood loss function for training the neural network, where \(\mathcal{L}(\theta)\) is the loss, \(G^{t}\) is sampled from the distribution \(\pi(G)\), and \(p_{\theta}(\tilde{G}^{0}\mid G^{t})\) is the predicted distribution of the original graph given the noisy graph at timestep \(t\)."
neurips_2024_oral_21,5,Q_{G}=Q_{V}&1_{N}^{\prime}\otimesQ_{VE}\\1_{N}\otimesQ_{EV}&1_{N\times N}\otimesQ_{ E},"where \(\otimes\) denotes the Kronecker product, \(\mathbf{1}_{N}\), \(\mathbf{1}_{N}^{\prime}\), and \(\mathbf{1}_{N\times N}\) represent the column vector, row vector, and matrix with all 1 elements, respectively",Q_{G}=Q_{V} &Q_{EV}\\Q_{VE} &Q_{E}=Q_{V} &1_{N}1_{N}^{\prime}\otimesQ_{EV}\\1_{N}1_{N}^{\prime}\otimesQ_{VE} &Q_{E},"where \(\otimes\) denotes the Kronecker product, \(\mathbf{1}_{N}\), \(\mathbf{1}_{N}^{\prime}\), and \(\mathbf{1}_{N\times N}\) represent the column vector, row vector, and matrix with all 1 elements, respectively"
neurips_2024_oral_21,6,q(X_{G}^{t}\midX_{G}^{t-1})=\widetilde{Cat}(X_{G}^{t};\tilde{p}=X_{G}^{t-1}Q_{G}^{t} ),"where \(\tilde{\mathbf{p}}\) is the unnormalized probability and \(\widetilde{\mathrm{Cat}}\) denotes categorical sampling: The first \(F_{V}\) columns of \(\tilde{\mathbf{p}}\) are normalized to sample \(\mathbf{X}_{V}^{t}\), while the remaining \(N\cdot E\) dimensions are reshaped and normalized to sample edges \(\mathbf{X}_{E}^{t}\)","\tilde{p}=X_{V}^{t-1}Q_{V}^{t}\\reshape(X_{E}^{t-1}Q_{E}^{t}),}\) is the unnormalized probability and\(\widetilde{Cat}\) denotes categorical sampling: The first\(F_{V}\) columns of\(\tilde{p}\) are normalized to sample\(X_{V}^{t}\), while the remaining\(N\cdot E\) dimensions are reshaped and normalized to sample edges\(X_{E}^{t}\).</description>","where \(\tilde{\mathbf{p}}\) is the unnormalized probability and \(\widetilde{\mathrm{Cat}}\) denotes categorical sampling: The first \(F_{V}\) columns of \(\tilde{\mathbf{p}}\) are normalized to sample \(\mathbf{X}_{V}^{t}\), while the remaining \(N\cdot E\) dimensions are reshaped and normalized to sample edges \(\mathbf{X}_{E}^{t}\)."
neurips_2024_oral_21,7,"\hat{p}_{\theta}(G^{t-1}\mid G^{t},C)=\log p_{\theta}(G^{t-1}\mid G^{ t})+s(\log p_{\theta}(G^{t-1}\mid G^{t},C)-\log p_{\theta}(G^{t-1}\mid G^{t}))",where \(s\) denotes the scale of conditional guidance,"\hat{p}_{\theta}(G^{t-1}\mid G^{t},C)=\log p_{\theta}(G^{t-1}\mid G^{t})+s\log q(C\mid G^{0})",where \(s\) denotes the scale of conditional guidance
neurips_2024_oral_21,8,"AdaLN(h,c)=\gamma_{\theta}(c)\odot\frac{h-\mu(h)}{\sigma(h )}+\beta_{\theta}(c)",where \(\mu(\cdot)\) and \(\sigma(\cdot)\) are mean and variance values,H=\frac{H-\mu(H)}{\sigma(H)}\cdot\gamma+\beta,where \(\mu(\cdot)\) and \(\sigma(\cdot)\) are mean and variance values
neurips_2024_oral_21,9,"AdaLN_{gate}(h,c)=\alpha_{\theta}(c)\odotAdaLN(h,c)",,"AdaLN_{gate}(h,c)=\gamma_{\theta}(c)\odot\frac{h-\mu(h)}{\sigma(h)}+\beta_{\theta}(c)+h",where \(\mathrm{AdaLN}_{gate}\) is a gated variant of adaptive layer normalization for residual connections
neurips_2024_oral_21,10,"\tilde{X}_{G}^{0}=AdaLN(MLP(H),c)",,\tilde{X}_{V}^{0}\\\tilde{X}_{E}^{0}=MLP(H),"where \(\tilde{\mathbf{X}}_{V}^{0}\) and \(\tilde{\mathbf{X}}_{E}^{0}\) are the predicted probabilities for nodes and edges, respectively"
neurips_2024_oral_22,1,"dx_{\sigma}\,=\,-\sigma\nabla_{x_{\sigma}}\!\log p (x_{\sigma};\sigma)\;d\sigma",,"p(x;\sigma)=p_{data}(x)*N(x;0,\sigma^{2}I)",The smoothed density \(p(\mathbf{x};\sigma)\) as a convolution of the data distribution \(p_{\text{data}}(\mathbf{x})\) and a Gaussian noise term.
neurips_2024_oral_22,2,"\theta\,=\,\arg\min_{\theta}E_{y\sim p_{data},\sigma\sim p_{train},n\simN(0,\sigma^{2}I)}\|D_{\theta}(y+n;\sigma)-y\|_{2}^ {2}",where \(p_{\text{train}}\) controls the noise level distribution during training,D_{\theta}(x;\sigma)\approx\nabla_{x}\log p(x;\sigma),where \(p_{\text{train}}\) controls the noise level distribution during training
neurips_2024_oral_22,3,"D_{w}(x;\sigma,c)\,=\,wD_{1}(x;\sigma,c)+( 1-w)D_{0}(x;\sigma,c)",,"x_{guided}=D_{0}(x;\sigma,c)+w\left( D_{1}(x;\sigma,c)-D_{0}(x;\sigma,c)\right)",where \(w\) is the guidance scale that controls the strength of the guidance effect.
neurips_2024_oral_22,4,"D_{w}(x;\sigma,c)\approxx+\sigma^{2}\nabla_{x}\log\underbrace{(p_ {0}(x|c;\sigma)[\frac{p_{1}(x|c;\sigma)}{p_{0}(x|c;\sigma)}]^{w})}_{\propto:\p_{w}(x|c;\sigma)}",,"D_{w}(x;\sigma,c)\,=\,D_{1}(x;\sigma,c)+\left(w-1\right)\left(D_{1}(x;\sigma,c)-D_{0}(x;\sigma,c)\right)","where \(D_{w}(\mathbf{x};\sigma,\mathbf{c})\) is the output of the guided denoiser, and \(w\) is the guidance scale factor."
neurips_2024_oral_22,5,\nabla_{x}\log p_{w}(x|c;\sigma)=\nabla_{x}\log p_{1}(x|c;\sigma)+(w-1)\nabla_{x}\log\frac{p_{1}(x|c;\sigma)}{p_{0}(x|c;\sigma)},,\nabla_{x}\log p_{w}(x|c;\sigma)\approx\nabla_{x}\log p_{0}(x|c;\sigma)+w\left(\nabla_{x}\log p_{1}(x|c;\sigma)-\nabla_{x}\log p_{0}(x|c;\sigma)\right),"where \(p_{0}\) and \(p_{1}\) are the densities associated with the two denoiser networks \(D_{0}\) and \(D_{1}\), respectively, and \(w\) is the guidance weight."
neurips_2024_oral_23,1,"|Cov(f(X),Y\mid X\in S)|\leq\alpha",,"S=\{ x\inX : |f(x)-E[Y | X=x]|\leq\alpha,\forall f\inF\}",A set \(S\) is defined as the collection of inputs \(x\) in the feature space \(\mathcal{X}\) such that the absolute difference between the prediction \(f(x)\) and the expected value of the outcome \(Y\) given \(X = x\) is less than or equal to a threshold \(\alpha\) for all functions \(f\) in the class \(\mathcal{F}\).
neurips_2024_oral_23,2,"\gamma^{*},\beta^{*}\in\operatorname*{arg\,min}_{\gamma\inR^{K},\beta\inR^{K}}\;E[(Y-\gamma_{J(X)}+\beta_{J(X)}\hat{Y})^{2}]",,"\gamma^{*}=\left(Cov_{k}(Y,\hat{Y})\right)_{k\in[K]},\quad\beta^{*}=\left(E_{k}[Y]-E_{k}[\hat{Y}]\right)_{k\in[K]}",Definitions of \(\gamma^{*}\) and \(\beta^{*}\) as vectors representing the conditional covariances and differences in expectations for each subset \(S_k\) in the multicalibrated partition.
neurips_2024_oral_23,3,"E_{k}[(Y-\gamma_{k}^{*}-\beta_{k}^{*}\hat{Y})^{2} ]+4Cov_{k}(Y,\hat{Y})^{2}\leqE_{k}[(Y-f(X) )^{2}]+2\alpha",,E[(Y-f(X))^{2}\mid J(X)=k]\geqE[(Y-\gamma^{*}_{k}+\beta^{*}_{k}\hat{Y})^{2}],Expected squared error of any predictor \(f\) given \(J(X) = k\) is at least as large as the expected squared error of the optimal predictor based on human judgment within the same subset.
neurips_2024_oral_23,4,E_{S}[(Y-g(H))^{2}]\leqE_{S}[(Y-\gamma-\beta g(H))^{2}]+\eta,,g(\beta)=E[Y\mid H=\beta]-\gamma,"Function \(g\) representing the expected value of \(Y\) given expert feedback \(H\) equal to \(\beta\), adjusted by a parameter \(\gamma\)."
neurips_2024_oral_23,5,"E_{S}[(Y-g(H))^{2}]+4Cov_{S}(Y,g(H))^{2}\leqE_ {S}[(Y-f(X))^{2}]+2\alpha+\eta",,"E_{S}[(Y-g(H))^{2}]+4Cov_{S}(Y,g(H))^{2}\leqE_{S}[(Y-f(X))^{2}]+2\alpha+\eta",Squared error incurred by the function \(g(H)\) predicting \(Y\) within the indistinguishable subset \(S\) compared to any \(f\in\mathcal{F}\).
neurips_2024_oral_23,6,"|Cov_{k}(Y,\hat{Y})|\leq\sqrt{\frac{\alpha}{2}}",,E_{k}[(Y-\hat{Y})^{2}]\geqE_{k}[(Y-\tilde{f}_{k}(X))^{2}]+2\alpha,Equation representing the lower bound on the expected squared error of the prediction \(\hat{Y}\) compared to a function \(\tilde{f}_{k}\) within an \(\alpha\)-multicalibrated partition for a binary-valued model class.
neurips_2024_oral_24,1,y^{\prime}=y+\Delta y=W_{0}x+BAx,where \(y\in R\)d is the output and the \(x\in R\)k denotes the input,y=W_{0}x+A(Bx),where \(y \in \mathbb{R}^{d}\) is the output and \(x \in \mathbb{R}^{k}\) denotes the input
neurips_2024_oral_24,2,"W&=\,W_{0}+\Delta\,W\\&=\,W_{0}+\sum_{i=1}^{N}\omega_{i}\cdot B_{i}A",,y^{\prime}=y+\Delta y=W_{0}x+\sum_{i=1}^{n}B_{i}A_{i}x,"where \(y^{\prime}\) is the updated output, \(y\) is the original output, \(\Delta y\) is the change in output, \(W_{0}\) is the frozen weight matrix, \(x\) is the input, \(B_{i}\) are the distinct matrices for each task, and \(A_{i}\) is the shared matrix across tasks."
neurips_2024_oral_24,3,y=W_{0}x+\sum_{i=1}^{N}\omega_{i}E_{i}Ax\quad(MoE),"where \(N\) denotes the number of experts, i",\Delta W=\sum_{i=1}^{N} E_{i}\cdot B_{i} A,"where \(N\) denotes the number of experts, \(E_{i}\) represents the expert weights, \(B_{i}\) are the distinct matrices, and \(A\) is the shared matrix."
neurips_2024_oral_24,4,\omega_{i}=softmax(W_{g}^{T}x)\quad(Router),,"\omega_{i}=\frac{\exp\left(W_{g}^{T}x\right)}{\sum_{j=1}^{N}\exp\left(W_{g}^{T}x\right)}\quadfor  i=1,\ldots,N","where \(\omega_{i}\) represents the gating score for expert \(E_{i}\), \(\mathit{W}_{g}\) is the transformation matrix, and \(N\) denotes the number of experts."
neurips_2024_oral_25,1,"{l}X=Y\beta_{Y\to X}+G^{\intercal}\gamma_{X}+\varepsilon_{X},\\Y=X\beta_{X\to Y}+G^{\intercal}\gamma_{Y}+\varepsilon_{Y}",where \(\beta_{Y\to X}\) is the causal effect of \(Y\) on \(X\) and \(\beta_{X\to Y}\) is that of \(X\) on \(Y\),X_{i}=\beta_{Y\to X} Y_{i}+G_{i}^{\intercal}\gamma_{X}+\varepsilon_{X_{i}},"where \( \beta_{Y\to X} \) is the causal effect of \( Y \) on \( X \), \( \mathbf{G}_{i} \) are the genetic variants, \( \boldsymbol{\gamma}_{X} \) are the coefficients for the genetic variants affecting \( X \), and \( \varepsilon_{X_{i}} \) is the error term for \( X \)."
neurips_2024_oral_25,2,"{l}X=(G^{\intercal}\gamma_{X}+G ^{\intercal}\gamma_{Y}\beta_{Y\to X}+\varepsilon_{X}+\varepsilon_{Y}\beta_{Y\to X})\Delta,\\Y=(G^{\intercal}\gamma_{X}\beta_{X\to Y}+G^{\intercal}\gamma_{Y}+\varepsilon_{X}\beta_{X\to Y}+\varepsilon_{Y})\Delta",,"{l}X=\Delta\left(Y\beta_{Y\to X}+G^{\intercal}\gamma_{X}+\varepsilon_{X}\right),\\Y=\Delta\left(X\beta_{X\to Y}+G^{\intercal}\gamma_{Y}+\varepsilon_{Y}\right)",where \(\Delta\) is a scaling factor based on the causal effects between \(X\) and \(Y\)
neurips_2024_oral_25,3,\hat{\beta}_{X\to Y}=[X^{\intercal}PX]^{-1}X^{\intercal}PY=\beta_{X\to Y},,\hat{\beta}_{X\to Y}=\frac{G_{V}^{X\to Y\intercal}Y}{G_{V}^{X\to Y\intercal}X},where \(\hat{\beta}_{X\to Y}\) is the estimated causal effect of \(X\) on \(Y\) using the valid IVs \(\mathbf{G}_{\mathcal{V}}^{X\to Y}\).
neurips_2024_oral_25,4,\hat{\beta}_{X\to Y}=[X^{\intercal}\tilde{P}X]^{-1}X^{\intercal}\tilde{P}Y=\beta_{X\to Y}+\underbrace{[X^{\intercal}\tilde{P}X ]^{-1}X^{\intercal}\tilde{P}(G^{\intercal}\gamma_{Y}+\varepsilon_{Y})}_{\beta_{bias}},,\hat{\beta}_{X\to Y}=[X^{\intercal}PX]^{-1}X^{\intercal}PY+Bias,where \text{Bias} represents the bias introduced by the inclusion of invalid IVs in the candidate set \(\mathbf{G}\).
neurips_2024_oral_25,5,"corr(Y-X\omega_{\{G_{3}\}},G_{1})=0,\qquadcorr(Y-X\omega_{\{ G_{1}\}},G_{3})=0","where \(\mathrm{corr}(\cdot)\) denotes the Pearson's correlation coefficient between two random variables, and \(\omega_{\{G_{i}\}}=\mathrm{TSLS}(X,Y,\{G_{i}\})\) with \(i\in\{1,3\}\)","corr(X,Y)=\frac{\omega_{\{G_{1}\}}+\omega_{\{G_{3}\}}}{2}","where \(\mathrm{corr}(\cdot)\) denotes the Pearson's correlation coefficient between two random variables, and \(\omega_{\{G_{i}\}}=\mathrm{TSLS}(X,Y,\{G_{i}\})\) with \(i\in\{1,3\}\)"
neurips_2024_oral_25,6,"corr(Y-X\omega_{\{G_{4},G_{5}\}},G_{2})\neq 0,corr(Y-X\omega_{\{ G_{2},G_{5}\}},G_{4})\neq 0,corr(Y-X\omega_{\{G_{2},G_{4}\}},G_{5})\neq 0","where \(\omega_{\{G_{i},G_{j}\}}=\mathrm{TSLS}(X,Y,\{G_{i},G_{j}\})\) with \(i\neq j\) and \(i,j\in\{2,4,5\}\)","corr(Y-X\omega_{\{G_{2},G_{4}\}},G_{5})\neq 0,\qquadcorr(Y-X\omega_{\{G_{2},G_{5}\}},G_{4})\neq 0,\qquadcorr(Y-X\omega_{\{G_{4},G_{5}\}},G_{2})\neq 0","where \(\omega_{\{G_{i},G_{j}\}}=\mathrm{TSLS}(X,Y,\{G_{i},G_{j}\})\) with \(i\neq j\) and \(i,j\in\{2,4,5\}\)"
neurips_2024_oral_25,7,"PR_{(X,Y\,|\,G)}:=Y-X\omega_{G}",,R_{G}=Y-X\hat{\beta}_{X\to Y}.\end{latex>,where \(R_{\mathbb{G}}\) represents the pseudo-residual of the phenotypes \(X\) and \(Y\) relative to the subset of candidate genetic variants \(\mathbb{G}\).
neurips_2024_oral_25,8,"corr(PR_{\langle X,Y\,|\,\{G_{j}\}\rangle},G_{j })\neq 0",,"G_{j} is a valid IV, then corr(Y-X\omega_{G\setminus\{G_{j}\}},G_{j})=0","where \(\omega_{\mathbb{G}\setminus\{G_{j}\}}=\mathrm{TSLS}(X,Y,\mathbb{G}\setminus\{G_{j}\})\)."
neurips_2024_oral_25,9,"corr(PR_{\langle X,Y\,|\,\{G_{2}\}\rangle},G_{1} )=0",,"corr(PR_{\langle X,Y\,|\,\{G_{1}\}\rangle},G_{1})\neq 0","where \(\mathcal{PR}_{\langle X,Y\,|\,\{G_{1}\}\rangle}\) is the pseudo-residual of \((X,Y)\) relative to the set \(\{G_{1}\}\)."
neurips_2024_oral_25,10,"corr(PR_{\langle X,Y\,|\,\{G_{1}\}\rangle}),G_{ 2})=0",,"corr(PR_{\langle X,Y\,|\,\{G_{1}\}\rangle},G_{2} )=0","where \(\mathcal{PR}_{(X,Y\,|\,\{G_{i}\})}\) is the pseudo-residual relative to the subset of genetic variants \(\{G_{i}\}\)."
neurips_2024_oral_25,11,"corr(PR_{\langle X,Y\,|\,\{G_{2}\}\rangle},G_{1} )=\frac{\gamma_{Y,1}\gamma_{X,2}-\gamma_{Y,2}\gamma_{X,1}}{\beta_{Y\to X }\gamma_{Y,2}+\gamma_{X,2}}",,"corr(PR_{\langle X,Y\,|\,\{G_{1}\}\rangle},G_{1})\neq 0","where \(\mathcal{PR}_{(X,Y\,|\,\{G_{1}\})}\) is the pseudo-residual relative to the valid IV \(G_{1}\)."
neurips_2024_oral_25,12,"corr(PR_{\langle X,Y\,|\,\{G_{1}\}\rangle},G_{2} )=\frac{\gamma_{Y,2}\gamma_{X,1}-\gamma_{Y,1}\gamma_{X,2}}{\beta_{Y\to X }\gamma_{Y,1}+\gamma_{X,1}}",,"corr(PR_{\langle X,Y\,|\,\{G_{1}\}\rangle},G_{2} )=\frac{\gamma_{Y,2}\gamma_{X,1}-\gamma_{Y,1}\gamma_{X,2}}{\beta_{X\to Y}\gamma_{X,1}+\gamma_{Y,1}}","where \(\gamma_{Y,1}\) and \(\gamma_{Y,2}\) are the direct effects of genetic variants on \(Y\), and \(\gamma_{X,1}\) and \(\gamma_{X,2}\) are the direct effects of genetic variants on \(X\)."
neurips_2024_oral_25,13,"corr(PR_{\langle X,Y\,|\,\{G_{2}\}\rangle},G_{1} )=0,\qquadcorr(PR_{\langle X,Y\,|\,\{G_{1}\}\rangle },G_{2})=0",,"corr(PR_{\langle X,Y\,|\,\{G_{1},G_{2}\}\rangle},G_{1})=0,\qquadcorr(PR_{\langle X,Y\,|\,\{G_{1},G_{2}\}\rangle},G_{2})=0","where \(\mathcal{PR}_{\langle X,Y\,|\,\{G_{1},G_{2}\}\rangle}\) denotes the pseudo-residual of \((X,Y)\) relative to the set of genetic variants \(\{G_{1},G_{2}\}\)."
neurips_2024_oral_25,14,"corr(PR_{(X,Y\midG\setminus G_{j})},G_{j})=0",,"corr(PR_{\langle X,Y\,|\,\{G_{j}\}\rangle},G_{j})=0,\quad j=1,\ldots,p,_{(X,Y\,|\,\{G_{j}\})}\) is the pseudo-residual relative to the subset of candidate genetic variants\(\{G_{j}\}\).</description>","where \(\mathcal{PR}_{(X,Y\,|\,\{G_{j}\})}\) is the pseudo-residual relative to the subset of candidate genetic variants \(\{G_{j}\}\)."
neurips_2024_oral_26,1,"s_{real}(x_{t},t)=\nabla_{x_{t}}\log p_{real,t}(x_{t})=-\frac{x_ {t}-\alpha_{t}\mu_{real}(x_{t},t)}{\sigma_{t}^{2}}",,"p_{real,t}(x_{t})=\int p_{real}(x)q(x_{t}|x)dx",Distribution of the diffused samples at timestep \(t\).
neurips_2024_oral_26,2,"\nablaL_{BMD}=E_{t}(\nabla_{\theta}KL( p_{fake,t}\|p_{real,t}))=-E_{t}(\int(s_{real}(F(G_{\theta}(z),t),t)-s_{fake}(F(G_{\theta}(z),t),t) )\frac{dG_{\theta}(z)}{d\theta}\,dz)","where \(z\sim\mathcal{N}(0,\mathbf{I})\) is a random Gaussian noise input, \(\theta\) are the generator parameters, \(F\) is the forward diffusion process (i","\nabla_{\theta}L_{DMD}=E_{t}[s_{real}(G(z,t),t)-s_{fake}(G(z,t),t)]","where \(z\sim\mathcal{N}(0,\mathbf{I})\) is a random Gaussian noise input, \(\theta\) are the generator parameters, \(F\) is the forward diffusion process."
neurips_2024_oral_26,3,"L_{reg}=E_{(z,y)}d(G_{\theta}(z),y)","where \(d\) is a distance function, such as LPIPS [54] in their implementation","\nablaL_{reg}=E_{(z,y)}\left[d(G_{\theta}(z),\mu_{real}(z))\right]","where \(d\) is a distance function, such as LPIPS [54] in their implementation."
neurips_2024_oral_26,4,"L_{GAN}=E_{x\sim p_{noise},t\sim[0,T]}[\log D (F(x,t))]+E_{z\sim p_{noise},t\sim[0,T]}[-\log(D(F(G_{\theta}(z ),t)))]","where \(D\) is the discriminator, and \(F\) is the forward diffusion process (i","L_{GAN}=E_{x\sim p_{real}}[\log D(x)]+E_{z\simN(0,I)}[\log(1-D(G_{\theta}(z)))]","where \(D\) is the discriminator, and \(F\) is the forward diffusion process."
neurips_2024_oral_27,1,"\langle S,A,O,I,T,R,\gamma,\Theta\rangle","where \(S\), \(A\) and \(O\) are the sets of states, actions, and observations, respectively","S=(S, A, O, T, R,\gamma)","where \(S\) is the set of states, \(A\) is the set of actions, \(O\) is the set of observations, \(T\) is the state transition function, \(R\) is the reward function, and \(\gamma\) is the discount factor."
neurips_2024_oral_27,2,"PVL^{\theta}(\pi)=\frac{1}{T}\sum_{t=0}^{T}\max(\sum_{k=t}^{T}(\gamma\lambda)^{k-t}\delta_{k}^{\theta},0)","where \(\lambda\) and \(T\) are the GAE discount factor and MDP horizon, respectively","\textit{PLR}^{\perp}(\pi,\theta)=E_{\pi}\left[\sum_{t=0}^{T}\lambda^{t}\left(R(s_{t},a_{t})-V^{\theta}(\pi)\right)\right]","where \(\lambda\) and \(T\) are the GAE discount factor and MDP horizon, respectively"
neurips_2024_oral_27,3,"P(\Gamma\mid\lambda_{\Gamma})=\prod_{j=1}^{J}\sum_{k=1}^{K}\alpha_{k}N (x_{j}\mid\mu_{k},\Sigma_{k})",where \(x_{j}\) is a state-action pair sample from \(\Gamma\),"P(\Gamma|\lambda_{\Gamma})=\prod_{j=1}^{|\Gamma|}\sum_{k=1}^{K}\alpha_{k}N(x_{j};\mu_{k},\Sigma_{k})",where \(x_{j}\) is a state-action pair sample from \(\Gamma\)
neurips_2024_oral_27,4,\textsc{Novelty}_{l_{\theta}}=-\frac{1}{|X_{\theta}|}\logL(X_{\theta}\mid\lambda_{\Gamma})=-\frac{1}{|X_{\theta}|}\sum_{t=1}^{T}\log p(x_{t}\mid\lambda_{\Gamma}),where \(x_{t}\) is a sample state-action pair from \(X_{\theta}\),N^{\theta}=\frac{1}{|X_{\theta}|}\sum_{t=1}^{|X_{\theta}|} P(x_{t}\mid\lambda_{\Gamma}),"where \(N^{\theta}\) is the novelty score of the candidate level \(l_{\theta}\), and \(P(x_{t} \mid \lambda_{\Gamma})\) is the posterior likelihood of the state-action pair \(x_{t}\) under the GMM fitted on the past experiences \(\Gamma\)."
neurips_2024_oral_27,5,P_{S}=\frac{h(S_{i})^{\beta}}{\sum_{j}h(S_{j})^{\beta}},where \(h\) is a prioritization function (e,P_{S}(l_{\theta})=\frac{\textsc{Novelty}_{l_{\theta}}^{\alpha}\cdot\textsc{Regret}_{l_{\theta}}^{\beta}}{\sum_{l_{j}\in L}\textsc{Novelty}_{l_{j}}^{\alpha}\cdot\textsc{Regret}_{l_{j}}^{\beta}},"where \(h\) is a prioritization function (e.g., a weighted sum), and \(\alpha\) and \(\beta\) are hyperparameters controlling the influence of novelty and regret, respectively."
neurips_2024_oral_27,6,P_{replay}=\alpha\cdot P_{N}+(1-\alpha)\cdot P_{R},"where \(P_{N}\) and \(P_{R}\) are the novelty-prioritized probability and regret-prioritized probability respectively, and \(\alpha\) allows us to adjust the weightage of each probability","P_{S}=\frac{P_{N}^{\alpha}P_{R}^{1-\alpha}}{\sum_{j}P_{N,j}^{\alpha}P_{R,j}^{1-\alpha}}","where \(P_{N}\) and \(P_{R}\) are the novelty-prioritized probability and regret-prioritized probability respectively, and \(\alpha\) allows us to adjust the weightage of each probability"
neurips_2024_oral_31,1,y^{*}=y-f_{\theta}(y)\nabla f_{\theta}(y),"where, by definition, \(f_{\theta}(\mathbf{y})\) is the distance between \(\mathbf{y}\) and \(\mathbf{y}^{*}\), and the gradient \(\nabla f_{\theta}(\mathbf{y})\) points in the opposite direction of the shortest path from \(\mathbf{y}\) to the surface of the object and is unit-norm",y^{*}=y-f_{\theta}(y)\cdot\frac{\nabla f_{\theta}(y)}{\|\nabla f_{\theta}(y)\|},"where \(f_{\theta}(\mathbf{y})\) is the signed distance from point \(\mathbf{y}\) to the object surface, and \(\nabla f_{\theta}(\mathbf{y})\) is the gradient of the signed distance function at point \(\mathbf{y}\)."
neurips_2024_oral_31,2,c^{j}_{ik}=n_{ik}-d^{j}_{ik}T_{j}\Big{(}\nabla f_{\theta_{j}}(T^{-1}_{j}(n_{ik}))\Big{)},,c^{j}_{ik}=T^{t}_{j}(n_{ik}-d^{j}_{ik}\nabla f_{\theta_{j}}(T^{-1}_{j}(n_{ik}))),"where \(d^{j}_{ik}\) is the signed distance from the node \(\mathbf{n}_{ik}\) to the surface of object \(O_{j}\), and \(\nabla f_{\theta_{j}}(\mathcal{T}^{-1}_{j}(\mathbf{n}_{ik}))\) is the gradient of the SDF at the transformed position of \(\mathbf{n}_{ik}\)."
neurips_2024_oral_34,1,"x_{0}\simN(0,I)\quadand\quadx_{t+1}=a_{t}x_{t}+\mu(x_{t},t)+\sigma_{t}\epsilon_{t}\quadfor\quad t=0,1,\dots,T-1","where \(\epsilon_{t}\sim\mathcal{N}(0,I)\) and \(\mu(\mathbf{x},t)\) is the output of a neural network","x_{t}=x_{t-1}+\sqrt{\beta_{t}}\epsilon_{t}+\mu(x_{t-1}, t)","where \(\epsilon_{t}\sim\mathcal{N}(0,I)\) and \(\mu(\mathbf{x},t)\) is the output of a neural network"
neurips_2024_oral_34,2,"q(x)=\frac{1}{Z}\exp(-E(x)/\tau),\quad E:X\toR","where \(\tau>0\) is temperature, \(\mathcal{X}\) is the compact domain of data, and \(Z=\int_{\mathcal{X}}\exp(-E(\mathbf{x})/\tau)d\mathbf{x}<\infty\) is the normalization constant","q(x)=\frac{1}{Z}\exp\left(-\frac{E(x)}{\tau}\right),\quad Z=\int_{X}\exp\left(-\frac{E(x)}{\tau}\right) dx","where \(\tau>0\) is temperature, \(\mathcal{X}\) is the compact domain of data, and \(Z\) is the normalization constant."
neurips_2024_oral_34,3,\min_{\pi\in\Pi}KL(\pi(x) || p(x))=\max_{\pi\in\Pi}E _{\pi}[\log p(x)]+H(\pi(x)),"where \(\Pi\) is the set of feasible \(\pi(\mathbf{x})\)'s, and \(\mathcal{H}(\pi)=-\int\pi(\mathbf{x})\log\pi(\mathbf{x})d\mathbf{x}\) is the differential entropy","\min_{\pi\in\Pi} KL(\pi || p)=\min_{\pi\in\Pi}\int\pi(x)\log\frac{\pi(x)}{p(x)} dx,\quadH(\pi)=-\int\pi(x)\log\pi(x)dx","where \(\Pi\) is the set of feasible \(\pi(\mathbf{x})\)'s, and \(\mathcal{H}(\pi)\) is the differential entropy."
neurips_2024_oral_34,4,\min_{\pi\in\Pi}KL(\pi(x) || q(x))\quadand\quad\min_{ q\inQ}KL(p(x) || q(x)),where \(\mathcal{Q}\) is the feasible set of EBMs,\min_{q\inQ}KL(p(x) || q(x))=E_{p}[\log p(x)]-E_{p}[\log q(x)],where \(\mathcal{Q}\) is the feasible set of EBMs.
neurips_2024_oral_34,5,\min_{q\inQ}\max_{\pi\in\Pi}KL(p(x) || q(x))-KL(\pi(x) || q(x)),,\min_{\pi\in\Pi}\max_{q\inQ} KL(p(x) || q(x))+\lambda KL(\pi(x) || q(x)),where \(\lambda > 0\) is a hyperparameter that balances the two KL divergences.
neurips_2024_oral_34,6,KL(\pi_{\phi}(x_{T}) || q_{\theta}(x_{T}))\leq KL(\pi_{\phi}(x_{0:T}) || q_{\theta}(x_{T})\tilde{q}(x_{0:T-1}|x_{T})),,\min_{\phi}E_{\pi_{\phi}(x)}[E_{\theta}(x)/\tau]-H(\pi_{\phi}(x))\leq KL(\pi_{\phi}(x_{T}) || q_{\theta}(x_{T})),where \(\tau>0\) is the temperature and \(\mathcal{H}(\pi_{\phi}(\mathbf{x}))\) is the differential entropy of the diffusion model.
neurips_2024_oral_34,7,"\tilde{q}(x_{0:T-1}|x_{T})=\prod_{t=0}^{T-1}\tilde{q}(x_{t}|x_{t+1}), where \tilde{q}(x_{t}|x_{t+1})=N(x_{t+1},s_{t}^{2}I),\quad s_{t}>0",,"\tilde{q}(x_{0:T-1}|x_{T})=\prod_{t=0}^{T-1}N(x_{t}|\mu_{t}(x_{T}),\Sigma_{t}(x_{T}))",where \(\mu_{t}(\mathbf{x}_{T})\) and \(\Sigma_{t}(\mathbf{x}_{T})\) are the mean and covariance functions parameterizing the conditional Gaussian distributions.
neurips_2024_oral_34,8,\min_{\phi}\mathop{E}_{\pi_{\phi}(x_{0:T})}[E_{\theta}(x_{T})+\tau\sum_{t=0}^{T-1}\log\pi_{\phi}(x_{t+1}|x _{t})+\tau\sum_{t=0}^{T-1}\frac{1}{2s_{t}^{2}} || x_{t+1}-x_ {t} || ^{2}],,\min_{\phi}E_{\pi_{\phi}(x_{0:T})}\left[E_{\theta}(x_{T})-\tau\sum_{t=0}^{T-1}\frac{E_{\theta}(x_{t})}{s_{t}^{2}}\right],"where \(\tau\) is the temperature, \(E_{\theta}(\mathbf{x})\) is the energy function, and \(s_{t}>0\) are the scaling factors for the Gaussian distributions."
neurips_2024_oral_34,9,V_{\psi}^{t}(x_{t})=E_{\pi}[E_{\theta}(x_{T})+\tau\sum_{t^{\prime}=t}^{T-1}\log\pi_{\phi}(x_{t^{\prime}+1}| x_{t^{\prime}})+\sum_{t^{\prime}=t}^{T-1}\frac{\tau}{2s_{t^{\prime}}^{2}} || x_{t^{\prime}+1}-x_{t^{\prime}} || ^{2}\bigg{|}x_{t} ],,V_{\psi}^{t}(x_{t})=E_{\pi_{\phi}}\left[ E_{\theta}(x_{T})+\tau\sum_{k=t}^{T-1}\log\pi_{\phi}(x_{k+1}|x_{k})+\tau\sum_{k=t}^{T-1}\frac{1}{2s_{k}^{2}} || x_{k+1}-x_{k} || ^{2}\bigg|x_{t}\right],"where \(V_{\psi}^{t}(\mathbf{x}_{t})\) is the cost-to-go function at time \(t\), and \(\psi\) are the parameters of the value function."
neurips_2024_oral_34,10,"\min_{\psi}E_{x_{t},x_{t+1}\sim\pi}[(sg[V_{\psi}^{t+1}(x_{t+1})]+\tau\log\pi_{\phi}(x_{t+1}|x _{t})+\frac{\tau}{2s_{t}^{2}} || x_{t}-x_{t+1} || ^{2}-V_{\psi}^ {t}(x_{t}))^{2}]",where \(\text{sg}[\cdot]\) denotes a stop-gradient operator indicating that gradient is not computed for the term,\min_{\psi}E_{\pi}\left[V_{\psi}^{t}(x_{t})-\left(E_{\theta}(x_{T})+\tau\sum_{t^{\prime}=t}^{T-1}\log\pi_{\phi}(x_{t^{\prime}+1}|x_{t^{\prime}})+\sum_{t^{\prime}=t}^{T-1}\frac{\tau}{2s_{t^{\prime}}^{2}} || x_{t^{\prime}+1}-x_{t^{\prime}} || ^{2}\right)\right]^{2},where \(\text{sg}[\cdot]\) denotes a stop-gradient operator indicating that gradient is not computed for the term.
neurips_2024_oral_34,11,\min_{\phi}E_{\pi_{\phi}(x_{t+1}|x_{t})}[V_{\psi}^{t+1}(x_{t+1})+\tau\log\pi_{\phi}(x_{t+1}|x _{t})+\frac{\tau}{2s_{t}^{2}} || x_{t}-x_{t+1} || ^{2}\bigg{|}x_{t}],,\min_{\phi}E_{\pi_{\phi}(x_{0:T})}[V_{\psi}^{t}(x_{t})+\tau\log\pi_{\phi}(x_{t+1}|x_{t})+\frac{\tau}{2s_{t}^{2}} || x_{t+1}-x_{t} || ^{2}],where \(V_{\psi}^{t}(\mathbf{x}_{t})\) is the value function and \(\tau\) is the temperature parameter.
neurips_2024_oral_34,12,"\min_{\psi}E_{x_{t},x_{t+1}\sim\pi}[(sg[V_{\psi}(x_{t+1})]+R(t)-V_{\psi}(x_{t}))^{2}]",,\min_{\phi}E_{\pi_{\phi}(x_{t+1}|x_{t})}[V_{\psi}(x_{t+1})+R(t)],where \(R(t)>0\) is a time cost function that replaces the running cost terms in the value update step for image generation experiments.
neurips_2024_oral_35,1,"varrowCrossAttention(Q=\{v\},K=\{p_{i}^{v}\}_{i=1}^{m}+\{v\},V=\{ p_{i}^{v}\}_{i=1}^{m}+\{v\})","Where \(v\) denotes a 3D voxel feature, and \(p_{i}^{v}\) denotes its projected 2D pixel feature from view \(i\), which is a concatenation of the RGB feature \(f_{i}^{v}\), the normal feature \(g_{i}^{v}\), and the RGB and normal values \(c_{i}^{v}\) and \(n_{i}^{v}\), respectively","A(v)=\sum_{i=1}^{m}Attention(v, p_{i}^{v})\\p_{i}^{v}=[f_{i}^{v}, g_{i}^{v}, c_{i}^{v}, n_{i}^{v}]","Where \(A(v)\) represents the aggregated features for a 3D voxel \(v\), \(p_{i}^{v}\) denotes its projected 2D pixel feature from view \(i\), which is a concatenation of the RGB feature \(f_{i}^{v}\), the normal feature \(g_{i}^{v}\), and the RGB and normal values \(c_{i}^{v}\) and \(n_{i}^{v}\), respectively."
neurips_2024_oral_35,2,L=\lambda_{1}L_{MSE}^{color}+\lambda_{2 }L_{LPIPS}^{color}+\lambda_{3}L_{MSE}^{normal}+\lambda_{4}L_{LPIPS}^{normal}+\lambda_{5}L_{occ}+\lambda_{6}L _{SDF},"where \(L_{\mathrm{occ}}\) and \(L_{\mathrm{SDF}}\) are MSE losses for occupancy and SDF volumes, and \(\lambda_{i}\) denotes the weight of each loss term",L=L_{occ}+\lambda_{1} L_{SDF}+\lambda_{2} L_{render},"where \(L_{\mathrm{occ}}\) and \(L_{\mathrm{SDF}}\) are MSE losses for occupancy and SDF volumes, and \(\lambda_{i}\) denotes the weight of each loss term."
neurips_2024_oral_40,1,"m_{Ada}(\varepsilon,\delta)=O\Bigg{(}\frac{d\ln(\frac{d}{\varepsilon\gamma})\ln(\frac{1}{\varepsilon\gamma})}{\gamma^{2}\varepsilon}+\frac{\ln(1/\delta)}{\varepsilon}\Bigg{)}",,"m_{Ada}(\varepsilon,\delta)\leq\frac{C\cdot d}{\varepsilon^{2}}\cdot\log\left(\frac{1}{\delta}\right)","Sample complexity of the AdaBoost algorithm, where \(C\) is a constant, \(d\) is the VC-dimension of the hypothesis set, \(\varepsilon\) is the desired accuracy, and \(\delta\) is the probability of failure."
neurips_2024_oral_40,2,L_{D}(f)=O\bigg{(}\frac{d\ln(m)\ln(m/d)+\ln(1/\delta)}{\gamma^{2}m}\bigg{)},,\varepsilon(m)\leq\frac{d}{m}+\frac{\ln(1/\delta)}{m},"Here, \(\varepsilon(m)\) represents the error rate of the output classifier \(f\) as a function of the number of samples \(m\), \(d\) is the VC-dimension of the hypothesis set, and \(\delta\) is the confidence parameter."
neurips_2024_oral_40,3,p=O\bigg{(}\frac{\ln m}{\gamma^{2}R}\bigg{)}\qquadand\qquad t=e^{O(dR)}\cdot\ln\frac{\ln m}{\delta\gamma^{2}},,"p=O\bigg{(}\frac{\gamma^{-2}\ln(m)}{R}\bigg{)},\quad t=\exp\bigg{(}O\bigg{(}dR^{2}\bigg{)}\ln\bigg{(}\frac{1}{\gamma}\bigg{)}\bigg{)}",Parallel complexity parameters \(p\) and \(t\) for the weak-to-strong learner \(\mathcal{A}_{R}\).
neurips_2024_oral_40,4,L_{D}(A_{R}(S))=O\bigg{(}\frac {d\ln(m)\ln(m/d)+\ln(1/\delta)}{\gamma^{2}m}\bigg{)},,L_{D}(f)=O\bigg{(}\frac{d\ln(m)\ln(m/d)+\ln(1/\delta)}{\gamma^{2}m}\bigg{)},"The expected loss of the output classifier \(f\) in terms of the sample size \(m\), VC-dimension \(d\), margin \(\gamma\), and failure probability \(\delta\)."
neurips_2024_oral_40,5,p\ln t=O\bigg{(}\frac{d\ln m}{\gamma^{2}}\bigg{)},,p=O\bigg{(}\frac{\ln m}{\gamma^{2}}\bigg{)}\qquadand\qquad t=e^{O(d)},Parallel complexity tradeoff for the proposed weak-to-strong learner.
neurips_2024_oral_40,6,"p\geq\frac{4\ln m}{\gamma^{2}R},\qquadand\qquad t\geq e^{16C_{ a}dR}\cdot R\ln\frac{pR}{\delta}",,C_{n}\cdotL_{D}(A_{R}(S))\leqO\bigg{(}\frac{d\ln(m)\ln(m/d)+\ln(1/\delta)}{\gamma^{2}m}\bigg{)},"where \(C_{\mathrm{n}}\) is a universal constant, \(\mathcal{L}_{\mathcal{D}}(\mathcal{A}_{R}(\mathbf{S}))\) is the expected loss of the learner \(\mathcal{A}_{R}\) over the sample \(\mathbf{S}\), \(d\) is the VC-dimension, \(m\) is the sample size, \(\delta\) is the probability of failure, and \(\gamma\) is the margin parameter."
neurips_2024_oral_40,7,L_{D}(sign(g))\leq C\cdot\frac{d\ln(m)\ln(m/d)+\ln(1/\delta)}{\gamma^{2}m},,c(x)g(x)\geq\frac{\gamma}{8}.\,Margin of the linear classifier \(g\) on input \(x\) with respect to the target concept \(c\).
neurips_2024_oral_40,8,|L_{D}(h)-L_{T}(h)|\leq\varepsilon,where \(\mathcal{L}_{T}(h)\coloneqq\mathcal{L}_{\mathrm{Uniform}(T)}(h)\) is the empirical loss of \(h\) on \(T\),L_{T}(h)\leqL_{D}(h)+\varepsilon,Empirical loss of hypothesis \(h\) on multiset \(T\) is within \(\varepsilon\) of its expected loss over distribution \(\mathcal{D}\).
neurips_2024_oral_40,9,"D_{\infty}(D_{r},D_{0})\coloneqq\ln\bigl{(}\sup_{x\inX}D_{ r}(x)/D_{0}(x)\bigr{)}",,|L_{D}(h)-L_{T}(h)|\leq\varepsilon,where \(\mathcal{L}_{T}(h)\coloneqq\mathcal{L}_{\mathrm{Uniform}(T)}(h)\) is the empirical loss of \(h\) on \(T\)
neurips_2024_oral_40,10,KL(D_{r}\parallel D_{0})\coloneqq\sum_{x\inX}D_{r}(x)\ln\frac{D_{r}(x)}{D_{0}(x)},,D_{KL}(D_{r}\|D_{0})\coloneqq\int_{X}D_{r}(x)\ln\frac{D_{r}(x)}{D_{0}(x)}dx,Kullback-Leibler divergence between distributions \(D_{r}\) and \(D_{0}\).
neurips_2024_oral_40,11,\lnE_{P}\big{[}e^{X}\big{]}\geqE_{Q}[X]-KL(Q\parallel P),,KL(D_{r}\parallel D_{0})\coloneqq\sum_{x\inX}D_{r}(x)\ln\frac{D_{r}(x)}{D_{0}(x)},The Kullback-Leibler divergence between two distributions \(D_{r}\) and \(D_{0}\).
neurips_2024_oral_40,12,KL(D\parallel\tilde{D})\leq 4\gamma^{2}R,,KL(D_{r}\parallel D_{0})\leq C_{n}\cdot\frac{d\ln(m)\ln(m/d)+\ln(1/\delta)}{\gamma^{2}m},"where \(D_{r}\) and \(D_{0}\) are distributions at round \(r\) and the initial distribution, respectively."
neurips_2024_oral_40,13,\Pr_{T\sim\tilde{D}^{n}}[T\inG]\geq\exp(-16C_{n}dR),,|L_{D}(h)-L_{\tilde{D}}(h)|\leq\gamma/2,where \(\mathcal{L}_{D}(h)\) is the loss of hypothesis \(h\) on distribution \(D\) and \(\mathcal{L}_{\tilde{D}}(h)\) is the loss of \(h\) on distribution \(\tilde{D}\).
neurips_2024_oral_40,14,\prod_{r=1}^{R}Z_{kR+r}<\exp(-\gamma^{2}R/2),,\prod_{\ell=1}^{pR}Z_{\ell}\leq\exp\bigg{(}-\frac{C_{n}dR}{\gamma^{2}}\bigg{)},Normalization factors produced by the Boosting steps.
neurips_2024_oral_41,1,\min_{x\inR^{d}_{c}}\phi(x),where \(\phi:\mathbb{R}^{d}\rightarrow\mathbb{R}\) is a differentiable function bounded from below (e,\phi:R^{d}\rightarrowR,where \(\phi\) is a differentiable function bounded from below.
neurips_2024_oral_41,2,x_{i}=x_{j}\quad\Leftrightarrow\quad\exists k\;:\;i\in P_{k} and j\in P_{k},,"P(x)=\{P_{1}(x),\ldots, P_{c}(x)\},\quadwhere \bigcup_{i=1}^{c} P_{i}(x)=\{1,\ldots, d\} and  P_{i}(x)\cap P_{j}(x)=\emptyset for  i\neq j","A partition \(P(x)\) of the set \(\{1,\ldots,d\}\) into \(c\) nonempty subsets \(P_{1}(x),\ldots,P_{c}(x)\)."
neurips_2024_oral_41,3,"M_{P}(x)=M_{P,\phi}(x):=\arg\min_{y\inR^{d}}\{\phi(y)\,:\,P(y)\sqsupseteq P (x)\}",,y=argmin_{y\inR^{d}_{\leq c}}\phi(y)\quads.t.\quad P(y)=P(x),"where \(y\) is the vector being optimized, \(\phi(y)\) is the objective function, and \(P(y)\) is the partition induced by \(y\)."
neurips_2024_oral_41,4,"M_{V}(y)=M_{V,\phi}(y):=\arg\min_{x\inR^{d}}\{\phi(x)\,:\,V(x)\subseteq V (y)\}",,"V_{P}(y)=V_{P,\phi}(y):=\arg\min_{x\inR^{d}}\{\phi(x)\,:\,V(x)=V(y)\}",where \(V_{P}(y)\) is the mapping that minimizes the objective function \(\phi\) while keeping the vector \(V\) fixed.
neurips_2024_oral_41,5,"\phi(x)\approx\widetilde{\phi}_{y}(x):=\phi(y)+\langle\nabla\phi(y),x-y\rangle+\tfrac{L}{2}\norm{x-y}^{2}",where \(L>0\) is a sufficiently large constant,"L_{V}(x,y)=\frac{1}{2}\|x-y\|^{2}+\frac{L}{2}\|\phi(x)-\phi(y)\|^{2}",where \(L>0\) is a sufficiently large constant.
neurips_2024_oral_41,6,"M_{V,\phi}(y)\overset{\eqref{eq:v_def}}{\approx}M_{V,\widetilde{\phi}_{y}}(y )\overset{\eqref{eq:v_def}}{=}\arg\min_{x\inR^{d}}\{\widetilde{\phi}_{y}(x)\:\V(x)\subseteq V(y)\}",,"\phi(x)\approx\widetilde{\phi}_{y}(x):=\phi(y)+\langle\nabla\phi(y),x-y\rangle+\tfrac{L}{2}\norm{x-y}^{2}",where \(L>0\) is a sufficiently large constant.
neurips_2024_oral_41,7,\widehat{\phi}_{y}(x):=\norm{x-(y-\tfrac{1}{L}\nabla\phi(y))}^{2}=\norm{x-y^{+}}^{2}=\sum\limits_{i=1}^{d}(x_{i}-y_{i}^{+})^{2},,"M_{V,\widehat{\phi}_{y}}(y)=\arg\min_{x\inR^{d}}\left\{\frac{1}{2}\norm{x-y^{+}}^{2}\:\V(x)\subseteq V(y)\right\}",where \(y^{+}=y-\frac{1}{L}\nabla\phi(y)\) is the point obtained after taking a single gradient descent step from \(y\) with learning rate \(\frac{1}{L}\).
neurips_2024_oral_41,8,"x:=M_{V,\phi}(y)\approx M_{V,\widehat{\phi}_{y}}(y):=\hat{x}",,"x=M_{V,\widehat{\phi}_{y}}(y)\approx\arg\min_{x\inR^{d}}\{\widehat{\phi}_{y}(x)\:\V(x)\subseteq V(y)\}","where \(M_{V,\widehat{\phi}_{y}}(y)\) is the approximation of the V step using the function \(\widehat{\phi}_{y}\)."
neurips_2024_oral_41,9,"\phi(x)\leq\phi(y)+\langle\nabla\phi(y),x-y\rangle+\tfrac{L}{2}\norm{x-y}^{2},\qquad\forall x,y\inR^{d}_{\leq c}",,"L-smooth:\quad\|\nabla\phi(x)-\nabla\phi(y)\|\leq L\|x-y\|,\quad\forall x,y\inR^{d}_{\leq c}","where \(L>0\) is a smoothness constant, \(\nabla\phi(x)\) is the gradient of the function \(\phi\), and \(\|\cdot\|\) denotes the norm."
neurips_2024_oral_41,10,"x^{+}:=M_{V,\widehat{\phi}_{y,S^{k}}}(y):=\operatorname*{arg\,min} _{x\inR^{d}}\{\widehat{\phi}_{y,S^{k}}(x)\:\V(x)\subseteq V(y)\} || where\quad\widehat{\phi}_{y,S^{k}}(x):=\|x-(y-\frac {1}{L_{S^{k}}}Z^{k}(\nabla\phi(y)))\|^{2}",,"\widehat{\phi}_{y,s^{k}}(x):=\left\|x-\left(y-\frac{1}{x_{S^{k}}}Z^{k}\left(\nabla\phi(y)\right)\right)\right\|^{2}","where \(\widehat{\phi}_{y,s^{k}}(x)\) is the modified objective function for the linearized subspace V step, \(y\) is the current point, \(x_{\mathcal{S}^{k}}\) is the scaling factor for the selected subspace, and \(Z^{k}\) is the linear mapping that zeroes out the coordinates not in the subspace \(\mathcal{S}^{k}\)."
neurips_2024_oral_42,1,"\min_{Q}\max_{\pi}\alpha(E_{\hat{s}\simD_{img},\alpha\sim\pi(a|\hat{s})}[Q(\hat{s},a)]-E_{\hat{s},\hat{a}\simD_{img}}[Q(\hat{s},\hat{a})]+R(\pi)) || +E_{\hat{s},\hat{a}\simD_{img}}[(Q(\hat{s},\hat{a})-\hat{B}^{\pi}\hat{Q}(\hat{s},\hat{a}))^{2}]",,"J(Q)=E_{(s, a, r, s')\simD_{img}}\left[ Q(s, a)-\frac{1}{2}\left( Q(s, a)-\hat{Q}(s, a)\right)^2\right]","Objective function for Conservative Q-learning, where \( J(Q) \) is the learning objective, \( Q(s, a) \) is the action-value function, and \( \hat{Q}(s, a) \) is the estimated action-value function from the offline dataset."
neurips_2024_oral_42,2,"\hat{B}_{T}^{\pi}\hat{Q}(\hat{s},\hat{a}):=\hat{r}-\eta_{R }R_{R}(\hat{s},\hat{a})-\eta_{T}R_{T}(\hat{s},\hat{a})+\gammaE_{\hat{s}^{\prime}\simD_{img},a^{\prime}\sim\pi_{k}(a^{\prime}|\hat{s}^{\prime})}[Q(\hat{s}^{\prime},a^{\prime})]","where \(\hat{s}^{\prime}\sim\mathcal{D}_{\mathrm{img}}\) is to sample the next state given \(\hat{s},\hat{a}\), \(\eta_{R}\) and \(\eta_{T}\) are two hyper-parameters to control the weighting of the uncertainty terms","R(\pi)=R_{R}(\hat{r},M_{R})+R_{T}(\hat{s},\hat{a})","\(\hat{s}^{\prime}\sim\mathcal{D}_{\mathrm{img}}\) is to sample the next state given \(\hat{s},\hat{a}\), \(\eta_{R}\) and \(\eta_{T}\) are two hyper-parameters to control the weighting of the uncertainty terms"
neurips_2024_oral_48,1,"x_{0}=x,\quadx_{\ell}=\sigma_{\ell}(W _{\ell}x_{\ell-1}+b_{\ell}),\quad u_{G,\theta}(x)=x_{L}","where \(L\): the number of layers, \(\mathbf{W}_{i}\in\mathbb{R}^{d_{\ell}\times d_{\ell-1}}\): the weights of the NN, \(\mathbf{b}_{i}\in\mathbb{R}^{d_{\ell}}\): the biases of the NN, \(d_{0}=d_{\text{in}}\), \(d_{L}=d_{\text{out}}\), \(\sigma_{\ell}:\mathbb{R}\to\mathbb{R}\) activation functions applied element-wise","u_{G,\theta}(x)=W_{L}\sigma_{L}\left(W_{L-1}\sigma_{L-1}\left(\cdots\sigma_{1}\left(W_{1}x+b_{1}\right)+b_{2}\right)\cdots+b_{L-1}\right)+b_{L}","where \(L\): the number of layers, \(\mathbf{W}_{i}\in\mathbb{R}^{d_{\ell}\times d_{\ell-1}}\): the weights of the NN, \(\mathbf{b}_{i}\in\mathbb{R}^{d_{\ell}}\): the biases of the NN, \(d_{0}=d_{\text{in}}\), \(d_{L}=d_{\text{out}}\), \(\sigma_{\ell}:\mathbb{R}\to\mathbb{R}\) activation functions applied element-wise"
neurips_2024_oral_48,2,"W_{\ell}^{\prime}=P_{\ell}W_{\ell}P_{\ell-1}^{-1},\,b_{\ell}^{\prime}=P_{\ell}b_{\ell}\Longrightarrow(W_{\ell}^{\prime},b_{\ell}^{\prime})_ {\ell=1}^{L}=\theta^{\prime}\simeq\theta=(W_ {\ell},b_{\ell})_{\ell=1}^{L}","where \(\ell\in\{1,\ldots,L\}\), \(\mathbf{P}_{0}=\mathbf{P}_{L}=\mathbf{I}\) and \(\mathbf{P}_{\ell}\in\mathbb{R}^{d_{\ell}\times d_{\ell}}\) are arbitrary permutation matrices","x_{\ell}=P_{\ell}x_{\ell},\quadW_{\ell}=P_{\ell}W_{\ell}P_{\ell-1},\quadb_{\ell}=P_{\ell}b_{\ell}","where \(\ell\in\{1,\ldots,L\}\), \(\mathbf{P}_{0}=\mathbf{P}_{L}=\mathbf{I}\) and \(\mathbf{P}_{\ell}\in\mathbb{R}^{d_{\ell}\times d_{\ell}}\) are arbitrary permutation matrices"
neurips_2024_oral_48,3,"h_{V}^{0}(i)=INIT_{V}(x_{V}(i )),\quadh_{E}^{0}(i,j)=INIT_{E}(x_{ E}(i,j))",,"x_{V}^{(t)}=\sigma\left(W^{(t)}x_{V}^{(t-1)}+b^{(t)}\right),\quadx_{E}^{(t)}=W_{E}^{(t)}x_{E}^{(t-1)}+b_{E}^{(t)}","where \(t\): the iteration (layer) index, \(\mathbf{W}^{(t)}\): the weight matrix for vertex features at iteration \(t\), \(\mathbf{b}^{(t)}\): the bias vector for vertex features at iteration \(t\), \(\mathbf{W}_{E}^{(t)}\): the weight matrix for edge features at iteration \(t\), \(\mathbf{x}_{V}^{(t)}\): the vertex features at iteration \(t\), \(\mathbf{x}_{E}^{(t)}\): the edge features at iteration \(t\), \(\sigma\): activation function applied element-wise."
neurips_2024_oral_48,4,"m_{V}^{t}(i)=\bigoplus_{j\inN(i)}\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!where\(h_{V}^{i},h_{E}^{i}\) are vertex and edge representations at iteration\(t\) and\(h_{G}\) is the overall graph (NN) representation. INIT, MSG, UPD are general function approximators (e.g. MLPs), while READ is a permutation invariant aggregator (e.g. DeepSets [81]). The above equations have appeared with several variations in the literature, e.g. in some cases the edge representations are not updated or the readout input involve edge representations as well. Another frequent strategy is to use _positional encodings_\(p_{V},p_{E}\) to break undesired symmetries. In FFNNs, Eq. (2) reveals that input and output vertices are not permutable, while vertices cannot be permuted across layers. Therefore, vertices (or edges) that are permutable share the same positional encoding (see Appendix A.1.2 for more details).**Remark:**Although, typically, the neighbourhood\(N(i)\) contains both incoming and outgoing edges, in Section 5 we will illustrate our method using only incoming edges: _forward neighbourhood_\(N_{FW}(i)=\{j\inV\midlayer\,(i)- layer\,(j)=1\}\) and _backward_ where layer\((i)\) gives the layer neuron\(i\) belongs. Backward neighbourhoods\(N_{BW}(i)\) are defined defined similarly. In Appendix A.2, we show a more elaborate _bidirectional version_ of our method, with both neighbourhoods considered. ## 4 Scaling symmetries in Feedforward Neural Networks**Scaling symmetries (activation functions).**Intuitively, permutation symmetries stem from the _graph structure_ of neural networks, or put differently, from the fact that hidden neurons do not possess any inherent ordering. Apart from the affine layers\(W_{\ell}\) that give rise to the graph structure, it is frequently the case that**activation functions**\(\sigma_{\ell}\) have inherent symmetries that are bestowed to the NN. Let us dive into certain illustrative examples: for the ReLU activation\(\sigma(x)=\max(x,0)\) it holds that\(\sigma(ax)=\max(ax,0)=a\max(x,0),\\forall a>0\). For the tanh and sine activations\(\sigma(x)=\tanh(x)\),\(\sigma(x)=\sin(x)\) respectively, it holds that\(\sigma(ax)=a\sigma(x),\\forall a\in\{-1,1\}\). In a slightly more complex example, polynomial activations\(\sigma(x)=x^{k}\), we have\(\sigma(ax)=a^{d}\sigma(x)\), i.e. the multiplier differs between input and output. In general, we will be talking about _scaling symmetries_ whenever there exist pairs\((a,b)\) for which it holds that\(\sigma(ax)=b\sigma(x)\). To see how such properties affect NN symmetries, let us focus on FFNNs (see Appendix A.3 for CNNs): for a neuron\(i\) (we omit layer subscripts) we have\(\sigma\big{(}aW(i,:)x+ab(i)\big{)}=\textit{b}\big{(}W(i,:)x+b(i)\big{)}\), i.e. _multiplying its bias and all incoming weights with a constant a results in scaling its output with a corresponding constant\(b\)_. Generalising this to linear transformations, we may ask the following: which are the pairs of matrices\((A,B)\) for which we have\(\sigma\big{(}AWx+Ab\big{)}=B\sigma\big{(}Wx+b\big{)}\)? Godfrey et al. [25] provide an answer for _any activation that respects certain conditions_. We restate here their most important results:**Proposition 4.1**(Lemma 3.1. and Theorem E.14 from [25]).: _Consider an activation function\(\sigma:R\toR\). Under mild conditions,5 the following hold:_ Footnote 5: See Appendix A.7.1 for the precise statement and more details about\(\phi_{\sigma,d}\).*_For any_\(d\inN^{+}\)_, there exists a (non-empty) group of invertible matrices defined as:_\(I_{\sigma,d}=\{A\inR^{d\times d}: invertible\mid\exists\B\inR^{d\times d} invertible, such that: \sigma(Ax)=B\sigma(x)\}\) _(_intertwiner group_)_, and a mapping function_\(\phi_{\sigma,d}\) _such that_\(B=\phi_{\sigma,d}(A)\)_._*_Every_\(A\in I_{\sigma,d}\) _is of the form_\(PQ\)_, where_\(P\)_: permutation matrix and_\(Q=\textit{diag}\big{(}q_{1},\ldots q_{d}\big{)}\) _diagonal, with_\(q_{i}\in D_{\sigma}=\{a\inR\setminus\{0\}\mid\sigma(ax)=\phi_{\sigma,1}(a)\sigma(x)\}\)_: the 1-dimensional group, and_\(\phi_{\sigma,d}(A)=P\textit{diag}\big{(}\phi_{\sigma,1}(q_{1} ),\ldots\phi_{\sigma,1}(q_{d})\big{)}\)_._ This is a powerful result that completely answers the question above for most practical activation functions. Importantly, not only does it recover permutation symmetries, but also reveals symmetries to diagonal matrix groups, which can be identified by solely examining\(\phi_{\sigma,1}\), i.e. the one-dimensional case and the set\(D_{\sigma}\) (easily proved to be a group) we have already discussed in our examples above. Using this statement, Godfrey et al. [25] characterised various activation functions (or recovered existing results), e.g. ReLU:\(I_{\sigma,d}\) contains**generalised permutation matrices with positive entries**of the form\(PQ\),\(Q=diag(q_{1},\ldots,q_{d})\),\(q_{i}>0\) and\(\phi_{\sigma,d}(PQ)=PQ\)[56]. Additionally, here we characterise the intertwiner group of sine (used in the popular SIREN architecture [70] for INRs). Not surprisingly, it has the same intertwiner group with tanh [11, 21] (we also recover this here using Proposition 4.1). Formally, (proof in Appendix A.7.1):**Corollary 4.2**.: _Hyperbolic tangent\(\sigma(x)=\tanh(x)\) and sine activation\(\sigma(x)=\sin(\omega x)\), satisfy the conditions of Proposition 4.1, when (for the latter)\(\omega\neq k\pi,k\inZ\). Additionally,\(I_{\sigma,d}\) contains**signed permutation matrices**of the form\(PQ\), with\(Q=diag(q_{1},\ldots,q_{d})\),\(q_{i}=\pm 1\) and\(\phi_{\sigma_{d}}(PQ)=PQ\)._It is straightforward to see that the symmetries of Proposition 4.1, induce equivalent parameterisations for FNNs. In particular, it follows directly from Proposition 3.4. in [25], that for activation functions\(\sigma_{\ell}\) satisfying the conditions of Proposition 4.1 and when\(\phi_{\sigma,\ell}(Q)=Q\), we have that:\[W^{\prime}_{\ell}=P_{\ell}Q_{\ell}W_{\ell}Q_{\ell-1}^{-1}P_{\ell-1}^{-1},\;b^{\prime}_{\ell}=P_{\ell}Q_{\ell}b_{\ell}\implies(W^{\prime }_{\ell},b^{\prime}_{\ell})_{\ell=1}^{L}=\theta^{\prime}\simeq\theta=(W_{\ell},b_{\ell})_{\ell=1}^{L}","where again \(\ell\in\{1,\ldots,L\}\), \(\mathbf{P}_{0}=\mathbf{Q}_{0}=\mathbf{P}_{L}=\mathbf{Q}_{L}=\mathbf{I}\)","h_{V}^{\ell}(i)=\sum_{j:(j,i)\inE}Q_{\ell}(i,j)h_{E}^{\ell-1}(j,i),\quadh_{E}^{\ell}(i,j)=P_{\ell}(i,j)h_{V}^{\ell}(i)","where again \(\ell\in\{1,\ldots,L\}\), \(\mathbf{P}_{0}=\mathbf{Q}_{0}=\mathbf{P}_{L}=\mathbf{Q}_{L}=\mathbf{I}\)"
neurips_2024_oral_48,5,"h^{\prime}_{V}(i)=q_{\ell}(\pi_{\ell}(i))h_{V} (\pi_{\ell}(i)),\quad\ell=layer(i)\in\{1,\ldots,L-1\} || h^{\prime}_{E}(i,j)=q_{\ell}(\pi_{\ell}(i))h_{E} (\pi_{\ell}(i),\pi_{\ell-1}(j))q_{\ell-1}^{-1}(\pi_{\ell-1}(j)),\;\ell=layer(i)\in\{2,\ldots,L-1\}",,"W^{\prime}_{\ell}=P_{\ell}Q_{\ell}W_{\ell}Q_{\ell-1}^{-1}P_{\ell-1}^{-1},\;b^{\prime}_{\ell}=P_{\ell}Q_{\ell}b_{\ell}\implies(W^{\prime }_{\ell},b^{\prime}_{\ell})_{\ell=1}^{L}=\theta^{\prime}\simeq\theta=(W_{\ell},b_{\ell})_{\ell=1}^{L}","where \(\ell\in\{1,\ldots,L\}\), \(\mathbf{P}_{\ell}\) and \(\mathbf{Q}_{\ell}\) are permutation and diagonal matrices respectively."
neurips_2024_oral_48,6,"g_{i}\big{(}q_{1}x_{1},\ldots,q_{n}x_{n}\big{)}=q_{i}g_{i}(x_{1},\ldots,x_{n}\big{)},\forall q_{i}\in D_{i},i\in\{1,\ldots,n\}",where \(D_{i}\) a 1-dimensional scaling group as defined in Proposition 4,"h^{\prime}_{V}(i)=q_{\ell}(i)h_{V}(i),\quad\ell=layer(i)\in\{1,\ldots,L-1\}",where \(q_{\ell}(i)\) is a scaling factor for vertex \(i\) in layer \(\ell\).
neurips_2024_oral_48,7,"ScaleInv^{k}(X)=\rho^{k}(\tilde{x }_{1},\ldots,\tilde{x}_{n})",,"g_{i}\big{(}q_{1}x_{1},\ldots,q_{n}x_{n}\big{)}=q_{i}g_{i}(x_{1},\ldots,x_{n}\big{)},\forall q_{i}\in D_{i},i\in\{1,\ldots,n\}",where \(D_{i}\) a 1-dimensional scaling group as defined in Proposition 4
neurips_2024_oral_48,8,"ScaleEq=f^{K}\circ\cdots\circf^{1},\;f^{k}(X)=\big{(}\Gamma^{k}_{1}x_{1},\ldots,\Gamma^{k}_{n}x_{n}\big{)}\odotScalelInv^{k }(X)",,"ScaleInv^{k}(X)=\rho^{k}(\tilde{x}_{1},\ldots,\tilde{x}_{n})",where \(\rho^{k}\) is a scaling function applied to the outputs of the scale invariant functions \(\tilde{\mathbf{x}}_{i}\).
neurips_2024_oral_48,9,"g\big{(}q_{1}x_{1},\ldots q_{n}x_{n}\big{)}=g(x_{1},\ldotsx_{n})\prod_{i=1}^{n}q_{i},\forall q_{i}\in D_{i}",,"g_{i}\big{(}q_{\ell}(i)x_{1},\ldots,q_{\ell}(i)x_{n}\big{)}=q_{\ell}(i)g_{i}\big{(}x_{1},\ldots,x_{n}\big{)},\forall q_{\ell}(i)\in D_{i},i\in\{1,\ldots,n\}",where \(D_{i}\) is a 1-dimensional scaling group as defined in Proposition 4.
neurips_2024_oral_48,10,"ReScaleEq(x_{1},\ldotsx_{n})=ScaleEq\big{(}vec(\textbf{X}_{n})\big{)}",,"h^{\prime}_{V}(i)=g\big{(}q_{\ell}(i)h_{V}(\pi_{\ell}(i)),\ldots,q_{\ell}(n)h_{V}(\pi_{\ell}(n))\big{)},\quad\ell=layer(i)\in\{1,\ldots,L-1\}",where \(g\) is a rescale equivariant function that combines the vertex representations scaled by their respective multipliers \(q_{\ell}(i)\).
neurips_2024_oral_48,11,"MSG_{V}(x,y,e)=ScaleEq ([x,ReScaleEq(y,e) ])","where \([\cdot,\cdot]\) denotes concatenation, \(\mathsf{ReScaleEq}(q_{y}\mathbf{y},q_{x}q_{y}^{-1}\mathbf{e})=q_{x}\mathsf{ ReScaleEq}(\mathbf{y},\mathbf{e})\)","MSG_{V}\left(q_{x}x,y,q_{x}q_{y}^{-1}e\right)=q_{x}MSG_{V}\left(x,y,e\right)","where \(\mathbf{x}\) is the central vertex feature, \(\mathbf{y}\) is the neighbor vertex feature, and \(\mathbf{e}\) is the edge feature connecting them."
neurips_2024_oral_48,12,"UPD_{V}(x,m)=ScaleEq( [x,m])",,"UPD_{V}(x,m)=ScaleEq([x,m])","where \(\mathbf{x}\) is the input vector and \(\mathbf{m}\) is the message vector, both scaled by the same multiplier \(q_{x}\)."
neurips_2024_oral_48,13,"READ_{V}(X):=DeepSets(\tilde{x}_{1},\ldots,\tilde{x}_{n}),\quad\tilde{x}_{ i}=canon_{i}(x_{i}) or \tilde{x}_{i}=symm_{i}(x_{i})",,READ(h_{V})=ScaleInv(symm(h_{V})),"where \(\mathbf{h}_{V}\) are the vertex representations of hidden neurons, \(\mathsf{ScaleInv}\) is a scale invariant function, and \(\mathsf{symm}\) is a symmetrisation function."
neurips_2024_oral_54,1,"V^{\pi}(s):=E[\sum_{t=0}^{\infty}\gamma^{t}r(s_{t},a_{t})| s_{0}=s];\quad Q^{\pi}(s,a):=E[\sum_{t=0}^{\infty}\gamma^{t}r (s_{t},a_{t})|s_{0}=s,a_{0}=a]","where \(a_{t}\sim\pi(\cdot|s_{t})\) and \(s_{t+1}\sim P(\,\cdot\,|s_{t},a_{t})\) for all \(t\geq 0\)","V^{\pi}(s)=E\left[\sum_{t=0}^{\infty}\gamma^{t} r(s_{t}, a_{t})\mid s_{0}=s\right]","where \(a_{t}\sim\pi(\cdot|s_{t})\) and \(s_{t+1}\sim P(\,\cdot\,|s_{t},a_{t})\) for all \(t\geq 0\)"
neurips_2024_oral_54,2,"(TQ)(s,a)=r(s,a)+\gamma\cdotE_{s^{\prime}\sim P(\cdot|s,a) }[\max_{a^{\prime}\inA}Q(s^{\prime},a^{\prime})]",,"Q^{\star}(s,a)=r(s,a)+\gamma\sum_{s^{\prime}\inS} P(s^{\prime}|s,a)\max_{a^{\prime}} Q^{\star}(s^{\prime},a^{\prime})","where \(s\) is the current state, \(a\) is the action taken, and \(s^{\prime}\) represents the next state resulting from the action \(a\)."
neurips_2024_oral_54,3,"(T_{Z}Q)(s,a)=r(s,a)+\gamma V(Z(s,a))","where \(V(s^{\prime})=\max_{a^{\prime}\in\mathcal{A}}Q(s^{\prime},a^{\prime})\)","(T_{Z}Q)(s,a)=r(s,a)+\gamma\cdotE_{s^{\prime}\sim Z}[\max_{a^{\prime}\inA}Q(s^{\prime},a^{\prime})]","where \(V(s^{\prime})=\max_{a^{\prime}\in\mathcal{A}}Q(s^{\prime},a^{\prime})\)"
neurips_2024_oral_54,4,"ER(A;N,M):=\sup_{M=(P,r)}E[\|\widehat{Q}_{M}(A,N,M)-Q^{\star}_{M}\|_{\infty }]",where the expectation is taken over the samples and any randomness in the algorithm,"ER(A;N,M)=E\left[\|\widehat{Q}_{M}(A,N,M)-Q^{\star}_{M}\|_{\infty}\right]",where the expectation is taken over the samples and any randomness in the algorithm
neurips_2024_oral_54,5,"SC(A;\varepsilon,M):=|S || A|\cdot\min\{N\inN:ER(A;N,M)\leq\varepsilon\}",,"SC(A;\varepsilon,M):=\inf\{N\geq 1:ER(A;N,M)\leq\varepsilon\}","where \(\mathsf{SC}(\mathscr{A};\varepsilon,M)\) represents the minimum number of samples required for the algorithm \(\mathscr{A}\) to achieve an error rate of at most \(\varepsilon\) across \(M\) agents."
neurips_2024_oral_54,6,"SC(A;\varepsilon,M,\delta):=|S || A|\cdot\min\{N\inN:\Pr(\sup_{M}\|\widehat{Q}_{M}(A,N,M)-Q^{\star}_{M}\|_{\infty}\leq\varepsilon)\geq 1-\delta\}",,"ER_{\delta}(A;N,M):=\sup_{M=(P,r)}P\left[\|\widehat{Q}_{M}(A,N,M)-Q^{\star}_{M}\|_{\infty }>\varepsilon\right]\leq\delta",where the expectation is taken over the samples and any randomness in the algorithm with a confidence level of \(1 - \delta\)
neurips_2024_oral_54,7,CC_{round}(A;N):=\frac{1}{M}\sum_{m=1}^{M}C^{m}_{round}(A;N);\quadCC_{bit}(A;N ):=\frac{1}{M}\sum_{m=1}^{M}C^{m}_{bit}(A;N),,C_{A}(N):=\sum_{m=1}^{M} C^{m}_{round}(A;N)+\sum_{m=1}^{M} C^{m}_{bit}(A;N),where \(C^{m}_{\mathsf{round}}(\mathscr{A};N)\) is the number of messages sent by agent \(m\) and \(C^{m}_{\mathsf{bit}}(\mathscr{A};N)\) is the total number of bits uploaded by agent \(m\).
neurips_2024_oral_54,8,Q_{t-\frac{1}{2}}^{m}=(1-\eta_{t})Q_{t-1}^{m}+\frac{\eta_{t}}{B}\sum_{b=1}^{B}T_{Zs}(Q_{t-1}^{m}),,Q_{t}^{m}=\frac{1}{B}\sum_{b=1}^{B}\widehat{T}_{Zs}(Q_{t-1}^{m}),"where \(a_{t}\sim\pi(\cdot|s_{t})\) and \(s_{t+1}\sim P(\,\cdot\,|s_{t},a_{t})\) for all \(t\geq 0\)"
neurips_2024_oral_54,9,"Q_{t}^{m}=\frac{1}{M}\sum_{j=1}^{M}Q_{t-\frac{1}{2}}^{j}& if t\inC,\\Q_{t-\frac{1}{2}}^{m}& otherwise.",,\overline{Q}_{t}^{m}=\frac{1}{R}\sum_{r=1}^{R}Q_{t_{r}}^{m},where \(\overline{Q}_{t}^{m}\) is the averaged estimate of the Q-function for agent \(m\) at time step \(t\).
neurips_2024_oral_54,10,R=\textsf{CC}_{\textsf{round}}(A;N)\leq\frac{c_{0}}{(1-\gamma)\log ^{2}N}; or\textsf{CC}_{\textsf{left}}(A;N)\leq\frac{c_{1}|S || A|}{(1-\gamma)\log^{2}N},,CC_{round}(A;T)\geq\Omega\left(\frac{T}{\log(T)}\right),where the lower bound holds for any algorithm \(\mathscr{A}\) under the specified conditions.
neurips_2024_oral_54,11,"\textsf{ER}(A;N,M)\geq\frac{C_{\gamma}}{\log^{3}N\sqrt{N}}",,"ER(A;N,M)\geq\frac{c_{2}|S || A|}{(1-\gamma)\log^{2}N}",where \(c_{2}>0\) is a universal constant.
neurips_2024_oral_56,1,"B_{Acc=K_{1}}(t|m)=\sup_{d}\{d|Acc(t|d,m)\leq K_{1}\}","where \(Acc(t|d,m)\) represents the accuracy of the model's accuracy on task \(t\) with difficulty \(d\)","RB(m, t)=\max_{d}\{ d\mid Acc(t|d, m)\geq K_{1}\}","where \(RB(m, t)\) is the reasoning boundary for model \(m\) on task \(t\), \(Acc(t|d, m)\) represents the accuracy of the model on task \(t\) with difficulty \(d\), and \(K_{1}\) is the predefined accuracy threshold."
neurips_2024_oral_56,2,"B_{Acc=K_{1}}(t_{1},t_{2},\dots,t_{n}|m)\approx\frac{1}{\sum _{i=1}^{n}\frac{N_{i}}{B_{Acc=K_{1}}(t_{i}|m)-b_{i}}}",where \(\mathcal{B}_{\text{Acc}=K_{1}}(t_{i}|m)\) denotes the reasoning boundary of model \(m\) for task \(t_{i}\),"B_{Acc=K_{1}}(t_{1},t_{2},\dots,t_{n}|m)=\max\{B_{Acc=K_{1}}(t_{i}|m)\,|\, i=1,2,\dots,n\}",where \(\mathcal{B}_{\text{Acc}=K_{1}}(t_{i}|m)\) denotes the reasoning boundary of model \(m\) for task \(t_{i}\)
neurips_2024_oral_56,3,"B^{\texttt{CGT}}(c,p)=\frac{1}{\frac{N_{1}}{(B(c)-b_{1})}+\frac{N_{2}}{(B(p)-b_{2})}}",,"B_{Acc=K_{1}}(p,c|m)\approx\frac{1}{\frac{1}{B(p)}+\frac{1}{B(c)}}",where \(\mathcal{B}(p)\) denotes the reasoning boundary for the step planning task and \(\mathcal{B}(c)\) denotes the reasoning boundary for the step calculation task.
neurips_2024_oral_56,4,"B^{Tool}(c,p)=\lim_{B(c)arrow+\infty}\frac{ 1}{\frac{N_{1}}{(B(c)-b_{1})}+\frac{N_{2}}{(B(p)-b_{2})}}=\frac{B(p)-b_{2}}{N_{2}}",,"B^{\texttt{TU}}(c,p)\rightarrow\frac{1}{\frac{N_{1}}{(B(c)-b_{1})}+\frac{N_{2}}{(B(p)-b_{2})}}.\","where \(\mathcal{B}^{\texttt{TU}}(c,p)\) denotes the combined reasoning boundary when tool usage is applied, and \(N_{1}, N_{2}, b_{1}, b_{2}\) are the same scaling factors as in previous equations."
neurips_2024_oral_57,1,"E_{t\simU[1,T],\;X^{0},C\sim q( X^{0},C)}(\|X^{0}-H(X^{t},t,C)\|^{2})","where \(t\) denotes the time step, \(\mathbf{X}^{0}=\mathbf{X}\) is the raw motion latent sequence, and \(\mathbf{X}^{t}\) is the noisy inputs generated by the diffusion forward process \(q(\mathbf{X}^{t}|\mathbf{X}^{t-1})=\mathcal{N}(\mathbf{X}^{t};\sqrt{1-\beta_{ t}}\mathbf{X}^{t-1},\beta_{t}\text{I})\)","L_{denoise}=E_{t,X^{0},X^{t}}\left[\left\|X^{0}-X^{t}\right\|^{2}\right]","where \(t\) denotes the time step, \(\mathbf{X}^{0}=\mathbf{X}\) is the raw motion latent sequence, and \(\mathbf{X}^{t}\) is the noisy inputs generated by the diffusion forward process \(q(\mathbf{X}^{t}|\mathbf{X}^{t-1})=\mathcal{N}(\mathbf{X}^{t};\sqrt{1-\beta_{ t}}\mathbf{X}^{t-1},\beta_{t}\text{I})\)"
neurips_2024_oral_57,2,"\hat{X}^{0}=(1+\sum_{e\inC}\lambda_{e})\cdotH(X^{t},t,C)-\sum_{e\inC}\lambda_{c}\cdotH(X^{t},t,C|_{e=\emptyset})",,"E_{t\simU[1,T],\;X^{0},C\sim q(X^{0},C)}(\|X^{0}-H(X^{t},t,C)\|^{2})+\lambdaE_{t\simU[1,T],\;X^{0},C\sim q(X^{0},C)}(\|X^{0}-H(X^{t},t,C_{drop})\|^{2})","where \(\lambda\) is the guidance scale, and \(\mathbf{C}_{drop}\) represents the condition signal with randomly dropped components."
neurips_2024_oral_58,1,"p(x_{1},x_{2},\dots,x_{T})=\prod_{t=1}^{T}p(x_{t}\mid x_{1},x_{2},\dots,x_{t-1 })",,"P(x)=\prod_{t=1}^{T} P(x_{t}\mid x_{1}, x_{2},\dots, x_{t-1})",The likelihood of the sequence \(x\) is expressed as the product of the conditional probabilities of each token given its preceding tokens.
neurips_2024_oral_58,2,"f=E(im),\\\\\q=Q(f)","where \(im\) denotes the raw image, \(\mathcal{E}(\cdot)\) a encoder, and \(\mathcal{Q}(\cdot)\) a quantizer",q=Q(E(im)),"where \(im\) denotes the raw image, \(\mathcal{E}(\cdot)\) an encoder, and \(\mathcal{Q}(\cdot)\) a quantizer"
neurips_2024_oral_58,3,"q^{(i,j)}=(\operatorname*{arg\,min}_{v\in[V]}\|lookup(Z,v)-f^{(i,j)}\|_{2})\in[V]","where \(\text{lookup}(Z,v)\) means taking the \(v\)-th vector in codebook \(Z\)","q^{(i,j)}=lookup(Z,\arg\min_{v}\|f^{(i,j)}-Z_{v}\|_{2})","where \(\text{lookup}(Z,v)\) means taking the \(v\)-th vector in codebook \(Z\)"
neurips_2024_oral_58,4,"\hat{f}=lookup(Z,q),\\\\\\\\\hat{im}=D(\hat{f}) || L=\|im-\hat{im}\|_{2}+\|f-\hat{f}\|_{2}+\lambda_{P}L_{P}(\hat{im})+\lambda_{G}L_{G}(\hat{im})",,"\hat{f}=lookup(Z,q),\\\\\\hat{im}=D(\hat{f}),\\\\\L=\|\hat{f}-f\|_{2}^{2}+\lambda\|\hat{im}-im\|_{2}^{2}","where \(\hat{f}\) is the reconstructed feature map, and \(\mathcal{D}(\cdot)\) is the decoder function."
neurips_2024_oral_58,5,"p(r_{1},r_{2},\dots,r_{K})=\prod_{k=1}^{K}p(r_{k}\mid r_{1},r_{2},\dots,r_{k-1 })","where each autoregressive unit \(r_{k}\in[V]^{h_{k}\times w_{k}}\) is the token map at scale \(k\) containing \(h_{k}\times w_{k}\) tokens, and the sequence \((r_{1},r_{2},\dots,r_{k-1})\) serves as the the ""prefix"" for \(r_{k}\)","p(r_{1},r_{2},\dots,r_{K})=\prod_{k=1}^{K}p(r_{k}\mid r_{1},r_{2},\dots,r_{k-1})","where each autoregressive unit \(r_{k}\in[V]^{h_{k}\times w_{k}}\) is the token map at scale \(k\) containing \(h_{k}\times w_{k}\) tokens, and the sequence \((r_{1},r_{2},\dots,r_{k-1})\) serves as the ""prefix"" for \(r_{k}\)."
neurips_2024_oral_58,6,"w=64d,\qquad h=d,\qquad dr=0.1\cdot d/24",,"p(r_{1},r_{2},\dots,r_{K})=\prod_{k=1}^{K}p(r_{k}\mid r_{1},r_{2},\dots,r_{k-1 })","where each autoregressive unit \(r_{k}\in[V]^{h_{k}\times w_{k}}\) is the token map at scale \(k\) containing \(h_{k}\times w_{k}\) tokens, and the sequence \((r_{1},r_{2},\dots,r_{k-1})\) serves as the ""prefix"" for \(r_{k}\)."
neurips_2024_oral_58,7,"N(d)=\underbrace{d\cdot 4w^{2}}_{self-attention}+\underbrace{d\cdot 8w^{2}} _{feed-forward}+\underbrace{d\cdot 6w^{2}}_{adaptive layernorm}=18\,dw^{2}=73728\,d^{3}",,"N\approx 12dw^{2}+6w^{2}\approx 49152\,d^{3}",The main parameter count \(N\) of a VAR transformer with depth \(d\) is given by
neurips_2024_oral_59,1,"c^{(t+1)}(v)\gets f^{(t+1)}(c^{(t)}(v),g^{(t+1)}(\{\{c ^{(t)}(u)\mid u\inN(v)\}\}))",,"c_v^{(t+1)}=Update(c_v^{(t)},\{c_u^{(t)}: u\inN(v)\})","where \(c_v^{(t)}\) is the color of node \(v\) at iteration \(t\), and \(\mathcal{N}(v)\) is the set of neighbors of node \(v\)."
neurips_2024_oral_59,2,c^{(t)}(G)\coloneqq h(\{\{c^{(t)}(v)\mid v\in V(G)\}\} ),,c^{(T)}(v)\gets g^{(T)}(\{c^{(T)}(u)\mid u\in V(G)\}),The graph output after \(T\) iterations for each node \(v\) in the graph \(G\).
neurips_2024_oral_59,3,"N_{r}(v)\coloneqq\{p\midp simple path of length r,\,p_{1},p_{r+1}\inN(v),v\notinp\}\",,N_{r}(v)\coloneqq\{u\in V(G)\midthere exists a simple path of length  r from  v to  u\},The \(r\)-neighborhood of a node \(v\) in graph \(G\) is defined as the set of nodes \(u\) that can be reached from \(v\) via a simple path of length \(r\).
neurips_2024_oral_59,4,"c_{r}^{(t+1)}(v)arrowHASH_{r}(c_{r}^{(t)}(v),\{\{c _{r}^{(t)}(p)\midp\inN_{0}(v)\}\},\ldots,\{\{c_{r}^{(t)}(p)\midp\inN_{r}(v )\}\})",,"c^{(t+1)}(v)\gets f^{(t+1)}(c^{(t)}(v),g^{(t+1)}(\{\{c^{(t)}(u)\mid u\inN_{r}(v)\}\}))","The color update rule for the \(r\)-loop Weisfeiler-Leman (\(r\)-\(\ell\)WL) test, where \(c^{(t)}(v)\) is the color of node \(v\) at iteration \(t\), and \(\mathcal{N}_{r}(v)\) is the \(r\)-neighborhood of node \(v\)."
neurips_2024_oral_59,5,c_{r}^{(t)}(G)=HASH_{r}(\{\{c_{r}^{(t)}(v)\mid v\in V(G)\}\}\}),,c_{r}^{(t)}(G)\coloneqq h_{r}(\{\{c_{r}^{(t)}(v)\mid v\in V(G)\}\} ),The final graph output after \(t\) iterations of \(r\)-\(\ell\)WL.
neurips_2024_oral_59,6,"m_{k}^{(t+1)}(v)&=f_{k}^{(t+1)}(\{\{c_{k}^{(t)}(p)\midp\inN_{k}(v)\}\}),\\c_{r}^{(t+1)}(v)&=g^{(t+1)}(c_{r}^{(t)}(v),\,m_{0 }^{(t+1)}(v),\ldots,m_{r}^{(t+1)}(v))",,"c_{r}^{(t+1)}(v)\getsUPDATE_{r}(c_{r}^{(t)}(v),\{\{c_{r}^{(t)}(u)\mid u\inN_{k}(v)\}\})","where \(\mathrm{UPDATE}_{r}\) is the update function for the \(r\)-\(\ell\)MPNN, and \(\mathcal{N}_{k}(v)\) denotes the \(k\)-neighborhood of node \(v\)."
neurips_2024_oral_59,7,f_{k}^{(t+1)}(\{\{c_{k}^{(t)}(p)\midp\inN_{k}(v)\}\}):=f(\sum_{p\inN_{k}(v)}g(p)),,c_{r}^{(T)}(G)=h\left(\left\{\left\{c_{r}^{(T)}(v)\mid v\in V(G)\right\}\right\}\right),"The final readout layer of the \(r\)-\(\ell\)MPNN, representing the graph output after \(T\) iterations."
neurips_2024_oral_59,8,x_{r}^{(t+1)}(v):=MLP(x_{r}^{(t)}(v)+(1+\varepsilon_{0})\sum_{u\inN_{0}(v)}x_{r}^{(t)}(u)+\sum_{k=1}^{r}(1+\varepsilon_{k})\sum_{p\inN_{k}(v)}GIN_{k}(p)),,f_{k}^{(t+1)}(\{\{c_{k}^{(t)}(p)\midp\inN_{k}(v)\}\})=MLP\left(\sum_{p\inN_{k}(v)}GIN(p)\right),"where \(f_{k}^{(t+1)}\) is the update function for the \(k\)-th layer at iteration \(t+1\), \(\mathrm{MLP}\) is a multi-layer perceptron, and \(\mathrm{GIN}\) is the Graph Isomorphism Network function applied to the paths in the neighborhood \(\mathcal{N}_{k}(v)\)."
neurips_2024_oral_6,1,"f_{z}(a_{1},a_{2})=E_{i\simS(z)\,,\,A_{1}\simB(a_{1})\,,\,A_{2}\simB(a_{2})}\Big{[}g_{i}(A_{1},A_{2 })\Big{]}=\sum_{i=0}^{15}\frac{\exp(z_{i})}{\sum_{j}\exp(z_{j})}\cdot g_{i}(a_ {1},a_{2})\",,"E[g(z)]=\sum_{i=0}^{15}softmax(z_i)\cdot g_i(a_1, a_2)","The expected output of the differentiable logic gate computed as the weighted sum of the outputs of all possible logic gate operations, where \(\mathbf{z}\) is the vector of trainable parameters and \(g_i\) represents the \(i\)-th logic gate operation."
neurips_2024_oral_6,2,"f_{3}(\,f_{1}(a_{1},a_{2}),f_{2}(a_{3},a_{4})\,)",,"f_{kernel}(a_{1},a_{2},a_{3},a_{4})=f_{3}(f_{1}(a_{1},a_{2}),f_{2}(a_{3},a_{4}))\","The output of the logic gate kernel, expressed as a binary tree of logic gates, given input activations \(a_{1}, a_{2}, a_{3}, a_{4}\)."
neurips_2024_oral_6,3,"A^{\prime}[k,i,j]=f_{3}^{k}\big{(}f_{1}^{k}\big{(}A\big{[}C_{M}[k,\!1]\!,C_{H}[k,\!1]\!+\!i,C_{W}[k,\!1]\!+\! j\big{]},A\big{[}C_{M}[k,\!2]\!,C_{H}[k,\!2]\!+\!i,C_{W}[k,\!2]\!+\!j\big{]}\big{)} || \qquad\qquad\qquad f_{2}^{k}\big{(}A\big{[}C_{M} [k,\!3]\!,C_{H}[k,\!3]\!+\!i,C_{W}[k,\!3]\!+\!j\big{]},A\big{[}C_{M}[k,\!4]\!,C_{H}[k,\!4]\!+\!i,C _{W}[k,\!4]\!+\!j\big{]}\big{)}\big{)}",,"Y=f_{1}(A_{C_{M,1,1}}, A_{C_{H,1,1}}, A_{C_{W,1,1}})\\f_{1}(A_{C_{M,1,2}}, A_{C_{H,1,2}}, A_{C_{W,1,2}})\\\vdots\\f_{n}(A_{C_{M,n,4}}, A_{C_{H,n,4}}, A_{C_{W,n,4}})","The output tensor \(\mathbf{Y}\) is constructed by applying the logic gate functions \(f_{i}\) to the selected input activations based on the connection index tensors \(\mathbf{C}_{M}, \mathbf{C}_{H}, \mathbf{C}_{W}\)."
neurips_2024_oral_60,1,Y^{l}=ESA(LN(X^{l}))+X^{l} || X^{l+1}=SwiGLU(LN(Y^{l}))+Y^{l},,X^{\nicefrac{L}{2}}=f(X^{0}),Intermediate vector representation computed from input embeddings.
neurips_2024_oral_60,2,"\hat{K}=LN(X^{\nicefrac{{L}}{{2}}})W_{K},\quad\hat{V}=LN(X^ {\nicefrac{{L}}{{2}}})W_{V}","where \(W_{K},W_{V}\in\mathbb{R}^{d\times d}\) are learnable","\hat{K}=X^{\nicefrac{L}{2}} W_{K},\quad\hat{V}=X^{\nicefrac{L}{2}} W_{V}","where \(W_{K},W_{V}\in\mathbb{R}^{d\times d}\) are learnable weight matrices for generating global key and value caches."
neurips_2024_oral_60,3,"Q^{l}=LN(X^{l})W_{Q}^{l} || Y^{l}=Attention(Q^{l},\hat{K},\hat{V})+X^{l} || X^{l+1}=SwiGLU(LN(Y^{l}))+Y^{l}",,"\hat{K}_{i}=\hat{K},\quad\hat{V}_{i}=\hat{V},\quadfor  i=1,\ldots,\frac{L}{2}",where \(\hat{K}_{i}\) and \(\hat{V}_{i}\) are the KV caches reused by each of the \(\frac{L}{2}\) cross-decoder modules.
neurips_2024_oral_60,4,"Q=XW_{Q},\quad K=XW_{K},\quad V=XW_{V}\\head_{i}=softmax(Q_{[i]}K_{[i]}^{\intercal}+B)V,\quad B_{ij}=0,&i-C<j\leq i\\-\infty,&otherwise\\SWA(X)=Concat(head_{1},\cdots,head_{h})W_ {O}","where \(W_{Q},W_{K},W_{V},W_{O}\in\mathbb{R}^{d\times d}\) are learnable matrices, and the window causal mask \(B\) controls each query only attends to the previous keys whose distances are less than \(C\)","Y^{l}=Attention(Q^{l},K^{l},V^{l},B)+X^{l}","where \(W_{Q},W_{K},W_{V},W_{O}\in\mathbb{R}^{d\times d}\) are learnable matrices, and the window causal mask \(B\) controls each query only attends to the previous keys whose distances are less than \(C\)"
neurips_2024_oral_60,5,"Q=(XW_{Q})\odot\Theta,\quad K=(XW_{K})\odot\overline{\Theta},\quad V=XW_{V},\quad\Theta_{n}=e^{in\theta}\\\gamma=sigmoid(XW_{\gamma})^{1/\tau},\quad D_{nm}=\prod\nolimits_{i=m+1}^{n}\gamma_{i},&n\geq m\\0,&n<m\\gRet(X)=(QK^{\intercal}\odot D)V","where \(W_{Q},W_{K},W_{V}\in\mathbb{R}^{d\times d}\) and \(W_{\gamma}\in\mathbb{R}^{d\times 1}\) are learnable weights, and the temperature term \(\tau\) encourages \(\gamma\) to 1 for better memorization [46]","H=softmax\left(\frac{QK^{\intercal}}{\sqrt{d}}+B\right)V,\quad\gamma=\sigma(XW_{\gamma})","where \(W_{Q},W_{K},W_{V}\in\mathbb{R}^{d\times d}\) and \(W_{\gamma}\in\mathbb{R}^{d\times 1}\) are learnable weights, and the temperature term \(\tau\) encourages \(\gamma\) to 1 for better memorization [46]"
neurips_2024_oral_60,6,"S_{n}=\gamma_{n}S_{n-1}+K_{n}^{\intercal}V_{n}\\gRet(X_{n})=Q_{n}S_{n},\quad n=1,\cdots,|x|","where \(Q,K,V,\gamma\) are the same as in Equation (5)",X^{n}=gRet(X^{n-1})+X^{n-1},"where \(X^{n}\) is the output at the \(n\)-th timestep, and \(X^{n-1}\) is the input from the previous timestep."
neurips_2024_oral_60,7,"\beta_{(i-1)B+j}&=\prod\limits_{k=(i-1)B+1}^{(i-1)B+j}\gamma_{k},\quad D_{[i]}(j,k)=\frac{\beta_{(i-1)B+k}}{\beta_{ (i-1)B+j}}\\if\\j\leq k\\else\\0\\R_{i}&=K_{[i]}^{\intercal}(V_{[i]}\odot\frac{\beta_{iB}}{\beta_{[i]}})+\beta_{iB}R_{i-1},\\\beta_{[i]}(j,k)=\beta_{(i-1)B+j}\\gRet(X)&=\underbrace{(Q_{[i]}K_{[i]}^{\intercal}\odot D_{[i]})V_{[i]}}_{Inner-Chunk}+\underbrace{(Q_{[i]}R_{i-1})\odot\beta_{[i]}}_{Cross-Chunk}","where \(R_{i}\) is the intermediate state of the \(i\)-th chunk, and \(\beta\) summarizes the data-controlled decay \(\gamma\)","R_{i}=\gamma_{i}R_{i-1}+K_{[i]}^{\intercal}V_{[i]}\\gRet(X_{[i]})=Q_{[i]}R_{i},\quad i=1,\cdots,\frac{|x|}{B}","where \(R_{i}\) is the intermediate state of the \(i\)-th chunk, and \(\beta\) summarizes the data-controlled decay \(\gamma\)"
neurips_2024_oral_60,8,"head_{i}=gRet(X) || Y=GroupNorm_{h}(Concat(head_{1},\cdots,head_{n})) || MHGR(X)=(swish(XW_{G})\odot Y)W_{O}",,"gRet_{multi-head}(X)=Concat(gRet(X)_{1},\cdots,gRet(X)_{h})W_{O}","where \(W_{O}\in\mathbb{R}^{d\times d}\) is a learnable matrix, and \(h\) is the number of heads"
neurips_2024_oral_8,1,w_{i}^{t}=\alpha^{t}W^{t-1}+(1-\alpha^{t})\hat{w}_{i}^{t},where \(\alpha^{t}\) is the fusion factor used to control the quality of the fused global model \(\mathcal{W}^{t-1}\),\hat{w}_{i}^{t}=(1-\alpha^{t})\hat{w}_{i}^{t}+\alpha^{t}W^{t-1},where \(\alpha^{t}\) is the fusion factor used to control the quality of the fused global model \(\mathcal{W}^{t-1}\)
neurips_2024_oral_8,2,"\alpha^{t}=\max\{(1-\epsilon)^{t-1}\alpha_{0},\alpha_{min}\}",,"\alpha^{t}=\max(\alpha_{min},\alpha_{0}-\epsilon\cdot (t-1))","where \(\alpha_{0}\) is the initial fusion factor, \(\epsilon\) is the sensitivity factor controlling the decrease rate, and \(\alpha_{min}\) is the minimum value for \(\alpha^{t}\)."
neurips_2024_oral_8,3,L^{DAR}_{i}= || g_{e}(w_{e}\odotM_{e};x_{i}) || _{2}^{2},,"L^{DAR}=\frac{1}{N_{i}}\sum_{j=1}^{N_{i}}L(y_{i}^{(j)},\hat{y}_{i}^{(j)})+\lambda\|w_{e}\odotM_{e}\|_{2}^{2}","where \(\mathcal{L}(y_{i}^{(j)},\hat{y}_{i}^{(j)})\) is the loss function for the prediction, \(\lambda\) is the regularization coefficient, and \(\|\mathbf{w}_{e}\odot\mathbf{M}_{e}\|_{2}^{2}\) represents the \(\ell_{2}\) norm of the pruned encoder weights."
neurips_2024_oral_8,4,"L^{CE}_{i}=-\frac{1}{|K_{i}|}\sum_{k\inK_{i}}y_{ i,k}\log(\hat{y}_{i,k})","where \(\mathcal{K}_{i}\) denotes the set of possible labels on the client \(i\), \(\hat{y}_{i,k}\) denotes predicting label, and \(y_{i,k}\) denotes ground-truth label","L^{CE}_{i}=-\sum_{k\inK_{i}} y_{i,k}\log(\hat{y}_{i,k})","where \(\mathcal{K}_{i}\) denotes the set of possible labels on the client \(i\), \(\hat{y}_{i,k}\) denotes predicting label, and \(y_{i,k}\) denotes ground-truth label."
neurips_2024_oral_8,5,L_{i}=L^{CE}_{i}+\gammaL^{DAR}_{i},where the \(\gamma\) is a pre-defined coefficient controlling the importance of \(\mathcal{L}^{DAR}_{i}\) relative to \(\mathcal{L}^{CE}_{i}\),L_{i}=L^{CE}_{i}+\gammaL^{DAR}_{i},where the \(\gamma\) is a pre-defined coefficient controlling the importance of \(\mathcal{L}^{DAR}_{i}\) relative to \(\mathcal{L}^{CE}_{i}\)
neurips_2024_oral_8,6,w^{t}_{i}:=\underbrace{w^{t}_{i}\odotM^{t}_{i}}_{local knowledge}+\underbrace{W^{t-1}\odot\overline{M}^{t}_{i}}_{global knowledge},"where \(\mathcal{W}^{t-1}\) is the global model aggregated at the \((t-1)\)-th round, and \(\overline{\mathbf{M}}^{t}_{i}\) denotes the logical NOT operation applied to \(\mathbf{M}^{t}_{i}\)",\hat{w}_{i}^{t}=w_{i}^{t}\odot\overline{M}^{t}_{i}+W^{t-1}\odotM^{t}_{i},"where \(\mathcal{W}^{t-1}\) is the global model aggregated at the \((t-1)\)-th round, and \(\overline{\mathbf{M}}^{t}_{i}\) denotes the logical NOT operation applied to \(\mathbf{M}^{t}_{i}\)"
neurips_2024_oral_8,7,W^{t}=\sum_{i\inC}\frac{|D_{i}|}{|D|}w_{i}^{t},"where \(|\mathcal{D}_{i}|\) is the sample number in the local dataset on client \(i\), and \(|\mathcal{D}|\) is the total number of samples in the entire FL system",W^{t}=\frac{1}{|D|}\sum_{i\inC}|D_{i}|w^{t}_{i},"where \(|\mathcal{D}_{i}|\) is the sample number in the local dataset on client \(i\), and \(|\mathcal{D}|\) is the total number of samples in the entire FL system."
neurips_2024_oral_9,1,"lh_{Q}=(Q_{i}^{d},subqs^{i})(i=1,2,...,K)",,Let  P be the demonstration pool of human-annotated question decomposition examples.\end{latex> <description>Let\( P\) be the demonstration pool of human-annotated question decomposition examples.</description>,Let \( P \) be the demonstration pool of human-annotated question decomposition examples.
neurips_2024_oral_9,2,"\{q_{t+1}^{j},j=1,...,J\}\gets Decompose(p_{\theta},\;h_{1},\;lh_{Q},\;q_ {t})",,"q_{t+1}^{j}=LLM(q_{t}, s_{t},\epsilon_{1}, lh_{Q})\quad (j=1,...,J)",Sub-questions generated by the LLM based on the coherence score and logic heuristics.
neurips_2024_oral_9,3,"r_{t+1}^{j}\gets Solve(p_{\theta},\;h_{2},\;q_{t+1}^{j})",,"r_{t+1}^{j}\gets Reason(p_{\theta},\;lh_{Q},\;q_{t+1}^{j})","Generate the essential rationale \(r_{t+1}^{j}\) for each sub-question \(q_{t+1}^{j}\) using the model \(p_{\theta}\), logic heuristics \(lh_{Q}\), and the sub-question itself."
neurips_2024_oral_9,4,"\hat{r}_{t+1}^{j}\gets Self\_Check(p_{\theta},\;h_{3},\;q_{t+1}^{j},\;r_{t+1}^{j})",,"\hat{r}_{t+1}^{j}\gets SelfCheck(p_{\theta},\;h_{3},\;r_{t+1}^{j})","Here, \(\hat{r}_{t+1}^{j}\) represents the corrected rationale for the sub-question \(q_{t+1}^{j}\) after the self-check process."
neurips_2024_oral_9,5,"s_{t+1}^{j}\gets Score(p_{\theta},\;h_{4},\;q_{t+1}^{j},\;\hat{r}_{t+1}^{j })",,"s_{t+1}^{j}\gets Evaluate(p_{\theta},\;h_{4},\;q_{t+1}^{j},\;\hat{r}_{t+1}^{j})",coherence score \(s_{t+1}^{j}\) for the refined rationale \(\hat{r}_{t+1}^{j}\) and the sub-question \(q_{t+1}^{j}\).
neurips_2024_oral_9,6,"n_{t+1}^{j}=(q_{t+1}^{j},\hat{r}_{t+1}^{j},s_{t+1}^{j})",where \(s_{t+1}^{j}\) can support the current or subsequent cycles in _Rethink_ (4,"n_{t+1}^{j}\gets (q_{t+1}^{j},\hat{r}_{t+1}^{j}, s_{t+1}^{j})","where \(n_{t+1}^{j}\) represents the node containing the sub-question \(q_{t+1}^{j}\), its refined rationale \(\hat{r}_{t+1}^{j}\), and the coherence score \(s_{t+1}^{j}\)."
neurips_2024_oral_9,7,"L_{k}\gets Extract(p_{\theta},\h_{5},\L,q_{t+1}^{j}),\L_{k}\subseteq L",where \(h_{5}\) is a prompt head (_Appendix_ A,"L_{k}=\{n_{l},\;l\leq t\;|\;Correlate(q_{t+1}^{j},\;q_{l})\;>\;0\}",where \(L_{k}\) is the subset of the most related nodes extracted based on their correlation with the sub-question \(q_{t+1}^{j}\).
neurips_2024_oral_9,8,"r^{\prime}\gets Update(p_{\theta},\h_{6},\n_{e}(q,r,s),\\hat{r}_{t+1}^{j})",,"r_{e}\gets Update(r_{e},\;\hat{r}_{t+1}^{j}),\;n_{e}\in L_{k}",where \(r_{e}\) is the rationale of the extracted node \(n_{e}\) that is being updated.
neurips_2024_oral_9,9,"n_{e}(q,r^{\prime},s)\gets n_{e}(q,r,s)",,r\gets r^{\prime},where \(r\) is the original rationale and \(r^{\prime}\) is the updated rationale for the node.
